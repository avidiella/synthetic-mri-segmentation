{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf74476f-414b-48dc-a93d-4950a50f6efc",
   "metadata": {},
   "source": [
    "# nnU-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809fb75-80f5-412b-9ad4-798eced7a4cb",
   "metadata": {},
   "source": [
    "## 1. Install compatible versions of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88afcd61-3f2c-4b3d-9461-d58d85c52f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.1\n",
      "Uninstalling torch-2.9.1:\n",
      "  Successfully uninstalled torch-2.9.1\n",
      "Found existing installation: torchvision 0.15.2+cu118\n",
      "Uninstalling torchvision-0.15.2+cu118:\n",
      "  Successfully uninstalled torchvision-0.15.2+cu118\n",
      "Found existing installation: torchaudio 2.0.2+cu118\n",
      "Uninstalling torchaudio-2.0.2+cu118:\n",
      "  Successfully uninstalled torchaudio-2.0.2+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torchmetrics as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio torchmetrics -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36bcdd47-826b-4ef4-a73b-0a84e2b9bdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.0.1+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-win_amd64.whl (2619.1 MB)\n",
      "Collecting torchvision==0.15.2+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-win_amd64.whl (4.9 MB)\n",
      "Requirement already satisfied: torchaudio==2.0.2+cu118 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (2.0.2+cu118)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (4.15.0)\n",
      "Requirement already satisfied: sympy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (3.1.6)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torchvision==0.15.2+cu118) (2.1.2)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torchvision==0.15.2+cu118) (2.32.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torchvision==0.15.2+cu118) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.9.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "    Uninstalling torch-2.9.1:\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "      Successfully uninstalled torch-2.9.1\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "  Attempting uninstall: torchvision\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "    Found existing installation: torchvision 0.24.1\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "    Uninstalling torchvision-0.24.1:\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "      Successfully uninstalled torchvision-0.24.1\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   ---------------------------------------- 2/2 [torchvision]\n",
      "\n",
      "Successfully installed torch-2.0.1+cu118 torchvision-0.15.2+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nnunetv2 2.6.2 requires torch>=2.1.2, but you have torch 2.0.1+cu118 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad9e66f2-9844-4aad-a844-94ca72887c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79652a-5129-45ee-abf4-1a9c51b1bdee",
   "metadata": {},
   "source": [
    "## 2. Install nnU-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789ad7e1-42fb-4866-8f61-bea769986789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnunetv2\n",
      "  Downloading nnunetv2-2.6.2.tar.gz (211 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting torch>=2.1.2 (from nnunetv2)\n",
      "  Downloading torch-2.9.1-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Collecting acvl-utils<0.3,>=0.2.3 (from nnunetv2)\n",
      "  Using cached acvl_utils-0.2.5.tar.gz (29 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting dynamic-network-architectures<0.5,>=0.4.1 (from nnunetv2)\n",
      "  Using cached dynamic_network_architectures-0.4.2.tar.gz (28 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tqdm (from nnunetv2)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy (from nnunetv2)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting batchgenerators>=0.25.1 (from nnunetv2)\n",
      "  Using cached batchgenerators-0.25.1.tar.gz (76 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.24 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2) (2.1.2)\n",
      "Collecting scikit-learn (from nnunetv2)\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scikit-image>=0.19.3 (from nnunetv2)\n",
      "  Using cached scikit_image-0.25.2-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting SimpleITK>=2.2.1 (from nnunetv2)\n",
      "  Using cached simpleitk-2.5.3-cp310-cp310-win_amd64.whl.metadata (7.3 kB)\n",
      "Collecting pandas (from nnunetv2)\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting graphviz (from nnunetv2)\n",
      "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tifffile (from nnunetv2)\n",
      "  Using cached tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2) (2.32.5)\n",
      "Collecting nibabel (from nnunetv2)\n",
      "  Downloading nibabel-5.3.3-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting matplotlib (from nnunetv2)\n",
      "  Using cached matplotlib-3.10.7-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from nnunetv2)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting imagecodecs (from nnunetv2)\n",
      "  Using cached imagecodecs-2025.3.30-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting yacs (from nnunetv2)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting batchgeneratorsv2>=0.3.0 (from nnunetv2)\n",
      "  Downloading batchgeneratorsv2-0.3.0.tar.gz (44 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting einops (from nnunetv2)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting blosc2>=3.0.0b1 (from nnunetv2)\n",
      "  Downloading blosc2-3.12.2-cp310-cp310-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2.3->nnunetv2)\n",
      "  Using cached connected_components_3d-3.26.1-cp310-cp310-win_amd64.whl.metadata (33 kB)\n",
      "Collecting timm (from dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached timm-1.0.22-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25.1->nnunetv2) (11.3.0)\n",
      "Collecting future (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting unittest2 (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting threadpoolctl (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.3.0->nnunetv2)\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting ndindex (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached ndindex-1.10.1-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting msgpack (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached msgpack-1.1.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: platformdirs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b1->nnunetv2) (4.5.0)\n",
      "Collecting numexpr>=2.14.1 (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached numexpr-2.14.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting py-cpuinfo (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2) (3.3)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image>=0.19.3->nnunetv2)\n",
      "  Using cached imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: packaging>=21 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2) (25.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.19.3->nnunetv2)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch>=2.1.2->nnunetv2)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->nnunetv2)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->nnunetv2)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->nnunetv2)\n",
      "  Using cached fonttools-4.61.0-cp310-cp310-win_amd64.whl.metadata (115 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->nnunetv2)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->nnunetv2)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\n",
      "Collecting importlib-resources>=5.12 (from nibabel->nnunetv2)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->nnunetv2)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->nnunetv2)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (2025.11.12)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->nnunetv2)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: torchvision in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.15.2+cu118)\n",
      "Requirement already satisfied: pyyaml in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (6.0.3)\n",
      "Collecting huggingface_hub (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Downloading huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.28.1)\n",
      "Collecting shellingham (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.16.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from tqdm->nnunetv2) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Downloading torchvision-0.24.1-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting traceback2 (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
      "Downloading blosc2-3.12.2-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 16.2 MB/s  0:00:00\n",
      "Downloading numexpr-2.14.1-cp310-cp310-win_amd64.whl (160 kB)\n",
      "Downloading scikit_image-0.25.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.8/12.8 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 40.1 MB/s  0:00:00\n",
      "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "   ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 21.8/41.3 MB 105.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.3 MB 109.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.3/41.3 MB 97.1 MB/s  0:00:00\n",
      "Downloading simpleitk-2.5.3-cp310-cp310-win_amd64.whl (18.8 MB)\n",
      "   ---------------------------------------- 0.0/18.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.1/18.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 5.2/18.8 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 6.6/18.8 MB 9.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.4/18.8 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 9.4/18.8 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 11.5/18.8 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 12.6/18.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.7/18.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.8/18.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.8/18.8 MB 8.9 MB/s  0:00:02\n",
      "Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "Downloading torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "   ---------------------------------------- 0.0/111.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 18.6/111.0 MB 83.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 37.7/111.0 MB 88.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 42.7/111.0 MB 67.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 59.5/111.0 MB 70.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 78.6/111.0 MB 74.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 98.8/111.0 MB 77.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 79.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 111.0/111.0 MB 73.8 MB/s  0:00:01\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading connected_components_3d-3.26.1-cp310-cp310-win_amd64.whl (521 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading imagecodecs-2025.3.30-cp310-cp310-win_amd64.whl (28.9 MB)\n",
      "   ---------------------------------------- 0.0/28.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 3.1/28.9 MB 14.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 5.2/28.9 MB 12.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 7.3/28.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 9.4/28.9 MB 12.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 12.6/28.9 MB 11.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 14.7/28.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.8/28.9 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.9/28.9 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 21.0/28.9 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 23.1/28.9 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 25.2/28.9 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 27.3/28.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.9/28.9 MB 10.8 MB/s  0:00:02\n",
      "Downloading matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 83.5 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 85.6 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading ndindex-1.10.1-cp310-cp310-win_amd64.whl (157 kB)\n",
      "Downloading nibabel-5.3.3-py3-none-any.whl (3.3 MB)\n",
      "   ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.3/3.3 MB 64.5 MB/s  0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.3/11.3 MB 88.7 MB/s  0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.9/8.9 MB 61.2 MB/s  0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading timm-1.0.22-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 70.9 MB/s  0:00:00\n",
      "Downloading huggingface_hub-1.2.1-py3-none-any.whl (520 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 85.3 MB/s  0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading torchvision-0.24.1-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.7/3.7 MB 110.8 MB/s  0:00:00\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: nnunetv2, acvl-utils, dynamic-network-architectures, batchgenerators, batchgeneratorsv2\n",
      "  Building wheel for nnunetv2 (pyproject.toml): started\n",
      "  Building wheel for nnunetv2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.6.2-py3-none-any.whl size=285951 sha256=a7a44d68002164ee30463aadda0c7df27ad8f6392cdcee74d03485d54c5ad3af\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\7b\\93\\c5\\742da4755f3e7838e41c346eb3ef391ed49ec53c620c7550ef\n",
      "  Building wheel for acvl-utils (pyproject.toml): started\n",
      "  Building wheel for acvl-utils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for acvl-utils: filename=acvl_utils-0.2.5-py3-none-any.whl size=27250 sha256=bb802262a73cd3726398181828eef4af2c33e33516c9a5ed575060d8cd3f8d91\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\11\\68\\32\\42aed9ef6a7c3ff868f84b243120481ef8a19d0ddbc2551133\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): started\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.4.2-py3-none-any.whl size=39089 sha256=5a3192eb7584824329d098ed04859ef5c230548e084466662184ec5778484418\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\08\\23\\62\\cbf95e3b955e69700d71fd056ba2ed436cfff0ba3f9ae2ebb9\n",
      "  Building wheel for batchgenerators (pyproject.toml): started\n",
      "  Building wheel for batchgenerators (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93167 sha256=8af42eec23d5219281538ea957556e6a14913677c919f54e9fbc764c7a6e1584\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\be\\1b\\30\\b3f066999ad01855fc903fe7c93c25682333dd5645d5c75434\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml): started\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.3.0-py3-none-any.whl size=65242 sha256=a54d62bc3f48ed0620dec0555264b448e05cc969a7ca0f134fb6e97b1d1edba7\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\85\\91\\18\\f246cf6cecb275d0e293271390794a0c090621cfc1ab1501c7\n",
      "Successfully built nnunetv2 acvl-utils dynamic-network-architectures batchgenerators batchgeneratorsv2\n",
      "Installing collected packages: SimpleITK, pytz, py-cpuinfo, linecache2, argparse, yacs, tzdata, traceback2, tqdm, tifffile, threadpoolctl, shellingham, scipy, safetensors, pyparsing, numexpr, ndindex, msgpack, lazy-loader, kiwisolver, joblib, importlib-resources, imageio, imagecodecs, hf-xet, graphviz, future, fsspec, fonttools, einops, cycler, contourpy, connected-components-3d, click, unittest2, typer-slim, torch, scikit-learn, scikit-image, pandas, nibabel, matplotlib, blosc2, torchvision, seaborn, fft-conv-pytorch, batchgenerators, huggingface_hub, batchgeneratorsv2, acvl-utils, timm, dynamic-network-architectures, nnunetv2\n",
      "\n",
      "   ----------------------------------------  0/53 [SimpleITK]\n",
      "   ----------------------------------------  0/53 [SimpleITK]\n",
      "   ----------------------------------------  0/53 [SimpleITK]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "   - --------------------------------------  2/53 [py-cpuinfo]\n",
      "   -- -------------------------------------  3/53 [linecache2]\n",
      "   --- ------------------------------------  4/53 [argparse]\n",
      "   ---- -----------------------------------  6/53 [tzdata]\n",
      "   ---- -----------------------------------  6/53 [tzdata]\n",
      "   ---- -----------------------------------  6/53 [tzdata]\n",
      "   ----- ----------------------------------  7/53 [traceback2]\n",
      "   ----- ----------------------------------  7/53 [traceback2]\n",
      "   ------ ---------------------------------  8/53 [tqdm]\n",
      "   ------ ---------------------------------  8/53 [tqdm]\n",
      "   ------ ---------------------------------  9/53 [tifffile]\n",
      "   ------- -------------------------------- 10/53 [threadpoolctl]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 13/53 [safetensors]\n",
      "   ---------- ----------------------------- 14/53 [pyparsing]\n",
      "   ----------- ---------------------------- 15/53 [numexpr]\n",
      "   ------------ --------------------------- 16/53 [ndindex]\n",
      "   ------------ --------------------------- 17/53 [msgpack]\n",
      "   -------------- ------------------------- 19/53 [kiwisolver]\n",
      "   --------------- ------------------------ 20/53 [joblib]\n",
      "   --------------- ------------------------ 20/53 [joblib]\n",
      "   --------------- ------------------------ 21/53 [importlib-resources]\n",
      "   --------------- ------------------------ 21/53 [importlib-resources]\n",
      "   ---------------- ----------------------- 22/53 [imageio]\n",
      "   ---------------- ----------------------- 22/53 [imageio]\n",
      "   ----------------- ---------------------- 23/53 [imagecodecs]\n",
      "   ----------------- ---------------------- 23/53 [imagecodecs]\n",
      "   ----------------- ---------------------- 23/53 [imagecodecs]\n",
      "   ------------------ --------------------- 24/53 [hf-xet]\n",
      "   ------------------ --------------------- 25/53 [graphviz]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   -------------------- ------------------- 27/53 [fsspec]\n",
      "   -------------------- ------------------- 27/53 [fsspec]\n",
      "   -------------------- ------------------- 27/53 [fsspec]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 29/53 [einops]\n",
      "   --------------------- ------------------ 29/53 [einops]\n",
      "   --------------------- ------------------ 29/53 [einops]\n",
      "   ---------------------- ----------------- 30/53 [cycler]\n",
      "   ----------------------- ---------------- 31/53 [contourpy]\n",
      "   ------------------------ --------------- 32/53 [connected-components-3d]\n",
      "   ------------------------ --------------- 33/53 [click]\n",
      "   ------------------------- -------------- 34/53 [unittest2]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "  Attempting uninstall: torch\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "  Attempting uninstall: torchvision\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "    Found existing installation: torchvision 0.15.2+cu118\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "    Uninstalling torchvision-0.15.2+cu118:\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "      Successfully uninstalled torchvision-0.15.2+cu118\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   --------------------------------- ------ 44/53 [seaborn]\n",
      "   --------------------------------- ------ 44/53 [seaborn]\n",
      "   --------------------------------- ------ 45/53 [fft-conv-pytorch]\n",
      "   ---------------------------------- ----- 46/53 [batchgenerators]\n",
      "   ---------------------------------- ----- 46/53 [batchgenerators]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ------------------------------------ --- 48/53 [batchgeneratorsv2]\n",
      "   ------------------------------------ --- 49/53 [acvl-utils]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------ - 51/53 [dynamic-network-architectures]\n",
      "   ------------------------------------ - 51/53 [dynamic-network-architectures]\n",
      "   ------------------------------------ - 51/53 [dynamic-network-architectures]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------- 53/53 [nnunetv2]\n",
      "\n",
      "Successfully installed SimpleITK-2.5.3 acvl-utils-0.2.5 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.3.0 blosc2-3.12.2 click-8.3.1 connected-components-3d-3.26.1 contourpy-1.3.2 cycler-0.12.1 dynamic-network-architectures-0.4.2 einops-0.8.1 fft-conv-pytorch-1.2.0 fonttools-4.61.0 fsspec-2025.12.0 future-1.0.0 graphviz-0.21 hf-xet-1.2.0 huggingface_hub-1.2.1 imagecodecs-2025.3.30 imageio-2.37.2 importlib-resources-6.5.2 joblib-1.5.2 kiwisolver-1.4.9 lazy-loader-0.4 linecache2-1.0.0 matplotlib-3.10.7 msgpack-1.1.2 ndindex-1.10.1 nibabel-5.3.3 nnunetv2-2.6.2 numexpr-2.14.1 pandas-2.3.3 py-cpuinfo-9.0.0 pyparsing-3.2.5 pytz-2025.2 safetensors-0.7.0 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.15.3 seaborn-0.13.2 shellingham-1.5.4 threadpoolctl-3.6.0 tifffile-2025.5.10 timm-1.0.22 torch-2.9.1 torchvision-0.24.1 tqdm-4.67.1 traceback2-1.4.0 typer-slim-0.20.0 tzdata-2025.2 unittest2-1.1.0 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\ProgramData\\Anaconda3\\envs\\nnunet\\Lib\\site-packages\\~vfuser'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\ProgramData\\Anaconda3\\envs\\nnunet\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install nnunetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adda6336-368a-4c71-b280-9eda0b7c21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: nnunetv2 2.6.2\n",
      "Uninstalling nnunetv2-2.6.2:\n",
      "  Successfully uninstalled nnunetv2-2.6.2\n",
      "Collecting nnunetv2==2.4.2\n",
      "  Downloading nnunetv2-2.4.2.tar.gz (184 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting torch>=2.1.2 (from nnunetv2==2.4.2)\n",
      "  Using cached torch-2.9.1-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: acvl-utils<0.3,>=0.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.2.5)\n",
      "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2==2.4.2)\n",
      "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (4.67.1)\n",
      "Collecting dicom2nifti (from nnunetv2==2.4.2)\n",
      "  Using cached dicom2nifti-2.6.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: scipy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (1.15.3)\n",
      "Requirement already satisfied: batchgenerators>=0.25 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.25.1)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (1.7.2)\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.25.2)\n",
      "Requirement already satisfied: SimpleITK>=2.2.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.5.3)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.3.3)\n",
      "Requirement already satisfied: graphviz in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.21)\n",
      "Requirement already satisfied: tifffile in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2025.5.10)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.32.5)\n",
      "Requirement already satisfied: nibabel in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (5.3.3)\n",
      "Requirement already satisfied: matplotlib in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (3.10.7)\n",
      "Requirement already satisfied: seaborn in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.13.2)\n",
      "Requirement already satisfied: imagecodecs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2025.3.30)\n",
      "Requirement already satisfied: yacs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.1.8)\n",
      "Requirement already satisfied: connected-components-3d in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (3.26.1)\n",
      "Requirement already satisfied: blosc2>=3.0.0b4 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (3.12.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (11.3.0)\n",
      "Requirement already satisfied: future in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (1.0.0)\n",
      "Requirement already satisfied: unittest2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (3.6.0)\n",
      "Requirement already satisfied: ndindex in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (1.10.1)\n",
      "Requirement already satisfied: msgpack in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (1.1.2)\n",
      "Requirement already satisfied: platformdirs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (4.5.0)\n",
      "Requirement already satisfied: numexpr>=2.14.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (2.14.1)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (9.0.0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (3.3)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (2.37.2)\n",
      "Requirement already satisfied: packaging>=21 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (0.4)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2==2.4.2) (1.3.0)\n",
      "Collecting pydicom>=3.0.0 (from dicom2nifti->nnunetv2==2.4.2)\n",
      "  Using cached pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting python-gdcm (from dicom2nifti->nnunetv2==2.4.2)\n",
      "  Using cached python_gdcm-3.2.2-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from jinja2->torch>=2.1.2->nnunetv2==2.4.2) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.4.2) (1.17.0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nibabel->nnunetv2==2.4.2) (6.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from pandas->nnunetv2==2.4.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from pandas->nnunetv2==2.4.2) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-learn->nnunetv2==2.4.2) (1.5.2)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from tqdm->nnunetv2==2.4.2) (0.4.6)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2==2.4.2)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: traceback2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from unittest2->batchgenerators>=0.25->nnunetv2==2.4.2) (1.4.0)\n",
      "Requirement already satisfied: linecache2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2==2.4.2) (1.0.0)\n",
      "Requirement already satisfied: PyYAML in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from yacs->nnunetv2==2.4.2) (6.0.3)\n",
      "Using cached torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "Downloading dicom2nifti-2.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 13.6 MB/s  0:00:00\n",
      "Downloading python_gdcm-3.2.2-cp310-cp310-win_amd64.whl (34.2 MB)\n",
      "   ---------------------------------------- 0.0/34.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 6.6/34.2 MB 33.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 21.0/34.2 MB 53.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  34.1/34.2 MB 56.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 34.2/34.2 MB 54.3 MB/s  0:00:00\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Building wheels for collected packages: nnunetv2, dynamic-network-architectures\n",
      "  Building wheel for nnunetv2 (pyproject.toml): started\n",
      "  Building wheel for nnunetv2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.4.2-py3-none-any.whl size=248849 sha256=b14498d8ae2982b5b94510857a830bc0dfc1d8570651be61e2f73bbaa1033683\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\a8\\24\\6a\\632a19c700123ed30f3f6380d84ea3f9e1d068bf5db817e804\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): started\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30114 sha256=7b4fbf41e49a08498b3c608efc6c13057f7a30ac9909566150c7ee15cbcd6bd6\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\55\\1b\\13\\a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
      "Successfully built nnunetv2 dynamic-network-architectures\n",
      "Installing collected packages: argparse, python-gdcm, pydicom, torch, dicom2nifti, dynamic-network-architectures, nnunetv2\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "  Attempting uninstall: torch\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "  Attempting uninstall: dynamic-network-architectures\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "    Found existing installation: dynamic_network_architectures 0.4.2\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "    Uninstalling dynamic_network_architectures-0.4.2:\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "      Successfully uninstalled dynamic_network_architectures-0.4.2\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------------- ----------- 5/7 [dynamic-network-architectures]\n",
      "   ---------------------------- ----------- 5/7 [dynamic-network-architectures]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------------- 7/7 [nnunetv2]\n",
      "\n",
      "Successfully installed argparse-1.4.0 dicom2nifti-2.6.2 dynamic-network-architectures-0.3.1 nnunetv2-2.4.2 pydicom-3.0.1 python-gdcm-3.2.2 torch-2.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall nnunetv2 -y\n",
    "!pip install nnunetv2==2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106d2f6-e26c-4c60-97ba-302628c4336a",
   "metadata": {},
   "source": [
    "## 3. System configuration and training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d85b8f3-9d23-4a74-b142-cc5773c06eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  7 12:01:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.80                 Driver Version: 576.80         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  |   00000000:05:00.0  On |                  N/A |\n",
      "|  0%   43C    P8             19W /  170W |     619MiB /  12288MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            8200    C+G   ...cord\\app-1.0.9217\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A            8440    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A            9796    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           12580    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           13160    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           15448    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           16700    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           18716    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18896    C+G   ...crosoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           23004    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           23180    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           31764    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           31904    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           31920    C+G   ...8wekyb3d8bbwe\\M365Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A           33440    C+G   ...1g1gvanyjgm\\WhatsApp.Root.exe      N/A      |\n",
      "|    0   N/A  N/A           34228    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd49396-f7ad-4943-843c-39cce140a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17102f3-e657-4d05-8b2d-f93c9e8f0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_raw\n",
      "nnUNet_preprocessed: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\n",
      "nnUNet_results: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_path = r\"C:\\Users\\Anna\\Documents\\TFM\"\n",
    "\n",
    "os.environ['nnUNet_raw'] = os.path.join(base_path, \"nnUNet_raw\")\n",
    "os.environ['nnUNet_preprocessed'] = os.path.join(base_path, \"nnUNet_preprocessed\")\n",
    "os.environ['nnUNet_results'] = os.path.join(base_path, \"nnUNet_results\")\n",
    "\n",
    "print(\"nnUNet_raw:\", os.environ['nnUNet_raw'])\n",
    "print(\"nnUNet_preprocessed:\", os.environ['nnUNet_preprocessed'])\n",
    "print(\"nnUNet_results:\", os.environ['nnUNet_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baadb9a-89d4-4f86-9872-80c4adccdde5",
   "metadata": {},
   "source": [
    "## 4. nnU-Net training\n",
    "### Verify dataset integrity and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9893577b-e6f6-4ca7-9558-5cfa1ca7c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint extraction...\n",
      "Dataset500_MRI\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.03 1.03 1.03]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [248.54368932 248.54368932 248.54368932]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.0609 1.0609 1.0609]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [241.30455274 241.30455274 241.30455274]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.092727 1.092727 1.092727]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [234.27626479 234.27626479 234.27626479]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.12550881 1.12550881 1.12550881]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [227.45268427 227.45268427 227.45268427]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.15927407 1.15927407 1.15927407]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [220.8278488 220.8278488 220.8278488]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.1940523 1.1940523 1.1940523]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [214.39596971 214.39596971 214.39596971]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.22987387 1.22987387 1.22987387]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [208.1514269 208.1514269 208.1514269]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.26677008 1.26677008 1.26677008]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [202.08876398 202.08876398 202.08876398]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 49, 'patch_size': (256, 256), 'median_image_size_in_voxels': array([256., 256.]), 'spacing': array([1., 1.]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D lowres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (128, 128, 128), 'median_image_size_in_voxels': (202, 202, 202), 'spacing': array([1.26677008, 1.26677008, 1.26677008]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'}\n",
      "\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (128, 128, 128), 'median_image_size_in_voxels': array([256., 256., 256.]), 'spacing': array([1., 1., 1.]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Plans were saved to C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset500_MRI\n",
      "Configuration: 2d...\n",
      "Configuration: 3d_fullres...\n",
      "Configuration: 3d_lowres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:03<31:36,  3.80s/it]\n",
      "  0%|          | 2/500 [00:03<13:30,  1.63s/it]\n",
      "  1%|          | 4/500 [00:04<05:18,  1.56it/s]\n",
      "  1%|1         | 6/500 [00:04<03:12,  2.56it/s]\n",
      "  2%|1         | 8/500 [00:04<02:05,  3.94it/s]\n",
      "  2%|2         | 10/500 [00:04<01:58,  4.15it/s]\n",
      "  2%|2         | 12/500 [00:04<01:26,  5.65it/s]\n",
      "  3%|2         | 14/500 [00:05<01:15,  6.47it/s]\n",
      "  3%|3         | 16/500 [00:05<00:59,  8.17it/s]\n",
      "  4%|3         | 18/500 [00:05<01:21,  5.91it/s]\n",
      "  4%|4         | 21/500 [00:06<01:09,  6.92it/s]\n",
      "  5%|5         | 25/500 [00:06<01:01,  7.76it/s]\n",
      "  5%|5         | 26/500 [00:06<00:59,  7.92it/s]\n",
      "  6%|5         | 29/500 [00:06<00:56,  8.36it/s]\n",
      "  6%|6         | 31/500 [00:07<00:48,  9.74it/s]\n",
      "  7%|6         | 33/500 [00:07<00:55,  8.39it/s]\n",
      "  7%|7         | 35/500 [00:07<00:47,  9.89it/s]\n",
      "  7%|7         | 37/500 [00:07<01:02,  7.45it/s]\n",
      "  8%|8         | 41/500 [00:08<00:50,  9.03it/s]\n",
      "  9%|8         | 43/500 [00:08<00:44, 10.29it/s]\n",
      "  9%|9         | 45/500 [00:08<00:58,  7.79it/s]\n",
      " 10%|9         | 48/500 [00:08<00:43, 10.45it/s]\n",
      " 10%|#         | 50/500 [00:09<00:50,  8.87it/s]\n",
      " 10%|#         | 52/500 [00:09<00:43, 10.29it/s]\n",
      " 11%|#         | 54/500 [00:09<00:58,  7.69it/s]\n",
      " 12%|#1        | 58/500 [00:10<00:53,  8.28it/s]\n",
      " 12%|#2        | 61/500 [00:10<00:56,  7.77it/s]\n",
      " 13%|#2        | 63/500 [00:10<00:48,  8.98it/s]\n",
      " 13%|#3        | 66/500 [00:11<00:47,  9.05it/s]\n",
      " 14%|#3        | 68/500 [00:11<00:41, 10.31it/s]\n",
      " 14%|#4        | 70/500 [00:11<00:55,  7.80it/s]\n",
      " 15%|#4        | 74/500 [00:12<00:51,  8.33it/s]\n",
      " 15%|#5        | 77/500 [00:12<00:54,  7.81it/s]\n",
      " 16%|#5        | 79/500 [00:12<00:46,  9.01it/s]\n",
      " 16%|#6        | 82/500 [00:12<00:46,  9.08it/s]\n",
      " 17%|#6        | 84/500 [00:13<00:40, 10.32it/s]\n",
      " 17%|#7        | 86/500 [00:13<00:52,  7.82it/s]\n",
      " 18%|#8        | 90/500 [00:13<00:49,  8.34it/s]\n",
      " 19%|#8        | 93/500 [00:14<00:47,  8.59it/s]\n",
      " 19%|#8        | 94/500 [00:14<00:46,  8.68it/s]\n",
      " 20%|#9        | 98/500 [00:14<00:45,  8.87it/s]\n",
      " 20%|##        | 101/500 [00:15<00:49,  8.13it/s]\n",
      " 20%|##        | 102/500 [00:15<00:48,  8.26it/s]\n",
      " 21%|##1       | 106/500 [00:15<00:45,  8.64it/s]\n",
      " 22%|##1       | 109/500 [00:16<00:44,  8.80it/s]\n",
      " 22%|##2       | 110/500 [00:16<00:44,  8.86it/s]\n",
      " 23%|##2       | 114/500 [00:16<00:42,  9.00it/s]\n",
      " 23%|##3       | 117/500 [00:16<00:37, 10.08it/s]\n",
      " 24%|##3       | 119/500 [00:17<00:48,  7.92it/s]\n",
      " 24%|##4       | 122/500 [00:17<00:45,  8.30it/s]\n",
      " 25%|##5       | 125/500 [00:17<00:35, 10.69it/s]\n",
      " 25%|##5       | 127/500 [00:18<00:45,  8.18it/s]\n",
      " 26%|##6       | 130/500 [00:18<00:48,  7.70it/s]\n",
      " 27%|##6       | 134/500 [00:19<00:48,  7.55it/s]\n",
      " 28%|##7       | 138/500 [00:19<00:44,  8.06it/s]\n",
      " 28%|##8       | 142/500 [00:20<00:42,  8.39it/s]\n",
      " 29%|##9       | 145/500 [00:20<00:34, 10.31it/s]\n",
      " 29%|##9       | 147/500 [00:20<00:42,  8.24it/s]\n",
      " 30%|###       | 150/500 [00:20<00:45,  7.78it/s]\n",
      " 31%|###       | 153/500 [00:21<00:34,  9.94it/s]\n",
      " 31%|###1      | 155/500 [00:21<00:43,  7.90it/s]\n",
      " 32%|###1      | 158/500 [00:21<00:45,  7.55it/s]\n",
      " 32%|###2      | 162/500 [00:22<00:41,  8.09it/s]\n",
      " 33%|###3      | 166/500 [00:22<00:39,  8.45it/s]\n",
      " 33%|###3      | 167/500 [00:22<00:39,  8.53it/s]\n",
      " 34%|###4      | 170/500 [00:23<00:41,  7.91it/s]\n",
      " 35%|###4      | 174/500 [00:23<00:38,  8.36it/s]\n",
      " 36%|###5      | 178/500 [00:24<00:37,  8.62it/s]\n",
      " 36%|###6      | 181/500 [00:24<00:30, 10.60it/s]\n",
      " 37%|###6      | 183/500 [00:24<00:37,  8.34it/s]\n",
      " 37%|###7      | 186/500 [00:25<00:40,  7.84it/s]\n",
      " 38%|###8      | 190/500 [00:25<00:37,  8.29it/s]\n",
      " 39%|###8      | 193/500 [00:25<00:29, 10.34it/s]\n",
      " 39%|###9      | 195/500 [00:26<00:33,  9.02it/s]\n",
      " 39%|###9      | 197/500 [00:26<00:29, 10.22it/s]\n",
      " 40%|###9      | 199/500 [00:26<00:38,  7.83it/s]\n",
      " 40%|####      | 202/500 [00:26<00:36,  8.24it/s]\n",
      " 41%|####      | 204/500 [00:27<00:30,  9.55it/s]\n",
      " 41%|####1     | 206/500 [00:27<00:39,  7.45it/s]\n",
      " 42%|####2     | 210/500 [00:27<00:32,  8.95it/s]\n",
      " 42%|####2     | 212/500 [00:27<00:28, 10.17it/s]\n",
      " 43%|####2     | 214/500 [00:28<00:32,  8.73it/s]\n",
      " 43%|####3     | 216/500 [00:28<00:28, 10.13it/s]\n",
      " 44%|####3     | 218/500 [00:28<00:32,  8.57it/s]\n",
      " 44%|####4     | 220/500 [00:28<00:32,  8.73it/s]\n",
      " 44%|####4     | 222/500 [00:29<00:35,  7.78it/s]\n",
      " 45%|####4     | 223/500 [00:29<00:34,  7.99it/s]\n",
      " 45%|####5     | 226/500 [00:29<00:28,  9.73it/s]\n",
      " 46%|####5     | 228/500 [00:29<00:28,  9.56it/s]\n",
      " 46%|####6     | 230/500 [00:30<00:32,  8.22it/s]\n",
      " 46%|####6     | 231/500 [00:30<00:32,  8.36it/s]\n",
      " 47%|####6     | 234/500 [00:30<00:22, 11.86it/s]\n",
      " 47%|####7     | 236/500 [00:30<00:28,  9.35it/s]\n",
      " 48%|####7     | 238/500 [00:31<00:32,  8.11it/s]\n",
      " 48%|####8     | 240/500 [00:31<00:26,  9.71it/s]\n",
      " 48%|####8     | 242/500 [00:31<00:27,  9.54it/s]\n",
      " 49%|####8     | 244/500 [00:31<00:27,  9.45it/s]\n",
      " 49%|####9     | 246/500 [00:31<00:31,  8.12it/s]\n",
      " 49%|####9     | 247/500 [00:31<00:30,  8.29it/s]\n",
      " 50%|#####     | 250/500 [00:32<00:21, 11.84it/s]\n",
      " 50%|#####     | 252/500 [00:32<00:30,  8.10it/s]\n",
      " 51%|#####     | 254/500 [00:32<00:33,  7.39it/s]\n",
      " 51%|#####1    | 256/500 [00:32<00:27,  8.98it/s]\n",
      " 52%|#####1    | 259/500 [00:33<00:29,  8.03it/s]\n",
      " 52%|#####2    | 262/500 [00:33<00:31,  7.58it/s]\n",
      " 53%|#####3    | 267/500 [00:34<00:25,  8.98it/s]\n",
      " 54%|#####3    | 269/500 [00:34<00:23, 10.04it/s]\n",
      " 54%|#####4    | 271/500 [00:34<00:29,  7.88it/s]\n",
      " 55%|#####5    | 275/500 [00:35<00:26,  8.35it/s]\n",
      " 56%|#####5    | 278/500 [00:35<00:25,  8.59it/s]\n",
      " 56%|#####5    | 279/500 [00:35<00:25,  8.66it/s]\n",
      " 56%|#####6    | 280/500 [00:35<00:25,  8.74it/s]\n",
      " 57%|#####6    | 283/500 [00:36<00:27,  7.91it/s]\n",
      " 57%|#####7    | 286/500 [00:36<00:22,  9.35it/s]\n",
      " 58%|#####7    | 288/500 [00:36<00:22,  9.31it/s]\n",
      " 58%|#####7    | 289/500 [00:36<00:22,  9.29it/s]\n",
      " 58%|#####8    | 291/500 [00:37<00:26,  8.04it/s]\n",
      " 59%|#####8    | 294/500 [00:37<00:21,  9.63it/s]\n",
      " 59%|#####9    | 296/500 [00:37<00:24,  8.33it/s]\n",
      " 60%|#####9    | 299/500 [00:38<00:25,  7.75it/s]\n",
      " 60%|######    | 302/500 [00:38<00:19, 10.31it/s]\n",
      " 61%|######    | 304/500 [00:38<00:24,  7.87it/s]\n",
      " 61%|######1   | 307/500 [00:39<00:25,  7.55it/s]\n",
      " 62%|######2   | 311/500 [00:39<00:23,  8.14it/s]\n",
      " 63%|######3   | 315/500 [00:40<00:23,  7.84it/s]\n",
      " 64%|######3   | 319/500 [00:40<00:21,  8.26it/s]\n",
      " 65%|######4   | 323/500 [00:40<00:20,  8.56it/s]\n",
      " 65%|######5   | 327/500 [00:41<00:19,  8.75it/s]\n",
      " 66%|######5   | 328/500 [00:41<00:19,  8.79it/s]\n",
      " 66%|######6   | 331/500 [00:41<00:20,  8.11it/s]\n",
      " 67%|######7   | 335/500 [00:42<00:19,  8.47it/s]\n",
      " 68%|######7   | 339/500 [00:42<00:18,  8.69it/s]\n",
      " 69%|######8   | 343/500 [00:43<00:17,  8.81it/s]\n",
      " 69%|######9   | 347/500 [00:43<00:18,  8.30it/s]\n",
      " 70%|#######   | 351/500 [00:44<00:16,  9.23it/s]\n",
      " 70%|#######   | 352/500 [00:44<00:16,  9.24it/s]\n",
      " 71%|#######1  | 355/500 [00:44<00:17,  8.39it/s]\n",
      " 72%|#######1  | 359/500 [00:45<00:16,  8.66it/s]\n",
      " 72%|#######2  | 362/500 [00:45<00:12, 10.74it/s]\n",
      " 73%|#######2  | 364/500 [00:45<00:16,  8.37it/s]\n",
      " 73%|#######3  | 367/500 [00:45<00:15,  8.61it/s]\n",
      " 74%|#######3  | 369/500 [00:46<00:13,  9.83it/s]\n",
      " 74%|#######4  | 371/500 [00:46<00:16,  7.67it/s]\n",
      " 75%|#######5  | 375/500 [00:46<00:15,  8.22it/s]\n",
      " 76%|#######5  | 378/500 [00:47<00:11, 10.48it/s]\n",
      " 76%|#######6  | 380/500 [00:47<00:13,  9.03it/s]\n",
      " 76%|#######6  | 382/500 [00:47<00:11, 10.31it/s]\n",
      " 77%|#######6  | 384/500 [00:47<00:14,  7.80it/s]\n",
      " 77%|#######7  | 387/500 [00:48<00:15,  7.45it/s]\n",
      " 78%|#######8  | 391/500 [00:48<00:13,  8.07it/s]\n",
      " 79%|#######8  | 393/500 [00:48<00:11,  9.23it/s]\n",
      " 79%|#######9  | 395/500 [00:49<00:14,  7.41it/s]\n",
      " 80%|#######9  | 399/500 [00:49<00:11,  8.85it/s]\n",
      " 80%|########  | 401/500 [00:49<00:09, 10.04it/s]\n",
      " 81%|########  | 403/500 [00:50<00:12,  7.79it/s]\n",
      " 81%|########1 | 405/500 [00:50<00:10,  9.14it/s]\n",
      " 81%|########1 | 407/500 [00:50<00:11,  8.08it/s]\n",
      " 82%|########1 | 409/500 [00:50<00:09,  9.56it/s]\n",
      " 82%|########2 | 411/500 [00:51<00:12,  7.32it/s]\n",
      " 83%|########2 | 415/500 [00:51<00:08, 10.04it/s]\n",
      " 83%|########3 | 417/500 [00:51<00:09,  8.71it/s]\n",
      " 84%|########3 | 419/500 [00:52<00:11,  7.07it/s]\n",
      " 85%|########4 | 423/500 [00:52<00:07, 10.76it/s]\n",
      " 85%|########5 | 425/500 [00:52<00:08,  9.15it/s]\n",
      " 85%|########5 | 427/500 [00:52<00:09,  7.34it/s]\n",
      " 86%|########5 | 429/500 [00:53<00:08,  8.72it/s]\n",
      " 86%|########6 | 432/500 [00:53<00:07,  8.89it/s]\n",
      " 87%|########6 | 434/500 [00:53<00:06, 10.24it/s]\n",
      " 87%|########7 | 436/500 [00:53<00:08,  7.71it/s]\n",
      " 88%|########7 | 439/500 [00:54<00:05, 10.46it/s]\n",
      " 88%|########8 | 441/500 [00:54<00:06,  8.85it/s]\n",
      " 89%|########8 | 443/500 [00:54<00:06,  8.94it/s]\n",
      " 89%|########9 | 445/500 [00:54<00:06,  7.91it/s]\n",
      " 90%|########9 | 448/500 [00:55<00:06,  7.50it/s]\n",
      " 90%|######### | 451/500 [00:55<00:04, 10.05it/s]\n",
      " 91%|######### | 453/500 [00:55<00:06,  7.76it/s]\n",
      " 91%|#########1| 456/500 [00:56<00:05,  7.43it/s]\n",
      " 92%|#########2| 460/500 [00:56<00:04,  8.04it/s]\n",
      " 93%|#########2| 464/500 [00:57<00:04,  8.42it/s]\n",
      " 93%|#########3| 466/500 [00:57<00:03,  9.49it/s]\n",
      " 94%|#########3| 468/500 [00:57<00:04,  7.65it/s]\n",
      " 94%|#########4| 472/500 [00:58<00:03,  8.19it/s]\n",
      " 95%|#########5| 476/500 [00:58<00:02,  8.53it/s]\n",
      " 96%|#########5| 478/500 [00:58<00:02,  9.58it/s]\n",
      " 96%|#########6| 480/500 [00:59<00:02,  8.49it/s]\n",
      " 96%|#########6| 482/500 [00:59<00:01,  9.79it/s]\n",
      " 97%|#########6| 484/500 [00:59<00:02,  7.58it/s]\n",
      " 98%|#########7| 488/500 [00:59<00:01,  8.98it/s]\n",
      " 98%|#########8| 490/500 [01:00<00:00, 10.25it/s]\n",
      " 98%|#########8| 492/500 [01:00<00:01,  7.84it/s]\n",
      " 99%|#########8| 494/500 [01:00<00:00,  9.22it/s]\n",
      " 99%|#########9| 496/500 [01:00<00:00,  9.24it/s]\n",
      "100%|#########9| 498/500 [01:00<00:00, 10.76it/s]\n",
      "100%|##########| 500/500 [01:01<00:00, 12.22it/s]\n",
      "100%|##########| 500/500 [01:01<00:00,  8.18it/s]\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:08<1:14:49,  9.00s/it]\n",
      "  1%|          | 3/500 [00:09<19:59,  2.41s/it]  \n",
      "  1%|          | 4/500 [00:09<13:58,  1.69s/it]\n",
      "  1%|1         | 5/500 [00:09<09:55,  1.20s/it]\n",
      "  1%|1         | 7/500 [00:09<05:30,  1.49it/s]\n",
      "  2%|1         | 8/500 [00:10<04:32,  1.80it/s]\n",
      "  2%|1         | 9/500 [00:14<12:52,  1.57s/it]\n",
      "  2%|2         | 10/500 [00:14<10:01,  1.23s/it]\n",
      "  2%|2         | 11/500 [00:14<07:24,  1.10it/s]\n",
      "  2%|2         | 12/500 [00:15<05:30,  1.47it/s]\n",
      "  3%|2         | 13/500 [00:15<04:09,  1.95it/s]\n",
      "  3%|2         | 14/500 [00:15<03:26,  2.35it/s]\n",
      "  3%|3         | 15/500 [00:15<03:12,  2.52it/s]\n",
      "  3%|3         | 17/500 [00:19<09:32,  1.19s/it]\n",
      "  4%|3         | 18/500 [00:20<07:36,  1.06it/s]\n",
      "  4%|4         | 20/500 [00:20<04:58,  1.61it/s]\n",
      "  4%|4         | 21/500 [00:20<04:13,  1.89it/s]\n",
      "  4%|4         | 22/500 [00:20<03:23,  2.35it/s]\n",
      "  5%|4         | 23/500 [00:21<03:24,  2.34it/s]\n",
      "  5%|5         | 25/500 [00:24<07:37,  1.04it/s]\n",
      "  5%|5         | 26/500 [00:25<07:37,  1.04it/s]\n",
      "  5%|5         | 27/500 [00:25<05:55,  1.33it/s]\n",
      "  6%|5         | 28/500 [00:25<04:48,  1.64it/s]\n",
      "  6%|5         | 29/500 [00:26<03:43,  2.11it/s]\n",
      "  6%|6         | 30/500 [00:26<02:54,  2.69it/s]\n",
      "  6%|6         | 31/500 [00:26<02:33,  3.06it/s]\n",
      "  6%|6         | 32/500 [00:26<02:18,  3.39it/s]\n",
      "  7%|6         | 33/500 [00:30<09:48,  1.26s/it]\n",
      "  7%|6         | 34/500 [00:30<07:54,  1.02s/it]\n",
      "  7%|7         | 35/500 [00:30<06:17,  1.23it/s]\n",
      "  7%|7         | 36/500 [00:31<04:39,  1.66it/s]\n",
      "  7%|7         | 37/500 [00:31<03:30,  2.19it/s]\n",
      "  8%|7         | 38/500 [00:31<02:57,  2.60it/s]\n",
      "  8%|7         | 39/500 [00:31<02:34,  2.99it/s]\n",
      "  8%|8         | 40/500 [00:31<02:17,  3.34it/s]\n",
      "  8%|8         | 41/500 [00:35<10:49,  1.42s/it]\n",
      "  8%|8         | 42/500 [00:36<08:03,  1.06s/it]\n",
      "  9%|8         | 43/500 [00:36<06:37,  1.15it/s]\n",
      "  9%|9         | 45/500 [00:36<03:44,  2.03it/s]\n",
      "  9%|9         | 46/500 [00:37<03:37,  2.09it/s]\n",
      "  9%|9         | 47/500 [00:37<03:06,  2.43it/s]\n",
      " 10%|9         | 49/500 [00:40<07:22,  1.02it/s]\n",
      " 10%|#         | 50/500 [00:41<06:23,  1.17it/s]\n",
      " 10%|#         | 51/500 [00:41<06:11,  1.21it/s]\n",
      " 11%|#         | 54/500 [00:42<03:14,  2.29it/s]\n",
      " 11%|#1        | 55/500 [00:42<02:45,  2.69it/s]\n",
      " 11%|#1        | 56/500 [00:42<03:01,  2.44it/s]\n",
      " 11%|#1        | 57/500 [00:46<08:04,  1.09s/it]\n",
      " 12%|#1        | 58/500 [00:46<06:48,  1.08it/s]\n",
      " 12%|#1        | 59/500 [00:46<05:37,  1.31it/s]\n",
      " 12%|#2        | 60/500 [00:47<04:56,  1.48it/s]\n",
      " 12%|#2        | 62/500 [00:47<02:55,  2.50it/s]\n",
      " 13%|#2        | 63/500 [00:48<03:32,  2.05it/s]\n",
      " 13%|#2        | 64/500 [00:48<03:26,  2.11it/s]\n",
      " 13%|#3        | 65/500 [00:50<06:43,  1.08it/s]\n",
      " 13%|#3        | 66/500 [00:51<06:48,  1.06it/s]\n",
      " 13%|#3        | 67/500 [00:52<06:25,  1.12it/s]\n",
      " 14%|#3        | 68/500 [00:52<04:47,  1.50it/s]\n",
      " 14%|#3        | 69/500 [00:52<04:18,  1.67it/s]\n",
      " 14%|#4        | 71/500 [00:53<02:31,  2.84it/s]\n",
      " 14%|#4        | 72/500 [00:53<02:50,  2.51it/s]\n",
      " 15%|#4        | 73/500 [00:55<05:30,  1.29it/s]\n",
      " 15%|#4        | 74/500 [00:56<06:44,  1.05it/s]\n",
      " 15%|#5        | 75/500 [00:57<05:42,  1.24it/s]\n",
      " 15%|#5        | 76/500 [00:58<05:22,  1.31it/s]\n",
      " 16%|#5        | 78/500 [00:58<03:07,  2.25it/s]\n",
      " 16%|#5        | 79/500 [00:58<02:55,  2.40it/s]\n",
      " 16%|#6        | 80/500 [00:59<03:20,  2.10it/s]\n",
      " 16%|#6        | 81/500 [01:00<04:52,  1.43it/s]\n",
      " 16%|#6        | 82/500 [01:01<06:27,  1.08it/s]\n",
      " 17%|#6        | 83/500 [01:02<06:06,  1.14it/s]\n",
      " 17%|#6        | 84/500 [01:02<04:33,  1.52it/s]\n",
      " 17%|#7        | 85/500 [01:03<03:52,  1.79it/s]\n",
      " 17%|#7        | 86/500 [01:03<03:36,  1.92it/s]\n",
      " 17%|#7        | 87/500 [01:04<03:38,  1.89it/s]\n",
      " 18%|#7        | 88/500 [01:04<02:59,  2.29it/s]\n",
      " 18%|#7        | 89/500 [01:05<04:45,  1.44it/s]\n",
      " 18%|#8        | 90/500 [01:07<06:12,  1.10it/s]\n",
      " 18%|#8        | 91/500 [01:07<04:33,  1.49it/s]\n",
      " 18%|#8        | 92/500 [01:08<05:23,  1.26it/s]\n",
      " 19%|#8        | 93/500 [01:08<04:12,  1.61it/s]\n",
      " 19%|#8        | 94/500 [01:08<03:49,  1.77it/s]\n",
      " 19%|#9        | 95/500 [01:09<03:33,  1.90it/s]\n",
      " 19%|#9        | 96/500 [01:09<03:21,  2.01it/s]\n",
      " 19%|#9        | 97/500 [01:11<05:11,  1.29it/s]\n",
      " 20%|#9        | 98/500 [01:12<06:14,  1.07it/s]\n",
      " 20%|#9        | 99/500 [01:12<04:34,  1.46it/s]\n",
      " 20%|##        | 100/500 [01:13<04:42,  1.41it/s]\n",
      " 20%|##        | 101/500 [01:13<03:56,  1.69it/s]\n",
      " 20%|##        | 102/500 [01:14<03:36,  1.84it/s]\n",
      " 21%|##        | 103/500 [01:14<02:44,  2.42it/s]\n",
      " 21%|##        | 104/500 [01:14<02:58,  2.21it/s]\n",
      " 21%|##1       | 105/500 [01:16<05:44,  1.15it/s]\n",
      " 21%|##1       | 106/500 [01:17<05:05,  1.29it/s]\n",
      " 21%|##1       | 107/500 [01:17<04:36,  1.42it/s]\n",
      " 22%|##1       | 108/500 [01:18<04:43,  1.38it/s]\n",
      " 22%|##1       | 109/500 [01:18<03:55,  1.66it/s]\n",
      " 22%|##2       | 110/500 [01:19<03:22,  1.92it/s]\n",
      " 22%|##2       | 111/500 [01:19<03:37,  1.79it/s]\n",
      " 23%|##2       | 113/500 [01:22<05:20,  1.21it/s]\n",
      " 23%|##2       | 114/500 [01:22<04:21,  1.48it/s]\n",
      " 23%|##3       | 115/500 [01:22<03:34,  1.80it/s]\n",
      " 23%|##3       | 116/500 [01:24<05:25,  1.18it/s]\n",
      " 23%|##3       | 117/500 [01:24<04:05,  1.56it/s]\n",
      " 24%|##3       | 118/500 [01:24<03:06,  2.05it/s]\n",
      " 24%|##3       | 119/500 [01:24<02:36,  2.44it/s]\n",
      " 24%|##4       | 120/500 [01:25<03:26,  1.84it/s]\n",
      " 24%|##4       | 121/500 [01:25<03:13,  1.96it/s]\n",
      " 24%|##4       | 122/500 [01:27<05:31,  1.14it/s]\n",
      " 25%|##4       | 123/500 [01:27<04:40,  1.34it/s]\n",
      " 25%|##4       | 124/500 [01:28<05:06,  1.23it/s]\n",
      " 25%|##5       | 125/500 [01:29<05:11,  1.20it/s]\n",
      " 25%|##5       | 127/500 [01:30<03:05,  2.01it/s]\n",
      " 26%|##5       | 128/500 [01:30<02:59,  2.07it/s]\n",
      " 26%|##5       | 129/500 [01:30<02:54,  2.13it/s]\n",
      " 26%|##6       | 130/500 [01:31<03:33,  1.73it/s]\n",
      " 26%|##6       | 131/500 [01:33<05:10,  1.19it/s]\n",
      " 26%|##6       | 132/500 [01:34<05:58,  1.03it/s]\n",
      " 27%|##6       | 133/500 [01:35<05:11,  1.18it/s]\n",
      " 27%|##6       | 134/500 [01:35<03:51,  1.58it/s]\n",
      " 27%|##7       | 135/500 [01:35<02:54,  2.09it/s]\n",
      " 27%|##7       | 136/500 [01:36<03:24,  1.78it/s]\n",
      " 28%|##7       | 138/500 [01:36<02:52,  2.10it/s]\n",
      " 28%|##7       | 139/500 [01:37<03:37,  1.66it/s]\n",
      " 28%|##8       | 140/500 [01:39<04:32,  1.32it/s]\n",
      " 28%|##8       | 141/500 [01:40<05:14,  1.14it/s]\n",
      " 28%|##8       | 142/500 [01:40<04:29,  1.33it/s]\n",
      " 29%|##8       | 143/500 [01:40<03:23,  1.76it/s]\n",
      " 29%|##8       | 144/500 [01:41<02:46,  2.14it/s]\n",
      " 29%|##9       | 145/500 [01:41<02:53,  2.05it/s]\n",
      " 29%|##9       | 146/500 [01:42<03:21,  1.76it/s]\n",
      " 29%|##9       | 147/500 [01:43<03:51,  1.52it/s]\n",
      " 30%|##9       | 148/500 [01:43<04:01,  1.46it/s]\n",
      " 30%|##9       | 149/500 [01:45<05:39,  1.03it/s]\n",
      " 30%|###       | 150/500 [01:45<04:19,  1.35it/s]\n",
      " 30%|###       | 151/500 [01:46<03:35,  1.62it/s]\n",
      " 30%|###       | 152/500 [01:46<03:04,  1.88it/s]\n",
      " 31%|###       | 153/500 [01:46<02:31,  2.29it/s]\n",
      " 31%|###       | 154/500 [01:47<02:30,  2.29it/s]\n",
      " 31%|###1      | 155/500 [01:47<02:41,  2.14it/s]\n",
      " 31%|###1      | 156/500 [01:48<03:55,  1.46it/s]\n",
      " 31%|###1      | 157/500 [01:50<05:10,  1.11it/s]\n",
      " 32%|###1      | 158/500 [01:50<03:58,  1.43it/s]\n",
      " 32%|###1      | 159/500 [01:51<04:04,  1.40it/s]\n",
      " 32%|###2      | 161/500 [01:51<03:01,  1.87it/s]\n",
      " 33%|###2      | 163/500 [01:52<02:10,  2.57it/s]\n",
      " 33%|###2      | 164/500 [01:53<03:34,  1.57it/s]\n",
      " 33%|###3      | 165/500 [01:55<05:10,  1.08it/s]\n",
      " 33%|###3      | 166/500 [01:55<04:27,  1.25it/s]\n",
      " 33%|###3      | 167/500 [01:56<03:54,  1.42it/s]\n",
      " 34%|###3      | 168/500 [01:56<03:08,  1.76it/s]\n",
      " 34%|###3      | 169/500 [01:56<02:25,  2.28it/s]\n",
      " 34%|###4      | 170/500 [01:57<02:34,  2.13it/s]\n",
      " 34%|###4      | 171/500 [01:57<02:30,  2.18it/s]\n",
      " 34%|###4      | 172/500 [01:58<02:38,  2.07it/s]\n",
      " 35%|###4      | 173/500 [02:00<06:03,  1.11s/it]\n",
      " 35%|###4      | 174/500 [02:01<05:28,  1.01s/it]\n",
      " 35%|###5      | 175/500 [02:01<04:21,  1.24it/s]\n",
      " 35%|###5      | 177/500 [02:02<02:52,  1.87it/s]\n",
      " 36%|###5      | 178/500 [02:02<02:43,  1.97it/s]\n",
      " 36%|###5      | 179/500 [02:03<02:27,  2.17it/s]\n",
      " 36%|###6      | 180/500 [02:03<02:24,  2.21it/s]\n",
      " 36%|###6      | 181/500 [02:06<05:36,  1.05s/it]\n",
      " 36%|###6      | 182/500 [02:06<04:38,  1.14it/s]\n",
      " 37%|###6      | 183/500 [02:06<03:47,  1.39it/s]\n",
      " 37%|###6      | 184/500 [02:07<03:50,  1.37it/s]\n",
      " 37%|###7      | 185/500 [02:08<03:22,  1.55it/s]\n",
      " 37%|###7      | 187/500 [02:08<01:57,  2.67it/s]\n",
      " 38%|###7      | 188/500 [02:09<02:43,  1.91it/s]\n",
      " 38%|###7      | 189/500 [02:11<05:04,  1.02it/s]\n",
      " 38%|###8      | 190/500 [02:11<04:17,  1.20it/s]\n",
      " 38%|###8      | 191/500 [02:12<03:14,  1.59it/s]\n",
      " 38%|###8      | 192/500 [02:13<04:13,  1.22it/s]\n",
      " 39%|###8      | 193/500 [02:13<03:28,  1.48it/s]\n",
      " 39%|###8      | 194/500 [02:14<02:55,  1.74it/s]\n",
      " 39%|###9      | 195/500 [02:14<02:13,  2.29it/s]\n",
      " 39%|###9      | 197/500 [02:16<04:12,  1.20it/s]\n",
      " 40%|###9      | 199/500 [02:16<02:37,  1.91it/s]\n",
      " 40%|####      | 200/500 [02:18<03:56,  1.27it/s]\n",
      " 40%|####      | 201/500 [02:18<03:07,  1.59it/s]\n",
      " 41%|####      | 203/500 [02:19<02:29,  1.98it/s]\n",
      " 41%|####      | 204/500 [02:19<02:17,  2.15it/s]\n",
      " 41%|####1     | 205/500 [02:21<04:03,  1.21it/s]\n",
      " 41%|####1     | 206/500 [02:22<03:32,  1.38it/s]\n",
      " 41%|####1     | 207/500 [02:22<02:43,  1.79it/s]\n",
      " 42%|####1     | 208/500 [02:22<03:08,  1.55it/s]\n",
      " 42%|####1     | 209/500 [02:23<02:41,  1.81it/s]\n",
      " 42%|####2     | 210/500 [02:23<02:12,  2.19it/s]\n",
      " 42%|####2     | 211/500 [02:24<03:32,  1.36it/s]\n",
      " 42%|####2     | 212/500 [02:25<02:47,  1.72it/s]\n",
      " 43%|####2     | 213/500 [02:25<03:02,  1.58it/s]\n",
      " 43%|####2     | 214/500 [02:26<03:21,  1.42it/s]\n",
      " 43%|####3     | 215/500 [02:27<03:16,  1.45it/s]\n",
      " 43%|####3     | 216/500 [02:28<03:40,  1.29it/s]\n",
      " 43%|####3     | 217/500 [02:28<02:52,  1.64it/s]\n",
      " 44%|####3     | 218/500 [02:29<03:04,  1.53it/s]\n",
      " 44%|####4     | 220/500 [02:30<02:34,  1.81it/s]\n",
      " 44%|####4     | 221/500 [02:31<03:10,  1.46it/s]\n",
      " 44%|####4     | 222/500 [02:31<02:51,  1.62it/s]\n",
      " 45%|####4     | 223/500 [02:32<03:10,  1.46it/s]\n",
      " 45%|####4     | 224/500 [02:33<03:31,  1.30it/s]\n",
      " 45%|####5     | 225/500 [02:34<03:30,  1.31it/s]\n",
      " 45%|####5     | 226/500 [02:34<02:46,  1.65it/s]\n",
      " 45%|####5     | 227/500 [02:35<02:40,  1.70it/s]\n",
      " 46%|####5     | 228/500 [02:35<02:27,  1.84it/s]\n",
      " 46%|####5     | 229/500 [02:36<03:10,  1.42it/s]\n",
      " 46%|####6     | 230/500 [02:36<02:22,  1.90it/s]\n",
      " 46%|####6     | 231/500 [02:38<03:41,  1.22it/s]\n",
      " 46%|####6     | 232/500 [02:39<03:52,  1.15it/s]\n",
      " 47%|####6     | 233/500 [02:39<03:00,  1.48it/s]\n",
      " 47%|####6     | 234/500 [02:39<02:40,  1.66it/s]\n",
      " 47%|####6     | 235/500 [02:40<02:34,  1.71it/s]\n",
      " 47%|####7     | 237/500 [02:41<02:54,  1.51it/s]\n",
      " 48%|####7     | 238/500 [02:42<02:18,  1.90it/s]\n",
      " 48%|####7     | 239/500 [02:43<03:47,  1.15it/s]\n",
      " 48%|####8     | 240/500 [02:45<04:01,  1.08it/s]\n",
      " 48%|####8     | 241/500 [02:45<03:01,  1.43it/s]\n",
      " 48%|####8     | 242/500 [02:45<02:16,  1.88it/s]\n",
      " 49%|####8     | 243/500 [02:45<02:09,  1.99it/s]\n",
      " 49%|####8     | 244/500 [02:46<02:11,  1.94it/s]\n",
      " 49%|####9     | 245/500 [02:46<02:29,  1.71it/s]\n",
      " 49%|####9     | 246/500 [02:47<02:41,  1.57it/s]\n",
      " 49%|####9     | 247/500 [02:48<02:34,  1.64it/s]\n",
      " 50%|####9     | 248/500 [02:50<04:06,  1.02it/s]\n",
      " 50%|####9     | 249/500 [02:50<03:08,  1.33it/s]\n",
      " 50%|#####     | 250/500 [02:50<02:36,  1.60it/s]\n",
      " 50%|#####     | 252/500 [02:50<01:29,  2.77it/s]\n",
      " 51%|#####     | 253/500 [02:51<01:46,  2.31it/s]\n",
      " 51%|#####     | 254/500 [02:53<03:17,  1.25it/s]\n",
      " 51%|#####1    | 255/500 [02:53<03:06,  1.31it/s]\n",
      " 51%|#####1    | 256/500 [02:55<03:27,  1.17it/s]\n",
      " 51%|#####1    | 257/500 [02:56<03:36,  1.12it/s]\n",
      " 52%|#####1    | 258/500 [02:56<02:48,  1.44it/s]\n",
      " 52%|#####2    | 260/500 [02:56<01:43,  2.33it/s]\n",
      " 52%|#####2    | 261/500 [02:56<01:49,  2.19it/s]\n",
      " 52%|#####2    | 262/500 [02:58<02:47,  1.42it/s]\n",
      " 53%|#####2    | 263/500 [02:59<03:11,  1.24it/s]\n",
      " 53%|#####2    | 264/500 [03:00<03:14,  1.21it/s]\n",
      " 53%|#####3    | 265/500 [03:01<03:09,  1.24it/s]\n",
      " 53%|#####3    | 266/500 [03:01<03:12,  1.21it/s]\n",
      " 53%|#####3    | 267/500 [03:02<02:30,  1.55it/s]\n",
      " 54%|#####3    | 268/500 [03:02<02:00,  1.92it/s]\n",
      " 54%|#####3    | 269/500 [03:02<01:39,  2.32it/s]\n",
      " 54%|#####4    | 270/500 [03:03<02:01,  1.89it/s]\n",
      " 54%|#####4    | 271/500 [03:04<02:23,  1.59it/s]\n",
      " 54%|#####4    | 272/500 [03:05<03:09,  1.20it/s]\n",
      " 55%|#####4    | 273/500 [03:06<03:32,  1.07it/s]\n",
      " 55%|#####4    | 274/500 [03:07<02:57,  1.27it/s]\n",
      " 55%|#####5    | 275/500 [03:07<02:11,  1.71it/s]\n",
      " 55%|#####5    | 276/500 [03:07<01:46,  2.11it/s]\n",
      " 55%|#####5    | 277/500 [03:07<01:35,  2.33it/s]\n",
      " 56%|#####5    | 278/500 [03:08<02:04,  1.78it/s]\n",
      " 56%|#####5    | 279/500 [03:09<02:45,  1.33it/s]\n",
      " 56%|#####6    | 280/500 [03:10<02:16,  1.61it/s]\n",
      " 56%|#####6    | 281/500 [03:12<03:50,  1.05s/it]\n",
      " 56%|#####6    | 282/500 [03:12<02:55,  1.24it/s]\n",
      " 57%|#####6    | 283/500 [03:12<02:09,  1.68it/s]\n",
      " 57%|#####6    | 284/500 [03:12<01:37,  2.23it/s]\n",
      " 57%|#####6    | 285/500 [03:13<01:56,  1.84it/s]\n",
      " 57%|#####7    | 286/500 [03:13<01:42,  2.10it/s]\n",
      " 57%|#####7    | 287/500 [03:14<02:13,  1.60it/s]\n",
      " 58%|#####7    | 288/500 [03:15<02:28,  1.43it/s]\n",
      " 58%|#####7    | 289/500 [03:17<03:47,  1.08s/it]\n",
      " 58%|#####8    | 290/500 [03:18<03:05,  1.13it/s]\n",
      " 58%|#####8    | 291/500 [03:18<02:22,  1.46it/s]\n",
      " 59%|#####8    | 293/500 [03:18<01:42,  2.03it/s]\n",
      " 59%|#####8    | 294/500 [03:19<01:33,  2.21it/s]\n",
      " 59%|#####8    | 295/500 [03:20<02:00,  1.70it/s]\n",
      " 59%|#####9    | 296/500 [03:20<02:09,  1.57it/s]\n",
      " 59%|#####9    | 297/500 [03:22<03:11,  1.06it/s]\n",
      " 60%|#####9    | 298/500 [03:22<02:28,  1.36it/s]\n",
      " 60%|#####9    | 299/500 [03:23<02:16,  1.47it/s]\n",
      " 60%|######    | 300/500 [03:23<01:42,  1.95it/s]\n",
      " 60%|######    | 301/500 [03:24<01:43,  1.92it/s]\n",
      " 60%|######    | 302/500 [03:24<02:03,  1.60it/s]\n",
      " 61%|######    | 303/500 [03:25<02:04,  1.58it/s]\n",
      " 61%|######    | 304/500 [03:25<01:52,  1.74it/s]\n",
      " 61%|######1   | 305/500 [03:27<03:00,  1.08it/s]\n",
      " 61%|######1   | 306/500 [03:27<02:18,  1.40it/s]\n",
      " 62%|######1   | 308/500 [03:28<01:18,  2.45it/s]\n",
      " 62%|######1   | 309/500 [03:29<02:00,  1.59it/s]\n",
      " 62%|######2   | 310/500 [03:29<01:39,  1.91it/s]\n",
      " 62%|######2   | 311/500 [03:30<02:07,  1.48it/s]\n",
      " 62%|######2   | 312/500 [03:30<01:48,  1.73it/s]\n",
      " 63%|######2   | 313/500 [03:33<03:07,  1.00s/it]\n",
      " 63%|######2   | 314/500 [03:33<02:24,  1.29it/s]\n",
      " 63%|######3   | 315/500 [03:33<01:58,  1.56it/s]\n",
      " 63%|######3   | 317/500 [03:34<01:44,  1.75it/s]\n",
      " 64%|######3   | 318/500 [03:34<01:33,  1.95it/s]\n",
      " 64%|######3   | 319/500 [03:34<01:13,  2.45it/s]\n",
      " 64%|######4   | 320/500 [03:36<01:57,  1.54it/s]\n",
      " 64%|######4   | 321/500 [03:38<03:06,  1.04s/it]\n",
      " 65%|######4   | 323/500 [03:38<01:51,  1.59it/s]\n",
      " 65%|######4   | 324/500 [03:39<01:47,  1.64it/s]\n",
      " 65%|######5   | 325/500 [03:39<01:29,  1.97it/s]\n",
      " 65%|######5   | 326/500 [03:39<01:09,  2.49it/s]\n",
      " 65%|######5   | 327/500 [03:41<02:08,  1.35it/s]\n",
      " 66%|######5   | 328/500 [03:41<02:08,  1.34it/s]\n",
      " 66%|######5   | 329/500 [03:43<02:35,  1.10it/s]\n",
      " 66%|######6   | 330/500 [03:43<02:26,  1.16it/s]\n",
      " 66%|######6   | 331/500 [03:43<01:48,  1.56it/s]\n",
      " 66%|######6   | 332/500 [03:44<01:37,  1.73it/s]\n",
      " 67%|######6   | 333/500 [03:44<01:18,  2.12it/s]\n",
      " 67%|######6   | 334/500 [03:45<01:16,  2.17it/s]\n",
      " 67%|######7   | 335/500 [03:45<01:14,  2.21it/s]\n",
      " 67%|######7   | 336/500 [03:46<02:01,  1.35it/s]\n",
      " 67%|######7   | 337/500 [03:47<01:45,  1.54it/s]\n",
      " 68%|######7   | 338/500 [03:48<02:11,  1.23it/s]\n",
      " 68%|######7   | 339/500 [03:48<01:52,  1.43it/s]\n",
      " 68%|######8   | 340/500 [03:49<01:54,  1.40it/s]\n",
      " 68%|######8   | 341/500 [03:49<01:30,  1.77it/s]\n",
      " 68%|######8   | 342/500 [03:50<01:23,  1.90it/s]\n",
      " 69%|######8   | 343/500 [03:50<01:13,  2.15it/s]\n",
      " 69%|######8   | 344/500 [03:52<02:17,  1.14it/s]\n",
      " 69%|######9   | 346/500 [03:53<01:59,  1.29it/s]\n",
      " 70%|######9   | 348/500 [03:54<01:17,  1.96it/s]\n",
      " 70%|######9   | 349/500 [03:54<01:07,  2.25it/s]\n",
      " 70%|#######   | 350/500 [03:55<01:29,  1.67it/s]\n",
      " 70%|#######   | 351/500 [03:56<01:39,  1.50it/s]\n",
      " 70%|#######   | 352/500 [03:57<02:12,  1.12it/s]\n",
      " 71%|#######   | 353/500 [03:57<01:44,  1.41it/s]\n",
      " 71%|#######   | 354/500 [03:58<01:45,  1.38it/s]\n",
      " 71%|#######1  | 355/500 [03:59<01:37,  1.49it/s]\n",
      " 71%|#######1  | 356/500 [03:59<01:31,  1.58it/s]\n",
      " 71%|#######1  | 357/500 [03:59<01:08,  2.09it/s]\n",
      " 72%|#######1  | 358/500 [04:00<01:24,  1.68it/s]\n",
      " 72%|#######1  | 359/500 [04:01<01:44,  1.35it/s]\n",
      " 72%|#######2  | 360/500 [04:02<01:58,  1.18it/s]\n",
      " 72%|#######2  | 361/500 [04:03<01:31,  1.52it/s]\n",
      " 72%|#######2  | 362/500 [04:04<01:39,  1.39it/s]\n",
      " 73%|#######2  | 363/500 [04:04<01:13,  1.86it/s]\n",
      " 73%|#######2  | 364/500 [04:04<00:55,  2.45it/s]\n",
      " 73%|#######3  | 365/500 [04:05<01:09,  1.95it/s]\n",
      " 73%|#######3  | 366/500 [04:06<01:57,  1.14it/s]\n",
      " 73%|#######3  | 367/500 [04:07<01:34,  1.40it/s]\n",
      " 74%|#######3  | 368/500 [04:07<01:40,  1.31it/s]\n",
      " 74%|#######3  | 369/500 [04:08<01:22,  1.59it/s]\n",
      " 74%|#######4  | 370/500 [04:08<01:01,  2.11it/s]\n",
      " 74%|#######4  | 371/500 [04:09<01:07,  1.90it/s]\n",
      " 74%|#######4  | 372/500 [04:09<01:12,  1.77it/s]\n",
      " 75%|#######4  | 373/500 [04:10<01:27,  1.46it/s]\n",
      " 75%|#######4  | 374/500 [04:12<01:53,  1.11it/s]\n",
      " 75%|#######5  | 375/500 [04:12<01:27,  1.43it/s]\n",
      " 75%|#######5  | 376/500 [04:12<01:24,  1.46it/s]\n",
      " 75%|#######5  | 377/500 [04:13<01:14,  1.64it/s]\n",
      " 76%|#######5  | 378/500 [04:13<01:11,  1.70it/s]\n",
      " 76%|#######5  | 379/500 [04:14<00:53,  2.25it/s]\n",
      " 76%|#######6  | 380/500 [04:14<01:08,  1.75it/s]\n",
      " 76%|#######6  | 381/500 [04:15<00:55,  2.14it/s]\n",
      " 76%|#######6  | 382/500 [04:17<02:10,  1.11s/it]\n",
      " 77%|#######6  | 384/500 [04:18<01:17,  1.49it/s]\n",
      " 77%|#######7  | 385/500 [04:18<01:10,  1.63it/s]\n",
      " 77%|#######7  | 386/500 [04:19<01:07,  1.68it/s]\n",
      " 77%|#######7  | 387/500 [04:19<01:12,  1.56it/s]\n",
      " 78%|#######7  | 388/500 [04:20<01:08,  1.63it/s]\n",
      " 78%|#######7  | 389/500 [04:20<00:55,  2.00it/s]\n",
      " 78%|#######8  | 390/500 [04:22<01:51,  1.01s/it]\n",
      " 78%|#######8  | 391/500 [04:22<01:21,  1.34it/s]\n",
      " 79%|#######8  | 393/500 [04:24<01:12,  1.47it/s]\n",
      " 79%|#######8  | 394/500 [04:24<00:57,  1.85it/s]\n",
      " 79%|#######9  | 395/500 [04:25<01:08,  1.53it/s]\n",
      " 79%|#######9  | 396/500 [04:25<01:07,  1.53it/s]\n",
      " 79%|#######9  | 397/500 [04:26<00:54,  1.88it/s]\n",
      " 80%|#######9  | 398/500 [04:27<01:26,  1.18it/s]\n",
      " 80%|#######9  | 399/500 [04:28<01:22,  1.22it/s]\n",
      " 80%|########  | 400/500 [04:28<01:04,  1.55it/s]\n",
      " 80%|########  | 401/500 [04:29<00:57,  1.72it/s]\n",
      " 80%|########  | 402/500 [04:29<00:55,  1.75it/s]\n",
      " 81%|########  | 403/500 [04:30<01:03,  1.52it/s]\n",
      " 81%|########  | 404/500 [04:31<01:12,  1.33it/s]\n",
      " 81%|########1 | 405/500 [04:31<00:53,  1.78it/s]\n",
      " 81%|########1 | 406/500 [04:32<01:07,  1.39it/s]\n",
      " 81%|########1 | 407/500 [04:33<01:04,  1.43it/s]\n",
      " 82%|########1 | 408/500 [04:34<01:14,  1.23it/s]\n",
      " 82%|########1 | 409/500 [04:35<01:09,  1.31it/s]\n",
      " 82%|########2 | 411/500 [04:36<00:56,  1.57it/s]\n",
      " 83%|########2 | 413/500 [04:37<00:50,  1.73it/s]\n",
      " 83%|########2 | 414/500 [04:37<00:44,  1.92it/s]\n",
      " 83%|########2 | 415/500 [04:39<01:06,  1.27it/s]\n",
      " 83%|########3 | 416/500 [04:39<01:10,  1.20it/s]\n",
      " 83%|########3 | 417/500 [04:40<00:57,  1.43it/s]\n",
      " 84%|########3 | 418/500 [04:40<00:48,  1.68it/s]\n",
      " 84%|########3 | 419/500 [04:41<00:49,  1.64it/s]\n",
      " 84%|########4 | 420/500 [04:41<00:44,  1.78it/s]\n",
      " 84%|########4 | 422/500 [04:43<00:48,  1.60it/s]\n",
      " 85%|########4 | 423/500 [04:44<01:04,  1.18it/s]\n",
      " 85%|########4 | 424/500 [04:44<00:53,  1.41it/s]\n",
      " 85%|########5 | 425/500 [04:45<00:43,  1.73it/s]\n",
      " 85%|########5 | 426/500 [04:45<00:42,  1.76it/s]\n",
      " 85%|########5 | 427/500 [04:46<00:40,  1.78it/s]\n",
      " 86%|########5 | 428/500 [04:46<00:37,  1.91it/s]\n",
      " 86%|########5 | 429/500 [04:47<00:37,  1.89it/s]\n",
      " 86%|########6 | 430/500 [04:47<00:32,  2.13it/s]\n",
      " 86%|########6 | 431/500 [04:50<01:20,  1.17s/it]\n",
      " 86%|########6 | 432/500 [04:50<01:04,  1.05it/s]\n",
      " 87%|########6 | 434/500 [04:51<00:42,  1.57it/s]\n",
      " 87%|########7 | 435/500 [04:51<00:38,  1.70it/s]\n",
      " 87%|########7 | 436/500 [04:52<00:36,  1.74it/s]\n",
      " 87%|########7 | 437/500 [04:52<00:32,  1.96it/s]\n",
      " 88%|########7 | 438/500 [04:52<00:26,  2.34it/s]\n",
      " 88%|########7 | 439/500 [04:55<01:09,  1.14s/it]\n",
      " 88%|########8 | 440/500 [04:55<00:50,  1.19it/s]\n",
      " 88%|########8 | 441/500 [04:56<00:42,  1.38it/s]\n",
      " 89%|########8 | 443/500 [04:57<00:33,  1.69it/s]\n",
      " 89%|########8 | 444/500 [04:57<00:29,  1.90it/s]\n",
      " 89%|########9 | 445/500 [04:57<00:26,  2.11it/s]\n",
      " 89%|########9 | 446/500 [04:58<00:25,  2.16it/s]\n",
      " 89%|########9 | 447/500 [05:00<00:56,  1.06s/it]\n",
      " 90%|########9 | 448/500 [05:01<00:41,  1.27it/s]\n",
      " 90%|########9 | 449/500 [05:01<00:36,  1.39it/s]\n",
      " 90%|######### | 450/500 [05:01<00:26,  1.85it/s]\n",
      " 90%|######### | 451/500 [05:02<00:23,  2.10it/s]\n",
      " 90%|######### | 452/500 [05:02<00:28,  1.69it/s]\n",
      " 91%|######### | 453/500 [05:03<00:30,  1.55it/s]\n",
      " 91%|######### | 454/500 [05:03<00:25,  1.82it/s]\n",
      " 91%|#########1| 455/500 [05:06<00:45,  1.00s/it]\n",
      " 91%|#########1| 456/500 [05:06<00:36,  1.20it/s]\n",
      " 91%|#########1| 457/500 [05:06<00:29,  1.47it/s]\n",
      " 92%|#########1| 458/500 [05:06<00:21,  1.96it/s]\n",
      " 92%|#########1| 459/500 [05:07<00:19,  2.05it/s]\n",
      " 92%|#########2| 460/500 [05:08<00:22,  1.76it/s]\n",
      " 92%|#########2| 461/500 [05:08<00:21,  1.78it/s]\n",
      " 92%|#########2| 462/500 [05:08<00:16,  2.35it/s]\n",
      " 93%|#########2| 463/500 [05:11<00:41,  1.12s/it]\n",
      " 93%|#########2| 464/500 [05:11<00:29,  1.23it/s]\n",
      " 93%|#########3| 465/500 [05:11<00:22,  1.58it/s]\n",
      " 93%|#########3| 466/500 [05:11<00:16,  2.10it/s]\n",
      " 93%|#########3| 467/500 [05:12<00:15,  2.16it/s]\n",
      " 94%|#########3| 468/500 [05:13<00:22,  1.40it/s]\n",
      " 94%|#########3| 469/500 [05:13<00:16,  1.87it/s]\n",
      " 94%|#########3| 470/500 [05:14<00:16,  1.86it/s]\n",
      " 94%|#########4| 471/500 [05:15<00:24,  1.20it/s]\n",
      " 94%|#########4| 472/500 [05:16<00:20,  1.34it/s]\n",
      " 95%|#########4| 473/500 [05:17<00:19,  1.39it/s]\n",
      " 95%|#########4| 474/500 [05:17<00:14,  1.76it/s]\n",
      " 95%|#########5| 475/500 [05:17<00:14,  1.68it/s]\n",
      " 95%|#########5| 476/500 [05:19<00:17,  1.35it/s]\n",
      " 95%|#########5| 477/500 [05:19<00:12,  1.81it/s]\n",
      " 96%|#########5| 478/500 [05:19<00:13,  1.63it/s]\n",
      " 96%|#########5| 479/500 [05:21<00:16,  1.27it/s]\n",
      " 96%|#########6| 480/500 [05:21<00:14,  1.40it/s]\n",
      " 96%|#########6| 481/500 [05:21<00:11,  1.67it/s]\n",
      " 96%|#########6| 482/500 [05:22<00:09,  1.94it/s]\n",
      " 97%|#########6| 483/500 [05:23<00:11,  1.53it/s]\n",
      " 97%|#########6| 484/500 [05:23<00:09,  1.70it/s]\n",
      " 97%|#########7| 485/500 [05:24<00:09,  1.56it/s]\n",
      " 97%|#########7| 486/500 [05:24<00:08,  1.64it/s]\n",
      " 97%|#########7| 487/500 [05:26<00:10,  1.22it/s]\n",
      " 98%|#########7| 488/500 [05:27<00:10,  1.11it/s]\n",
      " 98%|#########7| 489/500 [05:27<00:07,  1.51it/s]\n",
      " 98%|#########8| 490/500 [05:27<00:05,  1.79it/s]\n",
      " 98%|#########8| 491/500 [05:28<00:05,  1.53it/s]\n",
      " 98%|#########8| 492/500 [05:28<00:04,  1.92it/s]\n",
      " 99%|#########8| 493/500 [05:29<00:03,  2.16it/s]\n",
      " 99%|#########8| 494/500 [05:29<00:03,  1.82it/s]\n",
      " 99%|#########9| 495/500 [05:31<00:04,  1.14it/s]\n",
      " 99%|#########9| 496/500 [05:32<00:03,  1.06it/s]\n",
      " 99%|#########9| 497/500 [05:32<00:02,  1.45it/s]\n",
      "100%|#########9| 499/500 [05:33<00:00,  1.67it/s]\n",
      "100%|##########| 500/500 [05:33<00:00,  1.98it/s]\n",
      "100%|##########| 500/500 [05:34<00:00,  1.50it/s]\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:08<1:11:36,  8.61s/it]\n",
      "  1%|          | 3/500 [00:08<19:28,  2.35s/it]  \n",
      "  1%|          | 4/500 [00:09<13:19,  1.61s/it]\n",
      "  1%|1         | 5/500 [00:14<23:01,  2.79s/it]\n",
      "  1%|1         | 7/500 [00:14<12:12,  1.49s/it]\n",
      "  2%|1         | 8/500 [00:15<10:11,  1.24s/it]\n",
      "  2%|1         | 9/500 [00:19<18:15,  2.23s/it]\n",
      "  2%|2         | 10/500 [00:20<13:53,  1.70s/it]\n",
      "  2%|2         | 11/500 [00:20<10:10,  1.25s/it]\n",
      "  3%|2         | 13/500 [00:25<15:09,  1.87s/it]\n",
      "  3%|2         | 14/500 [00:25<11:52,  1.47s/it]\n",
      "  3%|3         | 16/500 [00:26<07:10,  1.12it/s]\n",
      "  3%|3         | 17/500 [00:31<14:45,  1.83s/it]\n",
      "  4%|3         | 18/500 [00:31<11:20,  1.41s/it]\n",
      "  4%|3         | 19/500 [00:31<09:04,  1.13s/it]\n",
      "  4%|4         | 20/500 [00:31<06:50,  1.17it/s]\n",
      "  4%|4         | 21/500 [00:35<14:22,  1.80s/it]\n",
      "  4%|4         | 22/500 [00:36<11:29,  1.44s/it]\n",
      "  5%|4         | 23/500 [00:36<09:08,  1.15s/it]\n",
      "  5%|5         | 25/500 [00:40<11:40,  1.47s/it]\n",
      "  5%|5         | 26/500 [00:41<11:19,  1.43s/it]\n",
      "  5%|5         | 27/500 [00:42<09:15,  1.17s/it]\n",
      "  6%|5         | 28/500 [00:42<07:40,  1.03it/s]\n",
      "  6%|5         | 29/500 [00:45<11:59,  1.53s/it]\n",
      "  6%|6         | 30/500 [00:47<11:28,  1.46s/it]\n",
      "  6%|6         | 31/500 [00:47<08:22,  1.07s/it]\n",
      "  6%|6         | 32/500 [00:47<07:39,  1.02it/s]\n",
      "  7%|6         | 33/500 [00:51<13:22,  1.72s/it]\n",
      "  7%|6         | 34/500 [00:52<10:53,  1.40s/it]\n",
      "  7%|7         | 35/500 [00:52<08:23,  1.08s/it]\n",
      "  7%|7         | 36/500 [00:53<07:52,  1.02s/it]\n",
      "  7%|7         | 37/500 [00:56<12:35,  1.63s/it]\n",
      "  8%|7         | 38/500 [00:56<09:33,  1.24s/it]\n",
      "  8%|7         | 39/500 [00:57<09:11,  1.20s/it]\n",
      "  8%|8         | 40/500 [00:58<07:39,  1.00it/s]\n",
      "  8%|8         | 41/500 [01:02<14:08,  1.85s/it]\n",
      "  8%|8         | 42/500 [01:02<10:37,  1.39s/it]\n",
      "  9%|8         | 43/500 [01:02<08:09,  1.07s/it]\n",
      "  9%|8         | 44/500 [01:03<07:40,  1.01s/it]\n",
      "  9%|9         | 45/500 [01:07<13:49,  1.82s/it]\n",
      "  9%|9         | 46/500 [01:08<11:37,  1.54s/it]\n",
      "  9%|9         | 47/500 [01:08<08:36,  1.14s/it]\n",
      " 10%|9         | 48/500 [01:08<06:30,  1.16it/s]\n",
      " 10%|9         | 49/500 [01:12<14:09,  1.88s/it]\n",
      " 10%|#         | 51/500 [01:13<09:28,  1.27s/it]\n",
      " 10%|#         | 52/500 [01:14<07:18,  1.02it/s]\n",
      " 11%|#         | 53/500 [01:17<12:39,  1.70s/it]\n",
      " 11%|#         | 54/500 [01:18<10:30,  1.41s/it]\n",
      " 11%|#1        | 55/500 [01:18<08:00,  1.08s/it]\n",
      " 11%|#1        | 56/500 [01:19<08:00,  1.08s/it]\n",
      " 11%|#1        | 57/500 [01:23<13:09,  1.78s/it]\n",
      " 12%|#1        | 58/500 [01:23<10:40,  1.45s/it]\n",
      " 12%|#1        | 59/500 [01:24<07:44,  1.05s/it]\n",
      " 12%|#2        | 60/500 [01:24<06:22,  1.15it/s]\n",
      " 12%|#2        | 61/500 [01:28<13:04,  1.79s/it]\n",
      " 12%|#2        | 62/500 [01:29<10:34,  1.45s/it]\n",
      " 13%|#2        | 63/500 [01:29<08:49,  1.21s/it]\n",
      " 13%|#2        | 64/500 [01:30<07:21,  1.01s/it]\n",
      " 13%|#3        | 65/500 [01:32<11:05,  1.53s/it]\n",
      " 13%|#3        | 66/500 [01:34<10:20,  1.43s/it]\n",
      " 13%|#3        | 67/500 [01:35<09:35,  1.33s/it]\n",
      " 14%|#3        | 68/500 [01:35<08:06,  1.13s/it]\n",
      " 14%|#3        | 69/500 [01:38<11:18,  1.57s/it]\n",
      " 14%|#4        | 70/500 [01:39<09:31,  1.33s/it]\n",
      " 14%|#4        | 71/500 [01:40<08:30,  1.19s/it]\n",
      " 14%|#4        | 72/500 [01:41<08:03,  1.13s/it]\n",
      " 15%|#4        | 73/500 [01:43<10:46,  1.51s/it]\n",
      " 15%|#4        | 74/500 [01:44<09:52,  1.39s/it]\n",
      " 15%|#5        | 75/500 [01:45<08:58,  1.27s/it]\n",
      " 15%|#5        | 76/500 [01:45<06:57,  1.01it/s]\n",
      " 15%|#5        | 77/500 [01:48<10:10,  1.44s/it]\n",
      " 16%|#5        | 78/500 [01:49<09:52,  1.40s/it]\n",
      " 16%|#5        | 79/500 [01:51<09:25,  1.34s/it]\n",
      " 16%|#6        | 80/500 [01:51<07:43,  1.10s/it]\n",
      " 16%|#6        | 81/500 [01:53<09:30,  1.36s/it]\n",
      " 16%|#6        | 82/500 [01:54<09:36,  1.38s/it]\n",
      " 17%|#6        | 83/500 [01:56<09:26,  1.36s/it]\n",
      " 17%|#6        | 84/500 [01:56<07:02,  1.02s/it]\n",
      " 17%|#7        | 85/500 [01:58<09:26,  1.37s/it]\n",
      " 17%|#7        | 86/500 [02:00<10:13,  1.48s/it]\n",
      " 17%|#7        | 87/500 [02:01<10:18,  1.50s/it]\n",
      " 18%|#7        | 89/500 [02:03<08:27,  1.23s/it]\n",
      " 18%|#8        | 90/500 [02:05<09:29,  1.39s/it]\n",
      " 18%|#8        | 91/500 [02:06<08:20,  1.22s/it]\n",
      " 18%|#8        | 92/500 [02:07<08:03,  1.19s/it]\n",
      " 19%|#8        | 93/500 [02:09<08:41,  1.28s/it]\n",
      " 19%|#8        | 94/500 [02:10<09:47,  1.45s/it]\n",
      " 19%|#9        | 95/500 [02:11<08:00,  1.19s/it]\n",
      " 19%|#9        | 96/500 [02:12<08:39,  1.29s/it]\n",
      " 19%|#9        | 97/500 [02:14<09:33,  1.42s/it]\n",
      " 20%|#9        | 98/500 [02:16<10:09,  1.52s/it]\n",
      " 20%|#9        | 99/500 [02:16<08:12,  1.23s/it]\n",
      " 20%|##        | 100/500 [02:18<08:20,  1.25s/it]\n",
      " 20%|##        | 101/500 [02:20<09:31,  1.43s/it]\n",
      " 20%|##        | 102/500 [02:21<09:54,  1.49s/it]\n",
      " 21%|##        | 103/500 [02:22<08:26,  1.27s/it]\n",
      " 21%|##        | 104/500 [02:22<06:45,  1.02s/it]\n",
      " 21%|##1       | 105/500 [02:25<10:21,  1.57s/it]\n",
      " 21%|##1       | 106/500 [02:27<09:35,  1.46s/it]\n",
      " 21%|##1       | 107/500 [02:27<06:54,  1.06s/it]\n",
      " 22%|##1       | 108/500 [02:28<07:49,  1.20s/it]\n",
      " 22%|##1       | 109/500 [02:30<09:19,  1.43s/it]\n",
      " 22%|##2       | 110/500 [02:32<09:42,  1.49s/it]\n",
      " 22%|##2       | 111/500 [02:32<07:49,  1.21s/it]\n",
      " 22%|##2       | 112/500 [02:33<07:21,  1.14s/it]\n",
      " 23%|##2       | 113/500 [02:36<10:14,  1.59s/it]\n",
      " 23%|##2       | 114/500 [02:37<08:36,  1.34s/it]\n",
      " 23%|##3       | 115/500 [02:37<06:51,  1.07s/it]\n",
      " 23%|##3       | 116/500 [02:39<08:22,  1.31s/it]\n",
      " 23%|##3       | 117/500 [02:41<09:50,  1.54s/it]\n",
      " 24%|##3       | 118/500 [02:42<08:06,  1.27s/it]\n",
      " 24%|##3       | 119/500 [02:42<06:04,  1.05it/s]\n",
      " 24%|##4       | 120/500 [02:45<09:12,  1.45s/it]\n",
      " 24%|##4       | 121/500 [02:45<07:15,  1.15s/it]\n",
      " 24%|##4       | 122/500 [02:47<09:23,  1.49s/it]\n",
      " 25%|##4       | 123/500 [02:48<06:57,  1.11s/it]\n",
      " 25%|##4       | 124/500 [02:49<08:32,  1.36s/it]\n",
      " 25%|##5       | 125/500 [02:51<07:59,  1.28s/it]\n",
      " 25%|##5       | 126/500 [02:53<09:52,  1.58s/it]\n",
      " 25%|##5       | 127/500 [02:53<07:17,  1.17s/it]\n",
      " 26%|##5       | 128/500 [02:55<08:57,  1.44s/it]\n",
      " 26%|##5       | 129/500 [02:55<06:39,  1.08s/it]\n",
      " 26%|##6       | 130/500 [02:57<08:04,  1.31s/it]\n",
      " 26%|##6       | 131/500 [02:59<08:14,  1.34s/it]\n",
      " 26%|##6       | 132/500 [03:01<09:45,  1.59s/it]\n",
      " 27%|##6       | 134/500 [03:03<08:08,  1.33s/it]\n",
      " 27%|##7       | 135/500 [03:04<07:34,  1.25s/it]\n",
      " 27%|##7       | 136/500 [03:06<09:32,  1.57s/it]\n",
      " 27%|##7       | 137/500 [03:07<07:38,  1.26s/it]\n",
      " 28%|##7       | 138/500 [03:08<07:53,  1.31s/it]\n",
      " 28%|##7       | 139/500 [03:09<06:10,  1.03s/it]\n",
      " 28%|##8       | 140/500 [03:11<08:31,  1.42s/it]\n",
      " 28%|##8       | 141/500 [03:12<08:29,  1.42s/it]\n",
      " 28%|##8       | 142/500 [03:13<07:18,  1.23s/it]\n",
      " 29%|##8       | 143/500 [03:14<06:28,  1.09s/it]\n",
      " 29%|##8       | 144/500 [03:16<07:58,  1.35s/it]\n",
      " 29%|##9       | 145/500 [03:18<08:50,  1.49s/it]\n",
      " 29%|##9       | 146/500 [03:19<08:05,  1.37s/it]\n",
      " 29%|##9       | 147/500 [03:19<06:48,  1.16s/it]\n",
      " 30%|##9       | 148/500 [03:21<07:03,  1.20s/it]\n",
      " 30%|##9       | 149/500 [03:23<09:20,  1.60s/it]\n",
      " 30%|###       | 150/500 [03:25<08:49,  1.51s/it]\n",
      " 30%|###       | 151/500 [03:25<06:32,  1.12s/it]\n",
      " 30%|###       | 152/500 [03:26<07:01,  1.21s/it]\n",
      " 31%|###       | 153/500 [03:27<07:10,  1.24s/it]\n",
      " 31%|###       | 154/500 [03:29<07:27,  1.29s/it]\n",
      " 31%|###1      | 155/500 [03:30<07:04,  1.23s/it]\n",
      " 31%|###1      | 156/500 [03:31<06:59,  1.22s/it]\n",
      " 31%|###1      | 157/500 [03:32<06:22,  1.12s/it]\n",
      " 32%|###1      | 158/500 [03:33<06:52,  1.21s/it]\n",
      " 32%|###1      | 159/500 [03:35<07:25,  1.31s/it]\n",
      " 32%|###2      | 160/500 [03:36<06:50,  1.21s/it]\n",
      " 32%|###2      | 161/500 [03:37<07:10,  1.27s/it]\n",
      " 32%|###2      | 162/500 [03:39<07:02,  1.25s/it]\n",
      " 33%|###2      | 163/500 [03:39<06:11,  1.10s/it]\n",
      " 33%|###2      | 164/500 [03:41<07:04,  1.26s/it]\n",
      " 33%|###3      | 165/500 [03:43<08:25,  1.51s/it]\n",
      " 33%|###3      | 166/500 [03:44<06:46,  1.22s/it]\n",
      " 33%|###3      | 167/500 [03:44<06:10,  1.11s/it]\n",
      " 34%|###3      | 168/500 [03:46<07:34,  1.37s/it]\n",
      " 34%|###3      | 169/500 [03:48<08:32,  1.55s/it]\n",
      " 34%|###4      | 170/500 [03:49<06:40,  1.21s/it]\n",
      " 34%|###4      | 171/500 [03:49<05:32,  1.01s/it]\n",
      " 34%|###4      | 172/500 [03:51<06:32,  1.20s/it]\n",
      " 35%|###4      | 173/500 [03:54<09:21,  1.72s/it]\n",
      " 35%|###4      | 174/500 [03:55<07:56,  1.46s/it]\n",
      " 35%|###5      | 175/500 [03:55<05:42,  1.06s/it]\n",
      " 35%|###5      | 176/500 [03:56<06:27,  1.20s/it]\n",
      " 35%|###5      | 177/500 [04:00<09:37,  1.79s/it]\n",
      " 36%|###5      | 178/500 [04:00<07:56,  1.48s/it]\n",
      " 36%|###5      | 179/500 [04:01<06:24,  1.20s/it]\n",
      " 36%|###6      | 180/500 [04:01<05:21,  1.00s/it]\n",
      " 36%|###6      | 181/500 [04:05<09:28,  1.78s/it]\n",
      " 36%|###6      | 182/500 [04:05<07:07,  1.35s/it]\n",
      " 37%|###6      | 183/500 [04:06<06:10,  1.17s/it]\n",
      " 37%|###6      | 184/500 [04:07<05:10,  1.02it/s]\n",
      " 37%|###7      | 185/500 [04:11<09:50,  1.87s/it]\n",
      " 37%|###7      | 186/500 [04:11<07:12,  1.38s/it]\n",
      " 37%|###7      | 187/500 [04:11<05:32,  1.06s/it]\n",
      " 38%|###7      | 188/500 [04:13<06:04,  1.17s/it]\n",
      " 38%|###7      | 189/500 [04:16<09:32,  1.84s/it]\n",
      " 38%|###8      | 190/500 [04:16<07:09,  1.39s/it]\n",
      " 38%|###8      | 191/500 [04:16<05:09,  1.00s/it]\n",
      " 38%|###8      | 192/500 [04:18<06:06,  1.19s/it]\n",
      " 39%|###8      | 193/500 [04:22<09:38,  1.88s/it]\n",
      " 39%|###8      | 194/500 [04:22<07:43,  1.51s/it]\n",
      " 39%|###9      | 195/500 [04:22<05:33,  1.09s/it]\n",
      " 39%|###9      | 196/500 [04:23<04:41,  1.08it/s]\n",
      " 39%|###9      | 197/500 [04:27<09:15,  1.83s/it]\n",
      " 40%|###9      | 198/500 [04:27<06:56,  1.38s/it]\n",
      " 40%|####      | 200/500 [04:28<05:13,  1.04s/it]\n",
      " 40%|####      | 201/500 [04:32<07:57,  1.60s/it]\n",
      " 40%|####      | 202/500 [04:32<06:00,  1.21s/it]\n",
      " 41%|####      | 203/500 [04:32<05:13,  1.06s/it]\n",
      " 41%|####      | 204/500 [04:34<05:50,  1.18s/it]\n",
      " 41%|####1     | 205/500 [04:37<08:08,  1.66s/it]\n",
      " 41%|####1     | 206/500 [04:37<06:32,  1.33s/it]\n",
      " 41%|####1     | 207/500 [04:38<05:13,  1.07s/it]\n",
      " 42%|####1     | 208/500 [04:39<04:55,  1.01s/it]\n",
      " 42%|####1     | 209/500 [04:42<07:32,  1.56s/it]\n",
      " 42%|####2     | 210/500 [04:42<06:22,  1.32s/it]\n",
      " 42%|####2     | 211/500 [04:44<06:20,  1.32s/it]\n",
      " 42%|####2     | 212/500 [04:44<05:12,  1.09s/it]\n",
      " 43%|####2     | 213/500 [04:46<06:18,  1.32s/it]\n",
      " 43%|####2     | 214/500 [04:47<05:48,  1.22s/it]\n",
      " 43%|####3     | 215/500 [04:49<06:50,  1.44s/it]\n",
      " 43%|####3     | 216/500 [04:50<05:41,  1.20s/it]\n",
      " 43%|####3     | 217/500 [04:51<06:34,  1.40s/it]\n",
      " 44%|####3     | 218/500 [04:53<06:36,  1.40s/it]\n",
      " 44%|####3     | 219/500 [04:54<05:31,  1.18s/it]\n",
      " 44%|####4     | 220/500 [04:55<05:22,  1.15s/it]\n",
      " 44%|####4     | 221/500 [04:57<06:57,  1.50s/it]\n",
      " 44%|####4     | 222/500 [04:58<06:04,  1.31s/it]\n",
      " 45%|####4     | 223/500 [04:59<05:44,  1.24s/it]\n",
      " 45%|####4     | 224/500 [05:00<05:30,  1.20s/it]\n",
      " 45%|####5     | 225/500 [05:03<07:45,  1.69s/it]\n",
      " 45%|####5     | 226/500 [05:04<06:36,  1.45s/it]\n",
      " 45%|####5     | 227/500 [05:04<05:20,  1.17s/it]\n",
      " 46%|####5     | 228/500 [05:05<05:21,  1.18s/it]\n",
      " 46%|####5     | 229/500 [05:08<07:43,  1.71s/it]\n",
      " 46%|####6     | 230/500 [05:09<06:06,  1.36s/it]\n",
      " 46%|####6     | 231/500 [05:10<05:44,  1.28s/it]\n",
      " 46%|####6     | 232/500 [05:11<05:44,  1.29s/it]\n",
      " 47%|####6     | 233/500 [05:14<07:03,  1.59s/it]\n",
      " 47%|####6     | 234/500 [05:14<05:47,  1.31s/it]\n",
      " 47%|####6     | 235/500 [05:15<05:37,  1.27s/it]\n",
      " 47%|####7     | 236/500 [05:16<05:03,  1.15s/it]\n",
      " 47%|####7     | 237/500 [05:19<07:08,  1.63s/it]\n",
      " 48%|####7     | 238/500 [05:20<05:40,  1.30s/it]\n",
      " 48%|####7     | 239/500 [05:21<05:57,  1.37s/it]\n",
      " 48%|####8     | 240/500 [05:22<05:16,  1.22s/it]\n",
      " 48%|####8     | 241/500 [05:25<07:12,  1.67s/it]\n",
      " 48%|####8     | 242/500 [05:25<05:18,  1.23s/it]\n",
      " 49%|####8     | 243/500 [05:26<05:30,  1.29s/it]\n",
      " 49%|####8     | 244/500 [05:28<05:30,  1.29s/it]\n",
      " 49%|####9     | 245/500 [05:30<06:46,  1.59s/it]\n",
      " 49%|####9     | 246/500 [05:30<05:24,  1.28s/it]\n",
      " 49%|####9     | 247/500 [05:31<04:02,  1.04it/s]\n",
      " 50%|####9     | 248/500 [05:33<06:07,  1.46s/it]\n",
      " 50%|####9     | 249/500 [05:35<06:28,  1.55s/it]\n",
      " 50%|#####     | 250/500 [05:35<04:47,  1.15s/it]\n",
      " 50%|#####     | 251/500 [05:36<04:08,  1.00it/s]\n",
      " 50%|#####     | 252/500 [05:38<05:10,  1.25s/it]\n",
      " 51%|#####     | 253/500 [05:40<06:02,  1.47s/it]\n",
      " 51%|#####     | 254/500 [05:41<05:25,  1.32s/it]\n",
      " 51%|#####1    | 255/500 [05:42<04:50,  1.19s/it]\n",
      " 51%|#####1    | 256/500 [05:42<04:26,  1.09s/it]\n",
      " 51%|#####1    | 257/500 [05:45<06:25,  1.59s/it]\n",
      " 52%|#####1    | 258/500 [05:46<05:16,  1.31s/it]\n",
      " 52%|#####1    | 259/500 [05:47<04:43,  1.17s/it]\n",
      " 52%|#####2    | 260/500 [05:48<04:35,  1.15s/it]\n",
      " 52%|#####2    | 261/500 [05:51<06:29,  1.63s/it]\n",
      " 53%|#####2    | 263/500 [05:52<05:02,  1.28s/it]\n",
      " 53%|#####2    | 264/500 [05:53<04:24,  1.12s/it]\n",
      " 53%|#####3    | 265/500 [05:55<05:49,  1.49s/it]\n",
      " 53%|#####3    | 266/500 [05:56<05:01,  1.29s/it]\n",
      " 53%|#####3    | 267/500 [05:58<05:44,  1.48s/it]\n",
      " 54%|#####3    | 268/500 [05:59<04:26,  1.15s/it]\n",
      " 54%|#####3    | 269/500 [06:01<05:57,  1.55s/it]\n",
      " 54%|#####4    | 270/500 [06:01<04:18,  1.13s/it]\n",
      " 54%|#####4    | 271/500 [06:03<05:21,  1.40s/it]\n",
      " 54%|#####4    | 272/500 [06:04<04:36,  1.21s/it]\n",
      " 55%|#####4    | 273/500 [06:07<06:10,  1.63s/it]\n",
      " 55%|#####4    | 274/500 [06:07<04:41,  1.24s/it]\n",
      " 55%|#####5    | 275/500 [06:09<05:20,  1.43s/it]\n",
      " 55%|#####5    | 276/500 [06:09<03:58,  1.06s/it]\n",
      " 55%|#####5    | 277/500 [06:12<05:40,  1.53s/it]\n",
      " 56%|#####5    | 278/500 [06:12<04:47,  1.30s/it]\n",
      " 56%|#####5    | 279/500 [06:14<05:01,  1.37s/it]\n",
      " 56%|#####6    | 280/500 [06:15<04:20,  1.19s/it]\n",
      " 56%|#####6    | 281/500 [06:17<05:53,  1.62s/it]\n",
      " 56%|#####6    | 282/500 [06:18<04:27,  1.23s/it]\n",
      " 57%|#####6    | 283/500 [06:19<04:39,  1.29s/it]\n",
      " 57%|#####6    | 284/500 [06:20<04:03,  1.13s/it]\n",
      " 57%|#####6    | 285/500 [06:23<06:08,  1.71s/it]\n",
      " 57%|#####7    | 286/500 [06:23<04:30,  1.26s/it]\n",
      " 57%|#####7    | 287/500 [06:24<04:10,  1.18s/it]\n",
      " 58%|#####7    | 288/500 [06:25<04:10,  1.18s/it]\n",
      " 58%|#####7    | 289/500 [06:28<05:47,  1.65s/it]\n",
      " 58%|#####8    | 290/500 [06:29<04:36,  1.31s/it]\n",
      " 58%|#####8    | 291/500 [06:30<04:27,  1.28s/it]\n",
      " 58%|#####8    | 292/500 [06:31<04:14,  1.22s/it]\n",
      " 59%|#####8    | 293/500 [06:34<05:47,  1.68s/it]\n",
      " 59%|#####8    | 294/500 [06:34<04:15,  1.24s/it]\n",
      " 59%|#####8    | 295/500 [06:35<04:44,  1.39s/it]\n",
      " 59%|#####9    | 296/500 [06:36<03:58,  1.17s/it]\n",
      " 59%|#####9    | 297/500 [06:39<05:25,  1.61s/it]\n",
      " 60%|#####9    | 298/500 [06:39<04:06,  1.22s/it]\n",
      " 60%|#####9    | 299/500 [06:41<04:30,  1.35s/it]\n",
      " 60%|######    | 300/500 [06:41<03:41,  1.11s/it]\n",
      " 60%|######    | 301/500 [06:44<04:57,  1.49s/it]\n",
      " 60%|######    | 302/500 [06:45<04:31,  1.37s/it]\n",
      " 61%|######    | 303/500 [06:46<04:40,  1.42s/it]\n",
      " 61%|######    | 304/500 [06:47<03:27,  1.06s/it]\n",
      " 61%|######1   | 305/500 [06:49<04:57,  1.53s/it]\n",
      " 61%|######1   | 306/500 [06:50<04:18,  1.33s/it]\n",
      " 61%|######1   | 307/500 [06:51<03:43,  1.16s/it]\n",
      " 62%|######1   | 308/500 [06:51<03:01,  1.06it/s]\n",
      " 62%|######1   | 309/500 [06:55<05:20,  1.68s/it]\n",
      " 62%|######2   | 310/500 [06:55<03:49,  1.21s/it]\n",
      " 62%|######2   | 311/500 [06:56<03:47,  1.20s/it]\n",
      " 62%|######2   | 312/500 [06:56<03:02,  1.03it/s]\n",
      " 63%|######2   | 313/500 [07:00<05:28,  1.76s/it]\n",
      " 63%|######2   | 314/500 [07:00<03:55,  1.26s/it]\n",
      " 63%|######3   | 315/500 [07:01<03:56,  1.28s/it]\n",
      " 63%|######3   | 316/500 [07:02<03:08,  1.03s/it]\n",
      " 63%|######3   | 317/500 [07:05<05:17,  1.74s/it]\n",
      " 64%|######3   | 318/500 [07:05<03:53,  1.28s/it]\n",
      " 64%|######3   | 319/500 [07:06<03:05,  1.03s/it]\n",
      " 64%|######4   | 320/500 [07:07<03:31,  1.18s/it]\n",
      " 64%|######4   | 321/500 [07:11<05:17,  1.77s/it]\n",
      " 65%|######4   | 323/500 [07:11<03:02,  1.03s/it]\n",
      " 65%|######4   | 324/500 [07:13<03:51,  1.31s/it]\n",
      " 65%|######5   | 325/500 [07:15<04:19,  1.48s/it]\n",
      " 65%|######5   | 326/500 [07:16<03:33,  1.23s/it]\n",
      " 65%|######5   | 327/500 [07:17<03:36,  1.25s/it]\n",
      " 66%|######5   | 328/500 [07:19<04:04,  1.42s/it]\n",
      " 66%|######5   | 329/500 [07:20<04:13,  1.49s/it]\n",
      " 66%|######6   | 330/500 [07:21<03:20,  1.18s/it]\n",
      " 66%|######6   | 331/500 [07:23<03:53,  1.38s/it]\n",
      " 66%|######6   | 332/500 [07:24<03:31,  1.26s/it]\n",
      " 67%|######6   | 333/500 [07:26<04:11,  1.51s/it]\n",
      " 67%|######6   | 334/500 [07:26<03:33,  1.28s/it]\n",
      " 67%|######7   | 335/500 [07:27<03:06,  1.13s/it]\n",
      " 67%|######7   | 336/500 [07:29<03:19,  1.22s/it]\n",
      " 67%|######7   | 337/500 [07:30<03:16,  1.21s/it]\n",
      " 68%|######7   | 338/500 [07:31<03:20,  1.24s/it]\n",
      " 68%|######7   | 339/500 [07:33<03:49,  1.42s/it]\n",
      " 68%|######8   | 340/500 [07:33<02:50,  1.06s/it]\n",
      " 68%|######8   | 341/500 [07:35<03:32,  1.33s/it]\n",
      " 68%|######8   | 342/500 [07:37<03:40,  1.40s/it]\n",
      " 69%|######8   | 343/500 [07:38<03:29,  1.34s/it]\n",
      " 69%|######8   | 344/500 [07:39<03:17,  1.26s/it]\n",
      " 69%|######9   | 345/500 [07:41<03:28,  1.34s/it]\n",
      " 69%|######9   | 346/500 [07:42<03:41,  1.44s/it]\n",
      " 69%|######9   | 347/500 [07:43<03:03,  1.20s/it]\n",
      " 70%|######9   | 348/500 [07:43<02:32,  1.00s/it]\n",
      " 70%|######9   | 349/500 [07:45<02:55,  1.16s/it]\n",
      " 70%|#######   | 350/500 [07:47<03:50,  1.53s/it]\n",
      " 70%|#######   | 351/500 [07:48<03:28,  1.40s/it]\n",
      " 70%|#######   | 352/500 [07:49<02:39,  1.08s/it]\n",
      " 71%|#######   | 353/500 [07:50<03:03,  1.25s/it]\n",
      " 71%|#######   | 354/500 [07:53<03:48,  1.56s/it]\n",
      " 71%|#######1  | 355/500 [07:53<03:06,  1.29s/it]\n",
      " 71%|#######1  | 356/500 [07:55<03:01,  1.26s/it]\n",
      " 71%|#######1  | 357/500 [07:56<03:07,  1.31s/it]\n",
      " 72%|#######1  | 358/500 [07:58<03:38,  1.54s/it]\n",
      " 72%|#######1  | 359/500 [07:59<03:08,  1.34s/it]\n",
      " 72%|#######2  | 360/500 [08:00<02:47,  1.20s/it]\n",
      " 72%|#######2  | 361/500 [08:01<02:51,  1.23s/it]\n",
      " 72%|#######2  | 362/500 [08:03<03:38,  1.58s/it]\n",
      " 73%|#######2  | 363/500 [08:04<02:40,  1.17s/it]\n",
      " 73%|#######2  | 364/500 [08:05<02:45,  1.21s/it]\n",
      " 73%|#######3  | 365/500 [08:05<02:07,  1.06it/s]\n",
      " 73%|#######3  | 366/500 [08:09<04:02,  1.81s/it]\n",
      " 73%|#######3  | 367/500 [08:09<03:01,  1.36s/it]\n",
      " 74%|#######3  | 368/500 [08:10<02:36,  1.18s/it]\n",
      " 74%|#######3  | 369/500 [08:10<01:56,  1.12it/s]\n",
      " 74%|#######4  | 370/500 [08:14<03:25,  1.58s/it]\n",
      " 74%|#######4  | 371/500 [08:15<03:13,  1.50s/it]\n",
      " 74%|#######4  | 372/500 [08:15<02:22,  1.12s/it]\n",
      " 75%|#######4  | 373/500 [08:16<02:20,  1.11s/it]\n",
      " 75%|#######4  | 374/500 [08:19<03:24,  1.63s/it]\n",
      " 75%|#######5  | 375/500 [08:20<02:58,  1.43s/it]\n",
      " 75%|#######5  | 376/500 [08:20<02:08,  1.03s/it]\n",
      " 75%|#######5  | 377/500 [08:21<02:17,  1.11s/it]\n",
      " 76%|#######5  | 378/500 [08:25<03:31,  1.73s/it]\n",
      " 76%|#######5  | 379/500 [08:25<02:42,  1.34s/it]\n",
      " 76%|#######6  | 380/500 [08:26<02:16,  1.14s/it]\n",
      " 76%|#######6  | 381/500 [08:26<01:38,  1.21it/s]\n",
      " 76%|#######6  | 382/500 [08:30<03:51,  1.96s/it]\n",
      " 77%|#######6  | 383/500 [08:31<02:48,  1.44s/it]\n",
      " 77%|#######6  | 384/500 [08:31<02:08,  1.11s/it]\n",
      " 77%|#######7  | 385/500 [08:31<01:36,  1.19it/s]\n",
      " 77%|#######7  | 386/500 [08:35<03:32,  1.87s/it]\n",
      " 77%|#######7  | 387/500 [08:37<03:04,  1.63s/it]\n",
      " 78%|#######7  | 388/500 [08:37<02:11,  1.17s/it]\n",
      " 78%|#######8  | 390/500 [08:41<02:49,  1.54s/it]\n",
      " 78%|#######8  | 391/500 [08:41<02:24,  1.32s/it]\n",
      " 78%|#######8  | 392/500 [08:42<02:00,  1.12s/it]\n",
      " 79%|#######8  | 393/500 [08:43<01:52,  1.05s/it]\n",
      " 79%|#######8  | 394/500 [08:46<02:54,  1.65s/it]\n",
      " 79%|#######9  | 395/500 [08:47<02:32,  1.46s/it]\n",
      " 79%|#######9  | 396/500 [08:48<02:07,  1.22s/it]\n",
      " 79%|#######9  | 397/500 [08:48<01:55,  1.12s/it]\n",
      " 80%|#######9  | 398/500 [08:52<02:59,  1.76s/it]\n",
      " 80%|########  | 400/500 [08:53<02:05,  1.25s/it]\n",
      " 80%|########  | 401/500 [08:54<01:59,  1.21s/it]\n",
      " 80%|########  | 402/500 [08:57<02:37,  1.61s/it]\n",
      " 81%|########  | 403/500 [08:57<02:08,  1.32s/it]\n",
      " 81%|########  | 404/500 [08:59<02:09,  1.35s/it]\n",
      " 81%|########1 | 405/500 [09:00<01:52,  1.18s/it]\n",
      " 81%|########1 | 406/500 [09:02<02:24,  1.54s/it]\n",
      " 81%|########1 | 407/500 [09:02<01:52,  1.21s/it]\n",
      " 82%|########1 | 408/500 [09:05<02:18,  1.50s/it]\n",
      " 82%|########1 | 409/500 [09:05<01:59,  1.31s/it]\n",
      " 82%|########2 | 410/500 [09:07<02:18,  1.54s/it]\n",
      " 82%|########2 | 411/500 [09:08<01:44,  1.18s/it]\n",
      " 82%|########2 | 412/500 [09:10<02:07,  1.44s/it]\n",
      " 83%|########2 | 413/500 [09:10<01:33,  1.08s/it]\n",
      " 83%|########2 | 414/500 [09:12<02:01,  1.41s/it]\n",
      " 83%|########2 | 415/500 [09:14<01:57,  1.38s/it]\n",
      " 83%|########3 | 416/500 [09:16<02:10,  1.55s/it]\n",
      " 83%|########3 | 417/500 [09:16<01:33,  1.12s/it]\n",
      " 84%|########3 | 418/500 [09:18<01:49,  1.34s/it]\n",
      " 84%|########3 | 419/500 [09:19<01:50,  1.36s/it]\n",
      " 84%|########4 | 420/500 [09:20<01:50,  1.38s/it]\n",
      " 84%|########4 | 421/500 [09:21<01:42,  1.29s/it]\n",
      " 84%|########4 | 422/500 [09:24<01:58,  1.53s/it]\n",
      " 85%|########4 | 423/500 [09:25<01:49,  1.43s/it]\n",
      " 85%|########4 | 424/500 [09:26<01:38,  1.29s/it]\n",
      " 85%|########5 | 425/500 [09:26<01:20,  1.07s/it]\n",
      " 85%|########5 | 426/500 [09:29<01:51,  1.50s/it]\n",
      " 85%|########5 | 427/500 [09:30<01:47,  1.48s/it]\n",
      " 86%|########5 | 428/500 [09:31<01:28,  1.23s/it]\n",
      " 86%|########5 | 429/500 [09:31<01:10,  1.01it/s]\n",
      " 86%|########6 | 430/500 [09:33<01:32,  1.32s/it]\n",
      " 86%|########6 | 431/500 [09:36<01:58,  1.72s/it]\n",
      " 86%|########6 | 432/500 [09:37<01:41,  1.49s/it]\n",
      " 87%|########6 | 434/500 [09:39<01:28,  1.33s/it]\n",
      " 87%|########7 | 435/500 [09:41<01:40,  1.55s/it]\n",
      " 87%|########7 | 436/500 [09:42<01:25,  1.34s/it]\n",
      " 87%|########7 | 437/500 [09:43<01:16,  1.21s/it]\n",
      " 88%|########7 | 438/500 [09:45<01:20,  1.30s/it]\n",
      " 88%|########7 | 439/500 [09:47<01:42,  1.68s/it]\n",
      " 88%|########8 | 440/500 [09:47<01:13,  1.22s/it]\n",
      " 88%|########8 | 441/500 [09:49<01:15,  1.28s/it]\n",
      " 88%|########8 | 442/500 [09:50<01:07,  1.16s/it]\n",
      " 89%|########8 | 443/500 [09:53<01:38,  1.73s/it]\n",
      " 89%|########8 | 444/500 [09:53<01:11,  1.28s/it]\n",
      " 89%|########9 | 445/500 [09:54<01:14,  1.35s/it]\n",
      " 89%|########9 | 446/500 [09:55<00:56,  1.05s/it]\n",
      " 89%|########9 | 447/500 [09:58<01:27,  1.65s/it]\n",
      " 90%|########9 | 448/500 [09:59<01:11,  1.38s/it]\n",
      " 90%|########9 | 449/500 [10:00<01:04,  1.26s/it]\n",
      " 90%|######### | 450/500 [10:00<00:53,  1.08s/it]\n",
      " 90%|######### | 451/500 [10:02<01:08,  1.41s/it]\n",
      " 90%|######### | 452/500 [10:04<01:09,  1.45s/it]\n",
      " 91%|######### | 453/500 [10:05<01:07,  1.44s/it]\n",
      " 91%|######### | 454/500 [10:06<00:53,  1.17s/it]\n",
      " 91%|#########1| 455/500 [10:08<01:04,  1.44s/it]\n",
      " 91%|#########1| 456/500 [10:09<00:57,  1.30s/it]\n",
      " 91%|#########1| 457/500 [10:11<01:00,  1.40s/it]\n",
      " 92%|#########1| 458/500 [10:11<00:52,  1.24s/it]\n",
      " 92%|#########1| 459/500 [10:13<00:50,  1.23s/it]\n",
      " 92%|#########2| 460/500 [10:14<00:55,  1.38s/it]\n",
      " 92%|#########2| 461/500 [10:16<00:53,  1.36s/it]\n",
      " 92%|#########2| 462/500 [10:16<00:42,  1.11s/it]\n",
      " 93%|#########2| 463/500 [10:19<00:54,  1.47s/it]\n",
      " 93%|#########2| 464/500 [10:20<00:52,  1.45s/it]\n",
      " 93%|#########3| 465/500 [10:21<00:45,  1.31s/it]\n",
      " 93%|#########3| 466/500 [10:21<00:33,  1.02it/s]\n",
      " 93%|#########3| 467/500 [10:23<00:40,  1.21s/it]\n",
      " 94%|#########3| 468/500 [10:26<00:53,  1.67s/it]\n",
      " 94%|#########3| 469/500 [10:26<00:40,  1.30s/it]\n",
      " 94%|#########3| 470/500 [10:27<00:34,  1.14s/it]\n",
      " 94%|#########4| 471/500 [10:28<00:30,  1.06s/it]\n",
      " 94%|#########4| 472/500 [10:30<00:38,  1.39s/it]\n",
      " 95%|#########4| 473/500 [10:31<00:37,  1.40s/it]\n",
      " 95%|#########4| 474/500 [10:32<00:33,  1.27s/it]\n",
      " 95%|#########5| 475/500 [10:33<00:31,  1.25s/it]\n",
      " 95%|#########5| 476/500 [10:36<00:35,  1.49s/it]\n",
      " 95%|#########5| 477/500 [10:37<00:32,  1.40s/it]\n",
      " 96%|#########5| 478/500 [10:38<00:30,  1.41s/it]\n",
      " 96%|#########5| 479/500 [10:39<00:25,  1.21s/it]\n",
      " 96%|#########6| 480/500 [10:41<00:29,  1.47s/it]\n",
      " 96%|#########6| 481/500 [10:42<00:25,  1.32s/it]\n",
      " 96%|#########6| 482/500 [10:43<00:20,  1.15s/it]\n",
      " 97%|#########6| 483/500 [10:44<00:21,  1.26s/it]\n",
      " 97%|#########6| 484/500 [10:46<00:20,  1.31s/it]\n",
      " 97%|#########7| 485/500 [10:47<00:21,  1.44s/it]\n",
      " 97%|#########7| 486/500 [10:48<00:16,  1.17s/it]\n",
      " 97%|#########7| 487/500 [10:50<00:17,  1.32s/it]\n",
      " 98%|#########7| 488/500 [10:51<00:17,  1.48s/it]\n",
      " 98%|#########7| 489/500 [10:53<00:16,  1.53s/it]\n",
      " 98%|#########8| 490/500 [10:54<00:12,  1.20s/it]\n",
      " 98%|#########8| 491/500 [10:55<00:11,  1.33s/it]\n",
      " 98%|#########8| 492/500 [10:57<00:11,  1.46s/it]\n",
      " 99%|#########8| 493/500 [10:58<00:09,  1.32s/it]\n",
      " 99%|#########8| 494/500 [10:59<00:07,  1.21s/it]\n",
      " 99%|#########9| 495/500 [11:01<00:07,  1.40s/it]\n",
      " 99%|#########9| 496/500 [11:03<00:06,  1.54s/it]\n",
      " 99%|#########9| 497/500 [11:04<00:04,  1.37s/it]\n",
      "100%|#########9| 498/500 [11:04<00:02,  1.16s/it]\n",
      "100%|#########9| 499/500 [11:06<00:01,  1.47s/it]\n",
      "100%|##########| 500/500 [11:08<00:00,  1.39s/it]\n",
      "100%|##########| 500/500 [11:08<00:00,  1.34s/it]\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:13<1:50:21, 13.27s/it]\n",
      "  0%|          | 2/500 [00:13<46:24,  5.59s/it]  \n",
      "  1%|          | 4/500 [00:13<17:31,  2.12s/it]\n",
      "  1%|1         | 6/500 [00:13<09:25,  1.14s/it]\n",
      "  2%|1         | 9/500 [00:21<15:58,  1.95s/it]\n",
      "  2%|2         | 10/500 [00:22<13:12,  1.62s/it]\n",
      "  2%|2         | 11/500 [00:22<10:31,  1.29s/it]\n",
      "  2%|2         | 12/500 [00:22<08:13,  1.01s/it]\n",
      "  3%|2         | 13/500 [00:22<06:21,  1.28it/s]\n",
      "  3%|2         | 14/500 [00:22<04:54,  1.65it/s]\n",
      "  3%|3         | 16/500 [00:22<02:58,  2.72it/s]\n",
      "  4%|3         | 18/500 [00:30<13:58,  1.74s/it]\n",
      "  4%|3         | 19/500 [00:30<11:08,  1.39s/it]\n",
      "  4%|4         | 21/500 [00:30<07:13,  1.11it/s]\n",
      "  4%|4         | 22/500 [00:31<05:51,  1.36it/s]\n",
      "  5%|4         | 24/500 [00:31<03:47,  2.09it/s]\n",
      "  5%|5         | 26/500 [00:39<13:36,  1.72s/it]\n",
      "  6%|5         | 28/500 [00:39<09:18,  1.18s/it]\n",
      "  6%|5         | 29/500 [00:39<07:39,  1.02it/s]\n",
      "  6%|6         | 30/500 [00:39<06:10,  1.27it/s]\n",
      "  6%|6         | 32/500 [00:39<03:58,  1.96it/s]\n",
      "  7%|6         | 34/500 [00:47<13:26,  1.73s/it]\n",
      "  7%|7         | 36/500 [00:47<09:10,  1.19s/it]\n",
      "  7%|7         | 37/500 [00:48<07:32,  1.02it/s]\n",
      "  8%|7         | 38/500 [00:48<06:05,  1.26it/s]\n",
      "  8%|8         | 40/500 [00:48<03:54,  1.96it/s]\n",
      "  8%|8         | 42/500 [00:56<13:20,  1.75s/it]\n",
      "  9%|8         | 43/500 [00:56<10:48,  1.42s/it]\n",
      "  9%|9         | 45/500 [00:56<07:07,  1.06it/s]\n",
      "  9%|9         | 46/500 [00:56<05:48,  1.30it/s]\n",
      " 10%|9         | 48/500 [00:56<03:47,  1.99it/s]\n",
      " 10%|#         | 50/500 [01:04<12:55,  1.72s/it]\n",
      " 10%|#         | 51/500 [01:05<10:30,  1.40s/it]\n",
      " 11%|#         | 53/500 [01:05<06:48,  1.09it/s]\n",
      " 11%|#1        | 55/500 [01:05<04:45,  1.56it/s]\n",
      " 11%|#1        | 57/500 [01:12<12:08,  1.64s/it]\n",
      " 12%|#1        | 58/500 [01:13<10:52,  1.48s/it]\n",
      " 12%|#2        | 60/500 [01:13<07:09,  1.02it/s]\n",
      " 12%|#2        | 61/500 [01:13<05:52,  1.25it/s]\n",
      " 12%|#2        | 62/500 [01:13<04:43,  1.54it/s]\n",
      " 13%|#2        | 63/500 [01:13<03:46,  1.93it/s]\n",
      " 13%|#2        | 64/500 [01:14<03:00,  2.42it/s]\n",
      " 13%|#3        | 65/500 [01:20<15:18,  2.11s/it]\n",
      " 13%|#3        | 66/500 [01:21<12:59,  1.80s/it]\n",
      " 14%|#3        | 68/500 [01:21<07:33,  1.05s/it]\n",
      " 14%|#3        | 69/500 [01:22<05:55,  1.21it/s]\n",
      " 14%|#4        | 72/500 [01:22<03:09,  2.25it/s]\n",
      " 15%|#4        | 73/500 [01:28<11:43,  1.65s/it]\n",
      " 15%|#4        | 74/500 [01:30<11:00,  1.55s/it]\n",
      " 15%|#5        | 75/500 [01:30<08:34,  1.21s/it]\n",
      " 15%|#5        | 76/500 [01:30<06:46,  1.04it/s]\n",
      " 16%|#5        | 78/500 [01:30<04:04,  1.73it/s]\n",
      " 16%|#5        | 79/500 [01:30<03:18,  2.13it/s]\n",
      " 16%|#6        | 80/500 [01:30<02:40,  2.62it/s]\n",
      " 16%|#6        | 81/500 [01:36<12:40,  1.82s/it]\n",
      " 16%|#6        | 82/500 [01:38<12:54,  1.85s/it]\n",
      " 17%|#6        | 84/500 [01:38<07:43,  1.11s/it]\n",
      " 17%|#7        | 87/500 [01:39<04:04,  1.69it/s]\n",
      " 18%|#7        | 89/500 [01:44<08:53,  1.30s/it]\n",
      " 18%|#8        | 90/500 [01:47<10:05,  1.48s/it]\n",
      " 18%|#8        | 91/500 [01:47<08:07,  1.19s/it]\n",
      " 18%|#8        | 92/500 [01:47<06:35,  1.03it/s]\n",
      " 19%|#8        | 93/500 [01:47<05:08,  1.32it/s]\n",
      " 19%|#9        | 96/500 [01:47<02:36,  2.58it/s]\n",
      " 20%|#9        | 98/500 [01:55<10:21,  1.55s/it]\n",
      " 20%|#9        | 99/500 [01:55<08:30,  1.27s/it]\n",
      " 20%|##        | 101/500 [01:55<05:44,  1.16it/s]\n",
      " 20%|##        | 102/500 [01:55<04:43,  1.41it/s]\n",
      " 21%|##        | 103/500 [01:55<03:49,  1.73it/s]\n",
      " 21%|##1       | 105/500 [02:00<08:45,  1.33s/it]\n",
      " 21%|##1       | 106/500 [02:03<10:33,  1.61s/it]\n",
      " 21%|##1       | 107/500 [02:03<08:42,  1.33s/it]\n",
      " 22%|##1       | 109/500 [02:04<05:18,  1.23it/s]\n",
      " 22%|##2       | 110/500 [02:04<04:25,  1.47it/s]\n",
      " 22%|##2       | 112/500 [02:04<02:49,  2.29it/s]\n",
      " 23%|##2       | 114/500 [02:11<10:18,  1.60s/it]\n",
      " 23%|##3       | 115/500 [02:12<08:45,  1.37s/it]\n",
      " 23%|##3       | 117/500 [02:12<05:45,  1.11it/s]\n",
      " 24%|##3       | 118/500 [02:12<04:41,  1.35it/s]\n",
      " 24%|##4       | 120/500 [02:12<03:11,  1.99it/s]\n",
      " 24%|##4       | 121/500 [02:17<08:17,  1.31s/it]\n",
      " 24%|##4       | 122/500 [02:19<10:28,  1.66s/it]\n",
      " 25%|##4       | 123/500 [02:20<08:41,  1.38s/it]\n",
      " 25%|##4       | 124/500 [02:20<06:45,  1.08s/it]\n",
      " 25%|##5       | 126/500 [02:20<03:59,  1.56it/s]\n",
      " 25%|##5       | 127/500 [02:20<03:12,  1.94it/s]\n",
      " 26%|##5       | 129/500 [02:25<07:10,  1.16s/it]\n",
      " 26%|##6       | 130/500 [02:28<09:34,  1.55s/it]\n",
      " 26%|##6       | 131/500 [02:28<08:12,  1.34s/it]\n",
      " 27%|##6       | 133/500 [02:29<05:22,  1.14it/s]\n",
      " 27%|##7       | 135/500 [02:29<03:30,  1.74it/s]\n",
      " 27%|##7       | 137/500 [02:33<06:28,  1.07s/it]\n",
      " 28%|##7       | 138/500 [02:36<08:24,  1.39s/it]\n",
      " 28%|##7       | 139/500 [02:37<08:16,  1.37s/it]\n",
      " 28%|##8       | 141/500 [02:37<05:10,  1.16it/s]\n",
      " 28%|##8       | 142/500 [02:37<04:11,  1.43it/s]\n",
      " 29%|##8       | 143/500 [02:37<03:29,  1.70it/s]\n",
      " 29%|##8       | 144/500 [02:37<02:45,  2.14it/s]\n",
      " 29%|##9       | 145/500 [02:41<07:40,  1.30s/it]\n",
      " 29%|##9       | 146/500 [02:43<09:13,  1.56s/it]\n",
      " 29%|##9       | 147/500 [02:45<09:40,  1.64s/it]\n",
      " 30%|##9       | 148/500 [02:45<07:14,  1.23s/it]\n",
      " 30%|###       | 150/500 [02:45<04:06,  1.42it/s]\n",
      " 30%|###       | 151/500 [02:46<03:15,  1.79it/s]\n",
      " 31%|###       | 153/500 [02:49<06:25,  1.11s/it]\n",
      " 31%|###       | 154/500 [02:51<07:38,  1.33s/it]\n",
      " 31%|###1      | 155/500 [02:53<08:30,  1.48s/it]\n",
      " 31%|###1      | 156/500 [02:53<06:27,  1.13s/it]\n",
      " 31%|###1      | 157/500 [02:54<05:01,  1.14it/s]\n",
      " 32%|###1      | 158/500 [02:54<03:47,  1.50it/s]\n",
      " 32%|###1      | 159/500 [02:54<02:52,  1.97it/s]\n",
      " 32%|###2      | 160/500 [02:54<02:13,  2.55it/s]\n",
      " 32%|###2      | 161/500 [02:57<06:58,  1.23s/it]\n",
      " 32%|###2      | 162/500 [02:59<08:19,  1.48s/it]\n",
      " 33%|###2      | 163/500 [03:02<10:00,  1.78s/it]\n",
      " 33%|###3      | 165/500 [03:02<05:31,  1.01it/s]\n",
      " 34%|###3      | 168/500 [03:02<03:03,  1.81it/s]\n",
      " 34%|###3      | 169/500 [03:05<05:47,  1.05s/it]\n",
      " 34%|###4      | 170/500 [03:07<06:50,  1.24s/it]\n",
      " 34%|###4      | 171/500 [03:10<08:42,  1.59s/it]\n",
      " 34%|###4      | 172/500 [03:10<06:39,  1.22s/it]\n",
      " 35%|###4      | 173/500 [03:10<05:11,  1.05it/s]\n",
      " 35%|###4      | 174/500 [03:11<03:55,  1.39it/s]\n",
      " 35%|###5      | 176/500 [03:11<02:18,  2.33it/s]\n",
      " 35%|###5      | 177/500 [03:14<05:32,  1.03s/it]\n",
      " 36%|###5      | 178/500 [03:16<06:47,  1.27s/it]\n",
      " 36%|###5      | 179/500 [03:18<08:59,  1.68s/it]\n",
      " 36%|###6      | 181/500 [03:19<05:19,  1.00s/it]\n",
      " 36%|###6      | 182/500 [03:19<04:11,  1.27it/s]\n",
      " 37%|###6      | 183/500 [03:19<03:24,  1.55it/s]\n",
      " 37%|###6      | 184/500 [03:19<02:39,  1.99it/s]\n",
      " 37%|###7      | 185/500 [03:22<05:31,  1.05s/it]\n",
      " 37%|###7      | 186/500 [03:24<07:10,  1.37s/it]\n",
      " 37%|###7      | 187/500 [03:27<09:38,  1.85s/it]\n",
      " 38%|###7      | 189/500 [03:27<05:23,  1.04s/it]\n",
      " 38%|###8      | 190/500 [03:27<04:12,  1.23it/s]\n",
      " 38%|###8      | 191/500 [03:27<03:32,  1.45it/s]\n",
      " 38%|###8      | 192/500 [03:28<02:52,  1.78it/s]\n",
      " 39%|###8      | 193/500 [03:30<05:18,  1.04s/it]\n",
      " 39%|###8      | 194/500 [03:32<06:37,  1.30s/it]\n",
      " 39%|###9      | 195/500 [03:35<09:10,  1.80s/it]\n",
      " 39%|###9      | 196/500 [03:35<06:57,  1.37s/it]\n",
      " 39%|###9      | 197/500 [03:35<05:03,  1.00s/it]\n",
      " 40%|###9      | 199/500 [03:35<02:58,  1.69it/s]\n",
      " 40%|####      | 200/500 [03:36<02:29,  2.00it/s]\n",
      " 40%|####      | 201/500 [03:38<04:47,  1.04it/s]\n",
      " 40%|####      | 202/500 [03:40<05:58,  1.20s/it]\n",
      " 41%|####      | 203/500 [03:43<09:05,  1.84s/it]\n",
      " 41%|####      | 204/500 [03:43<06:37,  1.34s/it]\n",
      " 41%|####1     | 205/500 [03:43<04:51,  1.01it/s]\n",
      " 41%|####1     | 206/500 [03:44<03:34,  1.37it/s]\n",
      " 41%|####1     | 207/500 [03:44<02:49,  1.73it/s]\n",
      " 42%|####1     | 209/500 [03:46<03:55,  1.23it/s]\n",
      " 42%|####2     | 210/500 [03:48<05:17,  1.09s/it]\n",
      " 42%|####2     | 211/500 [03:52<08:24,  1.74s/it]\n",
      " 42%|####2     | 212/500 [03:52<06:31,  1.36s/it]\n",
      " 43%|####2     | 214/500 [03:52<03:46,  1.27it/s]\n",
      " 43%|####3     | 216/500 [03:52<02:30,  1.89it/s]\n",
      " 43%|####3     | 217/500 [03:54<03:49,  1.23it/s]\n",
      " 44%|####3     | 218/500 [03:56<05:27,  1.16s/it]\n",
      " 44%|####3     | 219/500 [04:00<08:25,  1.80s/it]\n",
      " 44%|####4     | 221/500 [04:00<04:58,  1.07s/it]\n",
      " 44%|####4     | 222/500 [04:00<04:09,  1.11it/s]\n",
      " 45%|####4     | 224/500 [04:01<02:41,  1.71it/s]\n",
      " 45%|####5     | 225/500 [04:02<03:37,  1.26it/s]\n",
      " 45%|####5     | 226/500 [04:05<05:15,  1.15s/it]\n",
      " 45%|####5     | 227/500 [04:08<08:19,  1.83s/it]\n",
      " 46%|####5     | 228/500 [04:09<06:28,  1.43s/it]\n",
      " 46%|####6     | 230/500 [04:09<03:44,  1.20it/s]\n",
      " 46%|####6     | 231/500 [04:09<02:57,  1.51it/s]\n",
      " 46%|####6     | 232/500 [04:09<02:19,  1.92it/s]\n",
      " 47%|####6     | 233/500 [04:10<03:06,  1.43it/s]\n",
      " 47%|####6     | 234/500 [04:13<05:26,  1.23s/it]\n",
      " 47%|####6     | 235/500 [04:17<08:37,  1.95s/it]\n",
      " 47%|####7     | 236/500 [04:17<06:15,  1.42s/it]\n",
      " 47%|####7     | 237/500 [04:17<04:50,  1.10s/it]\n",
      " 48%|####7     | 238/500 [04:17<03:32,  1.23it/s]\n",
      " 48%|####7     | 239/500 [04:17<02:37,  1.65it/s]\n",
      " 48%|####8     | 240/500 [04:17<01:58,  2.19it/s]\n",
      " 48%|####8     | 241/500 [04:18<02:21,  1.83it/s]\n",
      " 48%|####8     | 242/500 [04:21<05:16,  1.23s/it]\n",
      " 49%|####8     | 243/500 [04:25<08:40,  2.03s/it]\n",
      " 49%|####8     | 244/500 [04:25<06:20,  1.48s/it]\n",
      " 49%|####9     | 245/500 [04:25<04:41,  1.10s/it]\n",
      " 49%|####9     | 247/500 [04:26<02:49,  1.49it/s]\n",
      " 50%|####9     | 248/500 [04:26<02:13,  1.88it/s]\n",
      " 50%|####9     | 249/500 [04:26<02:07,  1.97it/s]\n",
      " 50%|#####     | 250/500 [04:29<04:44,  1.14s/it]\n",
      " 50%|#####     | 251/500 [04:33<08:11,  1.97s/it]\n",
      " 51%|#####     | 253/500 [04:34<04:54,  1.19s/it]\n",
      " 51%|#####1    | 255/500 [04:34<03:14,  1.26it/s]\n",
      " 51%|#####1    | 256/500 [04:34<02:42,  1.50it/s]\n",
      " 51%|#####1    | 257/500 [04:34<02:16,  1.78it/s]\n",
      " 52%|#####1    | 258/500 [04:37<04:34,  1.13s/it]\n",
      " 52%|#####1    | 259/500 [04:42<08:05,  2.01s/it]\n",
      " 52%|#####2    | 261/500 [04:42<04:50,  1.22s/it]\n",
      " 53%|#####2    | 263/500 [04:42<03:08,  1.26it/s]\n",
      " 53%|#####2    | 264/500 [04:42<02:43,  1.44it/s]\n",
      " 53%|#####3    | 265/500 [04:43<02:16,  1.72it/s]\n",
      " 53%|#####3    | 266/500 [04:45<04:21,  1.12s/it]\n",
      " 53%|#####3    | 267/500 [04:50<07:38,  1.97s/it]\n",
      " 54%|#####3    | 269/500 [04:50<04:35,  1.19s/it]\n",
      " 54%|#####4    | 270/500 [04:50<03:35,  1.07it/s]\n",
      " 54%|#####4    | 271/500 [04:50<02:53,  1.32it/s]\n",
      " 54%|#####4    | 272/500 [04:51<02:26,  1.56it/s]\n",
      " 55%|#####4    | 273/500 [04:51<01:52,  2.01it/s]\n",
      " 55%|#####4    | 274/500 [04:53<03:52,  1.03s/it]\n",
      " 55%|#####5    | 275/500 [04:58<07:38,  2.04s/it]\n",
      " 55%|#####5    | 276/500 [04:58<05:45,  1.54s/it]\n",
      " 55%|#####5    | 277/500 [04:58<04:17,  1.15s/it]\n",
      " 56%|#####5    | 278/500 [04:58<03:14,  1.14it/s]\n",
      " 56%|#####5    | 279/500 [04:59<02:23,  1.54it/s]\n",
      " 56%|#####6    | 280/500 [04:59<02:08,  1.71it/s]\n",
      " 56%|#####6    | 282/500 [05:01<02:58,  1.22it/s]\n",
      " 57%|#####6    | 283/500 [05:06<06:23,  1.77s/it]\n",
      " 57%|#####6    | 284/500 [05:06<04:54,  1.36s/it]\n",
      " 57%|#####6    | 285/500 [05:06<03:46,  1.05s/it]\n",
      " 57%|#####7    | 286/500 [05:07<03:01,  1.18it/s]\n",
      " 57%|#####7    | 287/500 [05:07<02:15,  1.57it/s]\n",
      " 58%|#####7    | 288/500 [05:07<01:49,  1.94it/s]\n",
      " 58%|#####7    | 289/500 [05:07<01:30,  2.34it/s]\n",
      " 58%|#####8    | 290/500 [05:09<03:11,  1.10it/s]\n",
      " 58%|#####8    | 291/500 [05:14<07:10,  2.06s/it]\n",
      " 58%|#####8    | 292/500 [05:14<05:20,  1.54s/it]\n",
      " 59%|#####8    | 293/500 [05:15<03:57,  1.15s/it]\n",
      " 59%|#####8    | 294/500 [05:15<02:59,  1.15it/s]\n",
      " 59%|#####8    | 295/500 [05:15<02:11,  1.56it/s]\n",
      " 59%|#####9    | 296/500 [05:15<01:51,  1.83it/s]\n",
      " 59%|#####9    | 297/500 [05:15<01:24,  2.40it/s]\n",
      " 60%|#####9    | 298/500 [05:17<02:56,  1.14it/s]\n",
      " 60%|#####9    | 299/500 [05:22<06:44,  2.01s/it]\n",
      " 60%|######    | 300/500 [05:23<05:27,  1.64s/it]\n",
      " 60%|######    | 301/500 [05:23<03:54,  1.18s/it]\n",
      " 60%|######    | 302/500 [05:23<02:49,  1.17it/s]\n",
      " 61%|######    | 303/500 [05:23<02:11,  1.50it/s]\n",
      " 61%|######    | 304/500 [05:23<01:37,  2.01it/s]\n",
      " 61%|######1   | 305/500 [05:23<01:14,  2.62it/s]\n",
      " 61%|######1   | 306/500 [05:25<02:39,  1.22it/s]\n",
      " 61%|######1   | 307/500 [05:30<06:21,  1.98s/it]\n",
      " 62%|######1   | 308/500 [05:31<05:40,  1.78s/it]\n",
      " 62%|######1   | 309/500 [05:31<04:03,  1.28s/it]\n",
      " 62%|######2   | 310/500 [05:31<02:55,  1.08it/s]\n",
      " 62%|######2   | 311/500 [05:32<02:08,  1.47it/s]\n",
      " 62%|######2   | 312/500 [05:32<01:41,  1.85it/s]\n",
      " 63%|######2   | 313/500 [05:32<01:16,  2.43it/s]\n",
      " 63%|######2   | 314/500 [05:33<02:11,  1.41it/s]\n",
      " 63%|######3   | 315/500 [05:37<05:27,  1.77s/it]\n",
      " 63%|######3   | 316/500 [05:39<05:23,  1.76s/it]\n",
      " 63%|######3   | 317/500 [05:40<04:02,  1.33s/it]\n",
      " 64%|######3   | 318/500 [05:40<02:54,  1.04it/s]\n",
      " 64%|######4   | 320/500 [05:40<01:46,  1.69it/s]\n",
      " 64%|######4   | 322/500 [05:41<01:45,  1.69it/s]\n",
      " 65%|######4   | 323/500 [05:45<04:07,  1.40s/it]\n",
      " 65%|######4   | 324/500 [05:47<04:29,  1.53s/it]\n",
      " 65%|######5   | 325/500 [05:48<03:44,  1.28s/it]\n",
      " 65%|######5   | 326/500 [05:48<02:48,  1.03it/s]\n",
      " 65%|######5   | 327/500 [05:48<02:06,  1.37it/s]\n",
      " 66%|######5   | 329/500 [05:48<01:13,  2.31it/s]\n",
      " 66%|######6   | 330/500 [05:49<01:35,  1.78it/s]\n",
      " 66%|######6   | 331/500 [05:53<04:08,  1.47s/it]\n",
      " 66%|######6   | 332/500 [05:55<04:33,  1.63s/it]\n",
      " 67%|######6   | 333/500 [05:56<03:52,  1.39s/it]\n",
      " 67%|######6   | 334/500 [05:56<02:50,  1.03s/it]\n",
      " 67%|######7   | 336/500 [05:56<01:41,  1.62it/s]\n",
      " 67%|######7   | 337/500 [05:57<01:20,  2.03it/s]\n",
      " 68%|######7   | 338/500 [05:57<01:17,  2.09it/s]\n",
      " 68%|######7   | 339/500 [06:01<04:04,  1.52s/it]\n",
      " 68%|######8   | 340/500 [06:03<04:21,  1.64s/it]\n",
      " 68%|######8   | 341/500 [06:04<03:55,  1.48s/it]\n",
      " 68%|######8   | 342/500 [06:05<02:51,  1.09s/it]\n",
      " 69%|######8   | 343/500 [06:05<02:10,  1.20it/s]\n",
      " 69%|######9   | 346/500 [06:05<01:09,  2.22it/s]\n",
      " 69%|######9   | 347/500 [06:09<03:06,  1.22s/it]\n",
      " 70%|######9   | 348/500 [06:11<03:27,  1.37s/it]\n",
      " 70%|######9   | 349/500 [06:13<03:32,  1.41s/it]\n",
      " 70%|#######   | 351/500 [06:13<02:09,  1.15it/s]\n",
      " 70%|#######   | 352/500 [06:13<01:42,  1.44it/s]\n",
      " 71%|#######   | 353/500 [06:13<01:20,  1.82it/s]\n",
      " 71%|#######   | 354/500 [06:13<01:07,  2.16it/s]\n",
      " 71%|#######1  | 355/500 [06:17<03:22,  1.40s/it]\n",
      " 71%|#######1  | 356/500 [06:19<03:38,  1.52s/it]\n",
      " 71%|#######1  | 357/500 [06:21<03:55,  1.64s/it]\n",
      " 72%|#######1  | 359/500 [06:21<02:11,  1.07it/s]\n",
      " 72%|#######2  | 360/500 [06:21<01:46,  1.32it/s]\n",
      " 72%|#######2  | 361/500 [06:22<01:25,  1.62it/s]\n",
      " 72%|#######2  | 362/500 [06:22<01:06,  2.08it/s]\n",
      " 73%|#######2  | 363/500 [06:25<03:00,  1.31s/it]\n",
      " 73%|#######2  | 364/500 [06:27<03:19,  1.47s/it]\n",
      " 73%|#######3  | 365/500 [06:29<03:53,  1.73s/it]\n",
      " 73%|#######3  | 366/500 [06:30<02:52,  1.29s/it]\n",
      " 74%|#######3  | 369/500 [06:30<01:23,  1.58it/s]\n",
      " 74%|#######4  | 370/500 [06:30<01:08,  1.91it/s]\n",
      " 74%|#######4  | 371/500 [06:34<02:35,  1.21s/it]\n",
      " 74%|#######4  | 372/500 [06:35<02:47,  1.31s/it]\n",
      " 75%|#######4  | 373/500 [06:38<03:25,  1.62s/it]\n",
      " 75%|#######4  | 374/500 [06:38<02:36,  1.24s/it]\n",
      " 75%|#######5  | 375/500 [06:38<01:55,  1.08it/s]\n",
      " 75%|#######5  | 377/500 [06:38<01:09,  1.78it/s]\n",
      " 76%|#######5  | 378/500 [06:38<00:55,  2.21it/s]\n",
      " 76%|#######5  | 379/500 [06:42<02:22,  1.17s/it]\n",
      " 76%|#######6  | 380/500 [06:43<02:35,  1.29s/it]\n",
      " 76%|#######6  | 381/500 [06:46<03:13,  1.63s/it]\n",
      " 76%|#######6  | 382/500 [06:46<02:32,  1.29s/it]\n",
      " 77%|#######7  | 385/500 [06:46<01:11,  1.61it/s]\n",
      " 77%|#######7  | 386/500 [06:46<00:58,  1.94it/s]\n",
      " 77%|#######7  | 387/500 [06:50<02:06,  1.12s/it]\n",
      " 78%|#######7  | 388/500 [06:51<02:16,  1.22s/it]\n",
      " 78%|#######7  | 389/500 [06:54<03:07,  1.69s/it]\n",
      " 78%|#######8  | 390/500 [06:54<02:22,  1.29s/it]\n",
      " 78%|#######8  | 391/500 [06:55<01:44,  1.04it/s]\n",
      " 79%|#######8  | 393/500 [06:55<01:02,  1.71it/s]\n",
      " 79%|#######8  | 394/500 [06:55<00:49,  2.13it/s]\n",
      " 79%|#######9  | 395/500 [06:58<01:49,  1.04s/it]\n",
      " 79%|#######9  | 396/500 [06:59<02:08,  1.23s/it]\n",
      " 79%|#######9  | 397/500 [07:02<03:01,  1.76s/it]\n",
      " 80%|#######9  | 398/500 [07:03<02:21,  1.39s/it]\n",
      " 80%|########  | 400/500 [07:03<01:19,  1.26it/s]\n",
      " 80%|########  | 401/500 [07:03<01:01,  1.60it/s]\n",
      " 80%|########  | 402/500 [07:03<00:48,  2.03it/s]\n",
      " 81%|########  | 403/500 [07:06<01:37,  1.00s/it]\n",
      " 81%|########  | 404/500 [07:07<01:55,  1.21s/it]\n",
      " 81%|########1 | 405/500 [07:11<02:58,  1.88s/it]\n",
      " 81%|########1 | 406/500 [07:11<02:11,  1.40s/it]\n",
      " 81%|########1 | 407/500 [07:11<01:38,  1.06s/it]\n",
      " 82%|########1 | 408/500 [07:12<01:14,  1.24it/s]\n",
      " 82%|########2 | 411/500 [07:14<01:06,  1.35it/s]\n",
      " 82%|########2 | 412/500 [07:16<01:25,  1.02it/s]\n",
      " 83%|########2 | 413/500 [07:19<02:17,  1.58s/it]\n",
      " 83%|########2 | 414/500 [07:19<01:48,  1.27s/it]\n",
      " 83%|########2 | 415/500 [07:20<01:24,  1.01it/s]\n",
      " 83%|########3 | 416/500 [07:20<01:05,  1.28it/s]\n",
      " 83%|########3 | 417/500 [07:20<00:49,  1.69it/s]\n",
      " 84%|########3 | 418/500 [07:20<00:37,  2.20it/s]\n",
      " 84%|########3 | 419/500 [07:22<01:04,  1.26it/s]\n",
      " 84%|########4 | 420/500 [07:24<01:31,  1.14s/it]\n",
      " 84%|########4 | 421/500 [07:27<02:26,  1.86s/it]\n",
      " 84%|########4 | 422/500 [07:28<01:54,  1.47s/it]\n",
      " 85%|########4 | 423/500 [07:28<01:21,  1.06s/it]\n",
      " 85%|########4 | 424/500 [07:28<00:59,  1.28it/s]\n",
      " 85%|########5 | 425/500 [07:28<00:45,  1.63it/s]\n",
      " 85%|########5 | 426/500 [07:28<00:34,  2.17it/s]\n",
      " 85%|########5 | 427/500 [07:29<00:47,  1.54it/s]\n",
      " 86%|########5 | 428/500 [07:32<01:21,  1.14s/it]\n",
      " 86%|########5 | 429/500 [07:36<02:17,  1.94s/it]\n",
      " 86%|########6 | 430/500 [07:36<01:46,  1.52s/it]\n",
      " 86%|########6 | 431/500 [07:36<01:15,  1.10s/it]\n",
      " 86%|########6 | 432/500 [07:36<00:54,  1.25it/s]\n",
      " 87%|########6 | 433/500 [07:36<00:39,  1.69it/s]\n",
      " 87%|########6 | 434/500 [07:37<00:29,  2.23it/s]\n",
      " 87%|########7 | 435/500 [07:37<00:39,  1.65it/s]\n",
      " 87%|########7 | 436/500 [07:40<01:13,  1.14s/it]\n",
      " 87%|########7 | 437/500 [07:44<02:02,  1.94s/it]\n",
      " 88%|########7 | 438/500 [07:44<01:38,  1.59s/it]\n",
      " 88%|########7 | 439/500 [07:45<01:09,  1.14s/it]\n",
      " 88%|########8 | 440/500 [07:45<00:49,  1.20it/s]\n",
      " 88%|########8 | 441/500 [07:45<00:36,  1.62it/s]\n",
      " 89%|########8 | 443/500 [07:45<00:27,  2.07it/s]\n",
      " 89%|########8 | 444/500 [07:48<00:56,  1.01s/it]\n",
      " 89%|########9 | 445/500 [07:52<01:32,  1.68s/it]\n",
      " 89%|########9 | 446/500 [07:53<01:22,  1.52s/it]\n",
      " 90%|########9 | 448/500 [07:53<00:49,  1.05it/s]\n",
      " 90%|######### | 450/500 [07:53<00:30,  1.65it/s]\n",
      " 90%|######### | 451/500 [07:53<00:25,  1.92it/s]\n",
      " 90%|######### | 452/500 [07:56<00:49,  1.04s/it]\n",
      " 91%|######### | 453/500 [07:59<01:13,  1.57s/it]\n",
      " 91%|######### | 454/500 [08:01<01:11,  1.56s/it]\n",
      " 91%|#########1| 455/500 [08:01<00:53,  1.19s/it]\n",
      " 91%|#########1| 456/500 [08:01<00:38,  1.13it/s]\n",
      " 91%|#########1| 457/500 [08:01<00:29,  1.44it/s]\n",
      " 92%|#########1| 458/500 [08:01<00:21,  1.91it/s]\n",
      " 92%|#########1| 459/500 [08:02<00:16,  2.49it/s]\n",
      " 92%|#########2| 460/500 [08:04<00:39,  1.01it/s]\n",
      " 92%|#########2| 461/500 [08:07<01:06,  1.69s/it]\n",
      " 92%|#########2| 462/500 [08:09<01:03,  1.68s/it]\n",
      " 93%|#########2| 463/500 [08:09<00:44,  1.21s/it]\n",
      " 93%|#########2| 464/500 [08:09<00:32,  1.10it/s]\n",
      " 93%|#########3| 465/500 [08:09<00:23,  1.49it/s]\n",
      " 93%|#########3| 466/500 [08:10<00:19,  1.76it/s]\n",
      " 94%|#########3| 468/500 [08:12<00:26,  1.20it/s]\n",
      " 94%|#########3| 469/500 [08:15<00:43,  1.41s/it]\n",
      " 94%|#########3| 470/500 [08:17<00:46,  1.55s/it]\n",
      " 94%|#########4| 471/500 [08:17<00:33,  1.16s/it]\n",
      " 94%|#########4| 472/500 [08:17<00:25,  1.12it/s]\n",
      " 95%|#########4| 473/500 [08:18<00:18,  1.43it/s]\n",
      " 95%|#########4| 474/500 [08:18<00:14,  1.78it/s]\n",
      " 95%|#########5| 476/500 [08:20<00:18,  1.29it/s]\n",
      " 95%|#########5| 477/500 [08:23<00:31,  1.38s/it]\n",
      " 96%|#########5| 478/500 [08:25<00:34,  1.58s/it]\n",
      " 96%|#########5| 479/500 [08:25<00:24,  1.19s/it]\n",
      " 96%|#########6| 480/500 [08:26<00:18,  1.09it/s]\n",
      " 96%|#########6| 481/500 [08:26<00:13,  1.46it/s]\n",
      " 96%|#########6| 482/500 [08:26<00:10,  1.72it/s]\n",
      " 97%|#########6| 484/500 [08:28<00:11,  1.36it/s]\n",
      " 97%|#########7| 485/500 [08:31<00:20,  1.35s/it]\n",
      " 97%|#########7| 486/500 [08:34<00:22,  1.62s/it]\n",
      " 97%|#########7| 487/500 [08:34<00:16,  1.24s/it]\n",
      " 98%|#########7| 488/500 [08:34<00:11,  1.04it/s]\n",
      " 98%|#########8| 490/500 [08:34<00:06,  1.65it/s]\n",
      " 98%|#########8| 492/500 [08:36<00:05,  1.51it/s]\n",
      " 99%|#########8| 493/500 [08:39<00:08,  1.25s/it]\n",
      " 99%|#########8| 494/500 [08:42<00:09,  1.55s/it]\n",
      " 99%|#########9| 495/500 [08:42<00:05,  1.18s/it]\n",
      " 99%|#########9| 496/500 [08:42<00:03,  1.11it/s]\n",
      " 99%|#########9| 497/500 [08:42<00:02,  1.47it/s]\n",
      "100%|#########9| 498/500 [08:42<00:01,  1.82it/s]\n",
      "100%|##########| 500/500 [08:43<00:00,  2.00it/s]\n",
      "100%|##########| 500/500 [08:43<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset integrity\n",
    "!nnUNetv2_plan_and_preprocess -d 500 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2aaa22-893e-4763-801a-143fbc257cec",
   "metadata": {},
   "source": [
    "### Train 3d_lowres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a6aeec-5b7c-4763-8f39-30a0f07a4641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-12-07 12:02:42.624446: do_dummy_2d_data_aug: False\n",
      "2025-12-07 12:02:42.743926: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-07 12:02:42.745929: The split file contains 5 splits.\n",
      "2025-12-07 12:02:42.745929: Desired fold for training: 0\n",
      "2025-12-07 12:02:42.746932: This split has 400 training and 100 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2025-12-07 12:03:16.604067: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_lowres\n",
      " {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [202, 202, 202], 'spacing': [1.2667700813876164, 1.2667700813876164, 1.2667700813876164], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset500_MRI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [256, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.422696590423584, 'median': 0.4194243550300598, 'min': 0.0027002037968486547, 'percentile_00_5': 0.05628390982747078, 'percentile_99_5': 0.8565635681152344, 'std': 0.19347868859767914}}} \n",
      "\n",
      "2025-12-07 12:03:16.607102: unpacking dataset...\n",
      "2025-12-07 12:03:17.183792: unpacking done...\n",
      "2025-12-07 12:03:17.186795: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-12-07 12:03:17.238692: \n",
      "2025-12-07 12:03:17.238692: Epoch 0\n",
      "2025-12-07 12:03:17.238692: Current learning rate: 0.01\n",
      "2025-12-07 12:05:53.139965: train_loss 0.2196\n",
      "2025-12-07 12:05:53.142323: val_loss 0.0403\n",
      "2025-12-07 12:05:53.142323: Pseudo dice [0.4587, 0.5406, 0.1938]\n",
      "2025-12-07 12:05:53.144326: Epoch time: 155.9 s\n",
      "2025-12-07 12:05:53.144326: Yayy! New best EMA pseudo Dice: 0.3977\n",
      "2025-12-07 12:05:54.202162: \n",
      "2025-12-07 12:05:54.202162: Epoch 1\n",
      "2025-12-07 12:05:54.203249: Current learning rate: 0.00999\n",
      "2025-12-07 12:08:12.715917: train_loss -0.0151\n",
      "2025-12-07 12:08:12.715917: val_loss -0.0894\n",
      "2025-12-07 12:08:12.720767: Pseudo dice [0.5204, 0.5247, 0.4031]\n",
      "2025-12-07 12:08:12.720767: Epoch time: 138.51 s\n",
      "2025-12-07 12:08:12.722769: Yayy! New best EMA pseudo Dice: 0.4062\n",
      "2025-12-07 12:08:13.717835: \n",
      "2025-12-07 12:08:13.717835: Epoch 2\n",
      "2025-12-07 12:08:13.717835: Current learning rate: 0.00998\n",
      "2025-12-07 12:10:32.201926: train_loss -0.1272\n",
      "2025-12-07 12:10:32.201926: val_loss -0.2044\n",
      "2025-12-07 12:10:32.201926: Pseudo dice [0.5887, 0.6235, 0.4818]\n",
      "2025-12-07 12:10:32.201926: Epoch time: 138.48 s\n",
      "2025-12-07 12:10:32.201926: Yayy! New best EMA pseudo Dice: 0.422\n",
      "2025-12-07 12:10:33.311255: \n",
      "2025-12-07 12:10:33.311255: Epoch 3\n",
      "2025-12-07 12:10:33.311255: Current learning rate: 0.00997\n",
      "2025-12-07 12:12:51.732815: train_loss -0.1829\n",
      "2025-12-07 12:12:51.732815: val_loss -0.2453\n",
      "2025-12-07 12:12:51.734817: Pseudo dice [0.614, 0.6514, 0.5158]\n",
      "2025-12-07 12:12:51.734817: Epoch time: 138.42 s\n",
      "2025-12-07 12:12:51.736820: Yayy! New best EMA pseudo Dice: 0.4392\n",
      "2025-12-07 12:12:52.677979: \n",
      "2025-12-07 12:12:52.677979: Epoch 4\n",
      "2025-12-07 12:12:52.679981: Current learning rate: 0.00996\n",
      "2025-12-07 12:15:11.186821: train_loss -0.2242\n",
      "2025-12-07 12:15:11.186821: val_loss -0.2876\n",
      "2025-12-07 12:15:11.186821: Pseudo dice [0.6251, 0.6688, 0.5547]\n",
      "2025-12-07 12:15:11.186821: Epoch time: 138.51 s\n",
      "2025-12-07 12:15:11.186821: Yayy! New best EMA pseudo Dice: 0.4569\n",
      "2025-12-07 12:15:12.097795: \n",
      "2025-12-07 12:15:12.109192: Epoch 5\n",
      "2025-12-07 12:15:12.110253: Current learning rate: 0.00995\n",
      "2025-12-07 12:17:30.693334: train_loss -0.2805\n",
      "2025-12-07 12:17:30.694335: val_loss -0.3475\n",
      "2025-12-07 12:17:30.696335: Pseudo dice [0.6791, 0.7091, 0.6116]\n",
      "2025-12-07 12:17:30.697336: Epoch time: 138.6 s\n",
      "2025-12-07 12:17:30.698634: Yayy! New best EMA pseudo Dice: 0.4779\n",
      "2025-12-07 12:17:31.592087: \n",
      "2025-12-07 12:17:31.592087: Epoch 6\n",
      "2025-12-07 12:17:31.592087: Current learning rate: 0.00995\n",
      "2025-12-07 12:19:49.941831: train_loss -0.3535\n",
      "2025-12-07 12:19:49.941831: val_loss -0.4248\n",
      "2025-12-07 12:19:49.945835: Pseudo dice [0.7176, 0.7663, 0.6502]\n",
      "2025-12-07 12:19:49.945835: Epoch time: 138.35 s\n",
      "2025-12-07 12:19:49.947837: Yayy! New best EMA pseudo Dice: 0.5012\n",
      "2025-12-07 12:19:51.041833: \n",
      "2025-12-07 12:19:51.041833: Epoch 7\n",
      "2025-12-07 12:19:51.041833: Current learning rate: 0.00994\n",
      "2025-12-07 12:22:09.351999: train_loss -0.4049\n",
      "2025-12-07 12:22:09.351999: val_loss -0.461\n",
      "2025-12-07 12:22:09.354000: Pseudo dice [0.7487, 0.7912, 0.6669]\n",
      "2025-12-07 12:22:09.356002: Epoch time: 138.31 s\n",
      "2025-12-07 12:22:09.356002: Yayy! New best EMA pseudo Dice: 0.5247\n",
      "2025-12-07 12:22:10.270842: \n",
      "2025-12-07 12:22:10.270842: Epoch 8\n",
      "2025-12-07 12:22:10.271836: Current learning rate: 0.00993\n",
      "2025-12-07 12:24:28.592975: train_loss -0.4353\n",
      "2025-12-07 12:24:28.592975: val_loss -0.4571\n",
      "2025-12-07 12:24:28.594978: Pseudo dice [0.7581, 0.8073, 0.6306]\n",
      "2025-12-07 12:24:28.594978: Epoch time: 138.32 s\n",
      "2025-12-07 12:24:28.596981: Yayy! New best EMA pseudo Dice: 0.5454\n",
      "2025-12-07 12:24:29.511336: \n",
      "2025-12-07 12:24:29.511336: Epoch 9\n",
      "2025-12-07 12:24:29.511336: Current learning rate: 0.00992\n",
      "2025-12-07 12:26:47.756474: train_loss -0.4425\n",
      "2025-12-07 12:26:47.756474: val_loss -0.475\n",
      "2025-12-07 12:26:47.758476: Pseudo dice [0.7401, 0.8154, 0.6627]\n",
      "2025-12-07 12:26:47.758476: Epoch time: 138.25 s\n",
      "2025-12-07 12:26:47.760478: Yayy! New best EMA pseudo Dice: 0.5648\n",
      "2025-12-07 12:26:48.769609: \n",
      "2025-12-07 12:26:48.769609: Epoch 10\n",
      "2025-12-07 12:26:48.769609: Current learning rate: 0.00991\n",
      "2025-12-07 12:29:06.840700: train_loss -0.4845\n",
      "2025-12-07 12:29:06.842703: val_loss -0.5245\n",
      "2025-12-07 12:29:06.843822: Pseudo dice [0.7929, 0.8335, 0.6995]\n",
      "2025-12-07 12:29:06.844824: Epoch time: 138.07 s\n",
      "2025-12-07 12:29:06.844824: Yayy! New best EMA pseudo Dice: 0.5859\n",
      "2025-12-07 12:29:07.730428: \n",
      "2025-12-07 12:29:07.732430: Epoch 11\n",
      "2025-12-07 12:29:07.733517: Current learning rate: 0.0099\n",
      "2025-12-07 12:31:25.607714: train_loss -0.5058\n",
      "2025-12-07 12:31:25.607714: val_loss -0.5183\n",
      "2025-12-07 12:31:25.607714: Pseudo dice [0.7421, 0.8271, 0.7428]\n",
      "2025-12-07 12:31:25.615353: Epoch time: 137.88 s\n",
      "2025-12-07 12:31:25.615353: Yayy! New best EMA pseudo Dice: 0.6043\n",
      "2025-12-07 12:31:26.515118: \n",
      "2025-12-07 12:31:26.515118: Epoch 12\n",
      "2025-12-07 12:31:26.515118: Current learning rate: 0.00989\n",
      "2025-12-07 12:33:44.547827: train_loss -0.503\n",
      "2025-12-07 12:33:44.549829: val_loss -0.5551\n",
      "2025-12-07 12:33:44.549829: Pseudo dice [0.7826, 0.8474, 0.7459]\n",
      "2025-12-07 12:33:44.551833: Epoch time: 138.05 s\n",
      "2025-12-07 12:33:44.553840: Yayy! New best EMA pseudo Dice: 0.6231\n",
      "2025-12-07 12:33:45.812248: \n",
      "2025-12-07 12:33:45.812248: Epoch 13\n",
      "2025-12-07 12:33:45.812248: Current learning rate: 0.00988\n",
      "2025-12-07 12:36:03.736776: train_loss -0.5433\n",
      "2025-12-07 12:36:03.738779: val_loss -0.5758\n",
      "2025-12-07 12:36:03.738779: Pseudo dice [0.7995, 0.8558, 0.7635]\n",
      "2025-12-07 12:36:03.740782: Epoch time: 137.92 s\n",
      "2025-12-07 12:36:03.740782: Yayy! New best EMA pseudo Dice: 0.6414\n",
      "2025-12-07 12:36:04.655371: \n",
      "2025-12-07 12:36:04.655371: Epoch 14\n",
      "2025-12-07 12:36:04.655371: Current learning rate: 0.00987\n",
      "2025-12-07 12:38:22.670951: train_loss -0.5609\n",
      "2025-12-07 12:38:22.670951: val_loss -0.6053\n",
      "2025-12-07 12:38:22.670951: Pseudo dice [0.8259, 0.8745, 0.7606]\n",
      "2025-12-07 12:38:22.670951: Epoch time: 138.02 s\n",
      "2025-12-07 12:38:22.679574: Yayy! New best EMA pseudo Dice: 0.6593\n",
      "2025-12-07 12:38:23.561860: \n",
      "2025-12-07 12:38:23.561860: Epoch 15\n",
      "2025-12-07 12:38:23.561860: Current learning rate: 0.00986\n",
      "2025-12-07 12:40:41.843736: train_loss -0.5783\n",
      "2025-12-07 12:40:41.843736: val_loss -0.6026\n",
      "2025-12-07 12:40:41.843736: Pseudo dice [0.8147, 0.8636, 0.774]\n",
      "2025-12-07 12:40:41.859708: Epoch time: 138.28 s\n",
      "2025-12-07 12:40:41.859708: Yayy! New best EMA pseudo Dice: 0.6751\n",
      "2025-12-07 12:40:42.827409: \n",
      "2025-12-07 12:40:42.827409: Epoch 16\n",
      "2025-12-07 12:40:42.843043: Current learning rate: 0.00986\n",
      "2025-12-07 12:43:00.951350: train_loss -0.6051\n",
      "2025-12-07 12:43:00.951350: val_loss -0.6286\n",
      "2025-12-07 12:43:00.952852: Pseudo dice [0.8275, 0.8794, 0.7966]\n",
      "2025-12-07 12:43:00.954854: Epoch time: 138.12 s\n",
      "2025-12-07 12:43:00.954854: Yayy! New best EMA pseudo Dice: 0.6911\n",
      "2025-12-07 12:43:01.872051: \n",
      "2025-12-07 12:43:01.872051: Epoch 17\n",
      "2025-12-07 12:43:01.873054: Current learning rate: 0.00985\n",
      "2025-12-07 12:45:20.038627: train_loss -0.5668\n",
      "2025-12-07 12:45:20.040629: val_loss -0.5955\n",
      "2025-12-07 12:45:20.040629: Pseudo dice [0.8052, 0.865, 0.7676]\n",
      "2025-12-07 12:45:20.042631: Epoch time: 138.17 s\n",
      "2025-12-07 12:45:20.042631: Yayy! New best EMA pseudo Dice: 0.7032\n",
      "2025-12-07 12:45:20.970897: \n",
      "2025-12-07 12:45:20.971897: Epoch 18\n",
      "2025-12-07 12:45:20.973049: Current learning rate: 0.00984\n",
      "2025-12-07 12:47:39.014487: train_loss -0.586\n",
      "2025-12-07 12:47:39.016227: val_loss -0.6399\n",
      "2025-12-07 12:47:39.018230: Pseudo dice [0.8319, 0.8798, 0.7976]\n",
      "2025-12-07 12:47:39.018230: Epoch time: 138.04 s\n",
      "2025-12-07 12:47:39.020233: Yayy! New best EMA pseudo Dice: 0.7165\n",
      "2025-12-07 12:47:40.126882: \n",
      "2025-12-07 12:47:40.126882: Epoch 19\n",
      "2025-12-07 12:47:40.126882: Current learning rate: 0.00983\n",
      "2025-12-07 12:49:58.042298: train_loss -0.6258\n",
      "2025-12-07 12:49:58.042298: val_loss -0.6484\n",
      "2025-12-07 12:49:58.044300: Pseudo dice [0.8286, 0.8863, 0.8122]\n",
      "2025-12-07 12:49:58.045804: Epoch time: 137.92 s\n",
      "2025-12-07 12:49:58.047810: Yayy! New best EMA pseudo Dice: 0.7291\n",
      "2025-12-07 12:49:58.963232: \n",
      "2025-12-07 12:49:58.963232: Epoch 20\n",
      "2025-12-07 12:49:58.964236: Current learning rate: 0.00982\n",
      "2025-12-07 12:52:17.379574: train_loss -0.6257\n",
      "2025-12-07 12:52:17.379574: val_loss -0.6573\n",
      "2025-12-07 12:52:17.381579: Pseudo dice [0.832, 0.8862, 0.8171]\n",
      "2025-12-07 12:52:17.382582: Epoch time: 138.42 s\n",
      "2025-12-07 12:52:17.383585: Yayy! New best EMA pseudo Dice: 0.7407\n",
      "2025-12-07 12:52:18.296248: \n",
      "2025-12-07 12:52:18.296248: Epoch 21\n",
      "2025-12-07 12:52:18.296248: Current learning rate: 0.00981\n",
      "2025-12-07 12:54:37.452593: train_loss -0.6351\n",
      "2025-12-07 12:54:37.452593: val_loss -0.6621\n",
      "2025-12-07 12:54:37.452593: Pseudo dice [0.8363, 0.8972, 0.8063]\n",
      "2025-12-07 12:54:37.452593: Epoch time: 139.16 s\n",
      "2025-12-07 12:54:37.468580: Yayy! New best EMA pseudo Dice: 0.7513\n",
      "2025-12-07 12:54:38.344551: \n",
      "2025-12-07 12:54:38.344551: Epoch 22\n",
      "2025-12-07 12:54:38.344551: Current learning rate: 0.0098\n",
      "2025-12-07 12:56:56.899885: train_loss -0.6617\n",
      "2025-12-07 12:56:56.899885: val_loss -0.6814\n",
      "2025-12-07 12:56:56.901886: Pseudo dice [0.8482, 0.9002, 0.8436]\n",
      "2025-12-07 12:56:56.902886: Epoch time: 138.56 s\n",
      "2025-12-07 12:56:56.903886: Yayy! New best EMA pseudo Dice: 0.7626\n",
      "2025-12-07 12:56:57.956879: \n",
      "2025-12-07 12:56:57.956879: Epoch 23\n",
      "2025-12-07 12:56:57.956879: Current learning rate: 0.00979\n",
      "2025-12-07 12:59:16.344981: train_loss -0.6593\n",
      "2025-12-07 12:59:16.344981: val_loss -0.669\n",
      "2025-12-07 12:59:16.346983: Pseudo dice [0.8425, 0.8934, 0.8158]\n",
      "2025-12-07 12:59:16.346983: Epoch time: 138.39 s\n",
      "2025-12-07 12:59:16.346983: Yayy! New best EMA pseudo Dice: 0.7714\n",
      "2025-12-07 12:59:17.233570: \n",
      "2025-12-07 12:59:17.233570: Epoch 24\n",
      "2025-12-07 12:59:17.233570: Current learning rate: 0.00978\n",
      "2025-12-07 13:01:35.506209: train_loss -0.6638\n",
      "2025-12-07 13:01:35.506209: val_loss -0.697\n",
      "2025-12-07 13:01:35.508219: Pseudo dice [0.8536, 0.9118, 0.8405]\n",
      "2025-12-07 13:01:35.509218: Epoch time: 138.27 s\n",
      "2025-12-07 13:01:35.510219: Yayy! New best EMA pseudo Dice: 0.7811\n",
      "2025-12-07 13:01:36.388420: \n",
      "2025-12-07 13:01:36.388420: Epoch 25\n",
      "2025-12-07 13:01:36.388420: Current learning rate: 0.00977\n",
      "2025-12-07 13:03:54.779933: train_loss -0.6685\n",
      "2025-12-07 13:03:54.779933: val_loss -0.6985\n",
      "2025-12-07 13:03:54.782322: Pseudo dice [0.8537, 0.9031, 0.8396]\n",
      "2025-12-07 13:03:54.782322: Epoch time: 138.39 s\n",
      "2025-12-07 13:03:54.782322: Yayy! New best EMA pseudo Dice: 0.7895\n",
      "2025-12-07 13:03:55.703962: \n",
      "2025-12-07 13:03:55.704962: Epoch 26\n",
      "2025-12-07 13:03:55.706256: Current learning rate: 0.00977\n",
      "2025-12-07 13:06:14.180086: train_loss -0.6646\n",
      "2025-12-07 13:06:14.181088: val_loss -0.6694\n",
      "2025-12-07 13:06:14.183092: Pseudo dice [0.8369, 0.8871, 0.8324]\n",
      "2025-12-07 13:06:14.183092: Epoch time: 138.48 s\n",
      "2025-12-07 13:06:14.184093: Yayy! New best EMA pseudo Dice: 0.7958\n",
      "2025-12-07 13:06:15.092323: \n",
      "2025-12-07 13:06:15.092323: Epoch 27\n",
      "2025-12-07 13:06:15.092323: Current learning rate: 0.00976\n",
      "2025-12-07 13:08:33.145122: train_loss -0.6767\n",
      "2025-12-07 13:08:33.145122: val_loss -0.684\n",
      "2025-12-07 13:08:33.147125: Pseudo dice [0.8436, 0.8973, 0.8562]\n",
      "2025-12-07 13:08:33.149128: Epoch time: 138.05 s\n",
      "2025-12-07 13:08:33.151134: Yayy! New best EMA pseudo Dice: 0.8028\n",
      "2025-12-07 13:08:34.030535: \n",
      "2025-12-07 13:08:34.030535: Epoch 28\n",
      "2025-12-07 13:08:34.030535: Current learning rate: 0.00975\n",
      "2025-12-07 13:10:51.983656: train_loss -0.6823\n",
      "2025-12-07 13:10:51.983656: val_loss -0.6834\n",
      "2025-12-07 13:10:51.983656: Pseudo dice [0.8393, 0.9003, 0.8465]\n",
      "2025-12-07 13:10:51.983656: Epoch time: 137.95 s\n",
      "2025-12-07 13:10:51.983656: Yayy! New best EMA pseudo Dice: 0.8087\n",
      "2025-12-07 13:10:52.868313: \n",
      "2025-12-07 13:10:52.868313: Epoch 29\n",
      "2025-12-07 13:10:52.868313: Current learning rate: 0.00974\n",
      "2025-12-07 13:13:10.874206: train_loss -0.6954\n",
      "2025-12-07 13:13:10.874206: val_loss -0.699\n",
      "2025-12-07 13:13:10.874206: Pseudo dice [0.8432, 0.9005, 0.8688]\n",
      "2025-12-07 13:13:10.874206: Epoch time: 138.01 s\n",
      "2025-12-07 13:13:10.874206: Yayy! New best EMA pseudo Dice: 0.8149\n",
      "2025-12-07 13:13:11.932128: \n",
      "2025-12-07 13:13:11.932128: Epoch 30\n",
      "2025-12-07 13:13:11.932128: Current learning rate: 0.00973\n",
      "2025-12-07 13:15:29.999683: train_loss -0.6946\n",
      "2025-12-07 13:15:29.999683: val_loss -0.7117\n",
      "2025-12-07 13:15:29.999683: Pseudo dice [0.8577, 0.9027, 0.8537]\n",
      "2025-12-07 13:15:29.999683: Epoch time: 138.07 s\n",
      "2025-12-07 13:15:29.999683: Yayy! New best EMA pseudo Dice: 0.8206\n",
      "2025-12-07 13:15:30.888777: \n",
      "2025-12-07 13:15:30.888777: Epoch 31\n",
      "2025-12-07 13:15:30.888777: Current learning rate: 0.00972\n",
      "2025-12-07 13:17:48.909447: train_loss -0.7042\n",
      "2025-12-07 13:17:48.910448: val_loss -0.7234\n",
      "2025-12-07 13:17:48.911448: Pseudo dice [0.86, 0.9105, 0.8626]\n",
      "2025-12-07 13:17:48.912448: Epoch time: 138.02 s\n",
      "2025-12-07 13:17:48.914448: Yayy! New best EMA pseudo Dice: 0.8263\n",
      "2025-12-07 13:17:49.828179: \n",
      "2025-12-07 13:17:49.829184: Epoch 32\n",
      "2025-12-07 13:17:49.829184: Current learning rate: 0.00971\n",
      "2025-12-07 13:20:07.793944: train_loss -0.7064\n",
      "2025-12-07 13:20:07.793944: val_loss -0.7209\n",
      "2025-12-07 13:20:07.796949: Pseudo dice [0.8592, 0.9071, 0.8653]\n",
      "2025-12-07 13:20:07.798951: Epoch time: 137.97 s\n",
      "2025-12-07 13:20:07.798951: Yayy! New best EMA pseudo Dice: 0.8314\n",
      "2025-12-07 13:20:08.699880: \n",
      "2025-12-07 13:20:08.701621: Epoch 33\n",
      "2025-12-07 13:20:08.701621: Current learning rate: 0.0097\n",
      "2025-12-07 13:22:26.841862: train_loss -0.7052\n",
      "2025-12-07 13:22:26.841862: val_loss -0.7285\n",
      "2025-12-07 13:22:26.843602: Pseudo dice [0.8619, 0.9052, 0.8859]\n",
      "2025-12-07 13:22:26.843602: Epoch time: 138.14 s\n",
      "2025-12-07 13:22:26.843602: Yayy! New best EMA pseudo Dice: 0.8367\n",
      "2025-12-07 13:22:27.740093: \n",
      "2025-12-07 13:22:27.740093: Epoch 34\n",
      "2025-12-07 13:22:27.741402: Current learning rate: 0.00969\n",
      "2025-12-07 13:24:45.795722: train_loss -0.7063\n",
      "2025-12-07 13:24:45.798494: val_loss -0.7337\n",
      "2025-12-07 13:24:45.798494: Pseudo dice [0.8641, 0.9149, 0.8744]\n",
      "2025-12-07 13:24:45.800663: Epoch time: 138.06 s\n",
      "2025-12-07 13:24:45.800663: Yayy! New best EMA pseudo Dice: 0.8414\n",
      "2025-12-07 13:24:46.715203: \n",
      "2025-12-07 13:24:46.715203: Epoch 35\n",
      "2025-12-07 13:24:46.717796: Current learning rate: 0.00968\n",
      "2025-12-07 13:27:04.813298: train_loss -0.7245\n",
      "2025-12-07 13:27:04.813298: val_loss -0.7427\n",
      "2025-12-07 13:27:04.817307: Pseudo dice [0.8612, 0.9097, 0.8896]\n",
      "2025-12-07 13:27:04.819311: Epoch time: 138.1 s\n",
      "2025-12-07 13:27:04.821313: Yayy! New best EMA pseudo Dice: 0.846\n",
      "2025-12-07 13:27:05.921616: \n",
      "2025-12-07 13:27:05.921616: Epoch 36\n",
      "2025-12-07 13:27:05.921616: Current learning rate: 0.00968\n",
      "2025-12-07 13:29:24.091347: train_loss -0.7268\n",
      "2025-12-07 13:29:24.093348: val_loss -0.7458\n",
      "2025-12-07 13:29:24.093348: Pseudo dice [0.8731, 0.9194, 0.8733]\n",
      "2025-12-07 13:29:24.093348: Epoch time: 138.17 s\n",
      "2025-12-07 13:29:24.093348: Yayy! New best EMA pseudo Dice: 0.8503\n",
      "2025-12-07 13:29:24.998879: \n",
      "2025-12-07 13:29:24.999885: Epoch 37\n",
      "2025-12-07 13:29:25.000924: Current learning rate: 0.00967\n",
      "2025-12-07 13:31:43.165655: train_loss -0.7175\n",
      "2025-12-07 13:31:43.165655: val_loss -0.7488\n",
      "2025-12-07 13:31:43.165655: Pseudo dice [0.8687, 0.9196, 0.8825]\n",
      "2025-12-07 13:31:43.165655: Epoch time: 138.17 s\n",
      "2025-12-07 13:31:43.165655: Yayy! New best EMA pseudo Dice: 0.8543\n",
      "2025-12-07 13:31:44.078124: \n",
      "2025-12-07 13:31:44.078124: Epoch 38\n",
      "2025-12-07 13:31:44.078124: Current learning rate: 0.00966\n",
      "2025-12-07 13:34:02.928632: train_loss -0.7176\n",
      "2025-12-07 13:34:02.928632: val_loss -0.7436\n",
      "2025-12-07 13:34:02.930634: Pseudo dice [0.8716, 0.9153, 0.8855]\n",
      "2025-12-07 13:34:02.930634: Epoch time: 138.85 s\n",
      "2025-12-07 13:34:02.932636: Yayy! New best EMA pseudo Dice: 0.8579\n",
      "2025-12-07 13:34:03.874761: \n",
      "2025-12-07 13:34:03.874761: Epoch 39\n",
      "2025-12-07 13:34:03.874761: Current learning rate: 0.00965\n",
      "2025-12-07 13:36:22.391249: train_loss -0.708\n",
      "2025-12-07 13:36:22.391249: val_loss -0.7507\n",
      "2025-12-07 13:36:22.393255: Pseudo dice [0.8793, 0.9222, 0.8788]\n",
      "2025-12-07 13:36:22.393255: Epoch time: 138.52 s\n",
      "2025-12-07 13:36:22.394257: Yayy! New best EMA pseudo Dice: 0.8615\n",
      "2025-12-07 13:36:23.295635: \n",
      "2025-12-07 13:36:23.295635: Epoch 40\n",
      "2025-12-07 13:36:23.295635: Current learning rate: 0.00964\n",
      "2025-12-07 13:38:41.825699: train_loss -0.7291\n",
      "2025-12-07 13:38:41.826824: val_loss -0.7613\n",
      "2025-12-07 13:38:41.827859: Pseudo dice [0.8764, 0.9237, 0.889]\n",
      "2025-12-07 13:38:41.828870: Epoch time: 138.53 s\n",
      "2025-12-07 13:38:41.829872: Yayy! New best EMA pseudo Dice: 0.865\n",
      "2025-12-07 13:38:42.742896: \n",
      "2025-12-07 13:38:42.744899: Epoch 41\n",
      "2025-12-07 13:38:42.744899: Current learning rate: 0.00963\n",
      "2025-12-07 13:41:01.513389: train_loss -0.7332\n",
      "2025-12-07 13:41:01.513389: val_loss -0.7374\n",
      "2025-12-07 13:41:01.513389: Pseudo dice [0.854, 0.8996, 0.8852]\n",
      "2025-12-07 13:41:01.513389: Epoch time: 138.77 s\n",
      "2025-12-07 13:41:01.513389: Yayy! New best EMA pseudo Dice: 0.8664\n",
      "2025-12-07 13:41:02.762189: \n",
      "2025-12-07 13:41:02.762189: Epoch 42\n",
      "2025-12-07 13:41:02.762189: Current learning rate: 0.00962\n",
      "2025-12-07 13:43:21.392246: train_loss -0.7321\n",
      "2025-12-07 13:43:21.392246: val_loss -0.7654\n",
      "2025-12-07 13:43:21.396259: Pseudo dice [0.8763, 0.9273, 0.8869]\n",
      "2025-12-07 13:43:21.398261: Epoch time: 138.63 s\n",
      "2025-12-07 13:43:21.398261: Yayy! New best EMA pseudo Dice: 0.8695\n",
      "2025-12-07 13:43:22.296043: \n",
      "2025-12-07 13:43:22.296043: Epoch 43\n",
      "2025-12-07 13:43:22.296043: Current learning rate: 0.00961\n",
      "2025-12-07 13:45:40.435916: train_loss -0.7375\n",
      "2025-12-07 13:45:40.435916: val_loss -0.7605\n",
      "2025-12-07 13:45:40.439421: Pseudo dice [0.8744, 0.9184, 0.8994]\n",
      "2025-12-07 13:45:40.439421: Epoch time: 138.14 s\n",
      "2025-12-07 13:45:40.441425: Yayy! New best EMA pseudo Dice: 0.8723\n",
      "2025-12-07 13:45:41.341435: \n",
      "2025-12-07 13:45:41.343438: Epoch 44\n",
      "2025-12-07 13:45:41.343438: Current learning rate: 0.0096\n",
      "2025-12-07 13:47:59.490836: train_loss -0.7405\n",
      "2025-12-07 13:47:59.490836: val_loss -0.7674\n",
      "2025-12-07 13:47:59.490836: Pseudo dice [0.8785, 0.9207, 0.8934]\n",
      "2025-12-07 13:47:59.490836: Epoch time: 138.15 s\n",
      "2025-12-07 13:47:59.490836: Yayy! New best EMA pseudo Dice: 0.8748\n",
      "2025-12-07 13:48:00.418223: \n",
      "2025-12-07 13:48:00.418223: Epoch 45\n",
      "2025-12-07 13:48:00.420227: Current learning rate: 0.00959\n",
      "2025-12-07 13:50:18.262044: train_loss -0.7367\n",
      "2025-12-07 13:50:18.264782: val_loss -0.7574\n",
      "2025-12-07 13:50:18.265785: Pseudo dice [0.8693, 0.9164, 0.8974]\n",
      "2025-12-07 13:50:18.267915: Epoch time: 137.84 s\n",
      "2025-12-07 13:50:18.268916: Yayy! New best EMA pseudo Dice: 0.8767\n",
      "2025-12-07 13:50:19.171134: \n",
      "2025-12-07 13:50:19.171134: Epoch 46\n",
      "2025-12-07 13:50:19.173098: Current learning rate: 0.00959\n",
      "2025-12-07 13:52:37.202387: train_loss -0.7436\n",
      "2025-12-07 13:52:37.202387: val_loss -0.7717\n",
      "2025-12-07 13:52:37.202387: Pseudo dice [0.8737, 0.9235, 0.8935]\n",
      "2025-12-07 13:52:37.202387: Epoch time: 138.03 s\n",
      "2025-12-07 13:52:37.202387: Yayy! New best EMA pseudo Dice: 0.8788\n",
      "2025-12-07 13:52:38.061700: \n",
      "2025-12-07 13:52:38.061700: Epoch 47\n",
      "2025-12-07 13:52:38.077672: Current learning rate: 0.00958\n",
      "2025-12-07 13:54:56.026354: train_loss -0.7512\n",
      "2025-12-07 13:54:56.026354: val_loss -0.7578\n",
      "2025-12-07 13:54:56.026354: Pseudo dice [0.8669, 0.9171, 0.8977]\n",
      "2025-12-07 13:54:56.029906: Epoch time: 137.96 s\n",
      "2025-12-07 13:54:56.031908: Yayy! New best EMA pseudo Dice: 0.8803\n",
      "2025-12-07 13:54:57.078152: \n",
      "2025-12-07 13:54:57.078152: Epoch 48\n",
      "2025-12-07 13:54:57.078152: Current learning rate: 0.00957\n",
      "2025-12-07 13:57:15.120730: train_loss -0.7548\n",
      "2025-12-07 13:57:15.120730: val_loss -0.7752\n",
      "2025-12-07 13:57:15.122733: Pseudo dice [0.8817, 0.925, 0.8889]\n",
      "2025-12-07 13:57:15.124736: Epoch time: 138.04 s\n",
      "2025-12-07 13:57:15.124736: Yayy! New best EMA pseudo Dice: 0.8821\n",
      "2025-12-07 13:57:15.998859: \n",
      "2025-12-07 13:57:15.998859: Epoch 49\n",
      "2025-12-07 13:57:16.014501: Current learning rate: 0.00956\n",
      "2025-12-07 13:59:33.967801: train_loss -0.7621\n",
      "2025-12-07 13:59:33.967801: val_loss -0.7815\n",
      "2025-12-07 13:59:33.967801: Pseudo dice [0.882, 0.9293, 0.8922]\n",
      "2025-12-07 13:59:33.967801: Epoch time: 137.97 s\n",
      "2025-12-07 13:59:34.223984: Yayy! New best EMA pseudo Dice: 0.884\n",
      "2025-12-07 13:59:35.109556: \n",
      "2025-12-07 13:59:35.109556: Epoch 50\n",
      "2025-12-07 13:59:35.109556: Current learning rate: 0.00955\n",
      "2025-12-07 14:01:53.284263: train_loss -0.7651\n",
      "2025-12-07 14:01:53.284263: val_loss -0.7713\n",
      "2025-12-07 14:01:53.286264: Pseudo dice [0.8695, 0.9142, 0.9061]\n",
      "2025-12-07 14:01:53.288265: Epoch time: 138.17 s\n",
      "2025-12-07 14:01:53.289265: Yayy! New best EMA pseudo Dice: 0.8853\n",
      "2025-12-07 14:01:54.192549: \n",
      "2025-12-07 14:01:54.193554: Epoch 51\n",
      "2025-12-07 14:01:54.194742: Current learning rate: 0.00954\n",
      "2025-12-07 14:04:12.320916: train_loss -0.7568\n",
      "2025-12-07 14:04:12.323343: val_loss -0.7886\n",
      "2025-12-07 14:04:12.325345: Pseudo dice [0.8872, 0.9308, 0.8965]\n",
      "2025-12-07 14:04:12.325345: Epoch time: 138.13 s\n",
      "2025-12-07 14:04:12.327347: Yayy! New best EMA pseudo Dice: 0.8872\n",
      "2025-12-07 14:04:13.217274: \n",
      "2025-12-07 14:04:13.217274: Epoch 52\n",
      "2025-12-07 14:04:13.217274: Current learning rate: 0.00953\n",
      "2025-12-07 14:06:31.374332: train_loss -0.7599\n",
      "2025-12-07 14:06:31.376335: val_loss -0.7676\n",
      "2025-12-07 14:06:31.378337: Pseudo dice [0.8724, 0.9153, 0.897]\n",
      "2025-12-07 14:06:31.380339: Epoch time: 138.16 s\n",
      "2025-12-07 14:06:31.380339: Yayy! New best EMA pseudo Dice: 0.888\n",
      "2025-12-07 14:06:32.268285: \n",
      "2025-12-07 14:06:32.270287: Epoch 53\n",
      "2025-12-07 14:06:32.270287: Current learning rate: 0.00952\n",
      "2025-12-07 14:08:50.275688: train_loss -0.7644\n",
      "2025-12-07 14:08:50.277691: val_loss -0.7689\n",
      "2025-12-07 14:08:50.277691: Pseudo dice [0.867, 0.9207, 0.8894]\n",
      "2025-12-07 14:08:50.280437: Epoch time: 138.01 s\n",
      "2025-12-07 14:08:50.280437: Yayy! New best EMA pseudo Dice: 0.8884\n",
      "2025-12-07 14:08:51.337795: \n",
      "2025-12-07 14:08:51.337795: Epoch 54\n",
      "2025-12-07 14:08:51.339860: Current learning rate: 0.00951\n",
      "2025-12-07 14:11:09.496246: train_loss -0.742\n",
      "2025-12-07 14:11:09.497247: val_loss -0.7651\n",
      "2025-12-07 14:11:09.498261: Pseudo dice [0.8776, 0.92, 0.9047]\n",
      "2025-12-07 14:11:09.499264: Epoch time: 138.16 s\n",
      "2025-12-07 14:11:09.499264: Yayy! New best EMA pseudo Dice: 0.8897\n",
      "2025-12-07 14:11:10.379016: \n",
      "2025-12-07 14:11:10.379016: Epoch 55\n",
      "2025-12-07 14:11:10.381033: Current learning rate: 0.0095\n",
      "2025-12-07 14:13:28.727434: train_loss -0.7619\n",
      "2025-12-07 14:13:28.728439: val_loss -0.7768\n",
      "2025-12-07 14:13:28.729443: Pseudo dice [0.8759, 0.9226, 0.9032]\n",
      "2025-12-07 14:13:28.730887: Epoch time: 138.35 s\n",
      "2025-12-07 14:13:28.732020: Yayy! New best EMA pseudo Dice: 0.8908\n",
      "2025-12-07 14:13:29.632132: \n",
      "2025-12-07 14:13:29.634803: Epoch 56\n",
      "2025-12-07 14:13:29.634803: Current learning rate: 0.00949\n",
      "2025-12-07 14:15:47.516894: train_loss -0.7668\n",
      "2025-12-07 14:15:47.518897: val_loss -0.7955\n",
      "2025-12-07 14:15:47.522906: Pseudo dice [0.8852, 0.9302, 0.9092]\n",
      "2025-12-07 14:15:47.524910: Epoch time: 137.88 s\n",
      "2025-12-07 14:15:47.524910: Yayy! New best EMA pseudo Dice: 0.8925\n",
      "2025-12-07 14:15:48.431750: \n",
      "2025-12-07 14:15:48.431750: Epoch 57\n",
      "2025-12-07 14:15:48.431750: Current learning rate: 0.00949\n",
      "2025-12-07 14:18:06.530013: train_loss -0.7701\n",
      "2025-12-07 14:18:06.530013: val_loss -0.7885\n",
      "2025-12-07 14:18:06.533609: Pseudo dice [0.8838, 0.9245, 0.9017]\n",
      "2025-12-07 14:18:06.535612: Epoch time: 138.1 s\n",
      "2025-12-07 14:18:06.535612: Yayy! New best EMA pseudo Dice: 0.8936\n",
      "2025-12-07 14:18:07.414712: \n",
      "2025-12-07 14:18:07.416714: Epoch 58\n",
      "2025-12-07 14:18:07.416714: Current learning rate: 0.00948\n",
      "2025-12-07 14:20:25.404863: train_loss -0.7702\n",
      "2025-12-07 14:20:25.404863: val_loss -0.7962\n",
      "2025-12-07 14:20:25.411859: Pseudo dice [0.8844, 0.9327, 0.9039]\n",
      "2025-12-07 14:20:25.411859: Epoch time: 137.99 s\n",
      "2025-12-07 14:20:25.413862: Yayy! New best EMA pseudo Dice: 0.8949\n",
      "2025-12-07 14:20:26.309478: \n",
      "2025-12-07 14:20:26.309478: Epoch 59\n",
      "2025-12-07 14:20:26.311482: Current learning rate: 0.00947\n",
      "2025-12-07 14:22:44.464775: train_loss -0.7687\n",
      "2025-12-07 14:22:44.464775: val_loss -0.7907\n",
      "2025-12-07 14:22:44.466780: Pseudo dice [0.8877, 0.9297, 0.9076]\n",
      "2025-12-07 14:22:44.467783: Epoch time: 138.16 s\n",
      "2025-12-07 14:22:44.468822: Yayy! New best EMA pseudo Dice: 0.8963\n",
      "2025-12-07 14:22:45.529862: \n",
      "2025-12-07 14:22:45.529862: Epoch 60\n",
      "2025-12-07 14:22:45.529862: Current learning rate: 0.00946\n",
      "2025-12-07 14:25:03.547161: train_loss -0.7757\n",
      "2025-12-07 14:25:03.547161: val_loss -0.7721\n",
      "2025-12-07 14:25:03.563090: Pseudo dice [0.8716, 0.9133, 0.8963]\n",
      "2025-12-07 14:25:03.564569: Epoch time: 138.02 s\n",
      "2025-12-07 14:25:04.199976: \n",
      "2025-12-07 14:25:04.199976: Epoch 61\n",
      "2025-12-07 14:25:04.201984: Current learning rate: 0.00945\n",
      "2025-12-07 14:27:22.186749: train_loss -0.763\n",
      "2025-12-07 14:27:22.186749: val_loss -0.7915\n",
      "2025-12-07 14:27:22.186749: Pseudo dice [0.8848, 0.9282, 0.9078]\n",
      "2025-12-07 14:27:22.198249: Epoch time: 137.99 s\n",
      "2025-12-07 14:27:22.198249: Yayy! New best EMA pseudo Dice: 0.8971\n",
      "2025-12-07 14:27:23.119595: \n",
      "2025-12-07 14:27:23.120599: Epoch 62\n",
      "2025-12-07 14:27:23.122604: Current learning rate: 0.00944\n",
      "2025-12-07 14:29:41.109188: train_loss -0.7632\n",
      "2025-12-07 14:29:41.109188: val_loss -0.7862\n",
      "2025-12-07 14:29:41.113193: Pseudo dice [0.88, 0.9359, 0.8971]\n",
      "2025-12-07 14:29:41.113193: Epoch time: 137.99 s\n",
      "2025-12-07 14:29:41.113193: Yayy! New best EMA pseudo Dice: 0.8978\n",
      "2025-12-07 14:29:42.010019: \n",
      "2025-12-07 14:29:42.010019: Epoch 63\n",
      "2025-12-07 14:29:42.010019: Current learning rate: 0.00943\n",
      "2025-12-07 14:32:00.133271: train_loss -0.7658\n",
      "2025-12-07 14:32:00.133271: val_loss -0.7754\n",
      "2025-12-07 14:32:00.133271: Pseudo dice [0.8806, 0.9187, 0.8894]\n",
      "2025-12-07 14:32:00.133271: Epoch time: 138.12 s\n",
      "2025-12-07 14:32:00.773430: \n",
      "2025-12-07 14:32:00.773430: Epoch 64\n",
      "2025-12-07 14:32:00.775844: Current learning rate: 0.00942\n",
      "2025-12-07 14:34:18.834719: train_loss -0.77\n",
      "2025-12-07 14:34:18.836722: val_loss -0.7952\n",
      "2025-12-07 14:34:18.836722: Pseudo dice [0.883, 0.9313, 0.9046]\n",
      "2025-12-07 14:34:18.839950: Epoch time: 138.06 s\n",
      "2025-12-07 14:34:18.839950: Yayy! New best EMA pseudo Dice: 0.8985\n",
      "2025-12-07 14:34:19.749488: \n",
      "2025-12-07 14:34:19.749488: Epoch 65\n",
      "2025-12-07 14:34:19.749488: Current learning rate: 0.00941\n",
      "2025-12-07 14:36:37.855958: train_loss -0.77\n",
      "2025-12-07 14:36:37.856958: val_loss -0.8027\n",
      "2025-12-07 14:36:37.858999: Pseudo dice [0.8942, 0.9335, 0.9026]\n",
      "2025-12-07 14:36:37.860000: Epoch time: 138.12 s\n",
      "2025-12-07 14:36:37.861000: Yayy! New best EMA pseudo Dice: 0.8997\n",
      "2025-12-07 14:36:38.931678: \n",
      "2025-12-07 14:36:38.932681: Epoch 66\n",
      "2025-12-07 14:36:38.933684: Current learning rate: 0.0094\n",
      "2025-12-07 14:38:56.968285: train_loss -0.7727\n",
      "2025-12-07 14:38:56.968285: val_loss -0.7871\n",
      "2025-12-07 14:38:56.968285: Pseudo dice [0.8782, 0.9222, 0.9156]\n",
      "2025-12-07 14:38:56.968285: Epoch time: 138.04 s\n",
      "2025-12-07 14:38:56.968285: Yayy! New best EMA pseudo Dice: 0.9003\n",
      "2025-12-07 14:38:57.867096: \n",
      "2025-12-07 14:38:57.868098: Epoch 67\n",
      "2025-12-07 14:38:57.870107: Current learning rate: 0.00939\n",
      "2025-12-07 14:41:16.294418: train_loss -0.773\n",
      "2025-12-07 14:41:16.294418: val_loss -0.7866\n",
      "2025-12-07 14:41:16.296158: Pseudo dice [0.8861, 0.927, 0.9049]\n",
      "2025-12-07 14:41:16.298164: Epoch time: 138.43 s\n",
      "2025-12-07 14:41:16.300168: Yayy! New best EMA pseudo Dice: 0.9008\n",
      "2025-12-07 14:41:17.213991: \n",
      "2025-12-07 14:41:17.213991: Epoch 68\n",
      "2025-12-07 14:41:17.216001: Current learning rate: 0.00939\n",
      "2025-12-07 14:43:35.331965: train_loss -0.7749\n",
      "2025-12-07 14:43:35.332967: val_loss -0.7903\n",
      "2025-12-07 14:43:35.333970: Pseudo dice [0.8787, 0.9275, 0.8987]\n",
      "2025-12-07 14:43:35.335975: Epoch time: 138.12 s\n",
      "2025-12-07 14:43:35.336977: Yayy! New best EMA pseudo Dice: 0.9009\n",
      "2025-12-07 14:43:36.237662: \n",
      "2025-12-07 14:43:36.239664: Epoch 69\n",
      "2025-12-07 14:43:36.239664: Current learning rate: 0.00938\n",
      "2025-12-07 14:45:54.321488: train_loss -0.779\n",
      "2025-12-07 14:45:54.323490: val_loss -0.8105\n",
      "2025-12-07 14:45:54.325492: Pseudo dice [0.8885, 0.9362, 0.9259]\n",
      "2025-12-07 14:45:54.325492: Epoch time: 138.08 s\n",
      "2025-12-07 14:45:54.327494: Yayy! New best EMA pseudo Dice: 0.9025\n",
      "2025-12-07 14:45:55.405250: \n",
      "2025-12-07 14:45:55.407254: Epoch 70\n",
      "2025-12-07 14:45:55.407254: Current learning rate: 0.00937\n",
      "2025-12-07 14:48:13.466728: train_loss -0.7779\n",
      "2025-12-07 14:48:13.467728: val_loss -0.8034\n",
      "2025-12-07 14:48:13.467728: Pseudo dice [0.8929, 0.9339, 0.9079]\n",
      "2025-12-07 14:48:13.467728: Epoch time: 138.06 s\n",
      "2025-12-07 14:48:13.467728: Yayy! New best EMA pseudo Dice: 0.9034\n",
      "2025-12-07 14:48:14.372334: \n",
      "2025-12-07 14:48:14.372334: Epoch 71\n",
      "2025-12-07 14:48:14.372334: Current learning rate: 0.00936\n",
      "2025-12-07 14:50:32.549900: train_loss -0.7857\n",
      "2025-12-07 14:50:32.550900: val_loss -0.7929\n",
      "2025-12-07 14:50:32.552900: Pseudo dice [0.8838, 0.9297, 0.9097]\n",
      "2025-12-07 14:50:32.554215: Epoch time: 138.18 s\n",
      "2025-12-07 14:50:32.555215: Yayy! New best EMA pseudo Dice: 0.9038\n",
      "2025-12-07 14:50:33.647736: \n",
      "2025-12-07 14:50:33.647736: Epoch 72\n",
      "2025-12-07 14:50:33.650096: Current learning rate: 0.00935\n",
      "2025-12-07 14:52:51.641641: train_loss -0.7802\n",
      "2025-12-07 14:52:51.641641: val_loss -0.7876\n",
      "2025-12-07 14:52:51.641641: Pseudo dice [0.8753, 0.9276, 0.9093]\n",
      "2025-12-07 14:52:51.641641: Epoch time: 137.99 s\n",
      "2025-12-07 14:52:51.641641: Yayy! New best EMA pseudo Dice: 0.9039\n",
      "2025-12-07 14:52:52.662028: \n",
      "2025-12-07 14:52:52.662028: Epoch 73\n",
      "2025-12-07 14:52:52.664046: Current learning rate: 0.00934\n",
      "2025-12-07 14:55:10.733321: train_loss -0.7835\n",
      "2025-12-07 14:55:10.733321: val_loss -0.8145\n",
      "2025-12-07 14:55:10.733321: Pseudo dice [0.9013, 0.9313, 0.9134]\n",
      "2025-12-07 14:55:10.733321: Epoch time: 138.07 s\n",
      "2025-12-07 14:55:10.733321: Yayy! New best EMA pseudo Dice: 0.905\n",
      "2025-12-07 14:55:11.659639: \n",
      "2025-12-07 14:55:11.659639: Epoch 74\n",
      "2025-12-07 14:55:11.659639: Current learning rate: 0.00933\n",
      "2025-12-07 14:57:29.842820: train_loss -0.7836\n",
      "2025-12-07 14:57:29.842820: val_loss -0.8017\n",
      "2025-12-07 14:57:29.842820: Pseudo dice [0.8874, 0.9269, 0.9165]\n",
      "2025-12-07 14:57:29.842820: Epoch time: 138.19 s\n",
      "2025-12-07 14:57:29.842820: Yayy! New best EMA pseudo Dice: 0.9055\n",
      "2025-12-07 14:57:30.733202: \n",
      "2025-12-07 14:57:30.733202: Epoch 75\n",
      "2025-12-07 14:57:30.748980: Current learning rate: 0.00932\n",
      "2025-12-07 14:59:48.733162: train_loss -0.7836\n",
      "2025-12-07 14:59:48.733162: val_loss -0.8213\n",
      "2025-12-07 14:59:48.733162: Pseudo dice [0.9033, 0.9381, 0.9155]\n",
      "2025-12-07 14:59:48.738547: Epoch time: 138.0 s\n",
      "2025-12-07 14:59:48.738547: Yayy! New best EMA pseudo Dice: 0.9069\n",
      "2025-12-07 14:59:49.799754: \n",
      "2025-12-07 14:59:49.799754: Epoch 76\n",
      "2025-12-07 14:59:49.801580: Current learning rate: 0.00931\n",
      "2025-12-07 15:02:07.841862: train_loss -0.7839\n",
      "2025-12-07 15:02:07.841862: val_loss -0.8092\n",
      "2025-12-07 15:02:07.843864: Pseudo dice [0.8952, 0.9355, 0.9117]\n",
      "2025-12-07 15:02:07.843864: Epoch time: 138.04 s\n",
      "2025-12-07 15:02:07.843864: Yayy! New best EMA pseudo Dice: 0.9076\n",
      "2025-12-07 15:02:08.762301: \n",
      "2025-12-07 15:02:08.762301: Epoch 77\n",
      "2025-12-07 15:02:08.765147: Current learning rate: 0.0093\n",
      "2025-12-07 15:04:26.873888: train_loss -0.7768\n",
      "2025-12-07 15:04:26.873888: val_loss -0.7989\n",
      "2025-12-07 15:04:26.873888: Pseudo dice [0.8857, 0.9309, 0.9094]\n",
      "2025-12-07 15:04:26.873888: Epoch time: 138.11 s\n",
      "2025-12-07 15:04:26.873888: Yayy! New best EMA pseudo Dice: 0.9077\n",
      "2025-12-07 15:04:27.951750: \n",
      "2025-12-07 15:04:27.951750: Epoch 78\n",
      "2025-12-07 15:04:27.951750: Current learning rate: 0.0093\n",
      "2025-12-07 15:06:45.948339: train_loss -0.7869\n",
      "2025-12-07 15:06:45.950341: val_loss -0.7932\n",
      "2025-12-07 15:06:45.952081: Pseudo dice [0.887, 0.9312, 0.9065]\n",
      "2025-12-07 15:06:45.956087: Epoch time: 138.0 s\n",
      "2025-12-07 15:06:45.958091: Yayy! New best EMA pseudo Dice: 0.9078\n",
      "2025-12-07 15:06:47.059551: \n",
      "2025-12-07 15:06:47.059551: Epoch 79\n",
      "2025-12-07 15:06:47.061551: Current learning rate: 0.00929\n",
      "2025-12-07 15:09:05.310976: train_loss -0.7915\n",
      "2025-12-07 15:09:05.310976: val_loss -0.8168\n",
      "2025-12-07 15:09:05.310976: Pseudo dice [0.8971, 0.9438, 0.9092]\n",
      "2025-12-07 15:09:05.310976: Epoch time: 138.25 s\n",
      "2025-12-07 15:09:05.323732: Yayy! New best EMA pseudo Dice: 0.9087\n",
      "2025-12-07 15:09:06.234081: \n",
      "2025-12-07 15:09:06.234081: Epoch 80\n",
      "2025-12-07 15:09:06.234081: Current learning rate: 0.00928\n",
      "2025-12-07 15:11:25.072927: train_loss -0.7902\n",
      "2025-12-07 15:11:25.072927: val_loss -0.8031\n",
      "2025-12-07 15:11:25.074929: Pseudo dice [0.8901, 0.927, 0.9109]\n",
      "2025-12-07 15:11:25.076669: Epoch time: 138.84 s\n",
      "2025-12-07 15:11:25.078564: Yayy! New best EMA pseudo Dice: 0.9087\n",
      "2025-12-07 15:11:25.998330: \n",
      "2025-12-07 15:11:25.998330: Epoch 81\n",
      "2025-12-07 15:11:25.998330: Current learning rate: 0.00927\n",
      "2025-12-07 15:13:44.609713: train_loss -0.7953\n",
      "2025-12-07 15:13:44.609713: val_loss -0.8039\n",
      "2025-12-07 15:13:44.611716: Pseudo dice [0.8857, 0.9305, 0.9235]\n",
      "2025-12-07 15:13:44.613718: Epoch time: 138.61 s\n",
      "2025-12-07 15:13:44.615720: Yayy! New best EMA pseudo Dice: 0.9092\n",
      "2025-12-07 15:13:45.575633: \n",
      "2025-12-07 15:13:45.575633: Epoch 82\n",
      "2025-12-07 15:13:45.576768: Current learning rate: 0.00926\n",
      "2025-12-07 15:16:04.210169: train_loss -0.7891\n",
      "2025-12-07 15:16:04.210169: val_loss -0.8159\n",
      "2025-12-07 15:16:04.210169: Pseudo dice [0.8994, 0.9389, 0.9159]\n",
      "2025-12-07 15:16:04.210169: Epoch time: 138.64 s\n",
      "2025-12-07 15:16:04.210169: Yayy! New best EMA pseudo Dice: 0.9101\n",
      "2025-12-07 15:16:05.270648: \n",
      "2025-12-07 15:16:05.270648: Epoch 83\n",
      "2025-12-07 15:16:05.373798: Current learning rate: 0.00925\n",
      "2025-12-07 15:18:23.699862: train_loss -0.7844\n",
      "2025-12-07 15:18:23.699862: val_loss -0.8123\n",
      "2025-12-07 15:18:23.702919: Pseudo dice [0.896, 0.9363, 0.914]\n",
      "2025-12-07 15:18:23.703960: Epoch time: 138.43 s\n",
      "2025-12-07 15:18:23.705961: Yayy! New best EMA pseudo Dice: 0.9106\n",
      "2025-12-07 15:18:24.596686: \n",
      "2025-12-07 15:18:24.597991: Epoch 84\n",
      "2025-12-07 15:18:24.598994: Current learning rate: 0.00924\n",
      "2025-12-07 15:20:42.841209: train_loss -0.7959\n",
      "2025-12-07 15:20:42.841209: val_loss -0.8107\n",
      "2025-12-07 15:20:42.843213: Pseudo dice [0.8936, 0.9346, 0.9172]\n",
      "2025-12-07 15:20:42.843213: Epoch time: 138.24 s\n",
      "2025-12-07 15:20:42.843213: Yayy! New best EMA pseudo Dice: 0.9111\n",
      "2025-12-07 15:20:43.717208: \n",
      "2025-12-07 15:20:43.717208: Epoch 85\n",
      "2025-12-07 15:20:43.717208: Current learning rate: 0.00923\n",
      "2025-12-07 15:23:01.830331: train_loss -0.7809\n",
      "2025-12-07 15:23:01.832335: val_loss -0.8198\n",
      "2025-12-07 15:23:01.834337: Pseudo dice [0.9047, 0.9358, 0.9116]\n",
      "2025-12-07 15:23:01.834337: Epoch time: 138.11 s\n",
      "2025-12-07 15:23:01.836340: Yayy! New best EMA pseudo Dice: 0.9117\n",
      "2025-12-07 15:23:02.831616: \n",
      "2025-12-07 15:23:02.831616: Epoch 86\n",
      "2025-12-07 15:23:02.831616: Current learning rate: 0.00922\n",
      "2025-12-07 15:25:20.888710: train_loss -0.788\n",
      "2025-12-07 15:25:20.890451: val_loss -0.8091\n",
      "2025-12-07 15:25:20.892542: Pseudo dice [0.8888, 0.9323, 0.9229]\n",
      "2025-12-07 15:25:20.893543: Epoch time: 138.06 s\n",
      "2025-12-07 15:25:20.894543: Yayy! New best EMA pseudo Dice: 0.912\n",
      "2025-12-07 15:25:21.779858: \n",
      "2025-12-07 15:25:21.779858: Epoch 87\n",
      "2025-12-07 15:25:21.779858: Current learning rate: 0.00921\n",
      "2025-12-07 15:27:39.822057: train_loss -0.792\n",
      "2025-12-07 15:27:39.824059: val_loss -0.8135\n",
      "2025-12-07 15:27:39.826064: Pseudo dice [0.8935, 0.9328, 0.9186]\n",
      "2025-12-07 15:27:39.827804: Epoch time: 138.04 s\n",
      "2025-12-07 15:27:39.829807: Yayy! New best EMA pseudo Dice: 0.9123\n",
      "2025-12-07 15:27:40.729155: \n",
      "2025-12-07 15:27:40.729155: Epoch 88\n",
      "2025-12-07 15:27:40.729155: Current learning rate: 0.0092\n",
      "2025-12-07 15:29:58.702726: train_loss -0.7922\n",
      "2025-12-07 15:29:58.702726: val_loss -0.8049\n",
      "2025-12-07 15:29:58.716668: Pseudo dice [0.8953, 0.9353, 0.9104]\n",
      "2025-12-07 15:29:58.716668: Epoch time: 137.97 s\n",
      "2025-12-07 15:29:58.718410: Yayy! New best EMA pseudo Dice: 0.9124\n",
      "2025-12-07 15:29:59.877388: \n",
      "2025-12-07 15:29:59.877388: Epoch 89\n",
      "2025-12-07 15:29:59.877388: Current learning rate: 0.0092\n",
      "2025-12-07 15:32:17.845928: train_loss -0.7723\n",
      "2025-12-07 15:32:17.845928: val_loss -0.7628\n",
      "2025-12-07 15:32:17.847931: Pseudo dice [0.8638, 0.9116, 0.8958]\n",
      "2025-12-07 15:32:17.847931: Epoch time: 137.97 s\n",
      "2025-12-07 15:32:18.473625: \n",
      "2025-12-07 15:32:18.473625: Epoch 90\n",
      "2025-12-07 15:32:18.473625: Current learning rate: 0.00919\n",
      "2025-12-07 15:34:37.119003: train_loss -0.7698\n",
      "2025-12-07 15:34:37.121009: val_loss -0.8071\n",
      "2025-12-07 15:34:37.122013: Pseudo dice [0.8956, 0.9352, 0.9201]\n",
      "2025-12-07 15:34:37.123018: Epoch time: 138.65 s\n",
      "2025-12-07 15:34:37.742519: \n",
      "2025-12-07 15:34:37.742519: Epoch 91\n",
      "2025-12-07 15:34:37.742519: Current learning rate: 0.00918\n",
      "2025-12-07 15:36:56.155664: train_loss -0.7776\n",
      "2025-12-07 15:36:56.155664: val_loss -0.8155\n",
      "2025-12-07 15:36:56.157667: Pseudo dice [0.8978, 0.9313, 0.9096]\n",
      "2025-12-07 15:36:56.159669: Epoch time: 138.41 s\n",
      "2025-12-07 15:36:56.812711: \n",
      "2025-12-07 15:36:56.814675: Epoch 92\n",
      "2025-12-07 15:36:56.814675: Current learning rate: 0.00917\n",
      "2025-12-07 15:39:15.214092: train_loss -0.7966\n",
      "2025-12-07 15:39:15.216094: val_loss -0.8326\n",
      "2025-12-07 15:39:15.217835: Pseudo dice [0.9131, 0.9416, 0.9178]\n",
      "2025-12-07 15:39:15.220004: Epoch time: 138.4 s\n",
      "2025-12-07 15:39:15.843603: \n",
      "2025-12-07 15:39:15.843603: Epoch 93\n",
      "2025-12-07 15:39:15.843603: Current learning rate: 0.00916\n",
      "2025-12-07 15:41:34.076472: train_loss -0.7986\n",
      "2025-12-07 15:41:34.076472: val_loss -0.8028\n",
      "2025-12-07 15:41:34.078212: Pseudo dice [0.89, 0.9243, 0.9153]\n",
      "2025-12-07 15:41:34.078212: Epoch time: 138.23 s\n",
      "2025-12-07 15:41:34.695260: \n",
      "2025-12-07 15:41:34.697262: Epoch 94\n",
      "2025-12-07 15:41:34.697262: Current learning rate: 0.00915\n",
      "2025-12-07 15:43:53.047504: train_loss -0.7987\n",
      "2025-12-07 15:43:53.047504: val_loss -0.8202\n",
      "2025-12-07 15:43:53.049506: Pseudo dice [0.8975, 0.933, 0.9333]\n",
      "2025-12-07 15:43:53.051509: Epoch time: 138.35 s\n",
      "2025-12-07 15:43:53.053512: Yayy! New best EMA pseudo Dice: 0.9131\n",
      "2025-12-07 15:43:54.067632: \n",
      "2025-12-07 15:43:54.067632: Epoch 95\n",
      "2025-12-07 15:43:54.067632: Current learning rate: 0.00914\n",
      "2025-12-07 15:46:12.124059: train_loss -0.7989\n",
      "2025-12-07 15:46:12.126063: val_loss -0.8135\n",
      "2025-12-07 15:46:12.128068: Pseudo dice [0.8963, 0.9333, 0.9221]\n",
      "2025-12-07 15:46:12.130070: Epoch time: 138.06 s\n",
      "2025-12-07 15:46:12.132072: Yayy! New best EMA pseudo Dice: 0.9135\n",
      "2025-12-07 15:46:13.208620: \n",
      "2025-12-07 15:46:13.208620: Epoch 96\n",
      "2025-12-07 15:46:13.208620: Current learning rate: 0.00913\n",
      "2025-12-07 15:48:31.231319: train_loss -0.7992\n",
      "2025-12-07 15:48:31.233060: val_loss -0.8096\n",
      "2025-12-07 15:48:31.233060: Pseudo dice [0.8915, 0.936, 0.9051]\n",
      "2025-12-07 15:48:31.233060: Epoch time: 138.02 s\n",
      "2025-12-07 15:48:31.872406: \n",
      "2025-12-07 15:48:31.872406: Epoch 97\n",
      "2025-12-07 15:48:31.874149: Current learning rate: 0.00912\n",
      "2025-12-07 15:50:49.756040: train_loss -0.7994\n",
      "2025-12-07 15:50:49.758040: val_loss -0.8147\n",
      "2025-12-07 15:50:49.761040: Pseudo dice [0.896, 0.9368, 0.917]\n",
      "2025-12-07 15:50:49.763041: Epoch time: 137.88 s\n",
      "2025-12-07 15:50:49.764042: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2025-12-07 15:50:50.748638: \n",
      "2025-12-07 15:50:50.748638: Epoch 98\n",
      "2025-12-07 15:50:50.748638: Current learning rate: 0.00911\n",
      "2025-12-07 15:53:08.550105: train_loss -0.795\n",
      "2025-12-07 15:53:08.551105: val_loss -0.823\n",
      "2025-12-07 15:53:08.554105: Pseudo dice [0.8977, 0.9395, 0.9256]\n",
      "2025-12-07 15:53:08.555106: Epoch time: 137.8 s\n",
      "2025-12-07 15:53:08.557106: Yayy! New best EMA pseudo Dice: 0.9143\n",
      "2025-12-07 15:53:09.471509: \n",
      "2025-12-07 15:53:09.471509: Epoch 99\n",
      "2025-12-07 15:53:09.471509: Current learning rate: 0.0091\n",
      "2025-12-07 15:55:27.374797: train_loss -0.7977\n",
      "2025-12-07 15:55:27.375866: val_loss -0.8193\n",
      "2025-12-07 15:55:27.377870: Pseudo dice [0.8974, 0.9409, 0.918]\n",
      "2025-12-07 15:55:27.378870: Epoch time: 137.91 s\n",
      "2025-12-07 15:55:27.624727: Yayy! New best EMA pseudo Dice: 0.9147\n",
      "2025-12-07 15:55:28.519988: \n",
      "2025-12-07 15:55:28.519988: Epoch 100\n",
      "2025-12-07 15:55:28.519988: Current learning rate: 0.0091\n",
      "2025-12-07 15:57:46.514616: train_loss -0.7988\n",
      "2025-12-07 15:57:46.515687: val_loss -0.8197\n",
      "2025-12-07 15:57:46.517686: Pseudo dice [0.8954, 0.9328, 0.9284]\n",
      "2025-12-07 15:57:46.519687: Epoch time: 137.99 s\n",
      "2025-12-07 15:57:46.520687: Yayy! New best EMA pseudo Dice: 0.9152\n",
      "2025-12-07 15:57:47.581095: \n",
      "2025-12-07 15:57:47.582097: Epoch 101\n",
      "2025-12-07 15:57:47.584105: Current learning rate: 0.00909\n",
      "2025-12-07 16:00:05.891500: train_loss -0.7755\n",
      "2025-12-07 16:00:05.891500: val_loss -0.7492\n",
      "2025-12-07 16:00:05.908652: Pseudo dice [0.8527, 0.9113, 0.9045]\n",
      "2025-12-07 16:00:05.909656: Epoch time: 138.31 s\n",
      "2025-12-07 16:00:06.514517: \n",
      "2025-12-07 16:00:06.514517: Epoch 102\n",
      "2025-12-07 16:00:06.514517: Current learning rate: 0.00908\n",
      "2025-12-07 16:02:24.558972: train_loss -0.7171\n",
      "2025-12-07 16:02:24.558972: val_loss -0.7497\n",
      "2025-12-07 16:02:24.561977: Pseudo dice [0.8617, 0.9055, 0.8943]\n",
      "2025-12-07 16:02:24.563979: Epoch time: 138.04 s\n",
      "2025-12-07 16:02:25.186821: \n",
      "2025-12-07 16:02:25.186821: Epoch 103\n",
      "2025-12-07 16:02:25.186821: Current learning rate: 0.00907\n",
      "2025-12-07 16:04:43.179550: train_loss -0.7604\n",
      "2025-12-07 16:04:43.181552: val_loss -0.7805\n",
      "2025-12-07 16:04:43.185558: Pseudo dice [0.8799, 0.9247, 0.8995]\n",
      "2025-12-07 16:04:43.187300: Epoch time: 137.99 s\n",
      "2025-12-07 16:04:43.821460: \n",
      "2025-12-07 16:04:43.821460: Epoch 104\n",
      "2025-12-07 16:04:43.821460: Current learning rate: 0.00906\n",
      "2025-12-07 16:07:01.811475: train_loss -0.7787\n",
      "2025-12-07 16:07:01.811475: val_loss -0.8099\n",
      "2025-12-07 16:07:01.811475: Pseudo dice [0.8937, 0.9395, 0.9121]\n",
      "2025-12-07 16:07:01.816200: Epoch time: 137.99 s\n",
      "2025-12-07 16:07:02.448996: \n",
      "2025-12-07 16:07:02.448996: Epoch 105\n",
      "2025-12-07 16:07:02.451605: Current learning rate: 0.00905\n",
      "2025-12-07 16:09:20.368018: train_loss -0.7814\n",
      "2025-12-07 16:09:20.368018: val_loss -0.805\n",
      "2025-12-07 16:09:20.368018: Pseudo dice [0.8919, 0.9328, 0.9179]\n",
      "2025-12-07 16:09:20.368018: Epoch time: 137.92 s\n",
      "2025-12-07 16:09:20.983096: \n",
      "2025-12-07 16:09:20.983096: Epoch 106\n",
      "2025-12-07 16:09:20.983096: Current learning rate: 0.00904\n",
      "2025-12-07 16:11:38.946606: train_loss -0.7844\n",
      "2025-12-07 16:11:38.946606: val_loss -0.8045\n",
      "2025-12-07 16:11:38.949704: Pseudo dice [0.8915, 0.9313, 0.9177]\n",
      "2025-12-07 16:11:38.950704: Epoch time: 137.96 s\n",
      "2025-12-07 16:11:39.587979: \n",
      "2025-12-07 16:11:39.587979: Epoch 107\n",
      "2025-12-07 16:11:39.587979: Current learning rate: 0.00903\n",
      "2025-12-07 16:13:58.280770: train_loss -0.7941\n",
      "2025-12-07 16:13:58.280770: val_loss -0.8093\n",
      "2025-12-07 16:13:58.284748: Pseudo dice [0.8902, 0.9342, 0.925]\n",
      "2025-12-07 16:13:58.284748: Epoch time: 138.69 s\n",
      "2025-12-07 16:13:59.075996: \n",
      "2025-12-07 16:13:59.075996: Epoch 108\n",
      "2025-12-07 16:13:59.077999: Current learning rate: 0.00902\n",
      "2025-12-07 16:16:17.364313: train_loss -0.7948\n",
      "2025-12-07 16:16:17.364313: val_loss -0.8088\n",
      "2025-12-07 16:16:17.366314: Pseudo dice [0.8899, 0.9283, 0.9205]\n",
      "2025-12-07 16:16:17.368317: Epoch time: 138.29 s\n",
      "2025-12-07 16:16:17.999796: \n",
      "2025-12-07 16:16:17.999796: Epoch 109\n",
      "2025-12-07 16:16:17.999796: Current learning rate: 0.00901\n",
      "2025-12-07 16:18:36.327240: train_loss -0.7972\n",
      "2025-12-07 16:18:36.327240: val_loss -0.8378\n",
      "2025-12-07 16:18:36.343140: Pseudo dice [0.9119, 0.9463, 0.9225]\n",
      "2025-12-07 16:18:36.345294: Epoch time: 138.33 s\n",
      "2025-12-07 16:18:36.983928: \n",
      "2025-12-07 16:18:36.983928: Epoch 110\n",
      "2025-12-07 16:18:36.983928: Current learning rate: 0.009\n",
      "2025-12-07 16:20:55.467999: train_loss -0.7999\n",
      "2025-12-07 16:20:55.467999: val_loss -0.815\n",
      "2025-12-07 16:20:55.485145: Pseudo dice [0.8933, 0.9356, 0.9215]\n",
      "2025-12-07 16:20:55.486149: Epoch time: 138.48 s\n",
      "2025-12-07 16:20:56.092997: \n",
      "2025-12-07 16:20:56.092997: Epoch 111\n",
      "2025-12-07 16:20:56.108891: Current learning rate: 0.009\n",
      "2025-12-07 16:23:14.327790: train_loss -0.8044\n",
      "2025-12-07 16:23:14.327790: val_loss -0.8075\n",
      "2025-12-07 16:23:14.327790: Pseudo dice [0.8892, 0.9351, 0.9214]\n",
      "2025-12-07 16:23:14.327790: Epoch time: 138.23 s\n",
      "2025-12-07 16:23:14.967971: \n",
      "2025-12-07 16:23:14.967971: Epoch 112\n",
      "2025-12-07 16:23:14.967971: Current learning rate: 0.00899\n",
      "2025-12-07 16:25:33.175082: train_loss -0.7996\n",
      "2025-12-07 16:25:33.176084: val_loss -0.8305\n",
      "2025-12-07 16:25:33.178086: Pseudo dice [0.9082, 0.949, 0.9193]\n",
      "2025-12-07 16:25:33.179219: Epoch time: 138.21 s\n",
      "2025-12-07 16:25:33.796665: \n",
      "2025-12-07 16:25:33.796665: Epoch 113\n",
      "2025-12-07 16:25:33.796665: Current learning rate: 0.00898\n",
      "2025-12-07 16:27:51.807860: train_loss -0.8036\n",
      "2025-12-07 16:27:51.807860: val_loss -0.8201\n",
      "2025-12-07 16:27:51.810868: Pseudo dice [0.898, 0.9322, 0.9208]\n",
      "2025-12-07 16:27:51.810868: Epoch time: 138.01 s\n",
      "2025-12-07 16:27:52.609508: \n",
      "2025-12-07 16:27:52.609508: Epoch 114\n",
      "2025-12-07 16:27:52.609508: Current learning rate: 0.00897\n",
      "2025-12-07 16:30:10.639646: train_loss -0.7909\n",
      "2025-12-07 16:30:10.639646: val_loss -0.8089\n",
      "2025-12-07 16:30:10.639646: Pseudo dice [0.8893, 0.9352, 0.9184]\n",
      "2025-12-07 16:30:10.655335: Epoch time: 138.03 s\n",
      "2025-12-07 16:30:11.291274: \n",
      "2025-12-07 16:30:11.293277: Epoch 115\n",
      "2025-12-07 16:30:11.293277: Current learning rate: 0.00896\n",
      "2025-12-07 16:32:29.155187: train_loss -0.8004\n",
      "2025-12-07 16:32:29.155187: val_loss -0.825\n",
      "2025-12-07 16:32:29.155187: Pseudo dice [0.9033, 0.9392, 0.9234]\n",
      "2025-12-07 16:32:29.155187: Epoch time: 137.86 s\n",
      "2025-12-07 16:32:29.155187: Yayy! New best EMA pseudo Dice: 0.9156\n",
      "2025-12-07 16:32:30.061271: \n",
      "2025-12-07 16:32:30.061271: Epoch 116\n",
      "2025-12-07 16:32:30.061271: Current learning rate: 0.00895\n",
      "2025-12-07 16:34:48.156141: train_loss -0.7987\n",
      "2025-12-07 16:34:48.158144: val_loss -0.8332\n",
      "2025-12-07 16:34:48.160146: Pseudo dice [0.9103, 0.9425, 0.9241]\n",
      "2025-12-07 16:34:48.160146: Epoch time: 138.1 s\n",
      "2025-12-07 16:34:48.162148: Yayy! New best EMA pseudo Dice: 0.9166\n",
      "2025-12-07 16:34:49.068957: \n",
      "2025-12-07 16:34:49.068957: Epoch 117\n",
      "2025-12-07 16:34:49.068957: Current learning rate: 0.00894\n",
      "2025-12-07 16:37:06.967427: train_loss -0.806\n",
      "2025-12-07 16:37:06.967427: val_loss -0.828\n",
      "2025-12-07 16:37:06.969168: Pseudo dice [0.9015, 0.94, 0.9243]\n",
      "2025-12-07 16:37:06.969168: Epoch time: 137.9 s\n",
      "2025-12-07 16:37:06.969168: Yayy! New best EMA pseudo Dice: 0.9171\n",
      "2025-12-07 16:37:07.904633: \n",
      "2025-12-07 16:37:07.904633: Epoch 118\n",
      "2025-12-07 16:37:07.904633: Current learning rate: 0.00893\n",
      "2025-12-07 16:39:25.964246: train_loss -0.8013\n",
      "2025-12-07 16:39:25.965246: val_loss -0.8324\n",
      "2025-12-07 16:39:25.967246: Pseudo dice [0.9043, 0.943, 0.927]\n",
      "2025-12-07 16:39:25.969247: Epoch time: 138.06 s\n",
      "2025-12-07 16:39:25.971247: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2025-12-07 16:39:26.874269: \n",
      "2025-12-07 16:39:26.874269: Epoch 119\n",
      "2025-12-07 16:39:26.874269: Current learning rate: 0.00892\n",
      "2025-12-07 16:41:44.774969: train_loss -0.8063\n",
      "2025-12-07 16:41:44.775970: val_loss -0.8229\n",
      "2025-12-07 16:41:44.778970: Pseudo dice [0.8987, 0.933, 0.9229]\n",
      "2025-12-07 16:41:44.781009: Epoch time: 137.92 s\n",
      "2025-12-07 16:41:44.783009: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2025-12-07 16:41:45.869986: \n",
      "2025-12-07 16:41:45.869986: Epoch 120\n",
      "2025-12-07 16:41:45.872069: Current learning rate: 0.00891\n",
      "2025-12-07 16:44:04.077094: train_loss -0.8078\n",
      "2025-12-07 16:44:04.077094: val_loss -0.8139\n",
      "2025-12-07 16:44:04.077094: Pseudo dice [0.8876, 0.9318, 0.9276]\n",
      "2025-12-07 16:44:04.077094: Epoch time: 138.21 s\n",
      "2025-12-07 16:44:04.825780: \n",
      "2025-12-07 16:44:04.825780: Epoch 121\n",
      "2025-12-07 16:44:04.827784: Current learning rate: 0.0089\n",
      "2025-12-07 16:46:22.766015: train_loss -0.8088\n",
      "2025-12-07 16:46:22.766015: val_loss -0.8291\n",
      "2025-12-07 16:46:22.769016: Pseudo dice [0.9035, 0.9424, 0.922]\n",
      "2025-12-07 16:46:22.770015: Epoch time: 137.94 s\n",
      "2025-12-07 16:46:22.771016: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2025-12-07 16:46:23.655277: \n",
      "2025-12-07 16:46:23.655277: Epoch 122\n",
      "2025-12-07 16:46:23.655277: Current learning rate: 0.00889\n",
      "2025-12-07 16:48:41.561579: train_loss -0.8129\n",
      "2025-12-07 16:48:41.561579: val_loss -0.8455\n",
      "2025-12-07 16:48:41.561579: Pseudo dice [0.9133, 0.946, 0.9336]\n",
      "2025-12-07 16:48:41.561579: Epoch time: 137.91 s\n",
      "2025-12-07 16:48:41.569604: Yayy! New best EMA pseudo Dice: 0.9195\n",
      "2025-12-07 16:48:42.467548: \n",
      "2025-12-07 16:48:42.467548: Epoch 123\n",
      "2025-12-07 16:48:42.467548: Current learning rate: 0.00889\n",
      "2025-12-07 16:51:00.448020: train_loss -0.804\n",
      "2025-12-07 16:51:00.448020: val_loss -0.8282\n",
      "2025-12-07 16:51:00.448020: Pseudo dice [0.9026, 0.9379, 0.9197]\n",
      "2025-12-07 16:51:00.451008: Epoch time: 137.98 s\n",
      "2025-12-07 16:51:00.452510: Yayy! New best EMA pseudo Dice: 0.9195\n",
      "2025-12-07 16:51:01.457386: \n",
      "2025-12-07 16:51:01.457386: Epoch 124\n",
      "2025-12-07 16:51:01.462206: Current learning rate: 0.00888\n",
      "2025-12-07 16:53:19.457713: train_loss -0.8121\n",
      "2025-12-07 16:53:19.457713: val_loss -0.829\n",
      "2025-12-07 16:53:19.459715: Pseudo dice [0.9012, 0.938, 0.9278]\n",
      "2025-12-07 16:53:19.461717: Epoch time: 138.0 s\n",
      "2025-12-07 16:53:19.463719: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2025-12-07 16:53:20.359650: \n",
      "2025-12-07 16:53:20.359650: Epoch 125\n",
      "2025-12-07 16:53:20.359650: Current learning rate: 0.00887\n",
      "2025-12-07 16:55:38.324898: train_loss -0.8137\n",
      "2025-12-07 16:55:38.326901: val_loss -0.8415\n",
      "2025-12-07 16:55:38.328642: Pseudo dice [0.9095, 0.9396, 0.9289]\n",
      "2025-12-07 16:55:38.328642: Epoch time: 137.97 s\n",
      "2025-12-07 16:55:38.332147: Yayy! New best EMA pseudo Dice: 0.9204\n",
      "2025-12-07 16:55:39.406934: \n",
      "2025-12-07 16:55:39.406934: Epoch 126\n",
      "2025-12-07 16:55:39.406934: Current learning rate: 0.00886\n",
      "2025-12-07 16:57:57.317434: train_loss -0.8105\n",
      "2025-12-07 16:57:57.317434: val_loss -0.8386\n",
      "2025-12-07 16:57:57.319437: Pseudo dice [0.9063, 0.9453, 0.9321]\n",
      "2025-12-07 16:57:57.319437: Epoch time: 137.91 s\n",
      "2025-12-07 16:57:57.319437: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2025-12-07 16:57:58.311456: \n",
      "2025-12-07 16:57:58.311456: Epoch 127\n",
      "2025-12-07 16:57:58.311456: Current learning rate: 0.00885\n",
      "2025-12-07 17:00:17.405801: train_loss -0.8136\n",
      "2025-12-07 17:00:17.405801: val_loss -0.8313\n",
      "2025-12-07 17:00:17.405801: Pseudo dice [0.906, 0.9331, 0.9232]\n",
      "2025-12-07 17:00:17.421701: Epoch time: 139.09 s\n",
      "2025-12-07 17:00:18.045698: \n",
      "2025-12-07 17:00:18.045698: Epoch 128\n",
      "2025-12-07 17:00:18.045698: Current learning rate: 0.00884\n",
      "2025-12-07 17:02:36.500011: train_loss -0.8139\n",
      "2025-12-07 17:02:36.500011: val_loss -0.8176\n",
      "2025-12-07 17:02:36.515863: Pseudo dice [0.8942, 0.9302, 0.9295]\n",
      "2025-12-07 17:02:36.515863: Epoch time: 138.45 s\n",
      "2025-12-07 17:02:37.139918: \n",
      "2025-12-07 17:02:37.139918: Epoch 129\n",
      "2025-12-07 17:02:37.155756: Current learning rate: 0.00883\n",
      "2025-12-07 17:04:55.655053: train_loss -0.7855\n",
      "2025-12-07 17:04:55.656053: val_loss -0.7897\n",
      "2025-12-07 17:04:55.658070: Pseudo dice [0.891, 0.9279, 0.8986]\n",
      "2025-12-07 17:04:55.660072: Epoch time: 138.52 s\n",
      "2025-12-07 17:04:56.311609: \n",
      "2025-12-07 17:04:56.311609: Epoch 130\n",
      "2025-12-07 17:04:56.311609: Current learning rate: 0.00882\n",
      "2025-12-07 17:07:14.862648: train_loss -0.7718\n",
      "2025-12-07 17:07:14.862648: val_loss -0.7914\n",
      "2025-12-07 17:07:14.866652: Pseudo dice [0.882, 0.9234, 0.9139]\n",
      "2025-12-07 17:07:14.866652: Epoch time: 138.55 s\n",
      "2025-12-07 17:07:15.515313: \n",
      "2025-12-07 17:07:15.515313: Epoch 131\n",
      "2025-12-07 17:07:15.515313: Current learning rate: 0.00881\n",
      "2025-12-07 17:09:33.796189: train_loss -0.7823\n",
      "2025-12-07 17:09:33.796189: val_loss -0.8289\n",
      "2025-12-07 17:09:33.796189: Pseudo dice [0.9057, 0.9471, 0.9174]\n",
      "2025-12-07 17:09:33.796189: Epoch time: 138.28 s\n",
      "2025-12-07 17:09:34.623466: \n",
      "2025-12-07 17:09:34.623466: Epoch 132\n",
      "2025-12-07 17:09:34.623466: Current learning rate: 0.0088\n",
      "2025-12-07 17:11:52.643983: train_loss -0.8024\n",
      "2025-12-07 17:11:52.643983: val_loss -0.8341\n",
      "2025-12-07 17:11:52.647726: Pseudo dice [0.9091, 0.9439, 0.9227]\n",
      "2025-12-07 17:11:52.649852: Epoch time: 138.02 s\n",
      "2025-12-07 17:11:53.298155: \n",
      "2025-12-07 17:11:53.298155: Epoch 133\n",
      "2025-12-07 17:11:53.300162: Current learning rate: 0.00879\n",
      "2025-12-07 17:14:11.259985: train_loss -0.808\n",
      "2025-12-07 17:14:11.260984: val_loss -0.8228\n",
      "2025-12-07 17:14:11.263985: Pseudo dice [0.8961, 0.9345, 0.9263]\n",
      "2025-12-07 17:14:11.266268: Epoch time: 137.96 s\n",
      "2025-12-07 17:14:11.922715: \n",
      "2025-12-07 17:14:11.922715: Epoch 134\n",
      "2025-12-07 17:14:11.924720: Current learning rate: 0.00879\n",
      "2025-12-07 17:16:29.921141: train_loss -0.8093\n",
      "2025-12-07 17:16:29.921141: val_loss -0.8397\n",
      "2025-12-07 17:16:29.936005: Pseudo dice [0.9072, 0.9415, 0.9327]\n",
      "2025-12-07 17:16:29.937010: Epoch time: 138.0 s\n",
      "2025-12-07 17:16:30.561895: \n",
      "2025-12-07 17:16:30.577718: Epoch 135\n",
      "2025-12-07 17:16:30.577718: Current learning rate: 0.00878\n",
      "2025-12-07 17:18:48.555811: train_loss -0.8132\n",
      "2025-12-07 17:18:48.557814: val_loss -0.8256\n",
      "2025-12-07 17:18:48.557814: Pseudo dice [0.8993, 0.939, 0.9234]\n",
      "2025-12-07 17:18:48.557814: Epoch time: 137.99 s\n",
      "2025-12-07 17:18:49.202758: \n",
      "2025-12-07 17:18:49.202758: Epoch 136\n",
      "2025-12-07 17:18:49.210597: Current learning rate: 0.00877\n",
      "2025-12-07 17:21:07.139787: train_loss -0.8039\n",
      "2025-12-07 17:21:07.139787: val_loss -0.8217\n",
      "2025-12-07 17:21:07.155553: Pseudo dice [0.901, 0.9371, 0.9246]\n",
      "2025-12-07 17:21:07.155553: Epoch time: 137.94 s\n",
      "2025-12-07 17:21:07.811364: \n",
      "2025-12-07 17:21:07.811364: Epoch 137\n",
      "2025-12-07 17:21:07.811364: Current learning rate: 0.00876\n",
      "2025-12-07 17:23:25.937650: train_loss -0.7671\n",
      "2025-12-07 17:23:25.937650: val_loss -0.822\n",
      "2025-12-07 17:23:25.946115: Pseudo dice [0.9, 0.944, 0.9197]\n",
      "2025-12-07 17:23:25.946115: Epoch time: 138.14 s\n",
      "2025-12-07 17:23:26.773906: \n",
      "2025-12-07 17:23:26.774919: Epoch 138\n",
      "2025-12-07 17:23:26.776982: Current learning rate: 0.00875\n",
      "2025-12-07 17:25:44.794427: train_loss -0.7887\n",
      "2025-12-07 17:25:44.796169: val_loss -0.8158\n",
      "2025-12-07 17:25:44.798171: Pseudo dice [0.9024, 0.9362, 0.9011]\n",
      "2025-12-07 17:25:44.800174: Epoch time: 138.02 s\n",
      "2025-12-07 17:25:45.452436: \n",
      "2025-12-07 17:25:45.452436: Epoch 139\n",
      "2025-12-07 17:25:45.452436: Current learning rate: 0.00874\n",
      "2025-12-07 17:28:03.821480: train_loss -0.8007\n",
      "2025-12-07 17:28:03.823483: val_loss -0.8267\n",
      "2025-12-07 17:28:03.825486: Pseudo dice [0.9024, 0.9439, 0.9189]\n",
      "2025-12-07 17:28:03.827227: Epoch time: 138.37 s\n",
      "2025-12-07 17:28:04.483886: \n",
      "2025-12-07 17:28:04.483886: Epoch 140\n",
      "2025-12-07 17:28:04.483886: Current learning rate: 0.00873\n",
      "2025-12-07 17:30:24.088966: train_loss -0.8098\n",
      "2025-12-07 17:30:24.090969: val_loss -0.8107\n",
      "2025-12-07 17:30:24.093182: Pseudo dice [0.8943, 0.9312, 0.9172]\n",
      "2025-12-07 17:30:24.093182: Epoch time: 139.61 s\n",
      "2025-12-07 17:30:24.737122: \n",
      "2025-12-07 17:30:24.737122: Epoch 141\n",
      "2025-12-07 17:30:24.737122: Current learning rate: 0.00872\n",
      "2025-12-07 17:32:43.481237: train_loss -0.8128\n",
      "2025-12-07 17:32:43.482977: val_loss -0.8421\n",
      "2025-12-07 17:32:43.486983: Pseudo dice [0.9103, 0.9398, 0.9376]\n",
      "2025-12-07 17:32:43.488985: Epoch time: 138.75 s\n",
      "2025-12-07 17:32:44.151053: \n",
      "2025-12-07 17:32:44.151053: Epoch 142\n",
      "2025-12-07 17:32:44.153138: Current learning rate: 0.00871\n",
      "2025-12-07 17:35:02.927681: train_loss -0.8063\n",
      "2025-12-07 17:35:02.927681: val_loss -0.8428\n",
      "2025-12-07 17:35:02.930252: Pseudo dice [0.9111, 0.9524, 0.925]\n",
      "2025-12-07 17:35:02.932253: Epoch time: 138.78 s\n",
      "2025-12-07 17:35:03.686587: \n",
      "2025-12-07 17:35:03.686587: Epoch 143\n",
      "2025-12-07 17:35:03.702412: Current learning rate: 0.0087\n",
      "2025-12-07 17:37:22.536846: train_loss -0.8017\n",
      "2025-12-07 17:37:22.536846: val_loss -0.8295\n",
      "2025-12-07 17:37:22.538847: Pseudo dice [0.8999, 0.9379, 0.9332]\n",
      "2025-12-07 17:37:22.540849: Epoch time: 138.85 s\n",
      "2025-12-07 17:37:22.542851: Yayy! New best EMA pseudo Dice: 0.9214\n",
      "2025-12-07 17:37:23.659793: \n",
      "2025-12-07 17:37:23.659793: Epoch 144\n",
      "2025-12-07 17:37:23.659793: Current learning rate: 0.00869\n",
      "2025-12-07 17:39:42.089901: train_loss -0.8111\n",
      "2025-12-07 17:39:42.091906: val_loss -0.8318\n",
      "2025-12-07 17:39:42.097652: Pseudo dice [0.9071, 0.9429, 0.923]\n",
      "2025-12-07 17:39:42.099656: Epoch time: 138.43 s\n",
      "2025-12-07 17:39:42.101661: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2025-12-07 17:39:43.017161: \n",
      "2025-12-07 17:39:43.017161: Epoch 145\n",
      "2025-12-07 17:39:43.017161: Current learning rate: 0.00868\n",
      "2025-12-07 17:42:01.058166: train_loss -0.8121\n",
      "2025-12-07 17:42:01.058166: val_loss -0.8244\n",
      "2025-12-07 17:42:01.061909: Pseudo dice [0.898, 0.9388, 0.9324]\n",
      "2025-12-07 17:42:01.063912: Epoch time: 138.04 s\n",
      "2025-12-07 17:42:01.063912: Yayy! New best EMA pseudo Dice: 0.9218\n",
      "2025-12-07 17:42:02.014707: \n",
      "2025-12-07 17:42:02.014707: Epoch 146\n",
      "2025-12-07 17:42:02.014707: Current learning rate: 0.00868\n",
      "2025-12-07 17:44:19.997125: train_loss -0.8097\n",
      "2025-12-07 17:44:19.997125: val_loss -0.8341\n",
      "2025-12-07 17:44:20.000709: Pseudo dice [0.903, 0.9405, 0.9291]\n",
      "2025-12-07 17:44:20.002711: Epoch time: 137.98 s\n",
      "2025-12-07 17:44:20.004713: Yayy! New best EMA pseudo Dice: 0.9221\n",
      "2025-12-07 17:44:20.917750: \n",
      "2025-12-07 17:44:20.918749: Epoch 147\n",
      "2025-12-07 17:44:20.920387: Current learning rate: 0.00867\n",
      "2025-12-07 17:46:39.014913: train_loss -0.816\n",
      "2025-12-07 17:46:39.014913: val_loss -0.8485\n",
      "2025-12-07 17:46:39.017622: Pseudo dice [0.9127, 0.9456, 0.9335]\n",
      "2025-12-07 17:46:39.017622: Epoch time: 138.1 s\n",
      "2025-12-07 17:46:39.017622: Yayy! New best EMA pseudo Dice: 0.9229\n",
      "2025-12-07 17:46:39.930253: \n",
      "2025-12-07 17:46:39.930253: Epoch 148\n",
      "2025-12-07 17:46:39.932803: Current learning rate: 0.00866\n",
      "2025-12-07 17:48:57.969177: train_loss -0.8076\n",
      "2025-12-07 17:48:57.969177: val_loss -0.8384\n",
      "2025-12-07 17:48:57.976707: Pseudo dice [0.9076, 0.95, 0.9251]\n",
      "2025-12-07 17:48:57.978712: Epoch time: 138.04 s\n",
      "2025-12-07 17:48:57.978712: Yayy! New best EMA pseudo Dice: 0.9234\n",
      "2025-12-07 17:48:58.901242: \n",
      "2025-12-07 17:48:58.901242: Epoch 149\n",
      "2025-12-07 17:48:58.903246: Current learning rate: 0.00865\n",
      "2025-12-07 17:51:16.936489: train_loss -0.8105\n",
      "2025-12-07 17:51:16.938336: val_loss -0.8429\n",
      "2025-12-07 17:51:16.940338: Pseudo dice [0.9131, 0.9449, 0.9286]\n",
      "2025-12-07 17:51:16.942340: Epoch time: 138.04 s\n",
      "2025-12-07 17:51:17.192648: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2025-12-07 17:51:18.294205: \n",
      "2025-12-07 17:51:18.295945: Epoch 150\n",
      "2025-12-07 17:51:18.298096: Current learning rate: 0.00864\n",
      "2025-12-07 17:53:36.215772: train_loss -0.8147\n",
      "2025-12-07 17:53:36.216774: val_loss -0.8351\n",
      "2025-12-07 17:53:36.217776: Pseudo dice [0.905, 0.9453, 0.9263]\n",
      "2025-12-07 17:53:36.217776: Epoch time: 137.92 s\n",
      "2025-12-07 17:53:36.217776: Yayy! New best EMA pseudo Dice: 0.9241\n",
      "2025-12-07 17:53:37.148480: \n",
      "2025-12-07 17:53:37.148480: Epoch 151\n",
      "2025-12-07 17:53:37.150528: Current learning rate: 0.00863\n",
      "2025-12-07 17:55:55.216359: train_loss -0.821\n",
      "2025-12-07 17:55:55.217359: val_loss -0.8297\n",
      "2025-12-07 17:55:55.219403: Pseudo dice [0.9043, 0.9366, 0.9287]\n",
      "2025-12-07 17:55:55.221403: Epoch time: 138.07 s\n",
      "2025-12-07 17:55:55.883945: \n",
      "2025-12-07 17:55:55.883945: Epoch 152\n",
      "2025-12-07 17:55:55.886334: Current learning rate: 0.00862\n",
      "2025-12-07 17:58:13.901354: train_loss -0.8151\n",
      "2025-12-07 17:58:13.901354: val_loss -0.8464\n",
      "2025-12-07 17:58:13.903094: Pseudo dice [0.9128, 0.9509, 0.9303]\n",
      "2025-12-07 17:58:13.905096: Epoch time: 138.02 s\n",
      "2025-12-07 17:58:13.907099: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2025-12-07 17:58:14.812714: \n",
      "2025-12-07 17:58:14.812714: Epoch 153\n",
      "2025-12-07 17:58:14.824982: Current learning rate: 0.00861\n",
      "2025-12-07 18:00:32.796708: train_loss -0.8181\n",
      "2025-12-07 18:00:32.796708: val_loss -0.8428\n",
      "2025-12-07 18:00:32.805098: Pseudo dice [0.9102, 0.9464, 0.9244]\n",
      "2025-12-07 18:00:32.805098: Epoch time: 137.98 s\n",
      "2025-12-07 18:00:32.805098: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2025-12-07 18:00:33.717705: \n",
      "2025-12-07 18:00:33.717705: Epoch 154\n",
      "2025-12-07 18:00:33.717705: Current learning rate: 0.0086\n",
      "2025-12-07 18:02:51.797120: train_loss -0.8096\n",
      "2025-12-07 18:02:51.799124: val_loss -0.8315\n",
      "2025-12-07 18:02:51.801126: Pseudo dice [0.9022, 0.9381, 0.9271]\n",
      "2025-12-07 18:02:51.801126: Epoch time: 138.08 s\n",
      "2025-12-07 18:02:52.686293: \n",
      "2025-12-07 18:02:52.687296: Epoch 155\n",
      "2025-12-07 18:02:52.689314: Current learning rate: 0.00859\n",
      "2025-12-07 18:05:10.515537: train_loss -0.8249\n",
      "2025-12-07 18:05:10.515537: val_loss -0.84\n",
      "2025-12-07 18:05:10.515537: Pseudo dice [0.908, 0.9443, 0.9323]\n",
      "2025-12-07 18:05:10.524654: Epoch time: 137.83 s\n",
      "2025-12-07 18:05:10.524654: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2025-12-07 18:05:11.445362: \n",
      "2025-12-07 18:05:11.445362: Epoch 156\n",
      "2025-12-07 18:05:11.445362: Current learning rate: 0.00858\n",
      "2025-12-07 18:07:29.468419: train_loss -0.8198\n",
      "2025-12-07 18:07:29.468419: val_loss -0.8294\n",
      "2025-12-07 18:07:29.468419: Pseudo dice [0.9018, 0.9448, 0.9306]\n",
      "2025-12-07 18:07:29.482199: Epoch time: 138.02 s\n",
      "2025-12-07 18:07:29.484202: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2025-12-07 18:07:30.390563: \n",
      "2025-12-07 18:07:30.390563: Epoch 157\n",
      "2025-12-07 18:07:30.406241: Current learning rate: 0.00858\n",
      "2025-12-07 18:09:48.483290: train_loss -0.8227\n",
      "2025-12-07 18:09:48.484290: val_loss -0.8407\n",
      "2025-12-07 18:09:48.487290: Pseudo dice [0.9129, 0.9473, 0.926]\n",
      "2025-12-07 18:09:48.489291: Epoch time: 138.09 s\n",
      "2025-12-07 18:09:48.492486: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2025-12-07 18:09:49.572466: \n",
      "2025-12-07 18:09:49.572466: Epoch 158\n",
      "2025-12-07 18:09:49.572466: Current learning rate: 0.00857\n",
      "2025-12-07 18:12:07.422258: train_loss -0.8202\n",
      "2025-12-07 18:12:07.423407: val_loss -0.8358\n",
      "2025-12-07 18:12:07.425417: Pseudo dice [0.9077, 0.9398, 0.9263]\n",
      "2025-12-07 18:12:07.428424: Epoch time: 137.86 s\n",
      "2025-12-07 18:12:08.084292: \n",
      "2025-12-07 18:12:08.084292: Epoch 159\n",
      "2025-12-07 18:12:08.087030: Current learning rate: 0.00856\n",
      "2025-12-07 18:14:26.050752: train_loss -0.8172\n",
      "2025-12-07 18:14:26.050752: val_loss -0.8353\n",
      "2025-12-07 18:14:26.062822: Pseudo dice [0.9036, 0.9388, 0.9309]\n",
      "2025-12-07 18:14:26.062822: Epoch time: 137.97 s\n",
      "2025-12-07 18:14:26.719042: \n",
      "2025-12-07 18:14:26.719042: Epoch 160\n",
      "2025-12-07 18:14:26.734987: Current learning rate: 0.00855\n",
      "2025-12-07 18:16:44.701471: train_loss -0.8114\n",
      "2025-12-07 18:16:44.701471: val_loss -0.8426\n",
      "2025-12-07 18:16:44.701471: Pseudo dice [0.9114, 0.9477, 0.927]\n",
      "2025-12-07 18:16:44.701471: Epoch time: 137.98 s\n",
      "2025-12-07 18:16:44.701471: Yayy! New best EMA pseudo Dice: 0.9256\n",
      "2025-12-07 18:16:45.951902: \n",
      "2025-12-07 18:16:45.951902: Epoch 161\n",
      "2025-12-07 18:16:45.969745: Current learning rate: 0.00854\n",
      "2025-12-07 18:19:03.939879: train_loss -0.821\n",
      "2025-12-07 18:19:03.939879: val_loss -0.8272\n",
      "2025-12-07 18:19:03.942886: Pseudo dice [0.8942, 0.9334, 0.9273]\n",
      "2025-12-07 18:19:03.944944: Epoch time: 137.99 s\n",
      "2025-12-07 18:19:04.607598: \n",
      "2025-12-07 18:19:04.607598: Epoch 162\n",
      "2025-12-07 18:19:04.608600: Current learning rate: 0.00853\n",
      "2025-12-07 18:21:22.592535: train_loss -0.8177\n",
      "2025-12-07 18:21:22.592535: val_loss -0.8321\n",
      "2025-12-07 18:21:22.592535: Pseudo dice [0.908, 0.9433, 0.9248]\n",
      "2025-12-07 18:21:22.598449: Epoch time: 137.99 s\n",
      "2025-12-07 18:21:23.233416: \n",
      "2025-12-07 18:21:23.233416: Epoch 163\n",
      "2025-12-07 18:21:23.249347: Current learning rate: 0.00852\n",
      "2025-12-07 18:23:41.187200: train_loss -0.8207\n",
      "2025-12-07 18:23:41.188203: val_loss -0.8483\n",
      "2025-12-07 18:23:41.190207: Pseudo dice [0.9087, 0.9471, 0.9393]\n",
      "2025-12-07 18:23:41.192462: Epoch time: 137.95 s\n",
      "2025-12-07 18:23:41.960069: \n",
      "2025-12-07 18:23:41.961072: Epoch 164\n",
      "2025-12-07 18:23:41.963077: Current learning rate: 0.00851\n",
      "2025-12-07 18:26:00.074785: train_loss -0.8141\n",
      "2025-12-07 18:26:00.074785: val_loss -0.8365\n",
      "2025-12-07 18:26:00.077102: Pseudo dice [0.9054, 0.9399, 0.9308]\n",
      "2025-12-07 18:26:00.079104: Epoch time: 138.12 s\n",
      "2025-12-07 18:26:00.733460: \n",
      "2025-12-07 18:26:00.733460: Epoch 165\n",
      "2025-12-07 18:26:00.733460: Current learning rate: 0.0085\n",
      "2025-12-07 18:28:18.824225: train_loss -0.8197\n",
      "2025-12-07 18:28:18.824225: val_loss -0.8272\n",
      "2025-12-07 18:28:18.827729: Pseudo dice [0.9012, 0.9337, 0.9263]\n",
      "2025-12-07 18:28:18.829807: Epoch time: 138.09 s\n",
      "2025-12-07 18:28:19.477584: \n",
      "2025-12-07 18:28:19.477584: Epoch 166\n",
      "2025-12-07 18:28:19.477584: Current learning rate: 0.00849\n",
      "2025-12-07 18:30:37.590889: train_loss -0.816\n",
      "2025-12-07 18:30:37.591889: val_loss -0.8428\n",
      "2025-12-07 18:30:37.595044: Pseudo dice [0.907, 0.9432, 0.9294]\n",
      "2025-12-07 18:30:37.596044: Epoch time: 138.12 s\n",
      "2025-12-07 18:30:38.498636: \n",
      "2025-12-07 18:30:38.498636: Epoch 167\n",
      "2025-12-07 18:30:38.505867: Current learning rate: 0.00848\n",
      "2025-12-07 18:32:56.404827: train_loss -0.8164\n",
      "2025-12-07 18:32:56.404827: val_loss -0.8412\n",
      "2025-12-07 18:32:56.404827: Pseudo dice [0.9132, 0.9438, 0.9343]\n",
      "2025-12-07 18:32:56.420504: Epoch time: 137.91 s\n",
      "2025-12-07 18:32:56.420504: Yayy! New best EMA pseudo Dice: 0.9257\n",
      "2025-12-07 18:32:57.351302: \n",
      "2025-12-07 18:32:57.353304: Epoch 168\n",
      "2025-12-07 18:32:57.353304: Current learning rate: 0.00847\n",
      "2025-12-07 18:35:15.244566: train_loss -0.8269\n",
      "2025-12-07 18:35:15.244566: val_loss -0.849\n",
      "2025-12-07 18:35:15.248570: Pseudo dice [0.9148, 0.9472, 0.9373]\n",
      "2025-12-07 18:35:15.250572: Epoch time: 137.89 s\n",
      "2025-12-07 18:35:15.252575: Yayy! New best EMA pseudo Dice: 0.9265\n",
      "2025-12-07 18:35:16.213330: \n",
      "2025-12-07 18:35:16.213330: Epoch 169\n",
      "2025-12-07 18:35:16.213330: Current learning rate: 0.00847\n",
      "2025-12-07 18:37:34.295702: train_loss -0.8229\n",
      "2025-12-07 18:37:34.295702: val_loss -0.8571\n",
      "2025-12-07 18:37:34.295702: Pseudo dice [0.9244, 0.9495, 0.9374]\n",
      "2025-12-07 18:37:34.295702: Epoch time: 138.08 s\n",
      "2025-12-07 18:37:34.311500: Yayy! New best EMA pseudo Dice: 0.9275\n",
      "2025-12-07 18:37:35.279917: \n",
      "2025-12-07 18:37:35.279917: Epoch 170\n",
      "2025-12-07 18:37:35.279917: Current learning rate: 0.00846\n",
      "2025-12-07 18:39:53.562235: train_loss -0.8204\n",
      "2025-12-07 18:39:53.578196: val_loss -0.8447\n",
      "2025-12-07 18:39:53.578196: Pseudo dice [0.9127, 0.9456, 0.9264]\n",
      "2025-12-07 18:39:53.578196: Epoch time: 138.28 s\n",
      "2025-12-07 18:39:53.578196: Yayy! New best EMA pseudo Dice: 0.9276\n",
      "2025-12-07 18:39:54.486213: \n",
      "2025-12-07 18:39:54.486213: Epoch 171\n",
      "2025-12-07 18:39:54.489742: Current learning rate: 0.00845\n",
      "2025-12-07 18:42:12.404204: train_loss -0.818\n",
      "2025-12-07 18:42:12.404204: val_loss -0.8347\n",
      "2025-12-07 18:42:12.407221: Pseudo dice [0.9028, 0.9419, 0.9343]\n",
      "2025-12-07 18:42:12.408319: Epoch time: 137.92 s\n",
      "2025-12-07 18:42:13.045727: \n",
      "2025-12-07 18:42:13.045727: Epoch 172\n",
      "2025-12-07 18:42:13.045727: Current learning rate: 0.00844\n",
      "2025-12-07 18:44:31.201588: train_loss -0.8208\n",
      "2025-12-07 18:44:31.203329: val_loss -0.8364\n",
      "2025-12-07 18:44:31.203329: Pseudo dice [0.9068, 0.9445, 0.9271]\n",
      "2025-12-07 18:44:31.203329: Epoch time: 138.16 s\n",
      "2025-12-07 18:44:32.171478: \n",
      "2025-12-07 18:44:32.171478: Epoch 173\n",
      "2025-12-07 18:44:32.171478: Current learning rate: 0.00843\n",
      "2025-12-07 18:46:50.074674: train_loss -0.8237\n",
      "2025-12-07 18:46:50.074674: val_loss -0.846\n",
      "2025-12-07 18:46:50.078722: Pseudo dice [0.9114, 0.9458, 0.9368]\n",
      "2025-12-07 18:46:50.080723: Epoch time: 137.9 s\n",
      "2025-12-07 18:46:50.083727: Yayy! New best EMA pseudo Dice: 0.9278\n",
      "2025-12-07 18:46:50.999908: \n",
      "2025-12-07 18:46:50.999908: Epoch 174\n",
      "2025-12-07 18:46:50.999908: Current learning rate: 0.00842\n",
      "2025-12-07 18:49:09.075900: train_loss -0.8222\n",
      "2025-12-07 18:49:09.077402: val_loss -0.8432\n",
      "2025-12-07 18:49:09.079404: Pseudo dice [0.9121, 0.9439, 0.9297]\n",
      "2025-12-07 18:49:09.079404: Epoch time: 138.08 s\n",
      "2025-12-07 18:49:09.082844: Yayy! New best EMA pseudo Dice: 0.9278\n",
      "2025-12-07 18:49:09.998808: \n",
      "2025-12-07 18:49:09.998808: Epoch 175\n",
      "2025-12-07 18:49:09.998808: Current learning rate: 0.00841\n",
      "2025-12-07 18:51:28.003251: train_loss -0.8225\n",
      "2025-12-07 18:51:28.003251: val_loss -0.8305\n",
      "2025-12-07 18:51:28.006258: Pseudo dice [0.9037, 0.9343, 0.9351]\n",
      "2025-12-07 18:51:28.008262: Epoch time: 138.01 s\n",
      "2025-12-07 18:51:28.777461: \n",
      "2025-12-07 18:51:28.777461: Epoch 176\n",
      "2025-12-07 18:51:28.780472: Current learning rate: 0.0084\n",
      "2025-12-07 18:53:46.807074: train_loss -0.82\n",
      "2025-12-07 18:53:46.808074: val_loss -0.8381\n",
      "2025-12-07 18:53:46.810096: Pseudo dice [0.9015, 0.9411, 0.9418]\n",
      "2025-12-07 18:53:46.811098: Epoch time: 138.03 s\n",
      "2025-12-07 18:53:47.467961: \n",
      "2025-12-07 18:53:47.467961: Epoch 177\n",
      "2025-12-07 18:53:47.467961: Current learning rate: 0.00839\n",
      "2025-12-07 18:56:05.474582: train_loss -0.8167\n",
      "2025-12-07 18:56:05.474582: val_loss -0.8488\n",
      "2025-12-07 18:56:05.477583: Pseudo dice [0.9109, 0.9509, 0.9309]\n",
      "2025-12-07 18:56:05.479593: Epoch time: 138.01 s\n",
      "2025-12-07 18:56:05.481598: Yayy! New best EMA pseudo Dice: 0.9279\n",
      "2025-12-07 18:56:06.577782: \n",
      "2025-12-07 18:56:06.577782: Epoch 178\n",
      "2025-12-07 18:56:06.577782: Current learning rate: 0.00838\n",
      "2025-12-07 18:58:24.343248: train_loss -0.8201\n",
      "2025-12-07 18:58:24.343248: val_loss -0.8374\n",
      "2025-12-07 18:58:24.347252: Pseudo dice [0.9085, 0.9379, 0.93]\n",
      "2025-12-07 18:58:24.347252: Epoch time: 137.77 s\n",
      "2025-12-07 18:58:25.117040: \n",
      "2025-12-07 18:58:25.117040: Epoch 179\n",
      "2025-12-07 18:58:25.117040: Current learning rate: 0.00837\n",
      "2025-12-07 19:00:43.133048: train_loss -0.8219\n",
      "2025-12-07 19:00:43.134050: val_loss -0.8527\n",
      "2025-12-07 19:00:43.137057: Pseudo dice [0.9169, 0.9428, 0.9412]\n",
      "2025-12-07 19:00:43.139068: Epoch time: 138.02 s\n",
      "2025-12-07 19:00:43.139068: Yayy! New best EMA pseudo Dice: 0.9282\n",
      "2025-12-07 19:00:44.030402: \n",
      "2025-12-07 19:00:44.030402: Epoch 180\n",
      "2025-12-07 19:00:44.046318: Current learning rate: 0.00836\n",
      "2025-12-07 19:03:02.109035: train_loss -0.8195\n",
      "2025-12-07 19:03:02.109035: val_loss -0.8392\n",
      "2025-12-07 19:03:02.109035: Pseudo dice [0.9078, 0.9451, 0.9347]\n",
      "2025-12-07 19:03:02.125158: Epoch time: 138.08 s\n",
      "2025-12-07 19:03:02.127405: Yayy! New best EMA pseudo Dice: 0.9283\n",
      "2025-12-07 19:03:03.071138: \n",
      "2025-12-07 19:03:03.071138: Epoch 181\n",
      "2025-12-07 19:03:03.071138: Current learning rate: 0.00836\n",
      "2025-12-07 19:05:20.875661: train_loss -0.8243\n",
      "2025-12-07 19:05:20.875661: val_loss -0.8422\n",
      "2025-12-07 19:05:20.881678: Pseudo dice [0.9062, 0.9414, 0.939]\n",
      "2025-12-07 19:05:20.885689: Epoch time: 137.81 s\n",
      "2025-12-07 19:05:20.887496: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2025-12-07 19:05:22.007797: \n",
      "2025-12-07 19:05:22.007797: Epoch 182\n",
      "2025-12-07 19:05:22.009802: Current learning rate: 0.00835\n",
      "2025-12-07 19:07:40.175119: train_loss -0.8224\n",
      "2025-12-07 19:07:40.175119: val_loss -0.8345\n",
      "2025-12-07 19:07:40.178126: Pseudo dice [0.9069, 0.9406, 0.9318]\n",
      "2025-12-07 19:07:40.180132: Epoch time: 138.17 s\n",
      "2025-12-07 19:07:40.811682: \n",
      "2025-12-07 19:07:40.811682: Epoch 183\n",
      "2025-12-07 19:07:40.811682: Current learning rate: 0.00834\n",
      "2025-12-07 19:09:58.655071: train_loss -0.8258\n",
      "2025-12-07 19:09:58.655071: val_loss -0.8466\n",
      "2025-12-07 19:09:58.655071: Pseudo dice [0.91, 0.9447, 0.9372]\n",
      "2025-12-07 19:09:58.655071: Epoch time: 137.84 s\n",
      "2025-12-07 19:09:58.655071: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2025-12-07 19:09:59.749969: \n",
      "2025-12-07 19:09:59.749969: Epoch 184\n",
      "2025-12-07 19:09:59.749969: Current learning rate: 0.00833\n",
      "2025-12-07 19:12:17.827423: train_loss -0.8188\n",
      "2025-12-07 19:12:17.833464: val_loss -0.8451\n",
      "2025-12-07 19:12:17.835465: Pseudo dice [0.9057, 0.9461, 0.9348]\n",
      "2025-12-07 19:12:17.837468: Epoch time: 138.08 s\n",
      "2025-12-07 19:12:17.839470: Yayy! New best EMA pseudo Dice: 0.9285\n",
      "2025-12-07 19:12:18.853768: \n",
      "2025-12-07 19:12:18.853768: Epoch 185\n",
      "2025-12-07 19:12:18.856777: Current learning rate: 0.00832\n",
      "2025-12-07 19:14:36.909649: train_loss -0.8167\n",
      "2025-12-07 19:14:36.909649: val_loss -0.8371\n",
      "2025-12-07 19:14:36.913653: Pseudo dice [0.9056, 0.9451, 0.9298]\n",
      "2025-12-07 19:14:36.915655: Epoch time: 138.06 s\n",
      "2025-12-07 19:14:37.566916: \n",
      "2025-12-07 19:14:37.568918: Epoch 186\n",
      "2025-12-07 19:14:37.568918: Current learning rate: 0.00831\n",
      "2025-12-07 19:16:55.717859: train_loss -0.816\n",
      "2025-12-07 19:16:55.717859: val_loss -0.8362\n",
      "2025-12-07 19:16:55.717859: Pseudo dice [0.9042, 0.9403, 0.9279]\n",
      "2025-12-07 19:16:55.717859: Epoch time: 138.15 s\n",
      "2025-12-07 19:16:56.367774: \n",
      "2025-12-07 19:16:56.367774: Epoch 187\n",
      "2025-12-07 19:16:56.367774: Current learning rate: 0.0083\n",
      "2025-12-07 19:19:14.427472: train_loss -0.8212\n",
      "2025-12-07 19:19:14.429475: val_loss -0.8554\n",
      "2025-12-07 19:19:14.431478: Pseudo dice [0.9188, 0.9529, 0.9318]\n",
      "2025-12-07 19:19:14.433481: Epoch time: 138.06 s\n",
      "2025-12-07 19:19:14.435483: Yayy! New best EMA pseudo Dice: 0.9286\n",
      "2025-12-07 19:19:15.373972: \n",
      "2025-12-07 19:19:15.373972: Epoch 188\n",
      "2025-12-07 19:19:15.377150: Current learning rate: 0.00829\n",
      "2025-12-07 19:21:33.502789: train_loss -0.8243\n",
      "2025-12-07 19:21:33.502789: val_loss -0.8516\n",
      "2025-12-07 19:21:33.506793: Pseudo dice [0.9168, 0.9495, 0.9336]\n",
      "2025-12-07 19:21:33.508794: Epoch time: 138.13 s\n",
      "2025-12-07 19:21:33.511840: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2025-12-07 19:21:34.427148: \n",
      "2025-12-07 19:21:34.428889: Epoch 189\n",
      "2025-12-07 19:21:34.428889: Current learning rate: 0.00828\n",
      "2025-12-07 19:23:53.672543: train_loss -0.8254\n",
      "2025-12-07 19:23:53.673545: val_loss -0.8502\n",
      "2025-12-07 19:23:53.676548: Pseudo dice [0.9103, 0.9486, 0.9401]\n",
      "2025-12-07 19:23:53.678548: Epoch time: 139.25 s\n",
      "2025-12-07 19:23:53.680549: Yayy! New best EMA pseudo Dice: 0.9294\n",
      "2025-12-07 19:23:54.772289: \n",
      "2025-12-07 19:23:54.772289: Epoch 190\n",
      "2025-12-07 19:23:54.772289: Current learning rate: 0.00827\n",
      "2025-12-07 19:26:13.140501: train_loss -0.8251\n",
      "2025-12-07 19:26:13.140501: val_loss -0.8374\n",
      "2025-12-07 19:26:13.140501: Pseudo dice [0.9026, 0.9453, 0.9319]\n",
      "2025-12-07 19:26:13.148446: Epoch time: 138.37 s\n",
      "2025-12-07 19:26:13.967290: \n",
      "2025-12-07 19:26:13.967290: Epoch 191\n",
      "2025-12-07 19:26:13.972101: Current learning rate: 0.00826\n",
      "2025-12-07 19:28:32.276417: train_loss -0.8236\n",
      "2025-12-07 19:28:32.276417: val_loss -0.8456\n",
      "2025-12-07 19:28:32.282434: Pseudo dice [0.9068, 0.9443, 0.9371]\n",
      "2025-12-07 19:28:32.286445: Epoch time: 138.31 s\n",
      "2025-12-07 19:28:32.954730: \n",
      "2025-12-07 19:28:32.954730: Epoch 192\n",
      "2025-12-07 19:28:32.957001: Current learning rate: 0.00825\n",
      "2025-12-07 19:30:51.385888: train_loss -0.8112\n",
      "2025-12-07 19:30:51.387890: val_loss -0.8407\n",
      "2025-12-07 19:30:51.389392: Pseudo dice [0.9098, 0.9455, 0.929]\n",
      "2025-12-07 19:30:51.389392: Epoch time: 138.43 s\n",
      "2025-12-07 19:30:52.060239: \n",
      "2025-12-07 19:30:52.060239: Epoch 193\n",
      "2025-12-07 19:30:52.061743: Current learning rate: 0.00824\n",
      "2025-12-07 19:33:10.416735: train_loss -0.8228\n",
      "2025-12-07 19:33:10.416735: val_loss -0.8545\n",
      "2025-12-07 19:33:10.418738: Pseudo dice [0.9146, 0.9489, 0.9404]\n",
      "2025-12-07 19:33:10.420479: Epoch time: 138.36 s\n",
      "2025-12-07 19:33:10.420479: Yayy! New best EMA pseudo Dice: 0.9296\n",
      "2025-12-07 19:33:11.365000: \n",
      "2025-12-07 19:33:11.366005: Epoch 194\n",
      "2025-12-07 19:33:11.367747: Current learning rate: 0.00824\n",
      "2025-12-07 19:35:29.217067: train_loss -0.8256\n",
      "2025-12-07 19:35:29.217067: val_loss -0.833\n",
      "2025-12-07 19:35:29.234911: Pseudo dice [0.8962, 0.9376, 0.9317]\n",
      "2025-12-07 19:35:29.236913: Epoch time: 137.85 s\n",
      "2025-12-07 19:35:29.889876: \n",
      "2025-12-07 19:35:29.889876: Epoch 195\n",
      "2025-12-07 19:35:29.889876: Current learning rate: 0.00823\n",
      "2025-12-07 19:37:47.970228: train_loss -0.8265\n",
      "2025-12-07 19:37:47.970228: val_loss -0.8417\n",
      "2025-12-07 19:37:47.972231: Pseudo dice [0.9111, 0.9429, 0.9266]\n",
      "2025-12-07 19:37:47.972231: Epoch time: 138.08 s\n",
      "2025-12-07 19:37:48.889841: \n",
      "2025-12-07 19:37:48.889841: Epoch 196\n",
      "2025-12-07 19:37:48.889841: Current learning rate: 0.00822\n",
      "2025-12-07 19:40:07.002556: train_loss -0.8238\n",
      "2025-12-07 19:40:07.002556: val_loss -0.8395\n",
      "2025-12-07 19:40:07.005558: Pseudo dice [0.9061, 0.9477, 0.9338]\n",
      "2025-12-07 19:40:07.007559: Epoch time: 138.11 s\n",
      "2025-12-07 19:40:07.765582: \n",
      "2025-12-07 19:40:07.765582: Epoch 197\n",
      "2025-12-07 19:40:07.781512: Current learning rate: 0.00821\n",
      "2025-12-07 19:42:25.835589: train_loss -0.82\n",
      "2025-12-07 19:42:25.835589: val_loss -0.8436\n",
      "2025-12-07 19:42:25.845140: Pseudo dice [0.9152, 0.945, 0.9288]\n",
      "2025-12-07 19:42:25.847142: Epoch time: 138.07 s\n",
      "2025-12-07 19:42:26.499300: \n",
      "2025-12-07 19:42:26.499300: Epoch 198\n",
      "2025-12-07 19:42:26.514967: Current learning rate: 0.0082\n",
      "2025-12-07 19:44:44.458312: train_loss -0.8279\n",
      "2025-12-07 19:44:44.459312: val_loss -0.8525\n",
      "2025-12-07 19:44:44.461313: Pseudo dice [0.9124, 0.9574, 0.9283]\n",
      "2025-12-07 19:44:44.464313: Epoch time: 137.96 s\n",
      "2025-12-07 19:44:45.123506: \n",
      "2025-12-07 19:44:45.123506: Epoch 199\n",
      "2025-12-07 19:44:45.123506: Current learning rate: 0.00819\n",
      "2025-12-07 19:47:02.950369: train_loss -0.8179\n",
      "2025-12-07 19:47:02.950369: val_loss -0.847\n",
      "2025-12-07 19:47:02.952371: Pseudo dice [0.9144, 0.9478, 0.928]\n",
      "2025-12-07 19:47:02.955392: Epoch time: 137.83 s\n",
      "2025-12-07 19:47:04.034540: \n",
      "2025-12-07 19:47:04.034540: Epoch 200\n",
      "2025-12-07 19:47:04.034540: Current learning rate: 0.00818\n",
      "2025-12-07 19:49:22.014951: train_loss -0.8212\n",
      "2025-12-07 19:49:22.014951: val_loss -0.835\n",
      "2025-12-07 19:49:22.024478: Pseudo dice [0.901, 0.9364, 0.9307]\n",
      "2025-12-07 19:49:22.027309: Epoch time: 137.98 s\n",
      "2025-12-07 19:49:22.846105: \n",
      "2025-12-07 19:49:22.847108: Epoch 201\n",
      "2025-12-07 19:49:22.849117: Current learning rate: 0.00817\n",
      "2025-12-07 19:51:40.733444: train_loss -0.8258\n",
      "2025-12-07 19:51:40.733444: val_loss -0.8512\n",
      "2025-12-07 19:51:40.740831: Pseudo dice [0.915, 0.949, 0.9327]\n",
      "2025-12-07 19:51:40.742833: Epoch time: 137.89 s\n",
      "2025-12-07 19:51:41.404788: \n",
      "2025-12-07 19:51:41.404788: Epoch 202\n",
      "2025-12-07 19:51:41.408677: Current learning rate: 0.00816\n",
      "2025-12-07 19:53:59.282296: train_loss -0.8259\n",
      "2025-12-07 19:53:59.284299: val_loss -0.8431\n",
      "2025-12-07 19:53:59.286301: Pseudo dice [0.9097, 0.9404, 0.9382]\n",
      "2025-12-07 19:53:59.290306: Epoch time: 137.89 s\n",
      "2025-12-07 19:54:00.061769: \n",
      "2025-12-07 19:54:00.061769: Epoch 203\n",
      "2025-12-07 19:54:00.077420: Current learning rate: 0.00815\n",
      "2025-12-07 19:56:18.106199: train_loss -0.8202\n",
      "2025-12-07 19:56:18.106199: val_loss -0.845\n",
      "2025-12-07 19:56:18.108218: Pseudo dice [0.9073, 0.9494, 0.935]\n",
      "2025-12-07 19:56:18.108218: Epoch time: 138.04 s\n",
      "2025-12-07 19:56:18.764354: \n",
      "2025-12-07 19:56:18.764354: Epoch 204\n",
      "2025-12-07 19:56:18.764354: Current learning rate: 0.00814\n",
      "2025-12-07 19:58:36.915333: train_loss -0.8178\n",
      "2025-12-07 19:58:36.917335: val_loss -0.838\n",
      "2025-12-07 19:58:36.921079: Pseudo dice [0.9059, 0.9406, 0.9339]\n",
      "2025-12-07 19:58:36.923083: Epoch time: 138.15 s\n",
      "2025-12-07 19:58:37.592671: \n",
      "2025-12-07 19:58:37.592671: Epoch 205\n",
      "2025-12-07 19:58:37.592671: Current learning rate: 0.00813\n",
      "2025-12-07 20:00:55.452054: train_loss -0.8209\n",
      "2025-12-07 20:00:55.452054: val_loss -0.853\n",
      "2025-12-07 20:00:55.457753: Pseudo dice [0.9157, 0.9495, 0.9339]\n",
      "2025-12-07 20:00:55.457753: Epoch time: 137.86 s\n",
      "2025-12-07 20:00:56.078774: \n",
      "2025-12-07 20:00:56.078774: Epoch 206\n",
      "2025-12-07 20:00:56.081788: Current learning rate: 0.00813\n",
      "2025-12-07 20:03:14.140695: train_loss -0.8227\n",
      "2025-12-07 20:03:14.140695: val_loss -0.8243\n",
      "2025-12-07 20:03:14.140695: Pseudo dice [0.8941, 0.9312, 0.9341]\n",
      "2025-12-07 20:03:14.146950: Epoch time: 138.06 s\n",
      "2025-12-07 20:03:14.780078: \n",
      "2025-12-07 20:03:14.780078: Epoch 207\n",
      "2025-12-07 20:03:14.780078: Current learning rate: 0.00812\n",
      "2025-12-07 20:05:32.722013: train_loss -0.7927\n",
      "2025-12-07 20:05:32.722013: val_loss -0.8266\n",
      "2025-12-07 20:05:32.722013: Pseudo dice [0.9038, 0.9395, 0.92]\n",
      "2025-12-07 20:05:32.722013: Epoch time: 137.94 s\n",
      "2025-12-07 20:05:33.548093: \n",
      "2025-12-07 20:05:33.548093: Epoch 208\n",
      "2025-12-07 20:05:33.548093: Current learning rate: 0.00811\n",
      "2025-12-07 20:07:51.458176: train_loss -0.7879\n",
      "2025-12-07 20:07:51.458176: val_loss -0.8138\n",
      "2025-12-07 20:07:51.461197: Pseudo dice [0.8964, 0.9343, 0.9236]\n",
      "2025-12-07 20:07:51.463201: Epoch time: 137.91 s\n",
      "2025-12-07 20:07:52.217158: \n",
      "2025-12-07 20:07:52.232817: Epoch 209\n",
      "2025-12-07 20:07:52.232817: Current learning rate: 0.0081\n",
      "2025-12-07 20:10:10.092541: train_loss -0.8041\n",
      "2025-12-07 20:10:10.092541: val_loss -0.8375\n",
      "2025-12-07 20:10:10.092541: Pseudo dice [0.905, 0.941, 0.9282]\n",
      "2025-12-07 20:10:10.092541: Epoch time: 137.88 s\n",
      "2025-12-07 20:10:10.727145: \n",
      "2025-12-07 20:10:10.727145: Epoch 210\n",
      "2025-12-07 20:10:10.729157: Current learning rate: 0.00809\n",
      "2025-12-07 20:12:28.689175: train_loss -0.8133\n",
      "2025-12-07 20:12:28.689175: val_loss -0.8391\n",
      "2025-12-07 20:12:28.689175: Pseudo dice [0.9145, 0.9437, 0.9288]\n",
      "2025-12-07 20:12:28.689175: Epoch time: 137.96 s\n",
      "2025-12-07 20:12:29.327861: \n",
      "2025-12-07 20:12:29.327861: Epoch 211\n",
      "2025-12-07 20:12:29.327861: Current learning rate: 0.00808\n",
      "2025-12-07 20:14:47.327923: train_loss -0.812\n",
      "2025-12-07 20:14:47.327923: val_loss -0.8363\n",
      "2025-12-07 20:14:47.327923: Pseudo dice [0.9096, 0.945, 0.9256]\n",
      "2025-12-07 20:14:47.327923: Epoch time: 138.0 s\n",
      "2025-12-07 20:14:47.973922: \n",
      "2025-12-07 20:14:47.973922: Epoch 212\n",
      "2025-12-07 20:14:47.973922: Current learning rate: 0.00807\n",
      "2025-12-07 20:17:06.132528: train_loss -0.812\n",
      "2025-12-07 20:17:06.133530: val_loss -0.8395\n",
      "2025-12-07 20:17:06.136547: Pseudo dice [0.9105, 0.9498, 0.9238]\n",
      "2025-12-07 20:17:06.138554: Epoch time: 138.16 s\n",
      "2025-12-07 20:17:06.749240: \n",
      "2025-12-07 20:17:06.749240: Epoch 213\n",
      "2025-12-07 20:17:06.749240: Current learning rate: 0.00806\n",
      "2025-12-07 20:19:24.578490: train_loss -0.8107\n",
      "2025-12-07 20:19:24.578490: val_loss -0.8473\n",
      "2025-12-07 20:19:24.594319: Pseudo dice [0.915, 0.9518, 0.9315]\n",
      "2025-12-07 20:19:24.594319: Epoch time: 137.83 s\n",
      "2025-12-07 20:19:25.486346: \n",
      "2025-12-07 20:19:25.486346: Epoch 214\n",
      "2025-12-07 20:19:25.489359: Current learning rate: 0.00805\n",
      "2025-12-07 20:21:43.358577: train_loss -0.8168\n",
      "2025-12-07 20:21:43.358577: val_loss -0.8381\n",
      "2025-12-07 20:21:43.358577: Pseudo dice [0.9071, 0.9456, 0.9228]\n",
      "2025-12-07 20:21:43.358577: Epoch time: 137.87 s\n",
      "2025-12-07 20:21:43.983007: \n",
      "2025-12-07 20:21:43.983007: Epoch 215\n",
      "2025-12-07 20:21:43.983007: Current learning rate: 0.00804\n",
      "2025-12-07 20:24:01.857908: train_loss -0.8182\n",
      "2025-12-07 20:24:01.857908: val_loss -0.8425\n",
      "2025-12-07 20:24:01.863152: Pseudo dice [0.9107, 0.9429, 0.9345]\n",
      "2025-12-07 20:24:01.865155: Epoch time: 137.87 s\n",
      "2025-12-07 20:24:02.499253: \n",
      "2025-12-07 20:24:02.499253: Epoch 216\n",
      "2025-12-07 20:24:02.499253: Current learning rate: 0.00803\n",
      "2025-12-07 20:26:20.530557: train_loss -0.825\n",
      "2025-12-07 20:26:20.530557: val_loss -0.8476\n",
      "2025-12-07 20:26:20.530557: Pseudo dice [0.9153, 0.9458, 0.9377]\n",
      "2025-12-07 20:26:20.530557: Epoch time: 138.03 s\n",
      "2025-12-07 20:26:21.311542: \n",
      "2025-12-07 20:26:21.311542: Epoch 217\n",
      "2025-12-07 20:26:21.327261: Current learning rate: 0.00802\n",
      "2025-12-07 20:28:39.421726: train_loss -0.8189\n",
      "2025-12-07 20:28:39.421726: val_loss -0.8397\n",
      "2025-12-07 20:28:39.421726: Pseudo dice [0.906, 0.9436, 0.9337]\n",
      "2025-12-07 20:28:39.421726: Epoch time: 138.11 s\n",
      "2025-12-07 20:28:40.045655: \n",
      "2025-12-07 20:28:40.045655: Epoch 218\n",
      "2025-12-07 20:28:40.061619: Current learning rate: 0.00801\n",
      "2025-12-07 20:30:57.906782: train_loss -0.8248\n",
      "2025-12-07 20:30:57.906782: val_loss -0.8412\n",
      "2025-12-07 20:30:57.906782: Pseudo dice [0.9059, 0.9461, 0.9345]\n",
      "2025-12-07 20:30:57.906782: Epoch time: 137.86 s\n",
      "2025-12-07 20:30:58.562199: \n",
      "2025-12-07 20:30:58.562199: Epoch 219\n",
      "2025-12-07 20:30:58.562199: Current learning rate: 0.00801\n",
      "2025-12-07 20:33:16.500032: train_loss -0.8201\n",
      "2025-12-07 20:33:16.500032: val_loss -0.8524\n",
      "2025-12-07 20:33:16.500032: Pseudo dice [0.9192, 0.9524, 0.9255]\n",
      "2025-12-07 20:33:16.505838: Epoch time: 137.95 s\n",
      "2025-12-07 20:33:17.452533: \n",
      "2025-12-07 20:33:17.452533: Epoch 220\n",
      "2025-12-07 20:33:17.468335: Current learning rate: 0.008\n",
      "2025-12-07 20:35:35.382198: train_loss -0.8239\n",
      "2025-12-07 20:35:35.382198: val_loss -0.8395\n",
      "2025-12-07 20:35:35.386201: Pseudo dice [0.9031, 0.938, 0.9363]\n",
      "2025-12-07 20:35:35.388203: Epoch time: 137.93 s\n",
      "2025-12-07 20:35:36.018170: \n",
      "2025-12-07 20:35:36.018170: Epoch 221\n",
      "2025-12-07 20:35:36.018170: Current learning rate: 0.00799\n",
      "2025-12-07 20:37:53.940255: train_loss -0.8247\n",
      "2025-12-07 20:37:53.940255: val_loss -0.8469\n",
      "2025-12-07 20:37:53.943261: Pseudo dice [0.9104, 0.9463, 0.94]\n",
      "2025-12-07 20:37:53.945273: Epoch time: 137.92 s\n",
      "2025-12-07 20:37:54.592773: \n",
      "2025-12-07 20:37:54.592773: Epoch 222\n",
      "2025-12-07 20:37:54.608532: Current learning rate: 0.00798\n",
      "2025-12-07 20:40:12.644933: train_loss -0.8229\n",
      "2025-12-07 20:40:12.644933: val_loss -0.8444\n",
      "2025-12-07 20:40:12.648402: Pseudo dice [0.9086, 0.9472, 0.9347]\n",
      "2025-12-07 20:40:12.650404: Epoch time: 138.05 s\n",
      "2025-12-07 20:40:13.375329: \n",
      "2025-12-07 20:40:13.375329: Epoch 223\n",
      "2025-12-07 20:40:13.375329: Current learning rate: 0.00797\n",
      "2025-12-07 20:42:31.167648: train_loss -0.8253\n",
      "2025-12-07 20:42:31.169650: val_loss -0.8679\n",
      "2025-12-07 20:42:31.172509: Pseudo dice [0.9209, 0.9526, 0.9439]\n",
      "2025-12-07 20:42:31.175515: Epoch time: 137.79 s\n",
      "2025-12-07 20:42:31.177521: Yayy! New best EMA pseudo Dice: 0.9298\n",
      "2025-12-07 20:42:32.060571: \n",
      "2025-12-07 20:42:32.060571: Epoch 224\n",
      "2025-12-07 20:42:32.061657: Current learning rate: 0.00796\n",
      "2025-12-07 20:44:51.216740: train_loss -0.8328\n",
      "2025-12-07 20:44:51.216740: val_loss -0.8551\n",
      "2025-12-07 20:44:51.220483: Pseudo dice [0.9164, 0.9462, 0.9407]\n",
      "2025-12-07 20:44:51.222485: Epoch time: 139.16 s\n",
      "2025-12-07 20:44:51.224486: Yayy! New best EMA pseudo Dice: 0.9303\n",
      "2025-12-07 20:44:52.128910: \n",
      "2025-12-07 20:44:52.130913: Epoch 225\n",
      "2025-12-07 20:44:52.130913: Current learning rate: 0.00795\n",
      "2025-12-07 20:47:10.608327: train_loss -0.8276\n",
      "2025-12-07 20:47:10.608327: val_loss -0.8577\n",
      "2025-12-07 20:47:10.608327: Pseudo dice [0.9198, 0.9495, 0.9364]\n",
      "2025-12-07 20:47:10.608327: Epoch time: 138.48 s\n",
      "2025-12-07 20:47:10.608327: Yayy! New best EMA pseudo Dice: 0.9308\n",
      "2025-12-07 20:47:11.779070: \n",
      "2025-12-07 20:47:11.779070: Epoch 226\n",
      "2025-12-07 20:47:11.795063: Current learning rate: 0.00794\n",
      "2025-12-07 20:49:30.182772: train_loss -0.8235\n",
      "2025-12-07 20:49:30.182772: val_loss -0.8511\n",
      "2025-12-07 20:49:30.186777: Pseudo dice [0.9103, 0.9449, 0.9419]\n",
      "2025-12-07 20:49:30.188016: Epoch time: 138.4 s\n",
      "2025-12-07 20:49:30.191017: Yayy! New best EMA pseudo Dice: 0.9309\n",
      "2025-12-07 20:49:31.077151: \n",
      "2025-12-07 20:49:31.077151: Epoch 227\n",
      "2025-12-07 20:49:31.077151: Current learning rate: 0.00793\n",
      "2025-12-07 20:51:49.391018: train_loss -0.8273\n",
      "2025-12-07 20:51:49.391018: val_loss -0.8516\n",
      "2025-12-07 20:51:49.406980: Pseudo dice [0.9151, 0.9473, 0.9363]\n",
      "2025-12-07 20:51:49.409112: Epoch time: 138.31 s\n",
      "2025-12-07 20:51:49.410115: Yayy! New best EMA pseudo Dice: 0.9311\n",
      "2025-12-07 20:51:50.297872: \n",
      "2025-12-07 20:51:50.297872: Epoch 228\n",
      "2025-12-07 20:51:50.297872: Current learning rate: 0.00792\n",
      "2025-12-07 20:54:08.764357: train_loss -0.8291\n",
      "2025-12-07 20:54:08.764357: val_loss -0.8466\n",
      "2025-12-07 20:54:08.773907: Pseudo dice [0.912, 0.9508, 0.9384]\n",
      "2025-12-07 20:54:08.775910: Epoch time: 138.47 s\n",
      "2025-12-07 20:54:08.777912: Yayy! New best EMA pseudo Dice: 0.9314\n",
      "2025-12-07 20:54:09.670253: \n",
      "2025-12-07 20:54:09.670253: Epoch 229\n",
      "2025-12-07 20:54:09.672210: Current learning rate: 0.00791\n",
      "2025-12-07 20:56:27.608752: train_loss -0.8284\n",
      "2025-12-07 20:56:27.608752: val_loss -0.8492\n",
      "2025-12-07 20:56:27.614924: Pseudo dice [0.9098, 0.9493, 0.9386]\n",
      "2025-12-07 20:56:27.617476: Epoch time: 137.94 s\n",
      "2025-12-07 20:56:27.619478: Yayy! New best EMA pseudo Dice: 0.9315\n",
      "2025-12-07 20:56:28.498840: \n",
      "2025-12-07 20:56:28.498840: Epoch 230\n",
      "2025-12-07 20:56:28.498840: Current learning rate: 0.0079\n",
      "2025-12-07 20:58:46.662389: train_loss -0.831\n",
      "2025-12-07 20:58:46.664391: val_loss -0.8531\n",
      "2025-12-07 20:58:46.666393: Pseudo dice [0.9158, 0.9479, 0.9331]\n",
      "2025-12-07 20:58:46.668395: Epoch time: 138.16 s\n",
      "2025-12-07 20:58:46.670397: Yayy! New best EMA pseudo Dice: 0.9316\n",
      "2025-12-07 20:58:47.546303: \n",
      "2025-12-07 20:58:47.546303: Epoch 231\n",
      "2025-12-07 20:58:47.546303: Current learning rate: 0.00789\n",
      "2025-12-07 21:01:05.630060: train_loss -0.8247\n",
      "2025-12-07 21:01:05.630060: val_loss -0.8691\n",
      "2025-12-07 21:01:05.635821: Pseudo dice [0.9274, 0.9574, 0.9387]\n",
      "2025-12-07 21:01:05.637825: Epoch time: 138.08 s\n",
      "2025-12-07 21:01:05.639827: Yayy! New best EMA pseudo Dice: 0.9325\n",
      "2025-12-07 21:01:06.701637: \n",
      "2025-12-07 21:01:06.701637: Epoch 232\n",
      "2025-12-07 21:01:06.717332: Current learning rate: 0.00789\n",
      "2025-12-07 21:03:24.753635: train_loss -0.8231\n",
      "2025-12-07 21:03:24.753635: val_loss -0.8546\n",
      "2025-12-07 21:03:24.753635: Pseudo dice [0.9126, 0.9485, 0.9373]\n",
      "2025-12-07 21:03:24.764627: Epoch time: 138.05 s\n",
      "2025-12-07 21:03:24.766629: Yayy! New best EMA pseudo Dice: 0.9326\n",
      "2025-12-07 21:03:25.642739: \n",
      "2025-12-07 21:03:25.642739: Epoch 233\n",
      "2025-12-07 21:03:25.642739: Current learning rate: 0.00788\n",
      "2025-12-07 21:05:43.634435: train_loss -0.8312\n",
      "2025-12-07 21:05:43.635440: val_loss -0.8547\n",
      "2025-12-07 21:05:43.638450: Pseudo dice [0.9127, 0.9514, 0.94]\n",
      "2025-12-07 21:05:43.640517: Epoch time: 138.0 s\n",
      "2025-12-07 21:05:43.642525: Yayy! New best EMA pseudo Dice: 0.9328\n",
      "2025-12-07 21:05:44.499591: \n",
      "2025-12-07 21:05:44.499591: Epoch 234\n",
      "2025-12-07 21:05:44.516526: Current learning rate: 0.00787\n",
      "2025-12-07 21:08:02.615041: train_loss -0.8266\n",
      "2025-12-07 21:08:02.616041: val_loss -0.852\n",
      "2025-12-07 21:08:02.619042: Pseudo dice [0.916, 0.9464, 0.9354]\n",
      "2025-12-07 21:08:02.621043: Epoch time: 138.12 s\n",
      "2025-12-07 21:08:03.234133: \n",
      "2025-12-07 21:08:03.234133: Epoch 235\n",
      "2025-12-07 21:08:03.234133: Current learning rate: 0.00786\n",
      "2025-12-07 21:10:21.171710: train_loss -0.8297\n",
      "2025-12-07 21:10:21.171710: val_loss -0.8555\n",
      "2025-12-07 21:10:21.185844: Pseudo dice [0.9181, 0.9475, 0.9358]\n",
      "2025-12-07 21:10:21.187584: Epoch time: 137.94 s\n",
      "2025-12-07 21:10:21.187584: Yayy! New best EMA pseudo Dice: 0.9329\n",
      "2025-12-07 21:10:22.080126: \n",
      "2025-12-07 21:10:22.081125: Epoch 236\n",
      "2025-12-07 21:10:22.083573: Current learning rate: 0.00785\n",
      "2025-12-07 21:12:40.093121: train_loss -0.8291\n",
      "2025-12-07 21:12:40.093121: val_loss -0.8501\n",
      "2025-12-07 21:12:40.093121: Pseudo dice [0.9124, 0.9446, 0.9373]\n",
      "2025-12-07 21:12:40.093121: Epoch time: 138.01 s\n",
      "2025-12-07 21:12:40.868225: \n",
      "2025-12-07 21:12:40.868225: Epoch 237\n",
      "2025-12-07 21:12:40.868225: Current learning rate: 0.00784\n",
      "2025-12-07 21:14:58.995024: train_loss -0.832\n",
      "2025-12-07 21:14:58.995024: val_loss -0.8583\n",
      "2025-12-07 21:14:58.999028: Pseudo dice [0.919, 0.9506, 0.9406]\n",
      "2025-12-07 21:14:58.999028: Epoch time: 138.13 s\n",
      "2025-12-07 21:14:59.002529: Yayy! New best EMA pseudo Dice: 0.9331\n",
      "2025-12-07 21:14:59.922078: \n",
      "2025-12-07 21:14:59.923152: Epoch 238\n",
      "2025-12-07 21:14:59.925920: Current learning rate: 0.00783\n",
      "2025-12-07 21:17:17.795784: train_loss -0.8296\n",
      "2025-12-07 21:17:17.795784: val_loss -0.8522\n",
      "2025-12-07 21:17:17.795784: Pseudo dice [0.9124, 0.949, 0.9357]\n",
      "2025-12-07 21:17:17.795784: Epoch time: 137.89 s\n",
      "2025-12-07 21:17:18.406173: \n",
      "2025-12-07 21:17:18.406173: Epoch 239\n",
      "2025-12-07 21:17:18.406173: Current learning rate: 0.00782\n",
      "2025-12-07 21:19:36.430578: train_loss -0.8295\n",
      "2025-12-07 21:19:36.430578: val_loss -0.8465\n",
      "2025-12-07 21:19:36.434582: Pseudo dice [0.9149, 0.944, 0.9293]\n",
      "2025-12-07 21:19:36.436322: Epoch time: 138.02 s\n",
      "2025-12-07 21:19:37.072423: \n",
      "2025-12-07 21:19:37.073533: Epoch 240\n",
      "2025-12-07 21:19:37.075712: Current learning rate: 0.00781\n",
      "2025-12-07 21:21:55.078055: train_loss -0.8257\n",
      "2025-12-07 21:21:55.078055: val_loss -0.8577\n",
      "2025-12-07 21:21:55.094397: Pseudo dice [0.9146, 0.9535, 0.94]\n",
      "2025-12-07 21:21:55.096411: Epoch time: 138.01 s\n",
      "2025-12-07 21:21:55.889596: \n",
      "2025-12-07 21:21:55.889596: Epoch 241\n",
      "2025-12-07 21:21:55.889596: Current learning rate: 0.0078\n",
      "2025-12-07 21:24:13.749816: train_loss -0.8265\n",
      "2025-12-07 21:24:13.765867: val_loss -0.8666\n",
      "2025-12-07 21:24:13.768970: Pseudo dice [0.923, 0.9535, 0.9402]\n",
      "2025-12-07 21:24:13.771008: Epoch time: 137.86 s\n",
      "2025-12-07 21:24:13.773008: Yayy! New best EMA pseudo Dice: 0.9336\n",
      "2025-12-07 21:24:14.655297: \n",
      "2025-12-07 21:24:14.655297: Epoch 242\n",
      "2025-12-07 21:24:14.655297: Current learning rate: 0.00779\n",
      "2025-12-07 21:26:32.713045: train_loss -0.8304\n",
      "2025-12-07 21:26:32.715048: val_loss -0.8574\n",
      "2025-12-07 21:26:32.718052: Pseudo dice [0.9213, 0.9473, 0.9347]\n",
      "2025-12-07 21:26:32.720979: Epoch time: 138.06 s\n",
      "2025-12-07 21:26:32.722980: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-07 21:26:33.627991: \n",
      "2025-12-07 21:26:33.627991: Epoch 243\n",
      "2025-12-07 21:26:33.627991: Current learning rate: 0.00778\n",
      "2025-12-07 21:28:51.654844: train_loss -0.8316\n",
      "2025-12-07 21:28:51.654844: val_loss -0.8624\n",
      "2025-12-07 21:28:51.657147: Pseudo dice [0.9188, 0.9469, 0.9374]\n",
      "2025-12-07 21:28:51.660646: Epoch time: 138.03 s\n",
      "2025-12-07 21:28:51.662648: Yayy! New best EMA pseudo Dice: 0.9338\n",
      "2025-12-07 21:28:52.860897: \n",
      "2025-12-07 21:28:52.860897: Epoch 244\n",
      "2025-12-07 21:28:52.863908: Current learning rate: 0.00777\n",
      "2025-12-07 21:31:10.946009: train_loss -0.8266\n",
      "2025-12-07 21:31:10.948010: val_loss -0.8491\n",
      "2025-12-07 21:31:10.950012: Pseudo dice [0.913, 0.9513, 0.9363]\n",
      "2025-12-07 21:31:10.951753: Epoch time: 138.09 s\n",
      "2025-12-07 21:31:11.578771: \n",
      "2025-12-07 21:31:11.578771: Epoch 245\n",
      "2025-12-07 21:31:11.578771: Current learning rate: 0.00777\n",
      "2025-12-07 21:33:29.528265: train_loss -0.8295\n",
      "2025-12-07 21:33:29.530268: val_loss -0.8488\n",
      "2025-12-07 21:33:29.532270: Pseudo dice [0.9108, 0.95, 0.9295]\n",
      "2025-12-07 21:33:29.536274: Epoch time: 137.95 s\n",
      "2025-12-07 21:33:30.170769: \n",
      "2025-12-07 21:33:30.170769: Epoch 246\n",
      "2025-12-07 21:33:30.170769: Current learning rate: 0.00776\n",
      "2025-12-07 21:35:48.139231: train_loss -0.8264\n",
      "2025-12-07 21:35:48.139231: val_loss -0.8604\n",
      "2025-12-07 21:35:48.139231: Pseudo dice [0.9196, 0.953, 0.9347]\n",
      "2025-12-07 21:35:48.149452: Epoch time: 137.97 s\n",
      "2025-12-07 21:35:48.904662: \n",
      "2025-12-07 21:35:48.904662: Epoch 247\n",
      "2025-12-07 21:35:48.920391: Current learning rate: 0.00775\n",
      "2025-12-07 21:38:06.749270: train_loss -0.8339\n",
      "2025-12-07 21:38:06.749270: val_loss -0.8532\n",
      "2025-12-07 21:38:06.749270: Pseudo dice [0.9145, 0.9486, 0.9385]\n",
      "2025-12-07 21:38:06.766527: Epoch time: 137.84 s\n",
      "2025-12-07 21:38:07.390110: \n",
      "2025-12-07 21:38:07.390110: Epoch 248\n",
      "2025-12-07 21:38:07.390110: Current learning rate: 0.00774\n",
      "2025-12-07 21:40:25.536451: train_loss -0.8294\n",
      "2025-12-07 21:40:25.536451: val_loss -0.8607\n",
      "2025-12-07 21:40:25.536451: Pseudo dice [0.9158, 0.9571, 0.9426]\n",
      "2025-12-07 21:40:25.546232: Epoch time: 138.15 s\n",
      "2025-12-07 21:40:25.546232: Yayy! New best EMA pseudo Dice: 0.9341\n",
      "2025-12-07 21:40:26.421183: \n",
      "2025-12-07 21:40:26.421183: Epoch 249\n",
      "2025-12-07 21:40:26.436961: Current learning rate: 0.00773\n",
      "2025-12-07 21:42:44.547129: train_loss -0.8326\n",
      "2025-12-07 21:42:44.547129: val_loss -0.853\n",
      "2025-12-07 21:42:44.551135: Pseudo dice [0.9163, 0.9482, 0.9326]\n",
      "2025-12-07 21:42:44.555140: Epoch time: 138.13 s\n",
      "2025-12-07 21:42:45.749716: \n",
      "2025-12-07 21:42:45.749716: Epoch 250\n",
      "2025-12-07 21:42:45.753219: Current learning rate: 0.00772\n",
      "2025-12-07 21:45:03.689371: train_loss -0.8337\n",
      "2025-12-07 21:45:03.689371: val_loss -0.8532\n",
      "2025-12-07 21:45:03.692379: Pseudo dice [0.9143, 0.9474, 0.935]\n",
      "2025-12-07 21:45:03.694385: Epoch time: 137.94 s\n",
      "2025-12-07 21:45:04.310545: \n",
      "2025-12-07 21:45:04.310545: Epoch 251\n",
      "2025-12-07 21:45:04.310545: Current learning rate: 0.00771\n",
      "2025-12-07 21:47:22.239588: train_loss -0.8346\n",
      "2025-12-07 21:47:22.240591: val_loss -0.853\n",
      "2025-12-07 21:47:22.243597: Pseudo dice [0.9118, 0.9472, 0.9397]\n",
      "2025-12-07 21:47:22.246603: Epoch time: 137.93 s\n",
      "2025-12-07 21:47:22.874994: \n",
      "2025-12-07 21:47:22.874994: Epoch 252\n",
      "2025-12-07 21:47:22.874994: Current learning rate: 0.0077\n",
      "2025-12-07 21:49:40.702409: train_loss -0.8323\n",
      "2025-12-07 21:49:40.704412: val_loss -0.8576\n",
      "2025-12-07 21:49:40.708416: Pseudo dice [0.917, 0.9464, 0.9402]\n",
      "2025-12-07 21:49:40.712426: Epoch time: 137.83 s\n",
      "2025-12-07 21:49:41.448214: \n",
      "2025-12-07 21:49:41.448214: Epoch 253\n",
      "2025-12-07 21:49:41.451221: Current learning rate: 0.00769\n",
      "2025-12-07 21:51:59.733950: train_loss -0.8307\n",
      "2025-12-07 21:51:59.735953: val_loss -0.8629\n",
      "2025-12-07 21:51:59.738920: Pseudo dice [0.9205, 0.9529, 0.9382]\n",
      "2025-12-07 21:51:59.741266: Epoch time: 138.29 s\n",
      "2025-12-07 21:52:00.358394: \n",
      "2025-12-07 21:52:00.358394: Epoch 254\n",
      "2025-12-07 21:52:00.374187: Current learning rate: 0.00768\n",
      "2025-12-07 21:54:18.464573: train_loss -0.8334\n",
      "2025-12-07 21:54:18.466575: val_loss -0.8455\n",
      "2025-12-07 21:54:18.468315: Pseudo dice [0.9108, 0.9452, 0.9349]\n",
      "2025-12-07 21:54:18.472320: Epoch time: 138.11 s\n",
      "2025-12-07 21:54:19.093534: \n",
      "2025-12-07 21:54:19.093534: Epoch 255\n",
      "2025-12-07 21:54:19.093534: Current learning rate: 0.00767\n",
      "2025-12-07 21:56:37.170746: train_loss -0.8322\n",
      "2025-12-07 21:56:37.170746: val_loss -0.8575\n",
      "2025-12-07 21:56:37.186826: Pseudo dice [0.9148, 0.9506, 0.9424]\n",
      "2025-12-07 21:56:37.186826: Epoch time: 138.08 s\n",
      "2025-12-07 21:56:38.109541: \n",
      "2025-12-07 21:56:38.109541: Epoch 256\n",
      "2025-12-07 21:56:38.109541: Current learning rate: 0.00766\n",
      "2025-12-07 21:58:57.668011: train_loss -0.8373\n",
      "2025-12-07 21:58:57.669012: val_loss -0.8641\n",
      "2025-12-07 21:58:57.671012: Pseudo dice [0.9174, 0.9476, 0.9475]\n",
      "2025-12-07 21:58:57.674949: Epoch time: 139.56 s\n",
      "2025-12-07 21:58:57.676754: Yayy! New best EMA pseudo Dice: 0.9343\n",
      "2025-12-07 21:58:58.561255: \n",
      "2025-12-07 21:58:58.561255: Epoch 257\n",
      "2025-12-07 21:58:58.561255: Current learning rate: 0.00765\n",
      "2025-12-07 22:01:17.124300: train_loss -0.8242\n",
      "2025-12-07 22:01:17.126112: val_loss -0.8474\n",
      "2025-12-07 22:01:17.128115: Pseudo dice [0.9088, 0.948, 0.9426]\n",
      "2025-12-07 22:01:17.128115: Epoch time: 138.56 s\n",
      "2025-12-07 22:01:17.761024: \n",
      "2025-12-07 22:01:17.761024: Epoch 258\n",
      "2025-12-07 22:01:17.762027: Current learning rate: 0.00764\n",
      "2025-12-07 22:03:36.327931: train_loss -0.8316\n",
      "2025-12-07 22:03:36.327931: val_loss -0.856\n",
      "2025-12-07 22:03:36.327931: Pseudo dice [0.915, 0.947, 0.9394]\n",
      "2025-12-07 22:03:36.327931: Epoch time: 138.58 s\n",
      "2025-12-07 22:03:37.043583: \n",
      "2025-12-07 22:03:37.043583: Epoch 259\n",
      "2025-12-07 22:03:37.045591: Current learning rate: 0.00764\n",
      "2025-12-07 22:05:55.436077: train_loss -0.8351\n",
      "2025-12-07 22:05:55.436077: val_loss -0.8547\n",
      "2025-12-07 22:05:55.436077: Pseudo dice [0.9109, 0.9485, 0.9398]\n",
      "2025-12-07 22:05:55.436077: Epoch time: 138.39 s\n",
      "2025-12-07 22:05:56.083239: \n",
      "2025-12-07 22:05:56.083239: Epoch 260\n",
      "2025-12-07 22:05:56.086251: Current learning rate: 0.00763\n",
      "2025-12-07 22:08:14.575604: train_loss -0.8283\n",
      "2025-12-07 22:08:14.577606: val_loss -0.8558\n",
      "2025-12-07 22:08:14.580910: Pseudo dice [0.9217, 0.9509, 0.9339]\n",
      "2025-12-07 22:08:14.582910: Epoch time: 138.49 s\n",
      "2025-12-07 22:08:15.226619: \n",
      "2025-12-07 22:08:15.226619: Epoch 261\n",
      "2025-12-07 22:08:15.229722: Current learning rate: 0.00762\n",
      "2025-12-07 22:10:33.435554: train_loss -0.8382\n",
      "2025-12-07 22:10:33.437295: val_loss -0.8504\n",
      "2025-12-07 22:10:33.441546: Pseudo dice [0.9148, 0.9483, 0.9308]\n",
      "2025-12-07 22:10:33.444547: Epoch time: 138.21 s\n",
      "2025-12-07 22:10:34.298403: \n",
      "2025-12-07 22:10:34.298403: Epoch 262\n",
      "2025-12-07 22:10:34.298403: Current learning rate: 0.00761\n",
      "2025-12-07 22:12:52.277247: train_loss -0.8311\n",
      "2025-12-07 22:12:52.277247: val_loss -0.8636\n",
      "2025-12-07 22:12:52.281252: Pseudo dice [0.9198, 0.9548, 0.9429]\n",
      "2025-12-07 22:12:52.285256: Epoch time: 137.98 s\n",
      "2025-12-07 22:12:52.287258: Yayy! New best EMA pseudo Dice: 0.9344\n",
      "2025-12-07 22:12:53.187024: \n",
      "2025-12-07 22:12:53.187024: Epoch 263\n",
      "2025-12-07 22:12:53.190109: Current learning rate: 0.0076\n",
      "2025-12-07 22:15:11.218149: train_loss -0.831\n",
      "2025-12-07 22:15:11.218149: val_loss -0.86\n",
      "2025-12-07 22:15:11.233134: Pseudo dice [0.9179, 0.9498, 0.9436]\n",
      "2025-12-07 22:15:11.234136: Epoch time: 138.03 s\n",
      "2025-12-07 22:15:11.238141: Yayy! New best EMA pseudo Dice: 0.9347\n",
      "2025-12-07 22:15:12.124793: \n",
      "2025-12-07 22:15:12.124793: Epoch 264\n",
      "2025-12-07 22:15:12.124793: Current learning rate: 0.00759\n",
      "2025-12-07 22:17:30.340743: train_loss -0.8348\n",
      "2025-12-07 22:17:30.340743: val_loss -0.86\n",
      "2025-12-07 22:17:30.342484: Pseudo dice [0.9157, 0.9523, 0.9416]\n",
      "2025-12-07 22:17:30.347994: Epoch time: 138.22 s\n",
      "2025-12-07 22:17:30.349996: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2025-12-07 22:17:31.298331: \n",
      "2025-12-07 22:17:31.299333: Epoch 265\n",
      "2025-12-07 22:17:31.301639: Current learning rate: 0.00758\n",
      "2025-12-07 22:19:49.372313: train_loss -0.8284\n",
      "2025-12-07 22:19:49.372313: val_loss -0.8534\n",
      "2025-12-07 22:19:49.374053: Pseudo dice [0.9147, 0.945, 0.9351]\n",
      "2025-12-07 22:19:49.379553: Epoch time: 138.08 s\n",
      "2025-12-07 22:19:50.022130: \n",
      "2025-12-07 22:19:50.023871: Epoch 266\n",
      "2025-12-07 22:19:50.023871: Current learning rate: 0.00757\n",
      "2025-12-07 22:22:08.108613: train_loss -0.8332\n",
      "2025-12-07 22:22:08.108613: val_loss -0.8654\n",
      "2025-12-07 22:22:08.108613: Pseudo dice [0.9203, 0.9525, 0.9378]\n",
      "2025-12-07 22:22:08.124251: Epoch time: 138.09 s\n",
      "2025-12-07 22:22:08.758091: \n",
      "2025-12-07 22:22:08.760094: Epoch 267\n",
      "2025-12-07 22:22:08.760094: Current learning rate: 0.00756\n",
      "2025-12-07 22:24:26.763957: train_loss -0.828\n",
      "2025-12-07 22:24:26.765960: val_loss -0.8594\n",
      "2025-12-07 22:24:26.769966: Pseudo dice [0.9202, 0.95, 0.9408]\n",
      "2025-12-07 22:24:26.773970: Epoch time: 138.01 s\n",
      "2025-12-07 22:24:26.775710: Yayy! New best EMA pseudo Dice: 0.935\n",
      "2025-12-07 22:24:27.892126: \n",
      "2025-12-07 22:24:27.892126: Epoch 268\n",
      "2025-12-07 22:24:27.895138: Current learning rate: 0.00755\n",
      "2025-12-07 22:26:45.950401: train_loss -0.831\n",
      "2025-12-07 22:26:45.950401: val_loss -0.8697\n",
      "2025-12-07 22:26:45.952403: Pseudo dice [0.9244, 0.9561, 0.9409]\n",
      "2025-12-07 22:26:45.952403: Epoch time: 138.06 s\n",
      "2025-12-07 22:26:45.959905: Yayy! New best EMA pseudo Dice: 0.9356\n",
      "2025-12-07 22:26:46.889719: \n",
      "2025-12-07 22:26:46.889719: Epoch 269\n",
      "2025-12-07 22:26:46.905557: Current learning rate: 0.00754\n",
      "2025-12-07 22:29:04.827124: train_loss -0.8334\n",
      "2025-12-07 22:29:04.827124: val_loss -0.8494\n",
      "2025-12-07 22:29:04.834346: Pseudo dice [0.91, 0.9448, 0.9418]\n",
      "2025-12-07 22:29:04.836348: Epoch time: 137.94 s\n",
      "2025-12-07 22:29:05.467885: \n",
      "2025-12-07 22:29:05.467885: Epoch 270\n",
      "2025-12-07 22:29:05.478964: Current learning rate: 0.00753\n",
      "2025-12-07 22:31:23.547784: train_loss -0.8289\n",
      "2025-12-07 22:31:23.547784: val_loss -0.8627\n",
      "2025-12-07 22:31:23.565903: Pseudo dice [0.9171, 0.9534, 0.9406]\n",
      "2025-12-07 22:31:23.568177: Epoch time: 138.08 s\n",
      "2025-12-07 22:31:24.271743: \n",
      "2025-12-07 22:31:24.272745: Epoch 271\n",
      "2025-12-07 22:31:24.275752: Current learning rate: 0.00752\n",
      "2025-12-07 22:33:42.220912: train_loss -0.8294\n",
      "2025-12-07 22:33:42.220912: val_loss -0.8453\n",
      "2025-12-07 22:33:42.227914: Pseudo dice [0.9106, 0.9449, 0.929]\n",
      "2025-12-07 22:33:42.231915: Epoch time: 137.95 s\n",
      "2025-12-07 22:33:42.869613: \n",
      "2025-12-07 22:33:42.869613: Epoch 272\n",
      "2025-12-07 22:33:42.872625: Current learning rate: 0.00751\n",
      "2025-12-07 22:36:00.936425: train_loss -0.8321\n",
      "2025-12-07 22:36:00.936425: val_loss -0.8582\n",
      "2025-12-07 22:36:00.940253: Pseudo dice [0.9156, 0.9509, 0.9411]\n",
      "2025-12-07 22:36:00.942255: Epoch time: 138.07 s\n",
      "2025-12-07 22:36:01.584116: \n",
      "2025-12-07 22:36:01.584116: Epoch 273\n",
      "2025-12-07 22:36:01.584116: Current learning rate: 0.00751\n",
      "2025-12-07 22:38:19.890408: train_loss -0.8368\n",
      "2025-12-07 22:38:19.892410: val_loss -0.8643\n",
      "2025-12-07 22:38:19.894412: Pseudo dice [0.9183, 0.9543, 0.9437]\n",
      "2025-12-07 22:38:19.896414: Epoch time: 138.31 s\n",
      "2025-12-07 22:38:20.858843: \n",
      "2025-12-07 22:38:20.858843: Epoch 274\n",
      "2025-12-07 22:38:20.874658: Current learning rate: 0.0075\n",
      "2025-12-07 22:40:38.846540: train_loss -0.8326\n",
      "2025-12-07 22:40:38.846540: val_loss -0.8559\n",
      "2025-12-07 22:40:38.850545: Pseudo dice [0.9172, 0.9488, 0.9393]\n",
      "2025-12-07 22:40:38.852547: Epoch time: 137.99 s\n",
      "2025-12-07 22:40:39.485692: \n",
      "2025-12-07 22:40:39.485692: Epoch 275\n",
      "2025-12-07 22:40:39.485692: Current learning rate: 0.00749\n",
      "2025-12-07 22:42:57.607576: train_loss -0.8336\n",
      "2025-12-07 22:42:57.607576: val_loss -0.8494\n",
      "2025-12-07 22:42:57.610728: Pseudo dice [0.9128, 0.9499, 0.9327]\n",
      "2025-12-07 22:42:57.610728: Epoch time: 138.12 s\n",
      "2025-12-07 22:42:58.259645: \n",
      "2025-12-07 22:42:58.259645: Epoch 276\n",
      "2025-12-07 22:42:58.259645: Current learning rate: 0.00748\n",
      "2025-12-07 22:45:16.326764: train_loss -0.8303\n",
      "2025-12-07 22:45:16.326764: val_loss -0.8492\n",
      "2025-12-07 22:45:16.330818: Pseudo dice [0.9085, 0.9477, 0.9375]\n",
      "2025-12-07 22:45:16.332820: Epoch time: 138.07 s\n",
      "2025-12-07 22:45:17.086702: \n",
      "2025-12-07 22:45:17.087704: Epoch 277\n",
      "2025-12-07 22:45:17.090713: Current learning rate: 0.00747\n",
      "2025-12-07 22:47:35.171386: train_loss -0.8287\n",
      "2025-12-07 22:47:35.171386: val_loss -0.8554\n",
      "2025-12-07 22:47:35.171386: Pseudo dice [0.917, 0.9456, 0.9389]\n",
      "2025-12-07 22:47:35.171386: Epoch time: 138.09 s\n",
      "2025-12-07 22:47:35.810192: \n",
      "2025-12-07 22:47:35.810192: Epoch 278\n",
      "2025-12-07 22:47:35.810192: Current learning rate: 0.00746\n",
      "2025-12-07 22:49:53.717565: train_loss -0.8351\n",
      "2025-12-07 22:49:53.717565: val_loss -0.853\n",
      "2025-12-07 22:49:53.717565: Pseudo dice [0.9133, 0.944, 0.9385]\n",
      "2025-12-07 22:49:53.717565: Epoch time: 137.91 s\n",
      "2025-12-07 22:49:54.345709: \n",
      "2025-12-07 22:49:54.346714: Epoch 279\n",
      "2025-12-07 22:49:54.349069: Current learning rate: 0.00745\n",
      "2025-12-07 22:52:12.326843: train_loss -0.8296\n",
      "2025-12-07 22:52:12.326843: val_loss -0.857\n",
      "2025-12-07 22:52:12.331196: Pseudo dice [0.9143, 0.9534, 0.9365]\n",
      "2025-12-07 22:52:12.331196: Epoch time: 137.98 s\n",
      "2025-12-07 22:52:13.076559: \n",
      "2025-12-07 22:52:13.076559: Epoch 280\n",
      "2025-12-07 22:52:13.092209: Current learning rate: 0.00744\n",
      "2025-12-07 22:54:31.216544: train_loss -0.8194\n",
      "2025-12-07 22:54:31.217548: val_loss -0.8273\n",
      "2025-12-07 22:54:31.217548: Pseudo dice [0.9017, 0.9382, 0.9275]\n",
      "2025-12-07 22:54:31.217548: Epoch time: 138.14 s\n",
      "2025-12-07 22:54:32.024543: \n",
      "2025-12-07 22:54:32.024543: Epoch 281\n",
      "2025-12-07 22:54:32.024543: Current learning rate: 0.00743\n",
      "2025-12-07 22:56:50.005874: train_loss -0.7999\n",
      "2025-12-07 22:56:50.005874: val_loss -0.821\n",
      "2025-12-07 22:56:50.009616: Pseudo dice [0.8964, 0.9362, 0.9307]\n",
      "2025-12-07 22:56:50.013621: Epoch time: 137.98 s\n",
      "2025-12-07 22:56:50.655148: \n",
      "2025-12-07 22:56:50.655148: Epoch 282\n",
      "2025-12-07 22:56:50.655148: Current learning rate: 0.00742\n",
      "2025-12-07 22:59:08.811145: train_loss -0.8116\n",
      "2025-12-07 22:59:08.811145: val_loss -0.8453\n",
      "2025-12-07 22:59:08.815166: Pseudo dice [0.9179, 0.9498, 0.9184]\n",
      "2025-12-07 22:59:08.819172: Epoch time: 138.16 s\n",
      "2025-12-07 22:59:09.467377: \n",
      "2025-12-07 22:59:09.467377: Epoch 283\n",
      "2025-12-07 22:59:09.483089: Current learning rate: 0.00741\n",
      "2025-12-07 23:01:27.449691: train_loss -0.7979\n",
      "2025-12-07 23:01:27.449691: val_loss -0.8431\n",
      "2025-12-07 23:01:27.453728: Pseudo dice [0.9139, 0.9472, 0.9318]\n",
      "2025-12-07 23:01:27.456089: Epoch time: 137.98 s\n",
      "2025-12-07 23:01:28.077409: \n",
      "2025-12-07 23:01:28.077409: Epoch 284\n",
      "2025-12-07 23:01:28.088246: Current learning rate: 0.0074\n",
      "2025-12-07 23:03:46.029778: train_loss -0.8125\n",
      "2025-12-07 23:03:46.031811: val_loss -0.8368\n",
      "2025-12-07 23:03:46.037825: Pseudo dice [0.9048, 0.9467, 0.9344]\n",
      "2025-12-07 23:03:46.039828: Epoch time: 137.95 s\n",
      "2025-12-07 23:03:46.710685: \n",
      "2025-12-07 23:03:46.710685: Epoch 285\n",
      "2025-12-07 23:03:46.710685: Current learning rate: 0.00739\n",
      "2025-12-07 23:06:04.741943: train_loss -0.7908\n",
      "2025-12-07 23:06:04.741943: val_loss -0.8135\n",
      "2025-12-07 23:06:04.745947: Pseudo dice [0.8932, 0.9365, 0.9235]\n",
      "2025-12-07 23:06:04.747948: Epoch time: 138.03 s\n",
      "2025-12-07 23:06:05.386195: \n",
      "2025-12-07 23:06:05.386195: Epoch 286\n",
      "2025-12-07 23:06:05.390004: Current learning rate: 0.00738\n",
      "2025-12-07 23:08:23.308720: train_loss -0.8036\n",
      "2025-12-07 23:08:23.308720: val_loss -0.8378\n",
      "2025-12-07 23:08:23.312462: Pseudo dice [0.9066, 0.9452, 0.9297]\n",
      "2025-12-07 23:08:23.315462: Epoch time: 137.92 s\n",
      "2025-12-07 23:08:24.122929: \n",
      "2025-12-07 23:08:24.123934: Epoch 287\n",
      "2025-12-07 23:08:24.127041: Current learning rate: 0.00738\n",
      "2025-12-07 23:10:42.148884: train_loss -0.8064\n",
      "2025-12-07 23:10:42.150887: val_loss -0.8391\n",
      "2025-12-07 23:10:42.152889: Pseudo dice [0.9063, 0.9414, 0.9304]\n",
      "2025-12-07 23:10:42.154891: Epoch time: 138.03 s\n",
      "2025-12-07 23:10:42.805406: \n",
      "2025-12-07 23:10:42.805406: Epoch 288\n",
      "2025-12-07 23:10:42.805406: Current learning rate: 0.00737\n",
      "2025-12-07 23:13:00.671267: train_loss -0.822\n",
      "2025-12-07 23:13:00.671267: val_loss -0.8407\n",
      "2025-12-07 23:13:00.687845: Pseudo dice [0.9054, 0.9432, 0.9372]\n",
      "2025-12-07 23:13:00.690884: Epoch time: 137.87 s\n",
      "2025-12-07 23:13:01.312269: \n",
      "2025-12-07 23:13:01.312269: Epoch 289\n",
      "2025-12-07 23:13:01.328250: Current learning rate: 0.00736\n",
      "2025-12-07 23:15:19.280317: train_loss -0.8263\n",
      "2025-12-07 23:15:19.280317: val_loss -0.8572\n",
      "2025-12-07 23:15:19.280317: Pseudo dice [0.9187, 0.9522, 0.9345]\n",
      "2025-12-07 23:15:19.289331: Epoch time: 137.97 s\n",
      "2025-12-07 23:15:19.920658: \n",
      "2025-12-07 23:15:19.920658: Epoch 290\n",
      "2025-12-07 23:15:19.920658: Current learning rate: 0.00735\n",
      "2025-12-07 23:17:37.874197: train_loss -0.8236\n",
      "2025-12-07 23:17:37.890186: val_loss -0.8547\n",
      "2025-12-07 23:17:37.890186: Pseudo dice [0.9185, 0.9516, 0.9294]\n",
      "2025-12-07 23:17:37.890186: Epoch time: 137.95 s\n",
      "2025-12-07 23:17:38.514464: \n",
      "2025-12-07 23:17:38.514464: Epoch 291\n",
      "2025-12-07 23:17:38.514464: Current learning rate: 0.00734\n",
      "2025-12-07 23:19:56.467934: train_loss -0.8308\n",
      "2025-12-07 23:19:56.467934: val_loss -0.8706\n",
      "2025-12-07 23:19:56.483610: Pseudo dice [0.9255, 0.955, 0.9424]\n",
      "2025-12-07 23:19:56.483610: Epoch time: 137.95 s\n",
      "2025-12-07 23:19:57.107977: \n",
      "2025-12-07 23:19:57.107977: Epoch 292\n",
      "2025-12-07 23:19:57.107977: Current learning rate: 0.00733\n",
      "2025-12-07 23:22:15.014388: train_loss -0.8315\n",
      "2025-12-07 23:22:15.014388: val_loss -0.8504\n",
      "2025-12-07 23:22:15.014388: Pseudo dice [0.912, 0.9476, 0.9364]\n",
      "2025-12-07 23:22:15.014388: Epoch time: 137.91 s\n",
      "2025-12-07 23:22:15.827208: \n",
      "2025-12-07 23:22:15.827208: Epoch 293\n",
      "2025-12-07 23:22:15.838271: Current learning rate: 0.00732\n",
      "2025-12-07 23:24:33.743375: train_loss -0.8335\n",
      "2025-12-07 23:24:33.743375: val_loss -0.8525\n",
      "2025-12-07 23:24:33.749382: Pseudo dice [0.913, 0.9529, 0.939]\n",
      "2025-12-07 23:24:33.751097: Epoch time: 137.92 s\n",
      "2025-12-07 23:24:34.503320: \n",
      "2025-12-07 23:24:34.503320: Epoch 294\n",
      "2025-12-07 23:24:34.503320: Current learning rate: 0.00731\n",
      "2025-12-07 23:26:52.497139: train_loss -0.8301\n",
      "2025-12-07 23:26:52.497139: val_loss -0.8635\n",
      "2025-12-07 23:26:52.501012: Pseudo dice [0.922, 0.9537, 0.9401]\n",
      "2025-12-07 23:26:52.504754: Epoch time: 138.0 s\n",
      "2025-12-07 23:26:53.140279: \n",
      "2025-12-07 23:26:53.140279: Epoch 295\n",
      "2025-12-07 23:26:53.151899: Current learning rate: 0.0073\n",
      "2025-12-07 23:29:11.140428: train_loss -0.8316\n",
      "2025-12-07 23:29:11.140428: val_loss -0.8479\n",
      "2025-12-07 23:29:11.140428: Pseudo dice [0.9073, 0.945, 0.9451]\n",
      "2025-12-07 23:29:11.155829: Epoch time: 138.0 s\n",
      "2025-12-07 23:29:11.795320: \n",
      "2025-12-07 23:29:11.795320: Epoch 296\n",
      "2025-12-07 23:29:11.795320: Current learning rate: 0.00729\n",
      "2025-12-07 23:31:29.940754: train_loss -0.8346\n",
      "2025-12-07 23:31:29.941757: val_loss -0.8632\n",
      "2025-12-07 23:31:29.944775: Pseudo dice [0.9189, 0.9516, 0.9378]\n",
      "2025-12-07 23:31:29.946787: Epoch time: 138.15 s\n",
      "2025-12-07 23:31:30.687677: \n",
      "2025-12-07 23:31:30.687677: Epoch 297\n",
      "2025-12-07 23:31:30.687677: Current learning rate: 0.00728\n",
      "2025-12-07 23:33:49.086060: train_loss -0.8342\n",
      "2025-12-07 23:33:49.087062: val_loss -0.8636\n",
      "2025-12-07 23:33:49.091071: Pseudo dice [0.918, 0.9513, 0.9409]\n",
      "2025-12-07 23:33:49.093225: Epoch time: 138.4 s\n",
      "2025-12-07 23:33:49.728059: \n",
      "2025-12-07 23:33:49.728059: Epoch 298\n",
      "2025-12-07 23:33:49.744141: Current learning rate: 0.00727\n",
      "2025-12-07 23:36:08.140214: train_loss -0.8339\n",
      "2025-12-07 23:36:08.140214: val_loss -0.8646\n",
      "2025-12-07 23:36:08.140214: Pseudo dice [0.9159, 0.9496, 0.9452]\n",
      "2025-12-07 23:36:08.140214: Epoch time: 138.41 s\n",
      "2025-12-07 23:36:08.775344: \n",
      "2025-12-07 23:36:08.791089: Epoch 299\n",
      "2025-12-07 23:36:08.791089: Current learning rate: 0.00726\n",
      "2025-12-07 23:38:27.121349: train_loss -0.8343\n",
      "2025-12-07 23:38:27.123352: val_loss -0.8627\n",
      "2025-12-07 23:38:27.126856: Pseudo dice [0.919, 0.9496, 0.941]\n",
      "2025-12-07 23:38:27.128858: Epoch time: 138.35 s\n",
      "2025-12-07 23:38:28.327419: \n",
      "2025-12-07 23:38:28.327419: Epoch 300\n",
      "2025-12-07 23:38:28.343538: Current learning rate: 0.00725\n",
      "2025-12-07 23:40:46.722750: train_loss -0.8356\n",
      "2025-12-07 23:40:46.726755: val_loss -0.8571\n",
      "2025-12-07 23:40:46.731763: Pseudo dice [0.9096, 0.9502, 0.9478]\n",
      "2025-12-07 23:40:46.735768: Epoch time: 138.4 s\n",
      "2025-12-07 23:40:47.406730: \n",
      "2025-12-07 23:40:47.406730: Epoch 301\n",
      "2025-12-07 23:40:47.406730: Current learning rate: 0.00724\n",
      "2025-12-07 23:43:05.609263: train_loss -0.8345\n",
      "2025-12-07 23:43:05.609263: val_loss -0.8481\n",
      "2025-12-07 23:43:05.624980: Pseudo dice [0.9071, 0.9469, 0.9434]\n",
      "2025-12-07 23:43:05.626984: Epoch time: 138.2 s\n",
      "2025-12-07 23:43:06.265054: \n",
      "2025-12-07 23:43:06.265054: Epoch 302\n",
      "2025-12-07 23:43:06.265054: Current learning rate: 0.00724\n",
      "2025-12-07 23:45:24.298235: train_loss -0.832\n",
      "2025-12-07 23:45:24.298235: val_loss -0.8744\n",
      "2025-12-07 23:45:24.302242: Pseudo dice [0.9302, 0.9586, 0.9383]\n",
      "2025-12-07 23:45:24.304245: Epoch time: 138.03 s\n",
      "2025-12-07 23:45:24.998133: \n",
      "2025-12-07 23:45:24.998133: Epoch 303\n",
      "2025-12-07 23:45:24.998133: Current learning rate: 0.00723\n",
      "2025-12-07 23:47:43.064955: train_loss -0.8346\n",
      "2025-12-07 23:47:43.064955: val_loss -0.8548\n",
      "2025-12-07 23:47:43.068966: Pseudo dice [0.9112, 0.9494, 0.9437]\n",
      "2025-12-07 23:47:43.070993: Epoch time: 138.07 s\n",
      "2025-12-07 23:47:43.712336: \n",
      "2025-12-07 23:47:43.713340: Epoch 304\n",
      "2025-12-07 23:47:43.713340: Current learning rate: 0.00722\n",
      "2025-12-07 23:50:01.772634: train_loss -0.8341\n",
      "2025-12-07 23:50:01.772634: val_loss -0.8737\n",
      "2025-12-07 23:50:01.778641: Pseudo dice [0.9283, 0.9579, 0.9399]\n",
      "2025-12-07 23:50:01.780382: Epoch time: 138.07 s\n",
      "2025-12-07 23:50:02.422159: \n",
      "2025-12-07 23:50:02.422159: Epoch 305\n",
      "2025-12-07 23:50:02.422159: Current learning rate: 0.00721\n",
      "2025-12-07 23:52:20.515662: train_loss -0.8364\n",
      "2025-12-07 23:52:20.515662: val_loss -0.8719\n",
      "2025-12-07 23:52:20.515662: Pseudo dice [0.9265, 0.9583, 0.9424]\n",
      "2025-12-07 23:52:20.515662: Epoch time: 138.11 s\n",
      "2025-12-07 23:52:20.515662: Yayy! New best EMA pseudo Dice: 0.9361\n",
      "2025-12-07 23:52:21.790479: \n",
      "2025-12-07 23:52:21.790479: Epoch 306\n",
      "2025-12-07 23:52:21.794497: Current learning rate: 0.0072\n",
      "2025-12-07 23:54:39.765142: train_loss -0.8362\n",
      "2025-12-07 23:54:39.781145: val_loss -0.8619\n",
      "2025-12-07 23:54:39.784337: Pseudo dice [0.9201, 0.9503, 0.9313]\n",
      "2025-12-07 23:54:39.787501: Epoch time: 137.98 s\n",
      "2025-12-07 23:54:40.432220: \n",
      "2025-12-07 23:54:40.432220: Epoch 307\n",
      "2025-12-07 23:54:40.435117: Current learning rate: 0.00719\n",
      "2025-12-07 23:56:58.405448: train_loss -0.8356\n",
      "2025-12-07 23:56:58.407451: val_loss -0.8661\n",
      "2025-12-07 23:56:58.409453: Pseudo dice [0.9197, 0.9477, 0.9462]\n",
      "2025-12-07 23:56:58.412003: Epoch time: 137.97 s\n",
      "2025-12-07 23:56:59.045540: \n",
      "2025-12-07 23:56:59.045540: Epoch 308\n",
      "2025-12-07 23:56:59.045540: Current learning rate: 0.00718\n",
      "2025-12-07 23:59:17.092239: train_loss -0.8374\n",
      "2025-12-07 23:59:17.092239: val_loss -0.859\n",
      "2025-12-07 23:59:17.092239: Pseudo dice [0.9183, 0.9487, 0.9375]\n",
      "2025-12-07 23:59:17.092239: Epoch time: 138.05 s\n",
      "2025-12-07 23:59:17.830390: \n",
      "2025-12-07 23:59:17.830390: Epoch 309\n",
      "2025-12-07 23:59:17.832393: Current learning rate: 0.00717\n",
      "2025-12-08 00:01:35.780961: train_loss -0.8362\n",
      "2025-12-08 00:01:35.780961: val_loss -0.858\n",
      "2025-12-08 00:01:35.780961: Pseudo dice [0.9163, 0.9501, 0.9401]\n",
      "2025-12-08 00:01:35.780961: Epoch time: 137.95 s\n",
      "2025-12-08 00:01:36.422058: \n",
      "2025-12-08 00:01:36.422058: Epoch 310\n",
      "2025-12-08 00:01:36.437803: Current learning rate: 0.00716\n",
      "2025-12-08 00:03:54.439849: train_loss -0.8386\n",
      "2025-12-08 00:03:54.439849: val_loss -0.8592\n",
      "2025-12-08 00:03:54.445861: Pseudo dice [0.9155, 0.9502, 0.9394]\n",
      "2025-12-08 00:03:54.449874: Epoch time: 138.02 s\n",
      "2025-12-08 00:03:55.252268: \n",
      "2025-12-08 00:03:55.253270: Epoch 311\n",
      "2025-12-08 00:03:55.256277: Current learning rate: 0.00715\n",
      "2025-12-08 00:06:13.127785: train_loss -0.8401\n",
      "2025-12-08 00:06:13.128788: val_loss -0.8644\n",
      "2025-12-08 00:06:13.131797: Pseudo dice [0.92, 0.956, 0.9395]\n",
      "2025-12-08 00:06:13.133798: Epoch time: 137.88 s\n",
      "2025-12-08 00:06:13.843223: \n",
      "2025-12-08 00:06:13.843223: Epoch 312\n",
      "2025-12-08 00:06:13.856093: Current learning rate: 0.00714\n",
      "2025-12-08 00:08:32.048893: train_loss -0.8357\n",
      "2025-12-08 00:08:32.049895: val_loss -0.8647\n",
      "2025-12-08 00:08:32.052901: Pseudo dice [0.921, 0.9542, 0.9366]\n",
      "2025-12-08 00:08:32.055914: Epoch time: 138.21 s\n",
      "2025-12-08 00:08:32.057919: Yayy! New best EMA pseudo Dice: 0.9362\n",
      "2025-12-08 00:08:32.952033: \n",
      "2025-12-08 00:08:32.952033: Epoch 313\n",
      "2025-12-08 00:08:32.952033: Current learning rate: 0.00713\n",
      "2025-12-08 00:10:50.890639: train_loss -0.8333\n",
      "2025-12-08 00:10:50.890639: val_loss -0.8605\n",
      "2025-12-08 00:10:50.905008: Pseudo dice [0.9164, 0.9481, 0.9416]\n",
      "2025-12-08 00:10:50.906748: Epoch time: 137.94 s\n",
      "2025-12-08 00:10:51.586398: \n",
      "2025-12-08 00:10:51.586398: Epoch 314\n",
      "2025-12-08 00:10:51.590629: Current learning rate: 0.00712\n",
      "2025-12-08 00:13:09.561850: train_loss -0.838\n",
      "2025-12-08 00:13:09.561850: val_loss -0.87\n",
      "2025-12-08 00:13:09.577534: Pseudo dice [0.9191, 0.9573, 0.941]\n",
      "2025-12-08 00:13:09.579538: Epoch time: 137.98 s\n",
      "2025-12-08 00:13:09.581540: Yayy! New best EMA pseudo Dice: 0.9364\n",
      "2025-12-08 00:13:10.619074: \n",
      "2025-12-08 00:13:10.619074: Epoch 315\n",
      "2025-12-08 00:13:10.623226: Current learning rate: 0.00711\n",
      "2025-12-08 00:15:28.621215: train_loss -0.8367\n",
      "2025-12-08 00:15:28.621215: val_loss -0.8613\n",
      "2025-12-08 00:15:28.626116: Pseudo dice [0.916, 0.9518, 0.9412]\n",
      "2025-12-08 00:15:28.629124: Epoch time: 138.0 s\n",
      "2025-12-08 00:15:29.263845: \n",
      "2025-12-08 00:15:29.263845: Epoch 316\n",
      "2025-12-08 00:15:29.279652: Current learning rate: 0.0071\n",
      "2025-12-08 00:17:47.399035: train_loss -0.8347\n",
      "2025-12-08 00:17:47.400036: val_loss -0.8572\n",
      "2025-12-08 00:17:47.403522: Pseudo dice [0.9126, 0.9486, 0.9409]\n",
      "2025-12-08 00:17:47.406523: Epoch time: 138.14 s\n",
      "2025-12-08 00:17:48.217999: \n",
      "2025-12-08 00:17:48.217999: Epoch 317\n",
      "2025-12-08 00:17:48.217999: Current learning rate: 0.0071\n",
      "2025-12-08 00:20:06.287145: train_loss -0.8373\n",
      "2025-12-08 00:20:06.289148: val_loss -0.8614\n",
      "2025-12-08 00:20:06.295159: Pseudo dice [0.9197, 0.9511, 0.9445]\n",
      "2025-12-08 00:20:06.297162: Epoch time: 138.07 s\n",
      "2025-12-08 00:20:06.983799: \n",
      "2025-12-08 00:20:06.983799: Epoch 318\n",
      "2025-12-08 00:20:06.994499: Current learning rate: 0.00709\n",
      "2025-12-08 00:22:25.061189: train_loss -0.8318\n",
      "2025-12-08 00:22:25.061189: val_loss -0.8417\n",
      "2025-12-08 00:22:25.061189: Pseudo dice [0.9085, 0.9388, 0.9351]\n",
      "2025-12-08 00:22:25.061189: Epoch time: 138.08 s\n",
      "2025-12-08 00:22:25.718481: \n",
      "2025-12-08 00:22:25.718481: Epoch 319\n",
      "2025-12-08 00:22:25.718481: Current learning rate: 0.00708\n",
      "2025-12-08 00:24:43.704688: train_loss -0.8333\n",
      "2025-12-08 00:24:43.704688: val_loss -0.8489\n",
      "2025-12-08 00:24:43.722122: Pseudo dice [0.916, 0.9487, 0.9296]\n",
      "2025-12-08 00:24:43.724128: Epoch time: 138.0 s\n",
      "2025-12-08 00:24:44.358932: \n",
      "2025-12-08 00:24:44.358932: Epoch 320\n",
      "2025-12-08 00:24:44.358932: Current learning rate: 0.00707\n",
      "2025-12-08 00:27:03.726955: train_loss -0.8356\n",
      "2025-12-08 00:27:03.726955: val_loss -0.8494\n",
      "2025-12-08 00:27:03.726955: Pseudo dice [0.9102, 0.9473, 0.9368]\n",
      "2025-12-08 00:27:03.726955: Epoch time: 139.37 s\n",
      "2025-12-08 00:27:04.498457: \n",
      "2025-12-08 00:27:04.498457: Epoch 321\n",
      "2025-12-08 00:27:04.498457: Current learning rate: 0.00706\n",
      "2025-12-08 00:29:22.833615: train_loss -0.8397\n",
      "2025-12-08 00:29:22.833615: val_loss -0.8613\n",
      "2025-12-08 00:29:22.837620: Pseudo dice [0.9178, 0.9528, 0.9403]\n",
      "2025-12-08 00:29:22.839621: Epoch time: 138.34 s\n",
      "2025-12-08 00:29:23.500575: \n",
      "2025-12-08 00:29:23.500575: Epoch 322\n",
      "2025-12-08 00:29:23.504044: Current learning rate: 0.00705\n",
      "2025-12-08 00:31:42.093463: train_loss -0.8394\n",
      "2025-12-08 00:31:42.093463: val_loss -0.874\n",
      "2025-12-08 00:31:42.093463: Pseudo dice [0.9276, 0.9602, 0.9324]\n",
      "2025-12-08 00:31:42.110623: Epoch time: 138.59 s\n",
      "2025-12-08 00:31:42.751926: \n",
      "2025-12-08 00:31:42.751926: Epoch 323\n",
      "2025-12-08 00:31:42.751926: Current learning rate: 0.00704\n",
      "2025-12-08 00:34:01.202296: train_loss -0.8385\n",
      "2025-12-08 00:34:01.202296: val_loss -0.8637\n",
      "2025-12-08 00:34:01.202296: Pseudo dice [0.9195, 0.9504, 0.9429]\n",
      "2025-12-08 00:34:01.202296: Epoch time: 138.45 s\n",
      "2025-12-08 00:34:02.003758: \n",
      "2025-12-08 00:34:02.004763: Epoch 324\n",
      "2025-12-08 00:34:02.007787: Current learning rate: 0.00703\n",
      "2025-12-08 00:36:20.543963: train_loss -0.8371\n",
      "2025-12-08 00:36:20.545966: val_loss -0.8595\n",
      "2025-12-08 00:36:20.545966: Pseudo dice [0.9157, 0.9512, 0.942]\n",
      "2025-12-08 00:36:20.545966: Epoch time: 138.54 s\n",
      "2025-12-08 00:36:21.186123: \n",
      "2025-12-08 00:36:21.186123: Epoch 325\n",
      "2025-12-08 00:36:21.186123: Current learning rate: 0.00702\n",
      "2025-12-08 00:38:39.265363: train_loss -0.8381\n",
      "2025-12-08 00:38:39.266566: val_loss -0.8671\n",
      "2025-12-08 00:38:39.272568: Pseudo dice [0.921, 0.9508, 0.9439]\n",
      "2025-12-08 00:38:39.275568: Epoch time: 138.08 s\n",
      "2025-12-08 00:38:39.925719: \n",
      "2025-12-08 00:38:39.925719: Epoch 326\n",
      "2025-12-08 00:38:39.925719: Current learning rate: 0.00701\n",
      "2025-12-08 00:40:58.031832: train_loss -0.8404\n",
      "2025-12-08 00:40:58.031832: val_loss -0.8662\n",
      "2025-12-08 00:40:58.035837: Pseudo dice [0.9192, 0.9491, 0.9433]\n",
      "2025-12-08 00:40:58.039842: Epoch time: 138.11 s\n",
      "2025-12-08 00:40:58.811100: \n",
      "2025-12-08 00:40:58.811100: Epoch 327\n",
      "2025-12-08 00:40:58.811100: Current learning rate: 0.007\n",
      "2025-12-08 00:43:16.749996: train_loss -0.8433\n",
      "2025-12-08 00:43:16.749996: val_loss -0.8638\n",
      "2025-12-08 00:43:16.754000: Pseudo dice [0.9191, 0.9468, 0.9456]\n",
      "2025-12-08 00:43:16.758004: Epoch time: 137.95 s\n",
      "2025-12-08 00:43:17.413989: \n",
      "2025-12-08 00:43:17.413989: Epoch 328\n",
      "2025-12-08 00:43:17.413989: Current learning rate: 0.00699\n",
      "2025-12-08 00:45:35.344790: train_loss -0.8344\n",
      "2025-12-08 00:45:35.346792: val_loss -0.8561\n",
      "2025-12-08 00:45:35.350796: Pseudo dice [0.9144, 0.9511, 0.9426]\n",
      "2025-12-08 00:45:35.352798: Epoch time: 137.93 s\n",
      "2025-12-08 00:45:36.154520: \n",
      "2025-12-08 00:45:36.154520: Epoch 329\n",
      "2025-12-08 00:45:36.154520: Current learning rate: 0.00698\n",
      "2025-12-08 00:47:54.211820: train_loss -0.8371\n",
      "2025-12-08 00:47:54.213823: val_loss -0.8753\n",
      "2025-12-08 00:47:54.217566: Pseudo dice [0.9261, 0.956, 0.9473]\n",
      "2025-12-08 00:47:54.217566: Epoch time: 138.06 s\n",
      "2025-12-08 00:47:54.223063: Yayy! New best EMA pseudo Dice: 0.9369\n",
      "2025-12-08 00:47:55.242570: \n",
      "2025-12-08 00:47:55.242570: Epoch 330\n",
      "2025-12-08 00:47:55.246795: Current learning rate: 0.00697\n",
      "2025-12-08 00:50:13.170966: train_loss -0.8371\n",
      "2025-12-08 00:50:13.170966: val_loss -0.8652\n",
      "2025-12-08 00:50:13.176441: Pseudo dice [0.9231, 0.9542, 0.9376]\n",
      "2025-12-08 00:50:13.178443: Epoch time: 137.93 s\n",
      "2025-12-08 00:50:13.182447: Yayy! New best EMA pseudo Dice: 0.9371\n",
      "2025-12-08 00:50:14.093762: \n",
      "2025-12-08 00:50:14.093762: Epoch 331\n",
      "2025-12-08 00:50:14.093762: Current learning rate: 0.00696\n",
      "2025-12-08 00:52:32.236237: train_loss -0.8348\n",
      "2025-12-08 00:52:32.237237: val_loss -0.8538\n",
      "2025-12-08 00:52:32.241239: Pseudo dice [0.9123, 0.9469, 0.9399]\n",
      "2025-12-08 00:52:32.243239: Epoch time: 138.14 s\n",
      "2025-12-08 00:52:32.874624: \n",
      "2025-12-08 00:52:32.874624: Epoch 332\n",
      "2025-12-08 00:52:32.874624: Current learning rate: 0.00696\n",
      "2025-12-08 00:54:51.070688: train_loss -0.8361\n",
      "2025-12-08 00:54:51.070688: val_loss -0.8587\n",
      "2025-12-08 00:54:51.076432: Pseudo dice [0.9177, 0.9523, 0.9439]\n",
      "2025-12-08 00:54:51.080440: Epoch time: 138.2 s\n",
      "2025-12-08 00:54:51.827390: \n",
      "2025-12-08 00:54:51.827390: Epoch 333\n",
      "2025-12-08 00:54:51.827390: Current learning rate: 0.00695\n",
      "2025-12-08 00:57:10.046482: train_loss -0.8443\n",
      "2025-12-08 00:57:10.046482: val_loss -0.8656\n",
      "2025-12-08 00:57:10.062560: Pseudo dice [0.9211, 0.9543, 0.9408]\n",
      "2025-12-08 00:57:10.062560: Epoch time: 138.22 s\n",
      "2025-12-08 00:57:10.703147: \n",
      "2025-12-08 00:57:10.703147: Epoch 334\n",
      "2025-12-08 00:57:10.705663: Current learning rate: 0.00694\n",
      "2025-12-08 00:59:28.825096: train_loss -0.8399\n",
      "2025-12-08 00:59:28.825096: val_loss -0.8616\n",
      "2025-12-08 00:59:28.829157: Pseudo dice [0.9123, 0.948, 0.9473]\n",
      "2025-12-08 00:59:28.832157: Epoch time: 138.12 s\n",
      "2025-12-08 00:59:29.639856: \n",
      "2025-12-08 00:59:29.639856: Epoch 335\n",
      "2025-12-08 00:59:29.655511: Current learning rate: 0.00693\n",
      "2025-12-08 01:01:47.720448: train_loss -0.8444\n",
      "2025-12-08 01:01:47.720448: val_loss -0.8728\n",
      "2025-12-08 01:01:47.724528: Pseudo dice [0.9268, 0.9564, 0.9423]\n",
      "2025-12-08 01:01:47.727539: Epoch time: 138.08 s\n",
      "2025-12-08 01:01:47.729544: Yayy! New best EMA pseudo Dice: 0.9374\n",
      "2025-12-08 01:01:48.796588: \n",
      "2025-12-08 01:01:48.796588: Epoch 336\n",
      "2025-12-08 01:01:48.796588: Current learning rate: 0.00692\n",
      "2025-12-08 01:04:06.895915: train_loss -0.8412\n",
      "2025-12-08 01:04:06.895915: val_loss -0.8622\n",
      "2025-12-08 01:04:06.899920: Pseudo dice [0.9196, 0.9497, 0.9404]\n",
      "2025-12-08 01:04:06.901922: Epoch time: 138.1 s\n",
      "2025-12-08 01:04:07.555324: \n",
      "2025-12-08 01:04:07.555324: Epoch 337\n",
      "2025-12-08 01:04:07.559338: Current learning rate: 0.00691\n",
      "2025-12-08 01:06:25.592855: train_loss -0.8339\n",
      "2025-12-08 01:06:25.592855: val_loss -0.8533\n",
      "2025-12-08 01:06:25.592855: Pseudo dice [0.9088, 0.9481, 0.9409]\n",
      "2025-12-08 01:06:25.592855: Epoch time: 138.04 s\n",
      "2025-12-08 01:06:26.245044: \n",
      "2025-12-08 01:06:26.245044: Epoch 338\n",
      "2025-12-08 01:06:26.245044: Current learning rate: 0.0069\n",
      "2025-12-08 01:08:44.328248: train_loss -0.832\n",
      "2025-12-08 01:08:44.343976: val_loss -0.8684\n",
      "2025-12-08 01:08:44.343976: Pseudo dice [0.9254, 0.961, 0.9362]\n",
      "2025-12-08 01:08:44.343976: Epoch time: 138.08 s\n",
      "2025-12-08 01:08:45.040061: \n",
      "2025-12-08 01:08:45.041065: Epoch 339\n",
      "2025-12-08 01:08:45.044077: Current learning rate: 0.00689\n",
      "2025-12-08 01:11:03.092904: train_loss -0.8407\n",
      "2025-12-08 01:11:03.092904: val_loss -0.8619\n",
      "2025-12-08 01:11:03.098425: Pseudo dice [0.9176, 0.945, 0.9456]\n",
      "2025-12-08 01:11:03.100427: Epoch time: 138.05 s\n",
      "2025-12-08 01:11:03.748774: \n",
      "2025-12-08 01:11:03.748774: Epoch 340\n",
      "2025-12-08 01:11:03.748774: Current learning rate: 0.00688\n",
      "2025-12-08 01:13:21.807560: train_loss -0.8421\n",
      "2025-12-08 01:13:21.807560: val_loss -0.8628\n",
      "2025-12-08 01:13:21.811566: Pseudo dice [0.9191, 0.9501, 0.9396]\n",
      "2025-12-08 01:13:21.815308: Epoch time: 138.06 s\n",
      "2025-12-08 01:13:22.651364: \n",
      "2025-12-08 01:13:22.651364: Epoch 341\n",
      "2025-12-08 01:13:22.655158: Current learning rate: 0.00687\n",
      "2025-12-08 01:15:40.764422: train_loss -0.8392\n",
      "2025-12-08 01:15:40.764422: val_loss -0.8556\n",
      "2025-12-08 01:15:40.780231: Pseudo dice [0.9124, 0.9458, 0.9458]\n",
      "2025-12-08 01:15:40.780231: Epoch time: 138.11 s\n",
      "2025-12-08 01:15:41.421033: \n",
      "2025-12-08 01:15:41.421033: Epoch 342\n",
      "2025-12-08 01:15:41.421033: Current learning rate: 0.00686\n",
      "2025-12-08 01:17:59.605619: train_loss -0.7995\n",
      "2025-12-08 01:17:59.607621: val_loss -0.7946\n",
      "2025-12-08 01:17:59.609636: Pseudo dice [0.8933, 0.9287, 0.9201]\n",
      "2025-12-08 01:17:59.612760: Epoch time: 138.18 s\n",
      "2025-12-08 01:18:00.250533: \n",
      "2025-12-08 01:18:00.250533: Epoch 343\n",
      "2025-12-08 01:18:00.250533: Current learning rate: 0.00685\n",
      "2025-12-08 01:20:18.326825: train_loss -0.7811\n",
      "2025-12-08 01:20:18.326825: val_loss -0.8226\n",
      "2025-12-08 01:20:18.331953: Pseudo dice [0.8995, 0.9402, 0.9269]\n",
      "2025-12-08 01:20:18.335957: Epoch time: 138.08 s\n",
      "2025-12-08 01:20:18.984692: \n",
      "2025-12-08 01:20:18.984692: Epoch 344\n",
      "2025-12-08 01:20:18.984692: Current learning rate: 0.00684\n",
      "2025-12-08 01:22:36.951534: train_loss -0.8034\n",
      "2025-12-08 01:22:36.951534: val_loss -0.838\n",
      "2025-12-08 01:22:36.951534: Pseudo dice [0.9067, 0.947, 0.9338]\n",
      "2025-12-08 01:22:36.958352: Epoch time: 137.97 s\n",
      "2025-12-08 01:22:37.650197: \n",
      "2025-12-08 01:22:37.650197: Epoch 345\n",
      "2025-12-08 01:22:37.653610: Current learning rate: 0.00683\n",
      "2025-12-08 01:24:55.852500: train_loss -0.8204\n",
      "2025-12-08 01:24:55.852500: val_loss -0.8533\n",
      "2025-12-08 01:24:55.858245: Pseudo dice [0.9125, 0.9479, 0.9425]\n",
      "2025-12-08 01:24:55.860904: Epoch time: 138.2 s\n",
      "2025-12-08 01:24:56.514814: \n",
      "2025-12-08 01:24:56.514814: Epoch 346\n",
      "2025-12-08 01:24:56.514814: Current learning rate: 0.00682\n",
      "2025-12-08 01:27:14.725862: train_loss -0.8247\n",
      "2025-12-08 01:27:14.725862: val_loss -0.8462\n",
      "2025-12-08 01:27:14.731606: Pseudo dice [0.9152, 0.948, 0.9259]\n",
      "2025-12-08 01:27:14.733609: Epoch time: 138.21 s\n",
      "2025-12-08 01:27:15.577980: \n",
      "2025-12-08 01:27:15.577980: Epoch 347\n",
      "2025-12-08 01:27:15.577980: Current learning rate: 0.00681\n",
      "2025-12-08 01:29:33.735149: train_loss -0.8248\n",
      "2025-12-08 01:29:33.735149: val_loss -0.8482\n",
      "2025-12-08 01:29:33.738709: Pseudo dice [0.9112, 0.9427, 0.9371]\n",
      "2025-12-08 01:29:33.740711: Epoch time: 138.16 s\n",
      "2025-12-08 01:29:34.392215: \n",
      "2025-12-08 01:29:34.392215: Epoch 348\n",
      "2025-12-08 01:29:34.392215: Current learning rate: 0.0068\n",
      "2025-12-08 01:31:52.558987: train_loss -0.8316\n",
      "2025-12-08 01:31:52.560989: val_loss -0.8572\n",
      "2025-12-08 01:31:52.564088: Pseudo dice [0.9129, 0.948, 0.9471]\n",
      "2025-12-08 01:31:52.567147: Epoch time: 138.17 s\n",
      "2025-12-08 01:31:53.251940: \n",
      "2025-12-08 01:31:53.251940: Epoch 349\n",
      "2025-12-08 01:31:53.251940: Current learning rate: 0.0068\n",
      "2025-12-08 01:34:11.312023: train_loss -0.8365\n",
      "2025-12-08 01:34:11.312023: val_loss -0.8603\n",
      "2025-12-08 01:34:11.317610: Pseudo dice [0.9209, 0.9527, 0.9323]\n",
      "2025-12-08 01:34:11.319613: Epoch time: 138.06 s\n",
      "2025-12-08 01:34:12.230705: \n",
      "2025-12-08 01:34:12.230705: Epoch 350\n",
      "2025-12-08 01:34:12.232922: Current learning rate: 0.00679\n",
      "2025-12-08 01:36:30.561439: train_loss -0.8304\n",
      "2025-12-08 01:36:30.561439: val_loss -0.8519\n",
      "2025-12-08 01:36:30.577500: Pseudo dice [0.9085, 0.9462, 0.9415]\n",
      "2025-12-08 01:36:30.577500: Epoch time: 138.33 s\n",
      "2025-12-08 01:36:31.218225: \n",
      "2025-12-08 01:36:31.218225: Epoch 351\n",
      "2025-12-08 01:36:31.234223: Current learning rate: 0.00678\n",
      "2025-12-08 01:38:49.354671: train_loss -0.8347\n",
      "2025-12-08 01:38:49.354671: val_loss -0.8685\n",
      "2025-12-08 01:38:49.360415: Pseudo dice [0.9259, 0.9539, 0.9387]\n",
      "2025-12-08 01:38:49.362975: Epoch time: 138.14 s\n",
      "2025-12-08 01:38:50.015067: \n",
      "2025-12-08 01:38:50.015067: Epoch 352\n",
      "2025-12-08 01:38:50.015067: Current learning rate: 0.00677\n",
      "2025-12-08 01:41:08.108850: train_loss -0.8333\n",
      "2025-12-08 01:41:08.108850: val_loss -0.8684\n",
      "2025-12-08 01:41:08.124717: Pseudo dice [0.9172, 0.9557, 0.9453]\n",
      "2025-12-08 01:41:08.124717: Epoch time: 138.09 s\n",
      "2025-12-08 01:41:08.945081: \n",
      "2025-12-08 01:41:08.945081: Epoch 353\n",
      "2025-12-08 01:41:08.945081: Current learning rate: 0.00676\n",
      "2025-12-08 01:43:26.968684: train_loss -0.8279\n",
      "2025-12-08 01:43:26.968684: val_loss -0.8627\n",
      "2025-12-08 01:43:26.968684: Pseudo dice [0.9183, 0.953, 0.935]\n",
      "2025-12-08 01:43:26.984771: Epoch time: 138.03 s\n",
      "2025-12-08 01:43:27.639426: \n",
      "2025-12-08 01:43:27.639426: Epoch 354\n",
      "2025-12-08 01:43:27.639426: Current learning rate: 0.00675\n",
      "2025-12-08 01:45:45.717525: train_loss -0.8356\n",
      "2025-12-08 01:45:45.717525: val_loss -0.8698\n",
      "2025-12-08 01:45:45.733621: Pseudo dice [0.9221, 0.9576, 0.946]\n",
      "2025-12-08 01:45:45.733621: Epoch time: 138.09 s\n",
      "2025-12-08 01:45:46.389168: \n",
      "2025-12-08 01:45:46.389168: Epoch 355\n",
      "2025-12-08 01:45:46.389168: Current learning rate: 0.00674\n",
      "2025-12-08 01:48:04.371220: train_loss -0.8378\n",
      "2025-12-08 01:48:04.371220: val_loss -0.8699\n",
      "2025-12-08 01:48:04.376262: Pseudo dice [0.9244, 0.9558, 0.9416]\n",
      "2025-12-08 01:48:04.379269: Epoch time: 137.98 s\n",
      "2025-12-08 01:48:05.165694: \n",
      "2025-12-08 01:48:05.165694: Epoch 356\n",
      "2025-12-08 01:48:05.169703: Current learning rate: 0.00673\n",
      "2025-12-08 01:50:23.312317: train_loss -0.8368\n",
      "2025-12-08 01:50:23.312317: val_loss -0.8604\n",
      "2025-12-08 01:50:23.312317: Pseudo dice [0.9171, 0.9495, 0.9422]\n",
      "2025-12-08 01:50:23.328018: Epoch time: 138.15 s\n",
      "2025-12-08 01:50:23.952336: \n",
      "2025-12-08 01:50:23.952336: Epoch 357\n",
      "2025-12-08 01:50:23.968353: Current learning rate: 0.00672\n",
      "2025-12-08 01:52:42.171222: train_loss -0.8397\n",
      "2025-12-08 01:52:42.171222: val_loss -0.8647\n",
      "2025-12-08 01:52:42.176645: Pseudo dice [0.9231, 0.9532, 0.9369]\n",
      "2025-12-08 01:52:42.176645: Epoch time: 138.22 s\n",
      "2025-12-08 01:52:42.827331: \n",
      "2025-12-08 01:52:42.827331: Epoch 358\n",
      "2025-12-08 01:52:42.827331: Current learning rate: 0.00671\n",
      "2025-12-08 01:55:01.013851: train_loss -0.8443\n",
      "2025-12-08 01:55:01.014851: val_loss -0.8666\n",
      "2025-12-08 01:55:01.019182: Pseudo dice [0.9196, 0.954, 0.944]\n",
      "2025-12-08 01:55:01.022182: Epoch time: 138.19 s\n",
      "2025-12-08 01:55:02.041869: \n",
      "2025-12-08 01:55:02.041869: Epoch 359\n",
      "2025-12-08 01:55:02.046123: Current learning rate: 0.0067\n",
      "2025-12-08 01:57:20.124151: train_loss -0.8382\n",
      "2025-12-08 01:57:20.127131: val_loss -0.8574\n",
      "2025-12-08 01:57:20.130574: Pseudo dice [0.9165, 0.9488, 0.9435]\n",
      "2025-12-08 01:57:20.130574: Epoch time: 138.08 s\n",
      "2025-12-08 01:57:20.780443: \n",
      "2025-12-08 01:57:20.780443: Epoch 360\n",
      "2025-12-08 01:57:20.780443: Current learning rate: 0.00669\n",
      "2025-12-08 01:59:38.846765: train_loss -0.8385\n",
      "2025-12-08 01:59:38.847765: val_loss -0.8686\n",
      "2025-12-08 01:59:38.851766: Pseudo dice [0.9226, 0.9551, 0.9394]\n",
      "2025-12-08 01:59:38.854766: Epoch time: 138.07 s\n",
      "2025-12-08 01:59:39.499969: \n",
      "2025-12-08 01:59:39.499969: Epoch 361\n",
      "2025-12-08 01:59:39.518119: Current learning rate: 0.00668\n",
      "2025-12-08 02:01:57.582834: train_loss -0.8362\n",
      "2025-12-08 02:01:57.582834: val_loss -0.8644\n",
      "2025-12-08 02:01:57.586839: Pseudo dice [0.9205, 0.9573, 0.9392]\n",
      "2025-12-08 02:01:57.586839: Epoch time: 138.08 s\n",
      "2025-12-08 02:01:58.388101: \n",
      "2025-12-08 02:01:58.389105: Epoch 362\n",
      "2025-12-08 02:01:58.390108: Current learning rate: 0.00667\n",
      "2025-12-08 02:04:16.454195: train_loss -0.8386\n",
      "2025-12-08 02:04:16.455198: val_loss -0.8648\n",
      "2025-12-08 02:04:16.459213: Pseudo dice [0.9212, 0.952, 0.9433]\n",
      "2025-12-08 02:04:16.462224: Epoch time: 138.07 s\n",
      "2025-12-08 02:04:17.092880: \n",
      "2025-12-08 02:04:17.092880: Epoch 363\n",
      "2025-12-08 02:04:17.111848: Current learning rate: 0.00666\n",
      "2025-12-08 02:06:35.204291: train_loss -0.8296\n",
      "2025-12-08 02:06:35.204291: val_loss -0.8566\n",
      "2025-12-08 02:06:35.204291: Pseudo dice [0.9153, 0.9502, 0.9389]\n",
      "2025-12-08 02:06:35.204291: Epoch time: 138.11 s\n",
      "2025-12-08 02:06:35.842953: \n",
      "2025-12-08 02:06:35.842953: Epoch 364\n",
      "2025-12-08 02:06:35.858902: Current learning rate: 0.00665\n",
      "2025-12-08 02:08:54.124501: train_loss -0.8394\n",
      "2025-12-08 02:08:54.124501: val_loss -0.8642\n",
      "2025-12-08 02:08:54.128347: Pseudo dice [0.916, 0.9535, 0.945]\n",
      "2025-12-08 02:08:54.131843: Epoch time: 138.28 s\n",
      "2025-12-08 02:08:55.108622: \n",
      "2025-12-08 02:08:55.108622: Epoch 365\n",
      "2025-12-08 02:08:55.114011: Current learning rate: 0.00665\n",
      "2025-12-08 02:11:13.242517: train_loss -0.8414\n",
      "2025-12-08 02:11:13.244520: val_loss -0.8582\n",
      "2025-12-08 02:11:13.248523: Pseudo dice [0.9141, 0.9472, 0.9443]\n",
      "2025-12-08 02:11:13.252028: Epoch time: 138.13 s\n",
      "2025-12-08 02:11:13.951745: \n",
      "2025-12-08 02:11:13.951745: Epoch 366\n",
      "2025-12-08 02:11:13.967403: Current learning rate: 0.00664\n",
      "2025-12-08 02:13:32.140447: train_loss -0.8401\n",
      "2025-12-08 02:13:32.140447: val_loss -0.8699\n",
      "2025-12-08 02:13:32.158419: Pseudo dice [0.9207, 0.9599, 0.9378]\n",
      "2025-12-08 02:13:32.162423: Epoch time: 138.19 s\n",
      "2025-12-08 02:13:32.813192: \n",
      "2025-12-08 02:13:32.813192: Epoch 367\n",
      "2025-12-08 02:13:32.813192: Current learning rate: 0.00663\n",
      "2025-12-08 02:15:50.979984: train_loss -0.839\n",
      "2025-12-08 02:15:50.981987: val_loss -0.8521\n",
      "2025-12-08 02:15:50.985732: Pseudo dice [0.9093, 0.9418, 0.9429]\n",
      "2025-12-08 02:15:50.987735: Epoch time: 138.17 s\n",
      "2025-12-08 02:15:51.758043: \n",
      "2025-12-08 02:15:51.758043: Epoch 368\n",
      "2025-12-08 02:15:51.760046: Current learning rate: 0.00662\n",
      "2025-12-08 02:18:09.824323: train_loss -0.8444\n",
      "2025-12-08 02:18:09.824323: val_loss -0.8748\n",
      "2025-12-08 02:18:09.827139: Pseudo dice [0.9248, 0.9614, 0.9434]\n",
      "2025-12-08 02:18:09.827139: Epoch time: 138.07 s\n",
      "2025-12-08 02:18:10.483166: \n",
      "2025-12-08 02:18:10.483166: Epoch 369\n",
      "2025-12-08 02:18:10.483166: Current learning rate: 0.00661\n",
      "2025-12-08 02:20:28.647552: train_loss -0.8394\n",
      "2025-12-08 02:20:28.649554: val_loss -0.8763\n",
      "2025-12-08 02:20:28.651985: Pseudo dice [0.9246, 0.9604, 0.945]\n",
      "2025-12-08 02:20:28.655488: Epoch time: 138.16 s\n",
      "2025-12-08 02:20:28.659087: Yayy! New best EMA pseudo Dice: 0.9377\n",
      "2025-12-08 02:20:29.564861: \n",
      "2025-12-08 02:20:29.564861: Epoch 370\n",
      "2025-12-08 02:20:29.564861: Current learning rate: 0.0066\n",
      "2025-12-08 02:22:47.487225: train_loss -0.8282\n",
      "2025-12-08 02:22:47.488229: val_loss -0.8508\n",
      "2025-12-08 02:22:47.492233: Pseudo dice [0.9151, 0.9466, 0.9348]\n",
      "2025-12-08 02:22:47.495234: Epoch time: 137.92 s\n",
      "2025-12-08 02:22:48.405642: \n",
      "2025-12-08 02:22:48.405642: Epoch 371\n",
      "2025-12-08 02:22:48.405642: Current learning rate: 0.00659\n",
      "2025-12-08 02:25:06.483941: train_loss -0.8183\n",
      "2025-12-08 02:25:06.483941: val_loss -0.8434\n",
      "2025-12-08 02:25:06.491540: Pseudo dice [0.9136, 0.9454, 0.9262]\n",
      "2025-12-08 02:25:06.493543: Epoch time: 138.08 s\n",
      "2025-12-08 02:25:07.151245: \n",
      "2025-12-08 02:25:07.151245: Epoch 372\n",
      "2025-12-08 02:25:07.154933: Current learning rate: 0.00658\n",
      "2025-12-08 02:27:25.075832: train_loss -0.825\n",
      "2025-12-08 02:27:25.075832: val_loss -0.8476\n",
      "2025-12-08 02:27:25.081002: Pseudo dice [0.9109, 0.9462, 0.9405]\n",
      "2025-12-08 02:27:25.084002: Epoch time: 137.93 s\n",
      "2025-12-08 02:27:25.764031: \n",
      "2025-12-08 02:27:25.764031: Epoch 373\n",
      "2025-12-08 02:27:25.779832: Current learning rate: 0.00657\n",
      "2025-12-08 02:29:43.936963: train_loss -0.8283\n",
      "2025-12-08 02:29:43.938966: val_loss -0.873\n",
      "2025-12-08 02:29:43.942971: Pseudo dice [0.9268, 0.9564, 0.9432]\n",
      "2025-12-08 02:29:43.946713: Epoch time: 138.17 s\n",
      "2025-12-08 02:29:44.670276: \n",
      "2025-12-08 02:29:44.670276: Epoch 374\n",
      "2025-12-08 02:29:44.681874: Current learning rate: 0.00656\n",
      "2025-12-08 02:32:02.868458: train_loss -0.835\n",
      "2025-12-08 02:32:02.870461: val_loss -0.8624\n",
      "2025-12-08 02:32:02.874205: Pseudo dice [0.919, 0.9534, 0.9409]\n",
      "2025-12-08 02:32:02.874205: Epoch time: 138.2 s\n",
      "2025-12-08 02:32:03.530175: \n",
      "2025-12-08 02:32:03.530175: Epoch 375\n",
      "2025-12-08 02:32:03.530175: Current learning rate: 0.00655\n",
      "2025-12-08 02:34:21.592253: train_loss -0.8421\n",
      "2025-12-08 02:34:21.592253: val_loss -0.8704\n",
      "2025-12-08 02:34:21.608191: Pseudo dice [0.9219, 0.9569, 0.9439]\n",
      "2025-12-08 02:34:21.608191: Epoch time: 138.08 s\n",
      "2025-12-08 02:34:22.264355: \n",
      "2025-12-08 02:34:22.264355: Epoch 376\n",
      "2025-12-08 02:34:22.264355: Current learning rate: 0.00654\n",
      "2025-12-08 02:36:40.390229: train_loss -0.841\n",
      "2025-12-08 02:36:40.390229: val_loss -0.8527\n",
      "2025-12-08 02:36:40.399731: Pseudo dice [0.9141, 0.9436, 0.9419]\n",
      "2025-12-08 02:36:40.401734: Epoch time: 138.13 s\n",
      "2025-12-08 02:36:41.232527: \n",
      "2025-12-08 02:36:41.233535: Epoch 377\n",
      "2025-12-08 02:36:41.233535: Current learning rate: 0.00653\n",
      "2025-12-08 02:38:59.583930: train_loss -0.8316\n",
      "2025-12-08 02:38:59.587930: val_loss -0.8659\n",
      "2025-12-08 02:38:59.591931: Pseudo dice [0.92, 0.9551, 0.9401]\n",
      "2025-12-08 02:38:59.594999: Epoch time: 138.35 s\n",
      "2025-12-08 02:39:00.481476: \n",
      "2025-12-08 02:39:00.483479: Epoch 378\n",
      "2025-12-08 02:39:00.486238: Current learning rate: 0.00652\n",
      "2025-12-08 02:41:18.677762: train_loss -0.8364\n",
      "2025-12-08 02:41:18.678762: val_loss -0.8603\n",
      "2025-12-08 02:41:18.682194: Pseudo dice [0.9148, 0.9507, 0.9418]\n",
      "2025-12-08 02:41:18.685194: Epoch time: 138.2 s\n",
      "2025-12-08 02:41:19.346641: \n",
      "2025-12-08 02:41:19.347642: Epoch 379\n",
      "2025-12-08 02:41:19.350807: Current learning rate: 0.00651\n",
      "2025-12-08 02:43:37.487444: train_loss -0.8377\n",
      "2025-12-08 02:43:37.487444: val_loss -0.8735\n",
      "2025-12-08 02:43:37.487444: Pseudo dice [0.9245, 0.9589, 0.9465]\n",
      "2025-12-08 02:43:37.487444: Epoch time: 138.14 s\n",
      "2025-12-08 02:43:38.139395: \n",
      "2025-12-08 02:43:38.139395: Epoch 380\n",
      "2025-12-08 02:43:38.155071: Current learning rate: 0.0065\n",
      "2025-12-08 02:45:56.295943: train_loss -0.8448\n",
      "2025-12-08 02:45:56.295943: val_loss -0.8577\n",
      "2025-12-08 02:45:56.295943: Pseudo dice [0.913, 0.9456, 0.9423]\n",
      "2025-12-08 02:45:56.295943: Epoch time: 138.16 s\n",
      "2025-12-08 02:45:56.951688: \n",
      "2025-12-08 02:45:56.951688: Epoch 381\n",
      "2025-12-08 02:45:56.951688: Current learning rate: 0.00649\n",
      "2025-12-08 02:48:15.217619: train_loss -0.8397\n",
      "2025-12-08 02:48:15.217619: val_loss -0.872\n",
      "2025-12-08 02:48:15.221240: Pseudo dice [0.922, 0.9535, 0.9402]\n",
      "2025-12-08 02:48:15.225722: Epoch time: 138.27 s\n",
      "2025-12-08 02:48:15.873787: \n",
      "2025-12-08 02:48:15.873787: Epoch 382\n",
      "2025-12-08 02:48:15.873787: Current learning rate: 0.00648\n",
      "2025-12-08 02:50:34.107256: train_loss -0.8444\n",
      "2025-12-08 02:50:34.109001: val_loss -0.8674\n",
      "2025-12-08 02:50:34.113009: Pseudo dice [0.9207, 0.9543, 0.9361]\n",
      "2025-12-08 02:50:34.117013: Epoch time: 138.23 s\n",
      "2025-12-08 02:50:35.030485: \n",
      "2025-12-08 02:50:35.030485: Epoch 383\n",
      "2025-12-08 02:50:35.046127: Current learning rate: 0.00648\n",
      "2025-12-08 02:52:53.181716: train_loss -0.8495\n",
      "2025-12-08 02:52:53.181716: val_loss -0.86\n",
      "2025-12-08 02:52:53.187564: Pseudo dice [0.9168, 0.9484, 0.9478]\n",
      "2025-12-08 02:52:53.190568: Epoch time: 138.15 s\n",
      "2025-12-08 02:52:53.855164: \n",
      "2025-12-08 02:52:53.855164: Epoch 384\n",
      "2025-12-08 02:52:53.858153: Current learning rate: 0.00647\n",
      "2025-12-08 02:55:12.061866: train_loss -0.8437\n",
      "2025-12-08 02:55:12.061866: val_loss -0.8639\n",
      "2025-12-08 02:55:12.077723: Pseudo dice [0.9171, 0.9476, 0.9466]\n",
      "2025-12-08 02:55:12.077723: Epoch time: 138.21 s\n",
      "2025-12-08 02:55:12.733779: \n",
      "2025-12-08 02:55:12.733779: Epoch 385\n",
      "2025-12-08 02:55:12.749818: Current learning rate: 0.00646\n",
      "2025-12-08 02:57:30.735100: train_loss -0.8416\n",
      "2025-12-08 02:57:30.735100: val_loss -0.8721\n",
      "2025-12-08 02:57:30.735100: Pseudo dice [0.9287, 0.9572, 0.9451]\n",
      "2025-12-08 02:57:30.735100: Epoch time: 138.0 s\n",
      "2025-12-08 02:57:30.735100: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2025-12-08 02:57:31.786474: \n",
      "2025-12-08 02:57:31.788477: Epoch 386\n",
      "2025-12-08 02:57:31.792483: Current learning rate: 0.00645\n",
      "2025-12-08 02:59:49.942345: train_loss -0.8341\n",
      "2025-12-08 02:59:49.943347: val_loss -0.8669\n",
      "2025-12-08 02:59:49.947356: Pseudo dice [0.9189, 0.9476, 0.949]\n",
      "2025-12-08 02:59:49.950357: Epoch time: 138.16 s\n",
      "2025-12-08 02:59:49.954278: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2025-12-08 02:59:50.921900: \n",
      "2025-12-08 02:59:50.922902: Epoch 387\n",
      "2025-12-08 02:59:50.925962: Current learning rate: 0.00644\n",
      "2025-12-08 03:02:09.124265: train_loss -0.8449\n",
      "2025-12-08 03:02:09.124265: val_loss -0.8562\n",
      "2025-12-08 03:02:09.142216: Pseudo dice [0.9161, 0.9476, 0.9387]\n",
      "2025-12-08 03:02:09.144219: Epoch time: 138.2 s\n",
      "2025-12-08 03:02:09.808990: \n",
      "2025-12-08 03:02:09.808990: Epoch 388\n",
      "2025-12-08 03:02:09.813210: Current learning rate: 0.00643\n",
      "2025-12-08 03:04:27.968601: train_loss -0.8448\n",
      "2025-12-08 03:04:27.968601: val_loss -0.8572\n",
      "2025-12-08 03:04:27.968601: Pseudo dice [0.9146, 0.947, 0.937]\n",
      "2025-12-08 03:04:27.983935: Epoch time: 138.16 s\n",
      "2025-12-08 03:04:28.952362: \n",
      "2025-12-08 03:04:28.952362: Epoch 389\n",
      "2025-12-08 03:04:28.952362: Current learning rate: 0.00642\n",
      "2025-12-08 03:06:47.077363: train_loss -0.8412\n",
      "2025-12-08 03:06:47.077363: val_loss -0.8699\n",
      "2025-12-08 03:06:47.093205: Pseudo dice [0.9214, 0.9549, 0.9484]\n",
      "2025-12-08 03:06:47.093205: Epoch time: 138.14 s\n",
      "2025-12-08 03:06:47.784683: \n",
      "2025-12-08 03:06:47.785685: Epoch 390\n",
      "2025-12-08 03:06:47.788692: Current learning rate: 0.00641\n",
      "2025-12-08 03:09:06.078344: train_loss -0.8399\n",
      "2025-12-08 03:09:06.078344: val_loss -0.8639\n",
      "2025-12-08 03:09:06.083585: Pseudo dice [0.9156, 0.9525, 0.9453]\n",
      "2025-12-08 03:09:06.083585: Epoch time: 138.29 s\n",
      "2025-12-08 03:09:06.733888: \n",
      "2025-12-08 03:09:06.733888: Epoch 391\n",
      "2025-12-08 03:09:06.733888: Current learning rate: 0.0064\n",
      "2025-12-08 03:11:24.836907: train_loss -0.8387\n",
      "2025-12-08 03:11:24.836907: val_loss -0.8686\n",
      "2025-12-08 03:11:24.840911: Pseudo dice [0.9233, 0.954, 0.9407]\n",
      "2025-12-08 03:11:24.842912: Epoch time: 138.1 s\n",
      "2025-12-08 03:11:25.509063: \n",
      "2025-12-08 03:11:25.509063: Epoch 392\n",
      "2025-12-08 03:11:25.509063: Current learning rate: 0.00639\n",
      "2025-12-08 03:13:43.632042: train_loss -0.8453\n",
      "2025-12-08 03:13:43.632042: val_loss -0.8676\n",
      "2025-12-08 03:13:43.636046: Pseudo dice [0.9191, 0.956, 0.947]\n",
      "2025-12-08 03:13:43.641791: Epoch time: 138.12 s\n",
      "2025-12-08 03:13:43.643794: Yayy! New best EMA pseudo Dice: 0.938\n",
      "2025-12-08 03:13:44.609759: \n",
      "2025-12-08 03:13:44.609759: Epoch 393\n",
      "2025-12-08 03:13:44.609759: Current learning rate: 0.00638\n",
      "2025-12-08 03:16:02.657092: train_loss -0.843\n",
      "2025-12-08 03:16:02.657092: val_loss -0.8646\n",
      "2025-12-08 03:16:02.661096: Pseudo dice [0.9182, 0.9535, 0.947]\n",
      "2025-12-08 03:16:02.665100: Epoch time: 138.05 s\n",
      "2025-12-08 03:16:02.667102: Yayy! New best EMA pseudo Dice: 0.9382\n",
      "2025-12-08 03:16:03.612159: \n",
      "2025-12-08 03:16:03.612159: Epoch 394\n",
      "2025-12-08 03:16:03.616231: Current learning rate: 0.00637\n",
      "2025-12-08 03:18:21.594995: train_loss -0.8362\n",
      "2025-12-08 03:18:21.594995: val_loss -0.8691\n",
      "2025-12-08 03:18:21.599014: Pseudo dice [0.9263, 0.9552, 0.9373]\n",
      "2025-12-08 03:18:21.602023: Epoch time: 137.98 s\n",
      "2025-12-08 03:18:21.605032: Yayy! New best EMA pseudo Dice: 0.9383\n",
      "2025-12-08 03:18:22.515700: \n",
      "2025-12-08 03:18:22.515700: Epoch 395\n",
      "2025-12-08 03:18:22.530218: Current learning rate: 0.00636\n",
      "2025-12-08 03:20:40.606604: train_loss -0.8373\n",
      "2025-12-08 03:20:40.608344: val_loss -0.8663\n",
      "2025-12-08 03:20:40.614354: Pseudo dice [0.9227, 0.9539, 0.9458]\n",
      "2025-12-08 03:20:40.618360: Epoch time: 138.09 s\n",
      "2025-12-08 03:20:40.622364: Yayy! New best EMA pseudo Dice: 0.9386\n",
      "2025-12-08 03:20:41.578708: \n",
      "2025-12-08 03:20:41.578708: Epoch 396\n",
      "2025-12-08 03:20:41.578708: Current learning rate: 0.00635\n",
      "2025-12-08 03:22:59.704244: train_loss -0.842\n",
      "2025-12-08 03:22:59.704244: val_loss -0.8651\n",
      "2025-12-08 03:22:59.708248: Pseudo dice [0.9164, 0.9501, 0.9515]\n",
      "2025-12-08 03:22:59.710250: Epoch time: 138.13 s\n",
      "2025-12-08 03:22:59.714256: Yayy! New best EMA pseudo Dice: 0.9386\n",
      "2025-12-08 03:23:00.651760: \n",
      "2025-12-08 03:23:00.653500: Epoch 397\n",
      "2025-12-08 03:23:00.657289: Current learning rate: 0.00634\n",
      "2025-12-08 03:25:18.843692: train_loss -0.8387\n",
      "2025-12-08 03:25:18.843692: val_loss -0.8817\n",
      "2025-12-08 03:25:18.848744: Pseudo dice [0.93, 0.9595, 0.9471]\n",
      "2025-12-08 03:25:18.853226: Epoch time: 138.19 s\n",
      "2025-12-08 03:25:18.856241: Yayy! New best EMA pseudo Dice: 0.9393\n",
      "2025-12-08 03:25:19.796421: \n",
      "2025-12-08 03:25:19.796421: Epoch 398\n",
      "2025-12-08 03:25:19.796421: Current learning rate: 0.00633\n",
      "2025-12-08 03:27:37.898034: train_loss -0.8438\n",
      "2025-12-08 03:27:37.899036: val_loss -0.8692\n",
      "2025-12-08 03:27:37.902038: Pseudo dice [0.9198, 0.9604, 0.9428]\n",
      "2025-12-08 03:27:37.905398: Epoch time: 138.1 s\n",
      "2025-12-08 03:27:37.905398: Yayy! New best EMA pseudo Dice: 0.9395\n",
      "2025-12-08 03:27:38.844100: \n",
      "2025-12-08 03:27:38.844100: Epoch 399\n",
      "2025-12-08 03:27:38.855774: Current learning rate: 0.00632\n",
      "2025-12-08 03:29:56.905093: train_loss -0.8466\n",
      "2025-12-08 03:29:56.907132: val_loss -0.856\n",
      "2025-12-08 03:29:56.909134: Pseudo dice [0.9121, 0.9491, 0.9414]\n",
      "2025-12-08 03:29:56.914598: Epoch time: 138.06 s\n",
      "2025-12-08 03:29:57.997496: \n",
      "2025-12-08 03:29:57.998498: Epoch 400\n",
      "2025-12-08 03:29:57.999501: Current learning rate: 0.00631\n",
      "2025-12-08 03:32:15.973520: train_loss -0.8398\n",
      "2025-12-08 03:32:15.973520: val_loss -0.8713\n",
      "2025-12-08 03:32:15.975522: Pseudo dice [0.9202, 0.9549, 0.9446]\n",
      "2025-12-08 03:32:15.981361: Epoch time: 137.98 s\n",
      "2025-12-08 03:32:16.678240: \n",
      "2025-12-08 03:32:16.678240: Epoch 401\n",
      "2025-12-08 03:32:16.680244: Current learning rate: 0.0063\n",
      "2025-12-08 03:34:34.480013: train_loss -0.8459\n",
      "2025-12-08 03:34:34.480013: val_loss -0.8769\n",
      "2025-12-08 03:34:34.485519: Pseudo dice [0.9247, 0.9573, 0.9474]\n",
      "2025-12-08 03:34:34.489523: Epoch time: 137.81 s\n",
      "2025-12-08 03:34:35.139940: \n",
      "2025-12-08 03:34:35.139940: Epoch 402\n",
      "2025-12-08 03:34:35.158858: Current learning rate: 0.0063\n",
      "2025-12-08 03:36:53.233999: train_loss -0.8394\n",
      "2025-12-08 03:36:53.233999: val_loss -0.8556\n",
      "2025-12-08 03:36:53.233999: Pseudo dice [0.9148, 0.9464, 0.9412]\n",
      "2025-12-08 03:36:53.241812: Epoch time: 138.09 s\n",
      "2025-12-08 03:36:53.890372: \n",
      "2025-12-08 03:36:53.890372: Epoch 403\n",
      "2025-12-08 03:36:53.890372: Current learning rate: 0.00629\n",
      "2025-12-08 03:39:12.027005: train_loss -0.8364\n",
      "2025-12-08 03:39:12.028007: val_loss -0.8705\n",
      "2025-12-08 03:39:12.032205: Pseudo dice [0.9221, 0.955, 0.9471]\n",
      "2025-12-08 03:39:12.032205: Epoch time: 138.14 s\n",
      "2025-12-08 03:39:12.718785: \n",
      "2025-12-08 03:39:12.718785: Epoch 404\n",
      "2025-12-08 03:39:12.718785: Current learning rate: 0.00628\n",
      "2025-12-08 03:41:31.069227: train_loss -0.8408\n",
      "2025-12-08 03:41:31.071230: val_loss -0.8509\n",
      "2025-12-08 03:41:31.075236: Pseudo dice [0.9118, 0.9502, 0.9361]\n",
      "2025-12-08 03:41:31.077238: Epoch time: 138.35 s\n",
      "2025-12-08 03:41:31.904927: \n",
      "2025-12-08 03:41:31.904927: Epoch 405\n",
      "2025-12-08 03:41:31.904927: Current learning rate: 0.00627\n",
      "2025-12-08 03:43:50.056937: train_loss -0.8406\n",
      "2025-12-08 03:43:50.057938: val_loss -0.8555\n",
      "2025-12-08 03:43:50.060939: Pseudo dice [0.9133, 0.951, 0.9398]\n",
      "2025-12-08 03:43:50.066652: Epoch time: 138.15 s\n",
      "2025-12-08 03:43:50.719065: \n",
      "2025-12-08 03:43:50.719065: Epoch 406\n",
      "2025-12-08 03:43:50.719065: Current learning rate: 0.00626\n",
      "2025-12-08 03:46:08.916267: train_loss -0.8316\n",
      "2025-12-08 03:46:08.916267: val_loss -0.8566\n",
      "2025-12-08 03:46:08.920272: Pseudo dice [0.9154, 0.9492, 0.9412]\n",
      "2025-12-08 03:46:08.924170: Epoch time: 138.2 s\n",
      "2025-12-08 03:46:09.584572: \n",
      "2025-12-08 03:46:09.585577: Epoch 407\n",
      "2025-12-08 03:46:09.588320: Current learning rate: 0.00625\n",
      "2025-12-08 03:48:27.671250: train_loss -0.8335\n",
      "2025-12-08 03:48:27.671250: val_loss -0.8674\n",
      "2025-12-08 03:48:27.682055: Pseudo dice [0.923, 0.9537, 0.9422]\n",
      "2025-12-08 03:48:27.682055: Epoch time: 138.09 s\n",
      "2025-12-08 03:48:28.327754: \n",
      "2025-12-08 03:48:28.327754: Epoch 408\n",
      "2025-12-08 03:48:28.337674: Current learning rate: 0.00624\n",
      "2025-12-08 03:50:46.390258: train_loss -0.8379\n",
      "2025-12-08 03:50:46.390258: val_loss -0.8678\n",
      "2025-12-08 03:50:46.397342: Pseudo dice [0.9205, 0.9591, 0.9444]\n",
      "2025-12-08 03:50:46.397342: Epoch time: 138.06 s\n",
      "2025-12-08 03:50:47.055384: \n",
      "2025-12-08 03:50:47.056397: Epoch 409\n",
      "2025-12-08 03:50:47.056397: Current learning rate: 0.00623\n",
      "2025-12-08 03:53:05.265937: train_loss -0.8405\n",
      "2025-12-08 03:53:05.265937: val_loss -0.8636\n",
      "2025-12-08 03:53:05.269941: Pseudo dice [0.9153, 0.955, 0.9437]\n",
      "2025-12-08 03:53:05.273945: Epoch time: 138.22 s\n",
      "2025-12-08 03:53:05.937830: \n",
      "2025-12-08 03:53:05.937830: Epoch 410\n",
      "2025-12-08 03:53:05.937830: Current learning rate: 0.00622\n",
      "2025-12-08 03:55:24.264482: train_loss -0.8416\n",
      "2025-12-08 03:55:24.264482: val_loss -0.8626\n",
      "2025-12-08 03:55:24.280422: Pseudo dice [0.915, 0.9494, 0.9406]\n",
      "2025-12-08 03:55:24.280422: Epoch time: 138.33 s\n",
      "2025-12-08 03:55:25.078295: \n",
      "2025-12-08 03:55:25.078295: Epoch 411\n",
      "2025-12-08 03:55:25.078295: Current learning rate: 0.00621\n",
      "2025-12-08 03:57:43.248443: train_loss -0.8467\n",
      "2025-12-08 03:57:43.248443: val_loss -0.877\n",
      "2025-12-08 03:57:43.248443: Pseudo dice [0.926, 0.9551, 0.9504]\n",
      "2025-12-08 03:57:43.248443: Epoch time: 138.19 s\n",
      "2025-12-08 03:57:43.889479: \n",
      "2025-12-08 03:57:43.889479: Epoch 412\n",
      "2025-12-08 03:57:43.889479: Current learning rate: 0.0062\n",
      "2025-12-08 04:00:02.046167: train_loss -0.8395\n",
      "2025-12-08 04:00:02.046167: val_loss -0.8635\n",
      "2025-12-08 04:00:02.053979: Pseudo dice [0.922, 0.9522, 0.9394]\n",
      "2025-12-08 04:00:02.053979: Epoch time: 138.16 s\n",
      "2025-12-08 04:00:02.687552: \n",
      "2025-12-08 04:00:02.687552: Epoch 413\n",
      "2025-12-08 04:00:02.687552: Current learning rate: 0.00619\n",
      "2025-12-08 04:02:21.028466: train_loss -0.8401\n",
      "2025-12-08 04:02:21.030207: val_loss -0.8649\n",
      "2025-12-08 04:02:21.030207: Pseudo dice [0.9202, 0.9539, 0.9393]\n",
      "2025-12-08 04:02:21.036333: Epoch time: 138.34 s\n",
      "2025-12-08 04:02:21.654998: \n",
      "2025-12-08 04:02:21.654998: Epoch 414\n",
      "2025-12-08 04:02:21.670675: Current learning rate: 0.00618\n",
      "2025-12-08 04:04:39.889251: train_loss -0.8442\n",
      "2025-12-08 04:04:39.889251: val_loss -0.8683\n",
      "2025-12-08 04:04:39.895257: Pseudo dice [0.9224, 0.9538, 0.9472]\n",
      "2025-12-08 04:04:39.898999: Epoch time: 138.23 s\n",
      "2025-12-08 04:04:40.530149: \n",
      "2025-12-08 04:04:40.530149: Epoch 415\n",
      "2025-12-08 04:04:40.530149: Current learning rate: 0.00617\n",
      "2025-12-08 04:06:58.717176: train_loss -0.8432\n",
      "2025-12-08 04:06:58.718917: val_loss -0.8669\n",
      "2025-12-08 04:06:58.724041: Pseudo dice [0.9213, 0.9531, 0.9443]\n",
      "2025-12-08 04:06:58.728042: Epoch time: 138.19 s\n",
      "2025-12-08 04:06:59.367639: \n",
      "2025-12-08 04:06:59.367639: Epoch 416\n",
      "2025-12-08 04:06:59.371012: Current learning rate: 0.00616\n",
      "2025-12-08 04:09:17.525694: train_loss -0.8453\n",
      "2025-12-08 04:09:17.527696: val_loss -0.8649\n",
      "2025-12-08 04:09:17.530699: Pseudo dice [0.9157, 0.9489, 0.9474]\n",
      "2025-12-08 04:09:17.530699: Epoch time: 138.16 s\n",
      "2025-12-08 04:09:18.171477: \n",
      "2025-12-08 04:09:18.171477: Epoch 417\n",
      "2025-12-08 04:09:18.171477: Current learning rate: 0.00615\n",
      "2025-12-08 04:11:36.343222: train_loss -0.8472\n",
      "2025-12-08 04:11:36.343222: val_loss -0.8651\n",
      "2025-12-08 04:11:36.343222: Pseudo dice [0.9154, 0.9528, 0.9426]\n",
      "2025-12-08 04:11:36.343222: Epoch time: 138.17 s\n",
      "2025-12-08 04:11:37.159213: \n",
      "2025-12-08 04:11:37.159213: Epoch 418\n",
      "2025-12-08 04:11:37.159213: Current learning rate: 0.00614\n",
      "2025-12-08 04:13:55.369683: train_loss -0.8429\n",
      "2025-12-08 04:13:55.369683: val_loss -0.8663\n",
      "2025-12-08 04:13:55.373686: Pseudo dice [0.9221, 0.9541, 0.9398]\n",
      "2025-12-08 04:13:55.377691: Epoch time: 138.21 s\n",
      "2025-12-08 04:13:55.999895: \n",
      "2025-12-08 04:13:55.999895: Epoch 419\n",
      "2025-12-08 04:13:56.015930: Current learning rate: 0.00613\n",
      "2025-12-08 04:16:14.213716: train_loss -0.845\n",
      "2025-12-08 04:16:14.215718: val_loss -0.8661\n",
      "2025-12-08 04:16:14.219723: Pseudo dice [0.9187, 0.9508, 0.9463]\n",
      "2025-12-08 04:16:14.221725: Epoch time: 138.21 s\n",
      "2025-12-08 04:16:14.842891: \n",
      "2025-12-08 04:16:14.842891: Epoch 420\n",
      "2025-12-08 04:16:14.842891: Current learning rate: 0.00612\n",
      "2025-12-08 04:18:33.217798: train_loss -0.8374\n",
      "2025-12-08 04:18:33.217798: val_loss -0.8743\n",
      "2025-12-08 04:18:33.217798: Pseudo dice [0.9224, 0.9592, 0.9456]\n",
      "2025-12-08 04:18:33.217798: Epoch time: 138.37 s\n",
      "2025-12-08 04:18:33.874552: \n",
      "2025-12-08 04:18:33.874552: Epoch 421\n",
      "2025-12-08 04:18:33.874552: Current learning rate: 0.00612\n",
      "2025-12-08 04:20:52.074597: train_loss -0.8418\n",
      "2025-12-08 04:20:52.076601: val_loss -0.8718\n",
      "2025-12-08 04:20:52.082350: Pseudo dice [0.9258, 0.9589, 0.9427]\n",
      "2025-12-08 04:20:52.086355: Epoch time: 138.2 s\n",
      "2025-12-08 04:20:52.723272: \n",
      "2025-12-08 04:20:52.723272: Epoch 422\n",
      "2025-12-08 04:20:52.723272: Current learning rate: 0.00611\n",
      "2025-12-08 04:23:10.843787: train_loss -0.8362\n",
      "2025-12-08 04:23:10.843787: val_loss -0.8568\n",
      "2025-12-08 04:23:10.859681: Pseudo dice [0.9111, 0.9537, 0.9432]\n",
      "2025-12-08 04:23:10.862787: Epoch time: 138.12 s\n",
      "2025-12-08 04:23:11.483188: \n",
      "2025-12-08 04:23:11.483188: Epoch 423\n",
      "2025-12-08 04:23:11.483188: Current learning rate: 0.0061\n",
      "2025-12-08 04:25:29.608536: train_loss -0.8438\n",
      "2025-12-08 04:25:29.608536: val_loss -0.8714\n",
      "2025-12-08 04:25:29.608536: Pseudo dice [0.919, 0.9572, 0.9447]\n",
      "2025-12-08 04:25:29.608536: Epoch time: 138.14 s\n",
      "2025-12-08 04:25:30.493695: \n",
      "2025-12-08 04:25:30.494741: Epoch 424\n",
      "2025-12-08 04:25:30.497749: Current learning rate: 0.00609\n",
      "2025-12-08 04:27:48.656250: train_loss -0.8406\n",
      "2025-12-08 04:27:48.656250: val_loss -0.8562\n",
      "2025-12-08 04:27:48.656250: Pseudo dice [0.9143, 0.9465, 0.9446]\n",
      "2025-12-08 04:27:48.656250: Epoch time: 138.16 s\n",
      "2025-12-08 04:27:49.280776: \n",
      "2025-12-08 04:27:49.280776: Epoch 425\n",
      "2025-12-08 04:27:49.298748: Current learning rate: 0.00608\n",
      "2025-12-08 04:30:07.378622: train_loss -0.8444\n",
      "2025-12-08 04:30:07.378622: val_loss -0.8733\n",
      "2025-12-08 04:30:07.384628: Pseudo dice [0.9237, 0.9506, 0.9455]\n",
      "2025-12-08 04:30:07.388632: Epoch time: 138.1 s\n",
      "2025-12-08 04:30:08.026947: \n",
      "2025-12-08 04:30:08.026947: Epoch 426\n",
      "2025-12-08 04:30:08.030803: Current learning rate: 0.00607\n",
      "2025-12-08 04:32:26.185552: train_loss -0.8421\n",
      "2025-12-08 04:32:26.185552: val_loss -0.8702\n",
      "2025-12-08 04:32:26.186553: Pseudo dice [0.9218, 0.9546, 0.9471]\n",
      "2025-12-08 04:32:26.186553: Epoch time: 138.16 s\n",
      "2025-12-08 04:32:26.931798: \n",
      "2025-12-08 04:32:26.933800: Epoch 427\n",
      "2025-12-08 04:32:26.937544: Current learning rate: 0.00606\n",
      "2025-12-08 04:34:44.992664: train_loss -0.8372\n",
      "2025-12-08 04:34:44.993666: val_loss -0.867\n",
      "2025-12-08 04:34:44.997669: Pseudo dice [0.9162, 0.9517, 0.9483]\n",
      "2025-12-08 04:34:44.998670: Epoch time: 138.06 s\n",
      "2025-12-08 04:34:45.640112: \n",
      "2025-12-08 04:34:45.640112: Epoch 428\n",
      "2025-12-08 04:34:45.644178: Current learning rate: 0.00605\n",
      "2025-12-08 04:37:03.704918: train_loss -0.8415\n",
      "2025-12-08 04:37:03.705920: val_loss -0.8692\n",
      "2025-12-08 04:37:03.710933: Pseudo dice [0.9189, 0.9557, 0.9385]\n",
      "2025-12-08 04:37:03.713934: Epoch time: 138.07 s\n",
      "2025-12-08 04:37:04.385855: \n",
      "2025-12-08 04:37:04.386861: Epoch 429\n",
      "2025-12-08 04:37:04.390876: Current learning rate: 0.00604\n",
      "2025-12-08 04:39:22.468029: train_loss -0.8413\n",
      "2025-12-08 04:39:22.468029: val_loss -0.8531\n",
      "2025-12-08 04:39:22.484149: Pseudo dice [0.9128, 0.9491, 0.9399]\n",
      "2025-12-08 04:39:22.484149: Epoch time: 138.08 s\n",
      "2025-12-08 04:39:23.436585: \n",
      "2025-12-08 04:39:23.436585: Epoch 430\n",
      "2025-12-08 04:39:23.452533: Current learning rate: 0.00603\n",
      "2025-12-08 04:41:41.512358: train_loss -0.8445\n",
      "2025-12-08 04:41:41.514361: val_loss -0.8753\n",
      "2025-12-08 04:41:41.514361: Pseudo dice [0.9251, 0.9545, 0.9447]\n",
      "2025-12-08 04:41:41.521347: Epoch time: 138.08 s\n",
      "2025-12-08 04:41:42.167205: \n",
      "2025-12-08 04:41:42.167205: Epoch 431\n",
      "2025-12-08 04:41:42.171210: Current learning rate: 0.00602\n",
      "2025-12-08 04:44:00.317755: train_loss -0.8366\n",
      "2025-12-08 04:44:00.317755: val_loss -0.8517\n",
      "2025-12-08 04:44:00.323760: Pseudo dice [0.913, 0.9486, 0.9351]\n",
      "2025-12-08 04:44:00.325762: Epoch time: 138.15 s\n",
      "2025-12-08 04:44:00.952704: \n",
      "2025-12-08 04:44:00.952704: Epoch 432\n",
      "2025-12-08 04:44:00.968697: Current learning rate: 0.00601\n",
      "2025-12-08 04:46:19.030457: train_loss -0.8366\n",
      "2025-12-08 04:46:19.032427: val_loss -0.8595\n",
      "2025-12-08 04:46:19.036431: Pseudo dice [0.918, 0.955, 0.9379]\n",
      "2025-12-08 04:46:19.038433: Epoch time: 138.08 s\n",
      "2025-12-08 04:46:19.670521: \n",
      "2025-12-08 04:46:19.670521: Epoch 433\n",
      "2025-12-08 04:46:19.686603: Current learning rate: 0.006\n",
      "2025-12-08 04:48:37.870573: train_loss -0.8408\n",
      "2025-12-08 04:48:37.872576: val_loss -0.8628\n",
      "2025-12-08 04:48:37.878082: Pseudo dice [0.921, 0.9465, 0.942]\n",
      "2025-12-08 04:48:37.880085: Epoch time: 138.2 s\n",
      "2025-12-08 04:48:38.523712: \n",
      "2025-12-08 04:48:38.525715: Epoch 434\n",
      "2025-12-08 04:48:38.525715: Current learning rate: 0.00599\n",
      "2025-12-08 04:50:56.573106: train_loss -0.8452\n",
      "2025-12-08 04:50:56.573106: val_loss -0.8651\n",
      "2025-12-08 04:50:56.576848: Pseudo dice [0.9183, 0.9529, 0.9389]\n",
      "2025-12-08 04:50:56.580853: Epoch time: 138.05 s\n",
      "2025-12-08 04:50:57.217012: \n",
      "2025-12-08 04:50:57.217012: Epoch 435\n",
      "2025-12-08 04:50:57.217012: Current learning rate: 0.00598\n",
      "2025-12-08 04:53:15.324461: train_loss -0.843\n",
      "2025-12-08 04:53:15.324461: val_loss -0.8665\n",
      "2025-12-08 04:53:15.328343: Pseudo dice [0.9173, 0.9523, 0.944]\n",
      "2025-12-08 04:53:15.330345: Epoch time: 138.11 s\n",
      "2025-12-08 04:53:15.984052: \n",
      "2025-12-08 04:53:15.984052: Epoch 436\n",
      "2025-12-08 04:53:16.000031: Current learning rate: 0.00597\n",
      "2025-12-08 04:55:34.293838: train_loss -0.843\n",
      "2025-12-08 04:55:34.293838: val_loss -0.8758\n",
      "2025-12-08 04:55:34.296382: Pseudo dice [0.9248, 0.9541, 0.9483]\n",
      "2025-12-08 04:55:34.296382: Epoch time: 138.31 s\n",
      "2025-12-08 04:55:35.108673: \n",
      "2025-12-08 04:55:35.108673: Epoch 437\n",
      "2025-12-08 04:55:35.108673: Current learning rate: 0.00596\n",
      "2025-12-08 04:57:53.266028: train_loss -0.8441\n",
      "2025-12-08 04:57:53.266028: val_loss -0.8596\n",
      "2025-12-08 04:57:53.271029: Pseudo dice [0.9133, 0.9522, 0.9433]\n",
      "2025-12-08 04:57:53.275030: Epoch time: 138.16 s\n",
      "2025-12-08 04:57:53.952893: \n",
      "2025-12-08 04:57:53.968717: Epoch 438\n",
      "2025-12-08 04:57:53.968717: Current learning rate: 0.00595\n",
      "2025-12-08 05:00:12.168419: train_loss -0.8436\n",
      "2025-12-08 05:00:12.168419: val_loss -0.8756\n",
      "2025-12-08 05:00:12.177450: Pseudo dice [0.9279, 0.9539, 0.945]\n",
      "2025-12-08 05:00:12.181454: Epoch time: 138.22 s\n",
      "2025-12-08 05:00:12.813382: \n",
      "2025-12-08 05:00:12.813382: Epoch 439\n",
      "2025-12-08 05:00:12.827391: Current learning rate: 0.00594\n",
      "2025-12-08 05:02:31.155185: train_loss -0.8374\n",
      "2025-12-08 05:02:31.155185: val_loss -0.8682\n",
      "2025-12-08 05:02:31.160370: Pseudo dice [0.9187, 0.9539, 0.9439]\n",
      "2025-12-08 05:02:31.164088: Epoch time: 138.34 s\n",
      "2025-12-08 05:02:31.796096: \n",
      "2025-12-08 05:02:31.796096: Epoch 440\n",
      "2025-12-08 05:02:31.812061: Current learning rate: 0.00593\n",
      "2025-12-08 05:04:49.875850: train_loss -0.835\n",
      "2025-12-08 05:04:49.875850: val_loss -0.8644\n",
      "2025-12-08 05:04:49.875850: Pseudo dice [0.9222, 0.9531, 0.9357]\n",
      "2025-12-08 05:04:49.875850: Epoch time: 138.08 s\n",
      "2025-12-08 05:04:50.515215: \n",
      "2025-12-08 05:04:50.515215: Epoch 441\n",
      "2025-12-08 05:04:50.515215: Current learning rate: 0.00592\n",
      "2025-12-08 05:07:08.734112: train_loss -0.8409\n",
      "2025-12-08 05:07:08.734112: val_loss -0.8717\n",
      "2025-12-08 05:07:08.734112: Pseudo dice [0.9227, 0.9551, 0.946]\n",
      "2025-12-08 05:07:08.750076: Epoch time: 138.22 s\n",
      "2025-12-08 05:07:09.374291: \n",
      "2025-12-08 05:07:09.374291: Epoch 442\n",
      "2025-12-08 05:07:09.374291: Current learning rate: 0.00592\n",
      "2025-12-08 05:09:27.461574: train_loss -0.8416\n",
      "2025-12-08 05:09:27.462574: val_loss -0.8728\n",
      "2025-12-08 05:09:27.466580: Pseudo dice [0.9261, 0.9541, 0.9421]\n",
      "2025-12-08 05:09:27.467584: Epoch time: 138.09 s\n",
      "2025-12-08 05:09:28.296798: \n",
      "2025-12-08 05:09:28.296798: Epoch 443\n",
      "2025-12-08 05:09:28.310115: Current learning rate: 0.00591\n",
      "2025-12-08 05:11:46.414550: train_loss -0.8451\n",
      "2025-12-08 05:11:46.415550: val_loss -0.8632\n",
      "2025-12-08 05:11:46.419036: Pseudo dice [0.9197, 0.9546, 0.9444]\n",
      "2025-12-08 05:11:46.421163: Epoch time: 138.12 s\n",
      "2025-12-08 05:11:47.037868: \n",
      "2025-12-08 05:11:47.037868: Epoch 444\n",
      "2025-12-08 05:11:47.037868: Current learning rate: 0.0059\n",
      "2025-12-08 05:14:05.123485: train_loss -0.8437\n",
      "2025-12-08 05:14:05.123485: val_loss -0.8681\n",
      "2025-12-08 05:14:05.143210: Pseudo dice [0.9224, 0.9509, 0.9461]\n",
      "2025-12-08 05:14:05.145212: Epoch time: 138.09 s\n",
      "2025-12-08 05:14:05.775430: \n",
      "2025-12-08 05:14:05.775430: Epoch 445\n",
      "2025-12-08 05:14:05.779784: Current learning rate: 0.00589\n",
      "2025-12-08 05:16:23.951935: train_loss -0.844\n",
      "2025-12-08 05:16:23.951935: val_loss -0.8732\n",
      "2025-12-08 05:16:23.956809: Pseudo dice [0.9267, 0.9552, 0.9406]\n",
      "2025-12-08 05:16:23.960813: Epoch time: 138.18 s\n",
      "2025-12-08 05:16:24.584570: \n",
      "2025-12-08 05:16:24.585582: Epoch 446\n",
      "2025-12-08 05:16:24.588773: Current learning rate: 0.00588\n",
      "2025-12-08 05:18:42.721341: train_loss -0.8414\n",
      "2025-12-08 05:18:42.722341: val_loss -0.8724\n",
      "2025-12-08 05:18:42.726342: Pseudo dice [0.9241, 0.9585, 0.9441]\n",
      "2025-12-08 05:18:42.729343: Epoch time: 138.14 s\n",
      "2025-12-08 05:18:43.352829: \n",
      "2025-12-08 05:18:43.352829: Epoch 447\n",
      "2025-12-08 05:18:43.352829: Current learning rate: 0.00587\n",
      "2025-12-08 05:21:01.456614: train_loss -0.8425\n",
      "2025-12-08 05:21:01.456614: val_loss -0.8703\n",
      "2025-12-08 05:21:01.464511: Pseudo dice [0.9232, 0.9546, 0.9442]\n",
      "2025-12-08 05:21:01.470256: Epoch time: 138.11 s\n",
      "2025-12-08 05:21:01.472258: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-08 05:21:02.378389: \n",
      "2025-12-08 05:21:02.378389: Epoch 448\n",
      "2025-12-08 05:21:02.382407: Current learning rate: 0.00586\n",
      "2025-12-08 05:23:20.515336: train_loss -0.8461\n",
      "2025-12-08 05:23:20.515336: val_loss -0.8625\n",
      "2025-12-08 05:23:20.531023: Pseudo dice [0.9192, 0.9487, 0.94]\n",
      "2025-12-08 05:23:20.535611: Epoch time: 138.14 s\n",
      "2025-12-08 05:23:21.326668: \n",
      "2025-12-08 05:23:21.326668: Epoch 449\n",
      "2025-12-08 05:23:21.333194: Current learning rate: 0.00585\n",
      "2025-12-08 05:25:39.483210: train_loss -0.8384\n",
      "2025-12-08 05:25:39.483210: val_loss -0.8619\n",
      "2025-12-08 05:25:39.483210: Pseudo dice [0.9188, 0.9482, 0.9355]\n",
      "2025-12-08 05:25:39.490305: Epoch time: 138.16 s\n",
      "2025-12-08 05:25:40.374905: \n",
      "2025-12-08 05:25:40.374905: Epoch 450\n",
      "2025-12-08 05:25:40.384761: Current learning rate: 0.00584\n",
      "2025-12-08 05:27:58.421380: train_loss -0.8453\n",
      "2025-12-08 05:27:58.421380: val_loss -0.8689\n",
      "2025-12-08 05:27:58.421380: Pseudo dice [0.9262, 0.9558, 0.9365]\n",
      "2025-12-08 05:27:58.437293: Epoch time: 138.05 s\n",
      "2025-12-08 05:27:59.047946: \n",
      "2025-12-08 05:27:59.047946: Epoch 451\n",
      "2025-12-08 05:27:59.053955: Current learning rate: 0.00583\n",
      "2025-12-08 05:30:17.216994: train_loss -0.8334\n",
      "2025-12-08 05:30:17.216994: val_loss -0.8222\n",
      "2025-12-08 05:30:17.221119: Pseudo dice [0.8941, 0.943, 0.9254]\n",
      "2025-12-08 05:30:17.223811: Epoch time: 138.17 s\n",
      "2025-12-08 05:30:17.847966: \n",
      "2025-12-08 05:30:17.849149: Epoch 452\n",
      "2025-12-08 05:30:17.853165: Current learning rate: 0.00582\n",
      "2025-12-08 05:32:36.025360: train_loss -0.8021\n",
      "2025-12-08 05:32:36.026364: val_loss -0.8591\n",
      "2025-12-08 05:32:36.030375: Pseudo dice [0.9191, 0.9532, 0.9347]\n",
      "2025-12-08 05:32:36.030375: Epoch time: 138.18 s\n",
      "2025-12-08 05:32:36.639990: \n",
      "2025-12-08 05:32:36.639990: Epoch 453\n",
      "2025-12-08 05:32:36.655903: Current learning rate: 0.00581\n",
      "2025-12-08 05:34:54.808411: train_loss -0.8176\n",
      "2025-12-08 05:34:54.810413: val_loss -0.8524\n",
      "2025-12-08 05:34:54.814417: Pseudo dice [0.9154, 0.9497, 0.9387]\n",
      "2025-12-08 05:34:54.816404: Epoch time: 138.17 s\n",
      "2025-12-08 05:34:55.436534: \n",
      "2025-12-08 05:34:55.436534: Epoch 454\n",
      "2025-12-08 05:34:55.436534: Current learning rate: 0.0058\n",
      "2025-12-08 05:37:13.704451: train_loss -0.8302\n",
      "2025-12-08 05:37:13.706456: val_loss -0.8671\n",
      "2025-12-08 05:37:13.712477: Pseudo dice [0.9236, 0.9541, 0.9434]\n",
      "2025-12-08 05:37:13.717852: Epoch time: 138.27 s\n",
      "2025-12-08 05:37:14.545344: \n",
      "2025-12-08 05:37:14.545344: Epoch 455\n",
      "2025-12-08 05:37:14.561262: Current learning rate: 0.00579\n",
      "2025-12-08 05:39:32.745559: train_loss -0.8217\n",
      "2025-12-08 05:39:32.747562: val_loss -0.837\n",
      "2025-12-08 05:39:32.751306: Pseudo dice [0.9056, 0.9423, 0.9373]\n",
      "2025-12-08 05:39:32.756998: Epoch time: 138.2 s\n",
      "2025-12-08 05:39:33.373315: \n",
      "2025-12-08 05:39:33.373315: Epoch 456\n",
      "2025-12-08 05:39:33.389080: Current learning rate: 0.00578\n",
      "2025-12-08 05:41:51.714415: train_loss -0.7944\n",
      "2025-12-08 05:41:51.715417: val_loss -0.8499\n",
      "2025-12-08 05:41:51.719455: Pseudo dice [0.9154, 0.9536, 0.9351]\n",
      "2025-12-08 05:41:51.723460: Epoch time: 138.34 s\n",
      "2025-12-08 05:41:52.373981: \n",
      "2025-12-08 05:41:52.373981: Epoch 457\n",
      "2025-12-08 05:41:52.373981: Current learning rate: 0.00577\n",
      "2025-12-08 05:44:10.577240: train_loss -0.8114\n",
      "2025-12-08 05:44:10.579049: val_loss -0.8515\n",
      "2025-12-08 05:44:10.583055: Pseudo dice [0.9157, 0.9498, 0.9387]\n",
      "2025-12-08 05:44:10.587059: Epoch time: 138.2 s\n",
      "2025-12-08 05:44:11.218941: \n",
      "2025-12-08 05:44:11.218941: Epoch 458\n",
      "2025-12-08 05:44:11.218941: Current learning rate: 0.00576\n",
      "2025-12-08 05:46:29.300604: train_loss -0.8203\n",
      "2025-12-08 05:46:29.300604: val_loss -0.854\n",
      "2025-12-08 05:46:29.306612: Pseudo dice [0.9133, 0.9518, 0.9377]\n",
      "2025-12-08 05:46:29.308615: Epoch time: 138.08 s\n",
      "2025-12-08 05:46:29.936109: \n",
      "2025-12-08 05:46:29.936109: Epoch 459\n",
      "2025-12-08 05:46:29.936109: Current learning rate: 0.00575\n",
      "2025-12-08 05:48:48.109591: train_loss -0.8259\n",
      "2025-12-08 05:48:48.109591: val_loss -0.854\n",
      "2025-12-08 05:48:48.109591: Pseudo dice [0.9186, 0.9495, 0.9323]\n",
      "2025-12-08 05:48:48.109591: Epoch time: 138.17 s\n",
      "2025-12-08 05:48:48.733154: \n",
      "2025-12-08 05:48:48.733154: Epoch 460\n",
      "2025-12-08 05:48:48.733154: Current learning rate: 0.00574\n",
      "2025-12-08 05:51:06.782135: train_loss -0.8338\n",
      "2025-12-08 05:51:06.783139: val_loss -0.8543\n",
      "2025-12-08 05:51:06.787152: Pseudo dice [0.9138, 0.942, 0.9431]\n",
      "2025-12-08 05:51:06.790167: Epoch time: 138.05 s\n",
      "2025-12-08 05:51:07.405884: \n",
      "2025-12-08 05:51:07.405884: Epoch 461\n",
      "2025-12-08 05:51:07.405884: Current learning rate: 0.00573\n",
      "2025-12-08 05:53:25.325693: train_loss -0.8348\n",
      "2025-12-08 05:53:25.325693: val_loss -0.8574\n",
      "2025-12-08 05:53:25.326696: Pseudo dice [0.9181, 0.9502, 0.9418]\n",
      "2025-12-08 05:53:25.326696: Epoch time: 137.92 s\n",
      "2025-12-08 05:53:26.186282: \n",
      "2025-12-08 05:53:26.186282: Epoch 462\n",
      "2025-12-08 05:53:26.186282: Current learning rate: 0.00572\n",
      "2025-12-08 05:55:44.358374: train_loss -0.8347\n",
      "2025-12-08 05:55:44.358374: val_loss -0.863\n",
      "2025-12-08 05:55:44.374285: Pseudo dice [0.9197, 0.9519, 0.9441]\n",
      "2025-12-08 05:55:44.374285: Epoch time: 138.17 s\n",
      "2025-12-08 05:55:44.984594: \n",
      "2025-12-08 05:55:44.984594: Epoch 463\n",
      "2025-12-08 05:55:44.984594: Current learning rate: 0.00571\n",
      "2025-12-08 05:58:03.045187: train_loss -0.8398\n",
      "2025-12-08 05:58:03.045187: val_loss -0.8624\n",
      "2025-12-08 05:58:03.060919: Pseudo dice [0.9161, 0.9533, 0.948]\n",
      "2025-12-08 05:58:03.064924: Epoch time: 138.06 s\n",
      "2025-12-08 05:58:03.686740: \n",
      "2025-12-08 05:58:03.686740: Epoch 464\n",
      "2025-12-08 05:58:03.686740: Current learning rate: 0.0057\n",
      "2025-12-08 06:00:21.726662: train_loss -0.8429\n",
      "2025-12-08 06:00:21.728664: val_loss -0.8709\n",
      "2025-12-08 06:00:21.734409: Pseudo dice [0.921, 0.9527, 0.9518]\n",
      "2025-12-08 06:00:21.741626: Epoch time: 138.04 s\n",
      "2025-12-08 06:00:22.483474: \n",
      "2025-12-08 06:00:22.483474: Epoch 465\n",
      "2025-12-08 06:00:22.483474: Current learning rate: 0.0057\n",
      "2025-12-08 06:02:40.717708: train_loss -0.8372\n",
      "2025-12-08 06:02:40.717708: val_loss -0.871\n",
      "2025-12-08 06:02:40.717708: Pseudo dice [0.9236, 0.9544, 0.941]\n",
      "2025-12-08 06:02:40.727214: Epoch time: 138.23 s\n",
      "2025-12-08 06:02:41.359803: \n",
      "2025-12-08 06:02:41.359803: Epoch 466\n",
      "2025-12-08 06:02:41.359803: Current learning rate: 0.00569\n",
      "2025-12-08 06:04:59.577625: train_loss -0.8429\n",
      "2025-12-08 06:04:59.577625: val_loss -0.8647\n",
      "2025-12-08 06:04:59.593708: Pseudo dice [0.9191, 0.952, 0.9496]\n",
      "2025-12-08 06:04:59.593708: Epoch time: 138.22 s\n",
      "2025-12-08 06:05:00.202204: \n",
      "2025-12-08 06:05:00.202204: Epoch 467\n",
      "2025-12-08 06:05:00.202204: Current learning rate: 0.00568\n",
      "2025-12-08 06:07:18.271718: train_loss -0.8435\n",
      "2025-12-08 06:07:18.271718: val_loss -0.8632\n",
      "2025-12-08 06:07:18.276719: Pseudo dice [0.9203, 0.9548, 0.937]\n",
      "2025-12-08 06:07:18.279719: Epoch time: 138.07 s\n",
      "2025-12-08 06:07:19.046460: \n",
      "2025-12-08 06:07:19.046460: Epoch 468\n",
      "2025-12-08 06:07:19.061726: Current learning rate: 0.00567\n",
      "2025-12-08 06:09:37.204686: train_loss -0.8402\n",
      "2025-12-08 06:09:37.204686: val_loss -0.8657\n",
      "2025-12-08 06:09:37.218350: Pseudo dice [0.9212, 0.952, 0.9402]\n",
      "2025-12-08 06:09:37.223823: Epoch time: 138.16 s\n",
      "2025-12-08 06:09:38.014571: \n",
      "2025-12-08 06:09:38.014571: Epoch 469\n",
      "2025-12-08 06:09:38.014571: Current learning rate: 0.00566\n",
      "2025-12-08 06:11:56.101418: train_loss -0.846\n",
      "2025-12-08 06:11:56.101418: val_loss -0.8722\n",
      "2025-12-08 06:11:56.105421: Pseudo dice [0.9215, 0.9557, 0.9436]\n",
      "2025-12-08 06:11:56.109164: Epoch time: 138.09 s\n",
      "2025-12-08 06:11:56.737579: \n",
      "2025-12-08 06:11:56.737579: Epoch 470\n",
      "2025-12-08 06:11:56.737579: Current learning rate: 0.00565\n",
      "2025-12-08 06:14:14.915096: train_loss -0.846\n",
      "2025-12-08 06:14:14.916097: val_loss -0.857\n",
      "2025-12-08 06:14:14.919597: Pseudo dice [0.9136, 0.9494, 0.9413]\n",
      "2025-12-08 06:14:14.923831: Epoch time: 138.18 s\n",
      "2025-12-08 06:14:15.712318: \n",
      "2025-12-08 06:14:15.712318: Epoch 471\n",
      "2025-12-08 06:14:15.712318: Current learning rate: 0.00564\n",
      "2025-12-08 06:16:33.792442: train_loss -0.8427\n",
      "2025-12-08 06:16:33.794444: val_loss -0.8784\n",
      "2025-12-08 06:16:33.796445: Pseudo dice [0.9264, 0.9567, 0.9465]\n",
      "2025-12-08 06:16:33.802008: Epoch time: 138.08 s\n",
      "2025-12-08 06:16:34.422307: \n",
      "2025-12-08 06:16:34.422307: Epoch 472\n",
      "2025-12-08 06:16:34.424309: Current learning rate: 0.00563\n",
      "2025-12-08 06:18:52.473480: train_loss -0.8443\n",
      "2025-12-08 06:18:52.473480: val_loss -0.8626\n",
      "2025-12-08 06:18:52.477484: Pseudo dice [0.9185, 0.9497, 0.9421]\n",
      "2025-12-08 06:18:52.481488: Epoch time: 138.05 s\n",
      "2025-12-08 06:18:53.155477: \n",
      "2025-12-08 06:18:53.155477: Epoch 473\n",
      "2025-12-08 06:18:53.155477: Current learning rate: 0.00562\n",
      "2025-12-08 06:21:11.404872: train_loss -0.8417\n",
      "2025-12-08 06:21:11.404872: val_loss -0.8683\n",
      "2025-12-08 06:21:11.415123: Pseudo dice [0.9225, 0.9544, 0.9409]\n",
      "2025-12-08 06:21:11.417126: Epoch time: 138.27 s\n",
      "2025-12-08 06:21:12.092856: \n",
      "2025-12-08 06:21:12.092856: Epoch 474\n",
      "2025-12-08 06:21:12.110533: Current learning rate: 0.00561\n",
      "2025-12-08 06:23:30.216406: train_loss -0.8417\n",
      "2025-12-08 06:23:30.218147: val_loss -0.8659\n",
      "2025-12-08 06:23:30.222709: Pseudo dice [0.9174, 0.9484, 0.9467]\n",
      "2025-12-08 06:23:30.226713: Epoch time: 138.12 s\n",
      "2025-12-08 06:23:31.027474: \n",
      "2025-12-08 06:23:31.027474: Epoch 475\n",
      "2025-12-08 06:23:31.029976: Current learning rate: 0.0056\n",
      "2025-12-08 06:25:49.178159: train_loss -0.8383\n",
      "2025-12-08 06:25:49.180162: val_loss -0.8611\n",
      "2025-12-08 06:25:49.180162: Pseudo dice [0.9191, 0.9528, 0.9384]\n",
      "2025-12-08 06:25:49.185809: Epoch time: 138.15 s\n",
      "2025-12-08 06:25:49.795794: \n",
      "2025-12-08 06:25:49.795794: Epoch 476\n",
      "2025-12-08 06:25:49.795794: Current learning rate: 0.00559\n",
      "2025-12-08 06:28:07.895010: train_loss -0.8392\n",
      "2025-12-08 06:28:07.895010: val_loss -0.8686\n",
      "2025-12-08 06:28:07.901016: Pseudo dice [0.9272, 0.9527, 0.9345]\n",
      "2025-12-08 06:28:07.905760: Epoch time: 138.1 s\n",
      "2025-12-08 06:28:08.592438: \n",
      "2025-12-08 06:28:08.592438: Epoch 477\n",
      "2025-12-08 06:28:08.599945: Current learning rate: 0.00558\n",
      "2025-12-08 06:30:26.872214: train_loss -0.8427\n",
      "2025-12-08 06:30:26.872214: val_loss -0.8686\n",
      "2025-12-08 06:30:26.878296: Pseudo dice [0.9218, 0.9541, 0.9457]\n",
      "2025-12-08 06:30:26.882296: Epoch time: 138.28 s\n",
      "2025-12-08 06:30:27.522088: \n",
      "2025-12-08 06:30:27.522088: Epoch 478\n",
      "2025-12-08 06:30:27.522088: Current learning rate: 0.00557\n",
      "2025-12-08 06:32:45.859542: train_loss -0.8424\n",
      "2025-12-08 06:32:45.860628: val_loss -0.8639\n",
      "2025-12-08 06:32:45.864631: Pseudo dice [0.9198, 0.9506, 0.9447]\n",
      "2025-12-08 06:32:45.867632: Epoch time: 138.34 s\n",
      "2025-12-08 06:32:46.509107: \n",
      "2025-12-08 06:32:46.509107: Epoch 479\n",
      "2025-12-08 06:32:46.514996: Current learning rate: 0.00556\n",
      "2025-12-08 06:35:04.559544: train_loss -0.8449\n",
      "2025-12-08 06:35:04.560546: val_loss -0.8659\n",
      "2025-12-08 06:35:04.561548: Pseudo dice [0.9217, 0.9558, 0.9369]\n",
      "2025-12-08 06:35:04.567582: Epoch time: 138.05 s\n",
      "2025-12-08 06:35:05.265440: \n",
      "2025-12-08 06:35:05.265440: Epoch 480\n",
      "2025-12-08 06:35:05.268877: Current learning rate: 0.00555\n",
      "2025-12-08 06:37:23.378318: train_loss -0.8454\n",
      "2025-12-08 06:37:23.380320: val_loss -0.8694\n",
      "2025-12-08 06:37:23.384324: Pseudo dice [0.9238, 0.9522, 0.9495]\n",
      "2025-12-08 06:37:23.388327: Epoch time: 138.13 s\n",
      "2025-12-08 06:37:24.014509: \n",
      "2025-12-08 06:37:24.014509: Epoch 481\n",
      "2025-12-08 06:37:24.014509: Current learning rate: 0.00554\n",
      "2025-12-08 06:39:42.528482: train_loss -0.8376\n",
      "2025-12-08 06:39:42.533823: val_loss -0.8647\n",
      "2025-12-08 06:39:42.535825: Pseudo dice [0.9162, 0.9492, 0.9422]\n",
      "2025-12-08 06:39:42.539829: Epoch time: 138.51 s\n",
      "2025-12-08 06:39:43.352092: \n",
      "2025-12-08 06:39:43.352092: Epoch 482\n",
      "2025-12-08 06:39:43.355803: Current learning rate: 0.00553\n",
      "2025-12-08 06:42:01.220183: train_loss -0.8476\n",
      "2025-12-08 06:42:01.220183: val_loss -0.8692\n",
      "2025-12-08 06:42:01.226191: Pseudo dice [0.9234, 0.9573, 0.9431]\n",
      "2025-12-08 06:42:01.226191: Epoch time: 137.87 s\n",
      "2025-12-08 06:42:01.958833: \n",
      "2025-12-08 06:42:01.959835: Epoch 483\n",
      "2025-12-08 06:42:01.963845: Current learning rate: 0.00552\n",
      "2025-12-08 06:44:20.088468: train_loss -0.847\n",
      "2025-12-08 06:44:20.088468: val_loss -0.8694\n",
      "2025-12-08 06:44:20.093950: Pseudo dice [0.9198, 0.9549, 0.9477]\n",
      "2025-12-08 06:44:20.097954: Epoch time: 138.13 s\n",
      "2025-12-08 06:44:20.739543: \n",
      "2025-12-08 06:44:20.740547: Epoch 484\n",
      "2025-12-08 06:44:20.740547: Current learning rate: 0.00551\n",
      "2025-12-08 06:46:38.985071: train_loss -0.8453\n",
      "2025-12-08 06:46:38.985071: val_loss -0.8621\n",
      "2025-12-08 06:46:38.989077: Pseudo dice [0.9197, 0.9498, 0.9411]\n",
      "2025-12-08 06:46:38.993916: Epoch time: 138.25 s\n",
      "2025-12-08 06:46:39.625150: \n",
      "2025-12-08 06:46:39.625150: Epoch 485\n",
      "2025-12-08 06:46:39.625150: Current learning rate: 0.0055\n",
      "2025-12-08 06:48:57.811582: train_loss -0.8416\n",
      "2025-12-08 06:48:57.811582: val_loss -0.8657\n",
      "2025-12-08 06:48:57.811582: Pseudo dice [0.9184, 0.9534, 0.9436]\n",
      "2025-12-08 06:48:57.820384: Epoch time: 138.19 s\n",
      "2025-12-08 06:48:58.468880: \n",
      "2025-12-08 06:48:58.484672: Epoch 486\n",
      "2025-12-08 06:48:58.488679: Current learning rate: 0.00549\n",
      "2025-12-08 06:51:16.515791: train_loss -0.843\n",
      "2025-12-08 06:51:16.515791: val_loss -0.8694\n",
      "2025-12-08 06:51:16.515791: Pseudo dice [0.9235, 0.954, 0.9458]\n",
      "2025-12-08 06:51:16.531629: Epoch time: 138.05 s\n",
      "2025-12-08 06:51:17.139551: \n",
      "2025-12-08 06:51:17.139551: Epoch 487\n",
      "2025-12-08 06:51:17.155187: Current learning rate: 0.00548\n",
      "2025-12-08 06:53:35.373274: train_loss -0.8436\n",
      "2025-12-08 06:53:35.373274: val_loss -0.8714\n",
      "2025-12-08 06:53:35.375014: Pseudo dice [0.9236, 0.9561, 0.9455]\n",
      "2025-12-08 06:53:35.375014: Epoch time: 138.23 s\n",
      "2025-12-08 06:53:36.172153: \n",
      "2025-12-08 06:53:36.172153: Epoch 488\n",
      "2025-12-08 06:53:36.172153: Current learning rate: 0.00547\n",
      "2025-12-08 06:55:54.233932: train_loss -0.8435\n",
      "2025-12-08 06:55:54.233932: val_loss -0.861\n",
      "2025-12-08 06:55:54.233932: Pseudo dice [0.9175, 0.9514, 0.9412]\n",
      "2025-12-08 06:55:54.233932: Epoch time: 138.06 s\n",
      "2025-12-08 06:55:54.889833: \n",
      "2025-12-08 06:55:54.889833: Epoch 489\n",
      "2025-12-08 06:55:54.889833: Current learning rate: 0.00546\n",
      "2025-12-08 06:58:12.873818: train_loss -0.8478\n",
      "2025-12-08 06:58:12.873818: val_loss -0.8691\n",
      "2025-12-08 06:58:12.877962: Pseudo dice [0.922, 0.9524, 0.9447]\n",
      "2025-12-08 06:58:12.881966: Epoch time: 137.98 s\n",
      "2025-12-08 06:58:13.499788: \n",
      "2025-12-08 06:58:13.499788: Epoch 490\n",
      "2025-12-08 06:58:13.515759: Current learning rate: 0.00546\n",
      "2025-12-08 07:00:31.623969: train_loss -0.8445\n",
      "2025-12-08 07:00:31.623969: val_loss -0.8664\n",
      "2025-12-08 07:00:31.628327: Pseudo dice [0.9177, 0.9508, 0.9473]\n",
      "2025-12-08 07:00:31.632342: Epoch time: 138.12 s\n",
      "2025-12-08 07:00:32.265008: \n",
      "2025-12-08 07:00:32.265008: Epoch 491\n",
      "2025-12-08 07:00:32.265008: Current learning rate: 0.00545\n",
      "2025-12-08 07:02:50.344769: train_loss -0.8465\n",
      "2025-12-08 07:02:50.344769: val_loss -0.8664\n",
      "2025-12-08 07:02:50.358777: Pseudo dice [0.9205, 0.953, 0.9441]\n",
      "2025-12-08 07:02:50.360553: Epoch time: 138.08 s\n",
      "2025-12-08 07:02:51.015721: \n",
      "2025-12-08 07:02:51.015721: Epoch 492\n",
      "2025-12-08 07:02:51.015721: Current learning rate: 0.00544\n",
      "2025-12-08 07:05:09.149130: train_loss -0.844\n",
      "2025-12-08 07:05:09.149130: val_loss -0.8725\n",
      "2025-12-08 07:05:09.155099: Pseudo dice [0.9214, 0.9526, 0.9451]\n",
      "2025-12-08 07:05:09.156913: Epoch time: 138.14 s\n",
      "2025-12-08 07:05:09.779927: \n",
      "2025-12-08 07:05:09.779927: Epoch 493\n",
      "2025-12-08 07:05:09.779927: Current learning rate: 0.00543\n",
      "2025-12-08 07:07:27.968527: train_loss -0.8448\n",
      "2025-12-08 07:07:27.968527: val_loss -0.8745\n",
      "2025-12-08 07:07:27.986614: Pseudo dice [0.9254, 0.9525, 0.9489]\n",
      "2025-12-08 07:07:27.986614: Epoch time: 138.19 s\n",
      "2025-12-08 07:07:28.624251: \n",
      "2025-12-08 07:07:28.624251: Epoch 494\n",
      "2025-12-08 07:07:28.624251: Current learning rate: 0.00542\n",
      "2025-12-08 07:09:46.668461: train_loss -0.8442\n",
      "2025-12-08 07:09:46.670464: val_loss -0.8701\n",
      "2025-12-08 07:09:46.673487: Pseudo dice [0.9238, 0.9571, 0.9448]\n",
      "2025-12-08 07:09:46.673487: Epoch time: 138.04 s\n",
      "2025-12-08 07:09:46.673487: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-08 07:09:47.765152: \n",
      "2025-12-08 07:09:47.765152: Epoch 495\n",
      "2025-12-08 07:09:47.774718: Current learning rate: 0.00541\n",
      "2025-12-08 07:12:05.827110: train_loss -0.8456\n",
      "2025-12-08 07:12:05.827110: val_loss -0.8703\n",
      "2025-12-08 07:12:05.838676: Pseudo dice [0.9206, 0.9487, 0.9497]\n",
      "2025-12-08 07:12:05.838676: Epoch time: 138.06 s\n",
      "2025-12-08 07:12:05.843058: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-08 07:12:06.734819: \n",
      "2025-12-08 07:12:06.734819: Epoch 496\n",
      "2025-12-08 07:12:06.746399: Current learning rate: 0.0054\n",
      "2025-12-08 07:14:24.749332: train_loss -0.845\n",
      "2025-12-08 07:14:24.749332: val_loss -0.8747\n",
      "2025-12-08 07:14:24.765076: Pseudo dice [0.9291, 0.9561, 0.9413]\n",
      "2025-12-08 07:14:24.769814: Epoch time: 138.01 s\n",
      "2025-12-08 07:14:24.769814: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-08 07:14:25.703366: \n",
      "2025-12-08 07:14:25.703366: Epoch 497\n",
      "2025-12-08 07:14:25.703366: Current learning rate: 0.00539\n",
      "2025-12-08 07:16:43.809098: train_loss -0.8427\n",
      "2025-12-08 07:16:43.809098: val_loss -0.8734\n",
      "2025-12-08 07:16:43.811103: Pseudo dice [0.9235, 0.9532, 0.9454]\n",
      "2025-12-08 07:16:43.811103: Epoch time: 138.11 s\n",
      "2025-12-08 07:16:43.811103: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-08 07:16:44.723553: \n",
      "2025-12-08 07:16:44.725555: Epoch 498\n",
      "2025-12-08 07:16:44.725555: Current learning rate: 0.00538\n",
      "2025-12-08 07:19:02.798606: train_loss -0.839\n",
      "2025-12-08 07:19:02.798606: val_loss -0.8727\n",
      "2025-12-08 07:19:02.804612: Pseudo dice [0.9214, 0.9554, 0.9484]\n",
      "2025-12-08 07:19:02.808616: Epoch time: 138.08 s\n",
      "2025-12-08 07:19:02.814360: Yayy! New best EMA pseudo Dice: 0.9401\n",
      "2025-12-08 07:19:03.732311: \n",
      "2025-12-08 07:19:03.732311: Epoch 499\n",
      "2025-12-08 07:19:03.732311: Current learning rate: 0.00537\n",
      "2025-12-08 07:21:21.779516: train_loss -0.8455\n",
      "2025-12-08 07:21:21.779516: val_loss -0.8595\n",
      "2025-12-08 07:21:21.785523: Pseudo dice [0.9145, 0.9464, 0.9446]\n",
      "2025-12-08 07:21:21.789438: Epoch time: 138.05 s\n",
      "2025-12-08 07:21:22.867194: \n",
      "2025-12-08 07:21:22.868194: Epoch 500\n",
      "2025-12-08 07:21:22.872492: Current learning rate: 0.00536\n",
      "2025-12-08 07:23:40.958572: train_loss -0.8457\n",
      "2025-12-08 07:23:40.958572: val_loss -0.8658\n",
      "2025-12-08 07:23:40.963588: Pseudo dice [0.9197, 0.9511, 0.9519]\n",
      "2025-12-08 07:23:40.967606: Epoch time: 138.09 s\n",
      "2025-12-08 07:23:41.627552: \n",
      "2025-12-08 07:23:41.627552: Epoch 501\n",
      "2025-12-08 07:23:41.629555: Current learning rate: 0.00535\n",
      "2025-12-08 07:25:59.903878: train_loss -0.8442\n",
      "2025-12-08 07:25:59.903878: val_loss -0.8784\n",
      "2025-12-08 07:25:59.907884: Pseudo dice [0.9237, 0.9609, 0.9474]\n",
      "2025-12-08 07:25:59.911888: Epoch time: 138.28 s\n",
      "2025-12-08 07:25:59.917894: Yayy! New best EMA pseudo Dice: 0.9402\n",
      "2025-12-08 07:26:00.821723: \n",
      "2025-12-08 07:26:00.821723: Epoch 502\n",
      "2025-12-08 07:26:00.826736: Current learning rate: 0.00534\n",
      "2025-12-08 07:28:18.991899: train_loss -0.8359\n",
      "2025-12-08 07:28:18.991899: val_loss -0.8652\n",
      "2025-12-08 07:28:18.997905: Pseudo dice [0.9205, 0.9521, 0.9424]\n",
      "2025-12-08 07:28:19.001648: Epoch time: 138.17 s\n",
      "2025-12-08 07:28:19.639965: \n",
      "2025-12-08 07:28:19.639965: Epoch 503\n",
      "2025-12-08 07:28:19.639965: Current learning rate: 0.00533\n",
      "2025-12-08 07:30:37.744917: train_loss -0.8401\n",
      "2025-12-08 07:30:37.745919: val_loss -0.8718\n",
      "2025-12-08 07:30:37.748929: Pseudo dice [0.9215, 0.9538, 0.9484]\n",
      "2025-12-08 07:30:37.748929: Epoch time: 138.1 s\n",
      "2025-12-08 07:30:38.404575: \n",
      "2025-12-08 07:30:38.405580: Epoch 504\n",
      "2025-12-08 07:30:38.405580: Current learning rate: 0.00532\n",
      "2025-12-08 07:32:56.519256: train_loss -0.8461\n",
      "2025-12-08 07:32:56.519256: val_loss -0.869\n",
      "2025-12-08 07:32:56.525262: Pseudo dice [0.9205, 0.9538, 0.946]\n",
      "2025-12-08 07:32:56.531327: Epoch time: 138.12 s\n",
      "2025-12-08 07:32:57.155596: \n",
      "2025-12-08 07:32:57.155596: Epoch 505\n",
      "2025-12-08 07:32:57.164142: Current learning rate: 0.00531\n",
      "2025-12-08 07:35:15.372190: train_loss -0.8441\n",
      "2025-12-08 07:35:15.372190: val_loss -0.8717\n",
      "2025-12-08 07:35:15.380202: Pseudo dice [0.9211, 0.9522, 0.9502]\n",
      "2025-12-08 07:35:15.383271: Epoch time: 138.22 s\n",
      "2025-12-08 07:35:15.387139: Yayy! New best EMA pseudo Dice: 0.9402\n",
      "2025-12-08 07:35:16.477893: \n",
      "2025-12-08 07:35:16.479896: Epoch 506\n",
      "2025-12-08 07:35:16.483655: Current learning rate: 0.0053\n",
      "2025-12-08 07:37:34.585011: train_loss -0.8465\n",
      "2025-12-08 07:37:34.586011: val_loss -0.8751\n",
      "2025-12-08 07:37:34.592012: Pseudo dice [0.9259, 0.9573, 0.9471]\n",
      "2025-12-08 07:37:34.595326: Epoch time: 138.11 s\n",
      "2025-12-08 07:37:34.600327: Yayy! New best EMA pseudo Dice: 0.9405\n",
      "2025-12-08 07:37:35.656071: \n",
      "2025-12-08 07:37:35.656071: Epoch 507\n",
      "2025-12-08 07:37:35.656071: Current learning rate: 0.00529\n",
      "2025-12-08 07:39:53.605713: train_loss -0.8516\n",
      "2025-12-08 07:39:53.605713: val_loss -0.8725\n",
      "2025-12-08 07:39:53.609458: Pseudo dice [0.9216, 0.9571, 0.9455]\n",
      "2025-12-08 07:39:53.614959: Epoch time: 137.95 s\n",
      "2025-12-08 07:39:53.618963: Yayy! New best EMA pseudo Dice: 0.9406\n",
      "2025-12-08 07:39:54.530602: \n",
      "2025-12-08 07:39:54.530602: Epoch 508\n",
      "2025-12-08 07:39:54.530602: Current learning rate: 0.00528\n",
      "2025-12-08 07:42:12.578354: train_loss -0.8471\n",
      "2025-12-08 07:42:12.578354: val_loss -0.8683\n",
      "2025-12-08 07:42:12.593941: Pseudo dice [0.9149, 0.953, 0.9488]\n",
      "2025-12-08 07:42:12.593941: Epoch time: 138.06 s\n",
      "2025-12-08 07:42:13.233741: \n",
      "2025-12-08 07:42:13.233741: Epoch 509\n",
      "2025-12-08 07:42:13.233741: Current learning rate: 0.00527\n",
      "2025-12-08 07:44:31.203125: train_loss -0.8462\n",
      "2025-12-08 07:44:31.203125: val_loss -0.884\n",
      "2025-12-08 07:44:31.208144: Pseudo dice [0.9321, 0.957, 0.9488]\n",
      "2025-12-08 07:44:31.212155: Epoch time: 137.97 s\n",
      "2025-12-08 07:44:31.216165: Yayy! New best EMA pseudo Dice: 0.941\n",
      "2025-12-08 07:44:32.220291: \n",
      "2025-12-08 07:44:32.221303: Epoch 510\n",
      "2025-12-08 07:44:32.225307: Current learning rate: 0.00526\n",
      "2025-12-08 07:46:50.330163: train_loss -0.8476\n",
      "2025-12-08 07:46:50.330163: val_loss -0.8743\n",
      "2025-12-08 07:46:50.342403: Pseudo dice [0.9234, 0.956, 0.9503]\n",
      "2025-12-08 07:46:50.342403: Epoch time: 138.11 s\n",
      "2025-12-08 07:46:50.342403: Yayy! New best EMA pseudo Dice: 0.9412\n",
      "2025-12-08 07:46:51.249294: \n",
      "2025-12-08 07:46:51.249294: Epoch 511\n",
      "2025-12-08 07:46:51.249294: Current learning rate: 0.00525\n",
      "2025-12-08 07:49:09.315890: train_loss -0.8325\n",
      "2025-12-08 07:49:09.316895: val_loss -0.849\n",
      "2025-12-08 07:49:09.321898: Pseudo dice [0.913, 0.9424, 0.9373]\n",
      "2025-12-08 07:49:09.325898: Epoch time: 138.07 s\n",
      "2025-12-08 07:49:10.124212: \n",
      "2025-12-08 07:49:10.124212: Epoch 512\n",
      "2025-12-08 07:49:10.124212: Current learning rate: 0.00524\n",
      "2025-12-08 07:51:28.293964: train_loss -0.829\n",
      "2025-12-08 07:51:28.295705: val_loss -0.8596\n",
      "2025-12-08 07:51:28.301713: Pseudo dice [0.9124, 0.948, 0.9465]\n",
      "2025-12-08 07:51:28.305718: Epoch time: 138.17 s\n",
      "2025-12-08 07:51:29.046477: \n",
      "2025-12-08 07:51:29.046477: Epoch 513\n",
      "2025-12-08 07:51:29.046477: Current learning rate: 0.00523\n",
      "2025-12-08 07:53:47.156377: train_loss -0.8393\n",
      "2025-12-08 07:53:47.156377: val_loss -0.8636\n",
      "2025-12-08 07:53:47.156377: Pseudo dice [0.9218, 0.9518, 0.9449]\n",
      "2025-12-08 07:53:47.172035: Epoch time: 138.11 s\n",
      "2025-12-08 07:53:47.781952: \n",
      "2025-12-08 07:53:47.781952: Epoch 514\n",
      "2025-12-08 07:53:47.801920: Current learning rate: 0.00522\n",
      "2025-12-08 07:56:05.780976: train_loss -0.8378\n",
      "2025-12-08 07:56:05.780976: val_loss -0.8594\n",
      "2025-12-08 07:56:05.796868: Pseudo dice [0.9169, 0.9455, 0.9451]\n",
      "2025-12-08 07:56:05.796868: Epoch time: 138.0 s\n",
      "2025-12-08 07:56:06.428741: \n",
      "2025-12-08 07:56:06.428741: Epoch 515\n",
      "2025-12-08 07:56:06.433755: Current learning rate: 0.00521\n",
      "2025-12-08 07:58:24.617564: train_loss -0.8354\n",
      "2025-12-08 07:58:24.617564: val_loss -0.8599\n",
      "2025-12-08 07:58:24.623571: Pseudo dice [0.9194, 0.9554, 0.9325]\n",
      "2025-12-08 07:58:24.625312: Epoch time: 138.19 s\n",
      "2025-12-08 07:58:25.388320: \n",
      "2025-12-08 07:58:25.388320: Epoch 516\n",
      "2025-12-08 07:58:25.393124: Current learning rate: 0.0052\n",
      "2025-12-08 08:00:43.592191: train_loss -0.8393\n",
      "2025-12-08 08:00:43.592191: val_loss -0.8647\n",
      "2025-12-08 08:00:43.592191: Pseudo dice [0.9184, 0.9501, 0.9459]\n",
      "2025-12-08 08:00:43.592191: Epoch time: 138.21 s\n",
      "2025-12-08 08:00:44.266498: \n",
      "2025-12-08 08:00:44.266498: Epoch 517\n",
      "2025-12-08 08:00:44.266498: Current learning rate: 0.00519\n",
      "2025-12-08 08:03:02.467193: train_loss -0.8408\n",
      "2025-12-08 08:03:02.467193: val_loss -0.8686\n",
      "2025-12-08 08:03:02.482886: Pseudo dice [0.9202, 0.9492, 0.9459]\n",
      "2025-12-08 08:03:02.482886: Epoch time: 138.2 s\n",
      "2025-12-08 08:03:03.279865: \n",
      "2025-12-08 08:03:03.279865: Epoch 518\n",
      "2025-12-08 08:03:03.279865: Current learning rate: 0.00518\n",
      "2025-12-08 08:05:21.437251: train_loss -0.8422\n",
      "2025-12-08 08:05:21.437251: val_loss -0.8669\n",
      "2025-12-08 08:05:21.457336: Pseudo dice [0.9184, 0.9501, 0.9461]\n",
      "2025-12-08 08:05:21.461340: Epoch time: 138.16 s\n",
      "2025-12-08 08:05:22.187038: \n",
      "2025-12-08 08:05:22.187038: Epoch 519\n",
      "2025-12-08 08:05:22.187038: Current learning rate: 0.00518\n",
      "2025-12-08 08:07:40.173646: train_loss -0.8441\n",
      "2025-12-08 08:07:40.175648: val_loss -0.8673\n",
      "2025-12-08 08:07:40.179652: Pseudo dice [0.922, 0.9541, 0.9421]\n",
      "2025-12-08 08:07:40.183656: Epoch time: 137.99 s\n",
      "2025-12-08 08:07:40.811400: \n",
      "2025-12-08 08:07:40.811400: Epoch 520\n",
      "2025-12-08 08:07:40.811400: Current learning rate: 0.00517\n",
      "2025-12-08 08:09:59.019669: train_loss -0.8366\n",
      "2025-12-08 08:09:59.021672: val_loss -0.8625\n",
      "2025-12-08 08:09:59.026502: Pseudo dice [0.9174, 0.9522, 0.9386]\n",
      "2025-12-08 08:09:59.028505: Epoch time: 138.21 s\n",
      "2025-12-08 08:09:59.655467: \n",
      "2025-12-08 08:09:59.655467: Epoch 521\n",
      "2025-12-08 08:09:59.671438: Current learning rate: 0.00516\n",
      "2025-12-08 08:12:17.592239: train_loss -0.8467\n",
      "2025-12-08 08:12:17.592239: val_loss -0.8745\n",
      "2025-12-08 08:12:17.593979: Pseudo dice [0.9233, 0.9556, 0.9477]\n",
      "2025-12-08 08:12:17.593979: Epoch time: 137.94 s\n",
      "2025-12-08 08:12:18.233467: \n",
      "2025-12-08 08:12:18.233467: Epoch 522\n",
      "2025-12-08 08:12:18.233467: Current learning rate: 0.00515\n",
      "2025-12-08 08:14:36.499630: train_loss -0.844\n",
      "2025-12-08 08:14:36.499630: val_loss -0.8603\n",
      "2025-12-08 08:14:36.505805: Pseudo dice [0.9164, 0.9517, 0.9364]\n",
      "2025-12-08 08:14:36.509806: Epoch time: 138.27 s\n",
      "2025-12-08 08:14:37.140701: \n",
      "2025-12-08 08:14:37.140701: Epoch 523\n",
      "2025-12-08 08:14:37.140701: Current learning rate: 0.00514\n",
      "2025-12-08 08:16:55.165982: train_loss -0.8401\n",
      "2025-12-08 08:16:55.166982: val_loss -0.8511\n",
      "2025-12-08 08:16:55.170983: Pseudo dice [0.9099, 0.9452, 0.9417]\n",
      "2025-12-08 08:16:55.170983: Epoch time: 138.03 s\n",
      "2025-12-08 08:16:55.983569: \n",
      "2025-12-08 08:16:55.983569: Epoch 524\n",
      "2025-12-08 08:16:55.999230: Current learning rate: 0.00513\n",
      "2025-12-08 08:19:14.028756: train_loss -0.8465\n",
      "2025-12-08 08:19:14.030498: val_loss -0.8726\n",
      "2025-12-08 08:19:14.035598: Pseudo dice [0.9234, 0.9576, 0.9457]\n",
      "2025-12-08 08:19:14.039614: Epoch time: 138.05 s\n",
      "2025-12-08 08:19:14.701984: \n",
      "2025-12-08 08:19:14.701984: Epoch 525\n",
      "2025-12-08 08:19:14.701984: Current learning rate: 0.00512\n",
      "2025-12-08 08:21:32.860739: train_loss -0.8464\n",
      "2025-12-08 08:21:32.860739: val_loss -0.8563\n",
      "2025-12-08 08:21:32.870755: Pseudo dice [0.91, 0.9483, 0.9471]\n",
      "2025-12-08 08:21:32.874529: Epoch time: 138.16 s\n",
      "2025-12-08 08:21:33.514123: \n",
      "2025-12-08 08:21:33.514123: Epoch 526\n",
      "2025-12-08 08:21:33.514123: Current learning rate: 0.00511\n",
      "2025-12-08 08:23:51.810933: train_loss -0.8451\n",
      "2025-12-08 08:23:51.810933: val_loss -0.8715\n",
      "2025-12-08 08:23:51.810933: Pseudo dice [0.9232, 0.9525, 0.948]\n",
      "2025-12-08 08:23:51.810933: Epoch time: 138.3 s\n",
      "2025-12-08 08:23:52.448523: \n",
      "2025-12-08 08:23:52.448523: Epoch 527\n",
      "2025-12-08 08:23:52.451529: Current learning rate: 0.0051\n",
      "2025-12-08 08:26:10.549318: train_loss -0.8452\n",
      "2025-12-08 08:26:10.551320: val_loss -0.8641\n",
      "2025-12-08 08:26:10.559069: Pseudo dice [0.9162, 0.9493, 0.9437]\n",
      "2025-12-08 08:26:10.565076: Epoch time: 138.1 s\n",
      "2025-12-08 08:26:11.203157: \n",
      "2025-12-08 08:26:11.203157: Epoch 528\n",
      "2025-12-08 08:26:11.203157: Current learning rate: 0.00509\n",
      "2025-12-08 08:28:29.396263: train_loss -0.8446\n",
      "2025-12-08 08:28:29.396263: val_loss -0.8686\n",
      "2025-12-08 08:28:29.400267: Pseudo dice [0.922, 0.9567, 0.941]\n",
      "2025-12-08 08:28:29.406011: Epoch time: 138.19 s\n",
      "2025-12-08 08:28:30.031121: \n",
      "2025-12-08 08:28:30.031121: Epoch 529\n",
      "2025-12-08 08:28:30.031121: Current learning rate: 0.00508\n",
      "2025-12-08 08:30:48.144091: train_loss -0.845\n",
      "2025-12-08 08:30:48.144091: val_loss -0.8708\n",
      "2025-12-08 08:30:48.150098: Pseudo dice [0.923, 0.9556, 0.944]\n",
      "2025-12-08 08:30:48.154102: Epoch time: 138.11 s\n",
      "2025-12-08 08:30:49.000961: \n",
      "2025-12-08 08:30:49.000961: Epoch 530\n",
      "2025-12-08 08:30:49.000961: Current learning rate: 0.00507\n",
      "2025-12-08 08:33:07.213083: train_loss -0.8424\n",
      "2025-12-08 08:33:07.217677: val_loss -0.876\n",
      "2025-12-08 08:33:07.221500: Pseudo dice [0.9265, 0.9565, 0.9507]\n",
      "2025-12-08 08:33:07.226982: Epoch time: 138.21 s\n",
      "2025-12-08 08:33:07.857776: \n",
      "2025-12-08 08:33:07.857776: Epoch 531\n",
      "2025-12-08 08:33:07.873434: Current learning rate: 0.00506\n",
      "2025-12-08 08:35:25.929753: train_loss -0.8516\n",
      "2025-12-08 08:35:25.929753: val_loss -0.8754\n",
      "2025-12-08 08:35:25.934765: Pseudo dice [0.9246, 0.9568, 0.9467]\n",
      "2025-12-08 08:35:25.936770: Epoch time: 138.07 s\n",
      "2025-12-08 08:35:26.608288: \n",
      "2025-12-08 08:35:26.608288: Epoch 532\n",
      "2025-12-08 08:35:26.623954: Current learning rate: 0.00505\n",
      "2025-12-08 08:37:44.748628: train_loss -0.8498\n",
      "2025-12-08 08:37:44.748628: val_loss -0.8617\n",
      "2025-12-08 08:37:44.764560: Pseudo dice [0.915, 0.9501, 0.9444]\n",
      "2025-12-08 08:37:44.764560: Epoch time: 138.14 s\n",
      "2025-12-08 08:37:45.543654: \n",
      "2025-12-08 08:37:45.545041: Epoch 533\n",
      "2025-12-08 08:37:45.546044: Current learning rate: 0.00504\n",
      "2025-12-08 08:40:03.717402: train_loss -0.8433\n",
      "2025-12-08 08:40:03.717402: val_loss -0.8642\n",
      "2025-12-08 08:40:03.723543: Pseudo dice [0.915, 0.9496, 0.9428]\n",
      "2025-12-08 08:40:03.729116: Epoch time: 138.17 s\n",
      "2025-12-08 08:40:04.393683: \n",
      "2025-12-08 08:40:04.395101: Epoch 534\n",
      "2025-12-08 08:40:04.399103: Current learning rate: 0.00503\n",
      "2025-12-08 08:42:22.577264: train_loss -0.8446\n",
      "2025-12-08 08:42:22.577264: val_loss -0.8727\n",
      "2025-12-08 08:42:22.582350: Pseudo dice [0.9249, 0.9577, 0.9439]\n",
      "2025-12-08 08:42:22.588140: Epoch time: 138.18 s\n",
      "2025-12-08 08:42:23.220554: \n",
      "2025-12-08 08:42:23.220554: Epoch 535\n",
      "2025-12-08 08:42:23.224564: Current learning rate: 0.00502\n",
      "2025-12-08 08:44:41.353803: train_loss -0.8449\n",
      "2025-12-08 08:44:41.353803: val_loss -0.8812\n",
      "2025-12-08 08:44:41.359549: Pseudo dice [0.9295, 0.9612, 0.9465]\n",
      "2025-12-08 08:44:41.369564: Epoch time: 138.13 s\n",
      "2025-12-08 08:44:42.342937: \n",
      "2025-12-08 08:44:42.342937: Epoch 536\n",
      "2025-12-08 08:44:42.342937: Current learning rate: 0.00501\n",
      "2025-12-08 08:47:00.470068: train_loss -0.851\n",
      "2025-12-08 08:47:00.472069: val_loss -0.8726\n",
      "2025-12-08 08:47:00.478954: Pseudo dice [0.9217, 0.9528, 0.944]\n",
      "2025-12-08 08:47:00.481955: Epoch time: 138.13 s\n",
      "2025-12-08 08:47:01.157351: \n",
      "2025-12-08 08:47:01.157351: Epoch 537\n",
      "2025-12-08 08:47:01.157351: Current learning rate: 0.005\n",
      "2025-12-08 08:49:19.275853: train_loss -0.8485\n",
      "2025-12-08 08:49:19.275853: val_loss -0.8703\n",
      "2025-12-08 08:49:19.280433: Pseudo dice [0.9223, 0.951, 0.9459]\n",
      "2025-12-08 08:49:19.280433: Epoch time: 138.12 s\n",
      "2025-12-08 08:49:19.915541: \n",
      "2025-12-08 08:49:19.915541: Epoch 538\n",
      "2025-12-08 08:49:19.921118: Current learning rate: 0.00499\n",
      "2025-12-08 08:51:38.108196: train_loss -0.8477\n",
      "2025-12-08 08:51:38.108196: val_loss -0.8689\n",
      "2025-12-08 08:51:38.114202: Pseudo dice [0.9203, 0.9553, 0.9462]\n",
      "2025-12-08 08:51:38.120212: Epoch time: 138.19 s\n",
      "2025-12-08 08:51:38.936848: \n",
      "2025-12-08 08:51:38.936848: Epoch 539\n",
      "2025-12-08 08:51:38.942354: Current learning rate: 0.00498\n",
      "2025-12-08 08:53:57.042389: train_loss -0.8452\n",
      "2025-12-08 08:53:57.042389: val_loss -0.8769\n",
      "2025-12-08 08:53:57.049382: Pseudo dice [0.9255, 0.9529, 0.9519]\n",
      "2025-12-08 08:53:57.058384: Epoch time: 138.11 s\n",
      "2025-12-08 08:53:57.697227: \n",
      "2025-12-08 08:53:57.697227: Epoch 540\n",
      "2025-12-08 08:53:57.702594: Current learning rate: 0.00497\n",
      "2025-12-08 08:56:15.749310: train_loss -0.85\n",
      "2025-12-08 08:56:15.749310: val_loss -0.8761\n",
      "2025-12-08 08:56:15.765157: Pseudo dice [0.9259, 0.9586, 0.9458]\n",
      "2025-12-08 08:56:15.765157: Epoch time: 138.05 s\n",
      "2025-12-08 08:56:16.395381: \n",
      "2025-12-08 08:56:16.395381: Epoch 541\n",
      "2025-12-08 08:56:16.399394: Current learning rate: 0.00496\n",
      "2025-12-08 08:58:34.612226: train_loss -0.8521\n",
      "2025-12-08 08:58:34.612226: val_loss -0.8785\n",
      "2025-12-08 08:58:34.618232: Pseudo dice [0.9304, 0.9574, 0.9481]\n",
      "2025-12-08 08:58:34.623977: Epoch time: 138.22 s\n",
      "2025-12-08 08:58:35.286827: \n",
      "2025-12-08 08:58:35.287832: Epoch 542\n",
      "2025-12-08 08:58:35.291848: Current learning rate: 0.00495\n",
      "2025-12-08 09:00:53.295754: train_loss -0.8471\n",
      "2025-12-08 09:00:53.295754: val_loss -0.8598\n",
      "2025-12-08 09:00:53.308195: Pseudo dice [0.9145, 0.9441, 0.951]\n",
      "2025-12-08 09:00:53.313702: Epoch time: 138.01 s\n",
      "2025-12-08 09:00:54.109309: \n",
      "2025-12-08 09:00:54.109309: Epoch 543\n",
      "2025-12-08 09:00:54.109309: Current learning rate: 0.00494\n",
      "2025-12-08 09:03:12.248769: train_loss -0.8503\n",
      "2025-12-08 09:03:12.251712: val_loss -0.8711\n",
      "2025-12-08 09:03:12.256013: Pseudo dice [0.9229, 0.9557, 0.9423]\n",
      "2025-12-08 09:03:12.260017: Epoch time: 138.14 s\n",
      "2025-12-08 09:03:12.903055: \n",
      "2025-12-08 09:03:12.904058: Epoch 544\n",
      "2025-12-08 09:03:12.905076: Current learning rate: 0.00493\n",
      "2025-12-08 09:05:30.829271: train_loss -0.8484\n",
      "2025-12-08 09:05:30.829271: val_loss -0.866\n",
      "2025-12-08 09:05:30.835278: Pseudo dice [0.9203, 0.9514, 0.9394]\n",
      "2025-12-08 09:05:30.841283: Epoch time: 137.93 s\n",
      "2025-12-08 09:05:31.468428: \n",
      "2025-12-08 09:05:31.468428: Epoch 545\n",
      "2025-12-08 09:05:31.484177: Current learning rate: 0.00492\n",
      "2025-12-08 09:07:49.634703: train_loss -0.8436\n",
      "2025-12-08 09:07:49.635703: val_loss -0.8623\n",
      "2025-12-08 09:07:49.640704: Pseudo dice [0.9184, 0.9551, 0.9356]\n",
      "2025-12-08 09:07:49.644815: Epoch time: 138.17 s\n",
      "2025-12-08 09:07:50.428660: \n",
      "2025-12-08 09:07:50.428660: Epoch 546\n",
      "2025-12-08 09:07:50.433683: Current learning rate: 0.00491\n",
      "2025-12-08 09:10:08.622172: train_loss -0.842\n",
      "2025-12-08 09:10:08.622172: val_loss -0.8668\n",
      "2025-12-08 09:10:08.627962: Pseudo dice [0.919, 0.9554, 0.9496]\n",
      "2025-12-08 09:10:08.631966: Epoch time: 138.2 s\n",
      "2025-12-08 09:10:09.271993: \n",
      "2025-12-08 09:10:09.271993: Epoch 547\n",
      "2025-12-08 09:10:09.271993: Current learning rate: 0.0049\n",
      "2025-12-08 09:12:27.376120: train_loss -0.8478\n",
      "2025-12-08 09:12:27.378122: val_loss -0.8828\n",
      "2025-12-08 09:12:27.384128: Pseudo dice [0.9293, 0.9601, 0.9446]\n",
      "2025-12-08 09:12:27.388132: Epoch time: 138.11 s\n",
      "2025-12-08 09:12:28.025664: \n",
      "2025-12-08 09:12:28.026669: Epoch 548\n",
      "2025-12-08 09:12:28.030237: Current learning rate: 0.00489\n",
      "2025-12-08 09:14:46.187159: train_loss -0.8482\n",
      "2025-12-08 09:14:46.187159: val_loss -0.8685\n",
      "2025-12-08 09:14:46.202865: Pseudo dice [0.9207, 0.9523, 0.9438]\n",
      "2025-12-08 09:14:46.207516: Epoch time: 138.16 s\n",
      "2025-12-08 09:14:47.000050: \n",
      "2025-12-08 09:14:47.000050: Epoch 549\n",
      "2025-12-08 09:14:47.005127: Current learning rate: 0.00488\n",
      "2025-12-08 09:17:05.342887: train_loss -0.848\n",
      "2025-12-08 09:17:05.342887: val_loss -0.8684\n",
      "2025-12-08 09:17:05.342887: Pseudo dice [0.9206, 0.9549, 0.9471]\n",
      "2025-12-08 09:17:05.358592: Epoch time: 138.36 s\n",
      "2025-12-08 09:17:06.291917: \n",
      "2025-12-08 09:17:06.291917: Epoch 550\n",
      "2025-12-08 09:17:06.296480: Current learning rate: 0.00487\n",
      "2025-12-08 09:19:24.514457: train_loss -0.8459\n",
      "2025-12-08 09:19:24.516198: val_loss -0.8704\n",
      "2025-12-08 09:19:24.522381: Pseudo dice [0.9229, 0.9532, 0.9429]\n",
      "2025-12-08 09:19:24.527382: Epoch time: 138.22 s\n",
      "2025-12-08 09:19:25.173270: \n",
      "2025-12-08 09:19:25.173270: Epoch 551\n",
      "2025-12-08 09:19:25.177623: Current learning rate: 0.00486\n",
      "2025-12-08 09:21:43.368073: train_loss -0.848\n",
      "2025-12-08 09:21:43.368073: val_loss -0.8708\n",
      "2025-12-08 09:21:43.374151: Pseudo dice [0.9203, 0.9533, 0.9471]\n",
      "2025-12-08 09:21:43.378160: Epoch time: 138.2 s\n",
      "2025-12-08 09:21:44.082054: \n",
      "2025-12-08 09:21:44.082054: Epoch 552\n",
      "2025-12-08 09:21:44.087073: Current learning rate: 0.00485\n",
      "2025-12-08 09:24:02.249671: train_loss -0.8507\n",
      "2025-12-08 09:24:02.249671: val_loss -0.864\n",
      "2025-12-08 09:24:02.249671: Pseudo dice [0.9162, 0.9477, 0.9472]\n",
      "2025-12-08 09:24:02.265309: Epoch time: 138.17 s\n",
      "2025-12-08 09:24:02.890312: \n",
      "2025-12-08 09:24:02.890312: Epoch 553\n",
      "2025-12-08 09:24:02.890312: Current learning rate: 0.00484\n",
      "2025-12-08 09:26:21.017901: train_loss -0.8475\n",
      "2025-12-08 09:26:21.018901: val_loss -0.8783\n",
      "2025-12-08 09:26:21.024902: Pseudo dice [0.9284, 0.9552, 0.9477]\n",
      "2025-12-08 09:26:21.029904: Epoch time: 138.13 s\n",
      "2025-12-08 09:26:21.671298: \n",
      "2025-12-08 09:26:21.671298: Epoch 554\n",
      "2025-12-08 09:26:21.676367: Current learning rate: 0.00484\n",
      "2025-12-08 09:28:39.882823: train_loss -0.8444\n",
      "2025-12-08 09:28:39.884825: val_loss -0.8764\n",
      "2025-12-08 09:28:39.893783: Pseudo dice [0.9267, 0.9519, 0.9484]\n",
      "2025-12-08 09:28:39.897784: Epoch time: 138.23 s\n",
      "2025-12-08 09:28:40.696256: \n",
      "2025-12-08 09:28:40.697262: Epoch 555\n",
      "2025-12-08 09:28:40.700921: Current learning rate: 0.00483\n",
      "2025-12-08 09:30:58.788129: train_loss -0.8485\n",
      "2025-12-08 09:30:58.788129: val_loss -0.8703\n",
      "2025-12-08 09:30:58.795877: Pseudo dice [0.9208, 0.9535, 0.9437]\n",
      "2025-12-08 09:30:58.801897: Epoch time: 138.09 s\n",
      "2025-12-08 09:30:59.436984: \n",
      "2025-12-08 09:30:59.436984: Epoch 556\n",
      "2025-12-08 09:30:59.454298: Current learning rate: 0.00482\n",
      "2025-12-08 09:33:17.633950: train_loss -0.8497\n",
      "2025-12-08 09:33:17.633950: val_loss -0.8678\n",
      "2025-12-08 09:33:17.639956: Pseudo dice [0.9168, 0.9538, 0.9464]\n",
      "2025-12-08 09:33:17.643548: Epoch time: 138.2 s\n",
      "2025-12-08 09:33:18.285843: \n",
      "2025-12-08 09:33:18.285843: Epoch 557\n",
      "2025-12-08 09:33:18.285843: Current learning rate: 0.00481\n",
      "2025-12-08 09:35:36.608333: train_loss -0.8452\n",
      "2025-12-08 09:35:36.608333: val_loss -0.8697\n",
      "2025-12-08 09:35:36.613949: Pseudo dice [0.9212, 0.9504, 0.9456]\n",
      "2025-12-08 09:35:36.617953: Epoch time: 138.32 s\n",
      "2025-12-08 09:35:37.308359: \n",
      "2025-12-08 09:35:37.308359: Epoch 558\n",
      "2025-12-08 09:35:37.312446: Current learning rate: 0.0048\n",
      "2025-12-08 09:37:55.438079: train_loss -0.853\n",
      "2025-12-08 09:37:55.438079: val_loss -0.8748\n",
      "2025-12-08 09:37:55.451584: Pseudo dice [0.9281, 0.9572, 0.9429]\n",
      "2025-12-08 09:37:55.456745: Epoch time: 138.13 s\n",
      "2025-12-08 09:37:56.092868: \n",
      "2025-12-08 09:37:56.094871: Epoch 559\n",
      "2025-12-08 09:37:56.094871: Current learning rate: 0.00479\n",
      "2025-12-08 09:40:14.254556: train_loss -0.8392\n",
      "2025-12-08 09:40:14.256562: val_loss -0.8709\n",
      "2025-12-08 09:40:14.262572: Pseudo dice [0.9207, 0.9505, 0.9477]\n",
      "2025-12-08 09:40:14.270086: Epoch time: 138.16 s\n",
      "2025-12-08 09:40:14.910752: \n",
      "2025-12-08 09:40:14.910752: Epoch 560\n",
      "2025-12-08 09:40:14.910752: Current learning rate: 0.00478\n",
      "2025-12-08 09:42:33.014987: train_loss -0.8504\n",
      "2025-12-08 09:42:33.014987: val_loss -0.8782\n",
      "2025-12-08 09:42:33.024507: Pseudo dice [0.9286, 0.9575, 0.9415]\n",
      "2025-12-08 09:42:33.030513: Epoch time: 138.1 s\n",
      "2025-12-08 09:42:33.838246: \n",
      "2025-12-08 09:42:33.838246: Epoch 561\n",
      "2025-12-08 09:42:33.842721: Current learning rate: 0.00477\n",
      "2025-12-08 09:44:52.078267: train_loss -0.8453\n",
      "2025-12-08 09:44:52.078267: val_loss -0.8744\n",
      "2025-12-08 09:44:52.085787: Pseudo dice [0.9226, 0.9567, 0.949]\n",
      "2025-12-08 09:44:52.089792: Epoch time: 138.24 s\n",
      "2025-12-08 09:44:52.718794: \n",
      "2025-12-08 09:44:52.718794: Epoch 562\n",
      "2025-12-08 09:44:52.718794: Current learning rate: 0.00476\n",
      "2025-12-08 09:47:10.977910: train_loss -0.8442\n",
      "2025-12-08 09:47:10.979913: val_loss -0.8766\n",
      "2025-12-08 09:47:10.985657: Pseudo dice [0.9275, 0.9545, 0.9461]\n",
      "2025-12-08 09:47:10.993668: Epoch time: 138.26 s\n",
      "2025-12-08 09:47:11.640536: \n",
      "2025-12-08 09:47:11.640536: Epoch 563\n",
      "2025-12-08 09:47:11.640536: Current learning rate: 0.00475\n",
      "2025-12-08 09:49:29.864729: train_loss -0.8462\n",
      "2025-12-08 09:49:29.864729: val_loss -0.876\n",
      "2025-12-08 09:49:29.869737: Pseudo dice [0.926, 0.9516, 0.9466]\n",
      "2025-12-08 09:49:29.873739: Epoch time: 138.22 s\n",
      "2025-12-08 09:49:30.500667: \n",
      "2025-12-08 09:49:30.500667: Epoch 564\n",
      "2025-12-08 09:49:30.500667: Current learning rate: 0.00474\n",
      "2025-12-08 09:51:48.814432: train_loss -0.8394\n",
      "2025-12-08 09:51:48.814432: val_loss -0.8628\n",
      "2025-12-08 09:51:48.820441: Pseudo dice [0.9178, 0.9504, 0.9421]\n",
      "2025-12-08 09:51:48.824448: Epoch time: 138.31 s\n",
      "2025-12-08 09:51:49.640462: \n",
      "2025-12-08 09:51:49.640462: Epoch 565\n",
      "2025-12-08 09:51:49.644622: Current learning rate: 0.00473\n",
      "2025-12-08 09:54:07.780750: train_loss -0.8501\n",
      "2025-12-08 09:54:07.780750: val_loss -0.8792\n",
      "2025-12-08 09:54:07.796628: Pseudo dice [0.9316, 0.9565, 0.9423]\n",
      "2025-12-08 09:54:07.800516: Epoch time: 138.16 s\n",
      "2025-12-08 09:54:08.420606: \n",
      "2025-12-08 09:54:08.420606: Epoch 566\n",
      "2025-12-08 09:54:08.436584: Current learning rate: 0.00472\n",
      "2025-12-08 09:56:26.640449: train_loss -0.8461\n",
      "2025-12-08 09:56:26.640449: val_loss -0.8702\n",
      "2025-12-08 09:56:26.646455: Pseudo dice [0.9221, 0.9525, 0.9493]\n",
      "2025-12-08 09:56:26.650459: Epoch time: 138.22 s\n",
      "2025-12-08 09:56:27.297971: \n",
      "2025-12-08 09:56:27.297971: Epoch 567\n",
      "2025-12-08 09:56:27.297971: Current learning rate: 0.00471\n",
      "2025-12-08 09:58:45.562676: train_loss -0.8514\n",
      "2025-12-08 09:58:45.562676: val_loss -0.8766\n",
      "2025-12-08 09:58:45.571089: Pseudo dice [0.9264, 0.9567, 0.947]\n",
      "2025-12-08 09:58:45.575096: Epoch time: 138.27 s\n",
      "2025-12-08 09:58:46.546638: \n",
      "2025-12-08 09:58:46.546638: Epoch 568\n",
      "2025-12-08 09:58:46.559893: Current learning rate: 0.0047\n",
      "2025-12-08 10:01:04.826621: train_loss -0.8472\n",
      "2025-12-08 10:01:04.826621: val_loss -0.8567\n",
      "2025-12-08 10:01:04.837887: Pseudo dice [0.9144, 0.9494, 0.9393]\n",
      "2025-12-08 10:01:04.841891: Epoch time: 138.28 s\n",
      "2025-12-08 10:01:05.490356: \n",
      "2025-12-08 10:01:05.490356: Epoch 569\n",
      "2025-12-08 10:01:05.490356: Current learning rate: 0.00469\n",
      "2025-12-08 10:03:23.641619: train_loss -0.845\n",
      "2025-12-08 10:03:23.642621: val_loss -0.8753\n",
      "2025-12-08 10:03:23.647629: Pseudo dice [0.925, 0.9572, 0.9455]\n",
      "2025-12-08 10:03:23.651780: Epoch time: 138.15 s\n",
      "2025-12-08 10:03:24.297075: \n",
      "2025-12-08 10:03:24.298077: Epoch 570\n",
      "2025-12-08 10:03:24.302111: Current learning rate: 0.00468\n",
      "2025-12-08 10:05:42.391172: train_loss -0.8483\n",
      "2025-12-08 10:05:42.392175: val_loss -0.8776\n",
      "2025-12-08 10:05:42.396192: Pseudo dice [0.9238, 0.9568, 0.9465]\n",
      "2025-12-08 10:05:42.400202: Epoch time: 138.1 s\n",
      "2025-12-08 10:05:43.116364: \n",
      "2025-12-08 10:05:43.116364: Epoch 571\n",
      "2025-12-08 10:05:43.121372: Current learning rate: 0.00467\n",
      "2025-12-08 10:08:01.177659: train_loss -0.848\n",
      "2025-12-08 10:08:01.178661: val_loss -0.8739\n",
      "2025-12-08 10:08:01.182663: Pseudo dice [0.9229, 0.9532, 0.949]\n",
      "2025-12-08 10:08:01.185816: Epoch time: 138.06 s\n",
      "2025-12-08 10:08:01.814438: \n",
      "2025-12-08 10:08:01.815843: Epoch 572\n",
      "2025-12-08 10:08:01.819846: Current learning rate: 0.00466\n",
      "2025-12-08 10:10:19.952151: train_loss -0.8474\n",
      "2025-12-08 10:10:19.952151: val_loss -0.8706\n",
      "2025-12-08 10:10:19.968020: Pseudo dice [0.9229, 0.9553, 0.9489]\n",
      "2025-12-08 10:10:19.973167: Epoch time: 138.14 s\n",
      "2025-12-08 10:10:20.641559: \n",
      "2025-12-08 10:10:20.641559: Epoch 573\n",
      "2025-12-08 10:10:20.655317: Current learning rate: 0.00465\n",
      "2025-12-08 10:12:39.846951: train_loss -0.8478\n",
      "2025-12-08 10:12:39.848953: val_loss -0.8766\n",
      "2025-12-08 10:12:39.852958: Pseudo dice [0.9261, 0.9588, 0.9524]\n",
      "2025-12-08 10:12:39.856962: Epoch time: 139.21 s\n",
      "2025-12-08 10:12:39.860966: Yayy! New best EMA pseudo Dice: 0.9416\n",
      "2025-12-08 10:12:41.063480: \n",
      "2025-12-08 10:12:41.063480: Epoch 574\n",
      "2025-12-08 10:12:41.063480: Current learning rate: 0.00464\n",
      "2025-12-08 10:14:59.530400: train_loss -0.8469\n",
      "2025-12-08 10:14:59.530400: val_loss -0.8769\n",
      "2025-12-08 10:14:59.530400: Pseudo dice [0.9277, 0.9591, 0.9459]\n",
      "2025-12-08 10:14:59.530400: Epoch time: 138.47 s\n",
      "2025-12-08 10:14:59.547438: Yayy! New best EMA pseudo Dice: 0.9418\n",
      "2025-12-08 10:15:00.526584: \n",
      "2025-12-08 10:15:00.526584: Epoch 575\n",
      "2025-12-08 10:15:00.531934: Current learning rate: 0.00463\n",
      "2025-12-08 10:17:19.085349: train_loss -0.8524\n",
      "2025-12-08 10:17:19.087351: val_loss -0.8638\n",
      "2025-12-08 10:17:19.093356: Pseudo dice [0.9153, 0.9499, 0.9423]\n",
      "2025-12-08 10:17:19.095359: Epoch time: 138.56 s\n",
      "2025-12-08 10:17:19.744732: \n",
      "2025-12-08 10:17:19.744732: Epoch 576\n",
      "2025-12-08 10:17:19.744732: Current learning rate: 0.00462\n",
      "2025-12-08 10:19:38.315783: train_loss -0.8497\n",
      "2025-12-08 10:19:38.315783: val_loss -0.8752\n",
      "2025-12-08 10:19:38.321789: Pseudo dice [0.9279, 0.9562, 0.9412]\n",
      "2025-12-08 10:19:38.329535: Epoch time: 138.57 s\n",
      "2025-12-08 10:19:38.983898: \n",
      "2025-12-08 10:19:38.983898: Epoch 577\n",
      "2025-12-08 10:19:38.983898: Current learning rate: 0.00461\n",
      "2025-12-08 10:21:57.825167: train_loss -0.847\n",
      "2025-12-08 10:21:57.825167: val_loss -0.866\n",
      "2025-12-08 10:21:57.830132: Pseudo dice [0.9236, 0.9508, 0.945]\n",
      "2025-12-08 10:21:57.834136: Epoch time: 138.84 s\n",
      "2025-12-08 10:21:58.478226: \n",
      "2025-12-08 10:21:58.478226: Epoch 578\n",
      "2025-12-08 10:21:58.478226: Current learning rate: 0.0046\n",
      "2025-12-08 10:24:17.143631: train_loss -0.8426\n",
      "2025-12-08 10:24:17.143631: val_loss -0.8783\n",
      "2025-12-08 10:24:17.149376: Pseudo dice [0.927, 0.9572, 0.9486]\n",
      "2025-12-08 10:24:17.151378: Epoch time: 138.67 s\n",
      "2025-12-08 10:24:17.983595: \n",
      "2025-12-08 10:24:17.983595: Epoch 579\n",
      "2025-12-08 10:24:17.983595: Current learning rate: 0.00459\n",
      "2025-12-08 10:26:37.526812: train_loss -0.8486\n",
      "2025-12-08 10:26:37.526812: val_loss -0.8645\n",
      "2025-12-08 10:26:37.532819: Pseudo dice [0.9163, 0.9509, 0.9461]\n",
      "2025-12-08 10:26:37.536824: Epoch time: 139.54 s\n",
      "2025-12-08 10:26:38.181467: \n",
      "2025-12-08 10:26:38.181467: Epoch 580\n",
      "2025-12-08 10:26:38.185361: Current learning rate: 0.00458\n",
      "2025-12-08 10:28:56.716027: train_loss -0.8542\n",
      "2025-12-08 10:28:56.716027: val_loss -0.8691\n",
      "2025-12-08 10:28:56.730884: Pseudo dice [0.9194, 0.9507, 0.9463]\n",
      "2025-12-08 10:28:56.730884: Epoch time: 138.54 s\n",
      "2025-12-08 10:28:57.517035: \n",
      "2025-12-08 10:28:57.517035: Epoch 581\n",
      "2025-12-08 10:28:57.517035: Current learning rate: 0.00457\n",
      "2025-12-08 10:31:16.076910: train_loss -0.8498\n",
      "2025-12-08 10:31:16.076910: val_loss -0.8729\n",
      "2025-12-08 10:31:16.076910: Pseudo dice [0.9215, 0.9533, 0.951]\n",
      "2025-12-08 10:31:16.076910: Epoch time: 138.56 s\n",
      "2025-12-08 10:31:16.715948: \n",
      "2025-12-08 10:31:16.715948: Epoch 582\n",
      "2025-12-08 10:31:16.715948: Current learning rate: 0.00456\n",
      "2025-12-08 10:33:35.483987: train_loss -0.849\n",
      "2025-12-08 10:33:35.483987: val_loss -0.8757\n",
      "2025-12-08 10:33:35.489042: Pseudo dice [0.9231, 0.9559, 0.9491]\n",
      "2025-12-08 10:33:35.493053: Epoch time: 138.77 s\n",
      "2025-12-08 10:33:36.124057: \n",
      "2025-12-08 10:33:36.124057: Epoch 583\n",
      "2025-12-08 10:33:36.124057: Current learning rate: 0.00455\n",
      "2025-12-08 10:35:54.629456: train_loss -0.8506\n",
      "2025-12-08 10:35:54.629456: val_loss -0.8728\n",
      "2025-12-08 10:35:54.636457: Pseudo dice [0.9224, 0.9571, 0.9434]\n",
      "2025-12-08 10:35:54.643549: Epoch time: 138.51 s\n",
      "2025-12-08 10:35:55.373566: \n",
      "2025-12-08 10:35:55.373566: Epoch 584\n",
      "2025-12-08 10:35:55.389345: Current learning rate: 0.00454\n",
      "2025-12-08 10:38:13.646049: train_loss -0.8495\n",
      "2025-12-08 10:38:13.646049: val_loss -0.8739\n",
      "2025-12-08 10:38:13.652055: Pseudo dice [0.9226, 0.9544, 0.944]\n",
      "2025-12-08 10:38:13.655797: Epoch time: 138.27 s\n",
      "2025-12-08 10:38:14.478320: \n",
      "2025-12-08 10:38:14.478320: Epoch 585\n",
      "2025-12-08 10:38:14.483825: Current learning rate: 0.00453\n",
      "2025-12-08 10:40:32.619751: train_loss -0.8479\n",
      "2025-12-08 10:40:32.619751: val_loss -0.8789\n",
      "2025-12-08 10:40:32.627430: Pseudo dice [0.9293, 0.9587, 0.9473]\n",
      "2025-12-08 10:40:32.631435: Epoch time: 138.14 s\n",
      "2025-12-08 10:40:33.294771: \n",
      "2025-12-08 10:40:33.295876: Epoch 586\n",
      "2025-12-08 10:40:33.295876: Current learning rate: 0.00452\n",
      "2025-12-08 10:42:51.405753: train_loss -0.8431\n",
      "2025-12-08 10:42:51.405753: val_loss -0.8771\n",
      "2025-12-08 10:42:51.405753: Pseudo dice [0.9239, 0.955, 0.9463]\n",
      "2025-12-08 10:42:51.416932: Epoch time: 138.11 s\n",
      "2025-12-08 10:42:52.213477: \n",
      "2025-12-08 10:42:52.213477: Epoch 587\n",
      "2025-12-08 10:42:52.218479: Current learning rate: 0.00451\n",
      "2025-12-08 10:45:10.513573: train_loss -0.8486\n",
      "2025-12-08 10:45:10.515314: val_loss -0.8802\n",
      "2025-12-08 10:45:10.519178: Pseudo dice [0.927, 0.9517, 0.9465]\n",
      "2025-12-08 10:45:10.523182: Epoch time: 138.3 s\n",
      "2025-12-08 10:45:11.166549: \n",
      "2025-12-08 10:45:11.166549: Epoch 588\n",
      "2025-12-08 10:45:11.166549: Current learning rate: 0.0045\n",
      "2025-12-08 10:47:29.525609: train_loss -0.8468\n",
      "2025-12-08 10:47:29.525609: val_loss -0.8767\n",
      "2025-12-08 10:47:29.531540: Pseudo dice [0.9271, 0.9539, 0.9453]\n",
      "2025-12-08 10:47:29.534649: Epoch time: 138.36 s\n",
      "2025-12-08 10:47:30.172115: \n",
      "2025-12-08 10:47:30.172115: Epoch 589\n",
      "2025-12-08 10:47:30.190361: Current learning rate: 0.00449\n",
      "2025-12-08 10:49:48.393316: train_loss -0.8497\n",
      "2025-12-08 10:49:48.393316: val_loss -0.8744\n",
      "2025-12-08 10:49:48.398319: Pseudo dice [0.9228, 0.9575, 0.9441]\n",
      "2025-12-08 10:49:48.402782: Epoch time: 138.22 s\n",
      "2025-12-08 10:49:49.171736: \n",
      "2025-12-08 10:49:49.171736: Epoch 590\n",
      "2025-12-08 10:49:49.190457: Current learning rate: 0.00448\n",
      "2025-12-08 10:52:07.562145: train_loss -0.8444\n",
      "2025-12-08 10:52:07.562145: val_loss -0.8665\n",
      "2025-12-08 10:52:07.583274: Pseudo dice [0.9191, 0.9512, 0.949]\n",
      "2025-12-08 10:52:07.588285: Epoch time: 138.39 s\n",
      "2025-12-08 10:52:08.220322: \n",
      "2025-12-08 10:52:08.220322: Epoch 591\n",
      "2025-12-08 10:52:08.220322: Current learning rate: 0.00447\n",
      "2025-12-08 10:54:26.444059: train_loss -0.8449\n",
      "2025-12-08 10:54:26.444059: val_loss -0.8641\n",
      "2025-12-08 10:54:26.450069: Pseudo dice [0.9206, 0.9503, 0.9404]\n",
      "2025-12-08 10:54:26.454211: Epoch time: 138.22 s\n",
      "2025-12-08 10:54:27.267821: \n",
      "2025-12-08 10:54:27.268823: Epoch 592\n",
      "2025-12-08 10:54:27.273608: Current learning rate: 0.00446\n",
      "2025-12-08 10:56:45.425279: train_loss -0.8493\n",
      "2025-12-08 10:56:45.425279: val_loss -0.8589\n",
      "2025-12-08 10:56:45.427282: Pseudo dice [0.9133, 0.9529, 0.9455]\n",
      "2025-12-08 10:56:45.427282: Epoch time: 138.16 s\n",
      "2025-12-08 10:56:46.198904: \n",
      "2025-12-08 10:56:46.198904: Epoch 593\n",
      "2025-12-08 10:56:46.204860: Current learning rate: 0.00445\n",
      "2025-12-08 10:59:04.405008: train_loss -0.8498\n",
      "2025-12-08 10:59:04.405008: val_loss -0.86\n",
      "2025-12-08 10:59:04.422963: Pseudo dice [0.9125, 0.9496, 0.9448]\n",
      "2025-12-08 10:59:04.422963: Epoch time: 138.21 s\n",
      "2025-12-08 10:59:05.061343: \n",
      "2025-12-08 10:59:05.061343: Epoch 594\n",
      "2025-12-08 10:59:05.061343: Current learning rate: 0.00444\n",
      "2025-12-08 11:01:23.142632: train_loss -0.8531\n",
      "2025-12-08 11:01:23.142632: val_loss -0.8716\n",
      "2025-12-08 11:01:23.150194: Pseudo dice [0.9204, 0.9523, 0.9498]\n",
      "2025-12-08 11:01:23.155939: Epoch time: 138.08 s\n",
      "2025-12-08 11:01:23.795926: \n",
      "2025-12-08 11:01:23.795926: Epoch 595\n",
      "2025-12-08 11:01:23.811869: Current learning rate: 0.00443\n",
      "2025-12-08 11:03:42.064077: train_loss -0.8506\n",
      "2025-12-08 11:03:42.066079: val_loss -0.8724\n",
      "2025-12-08 11:03:42.074091: Pseudo dice [0.9214, 0.9482, 0.9457]\n",
      "2025-12-08 11:03:42.077834: Epoch time: 138.27 s\n",
      "2025-12-08 11:03:42.831482: \n",
      "2025-12-08 11:03:42.833485: Epoch 596\n",
      "2025-12-08 11:03:42.833485: Current learning rate: 0.00442\n",
      "2025-12-08 11:06:01.109091: train_loss -0.846\n",
      "2025-12-08 11:06:01.109091: val_loss -0.8724\n",
      "2025-12-08 11:06:01.123594: Pseudo dice [0.9225, 0.9553, 0.9458]\n",
      "2025-12-08 11:06:01.129100: Epoch time: 138.28 s\n",
      "2025-12-08 11:06:01.766088: \n",
      "2025-12-08 11:06:01.766088: Epoch 597\n",
      "2025-12-08 11:06:01.766088: Current learning rate: 0.00441\n",
      "2025-12-08 11:08:20.093657: train_loss -0.8482\n",
      "2025-12-08 11:08:20.093657: val_loss -0.8759\n",
      "2025-12-08 11:08:20.093657: Pseudo dice [0.9246, 0.9557, 0.9481]\n",
      "2025-12-08 11:08:20.093657: Epoch time: 138.33 s\n",
      "2025-12-08 11:08:20.905095: \n",
      "2025-12-08 11:08:20.905095: Epoch 598\n",
      "2025-12-08 11:08:20.905095: Current learning rate: 0.0044\n",
      "2025-12-08 11:10:39.106498: train_loss -0.8515\n",
      "2025-12-08 11:10:39.107498: val_loss -0.884\n",
      "2025-12-08 11:10:39.108545: Pseudo dice [0.9292, 0.9564, 0.9508]\n",
      "2025-12-08 11:10:39.108545: Epoch time: 138.2 s\n",
      "2025-12-08 11:10:39.873326: \n",
      "2025-12-08 11:10:39.873326: Epoch 599\n",
      "2025-12-08 11:10:39.880598: Current learning rate: 0.00439\n",
      "2025-12-08 11:12:58.171264: train_loss -0.8531\n",
      "2025-12-08 11:12:58.171264: val_loss -0.8754\n",
      "2025-12-08 11:12:58.171264: Pseudo dice [0.9233, 0.9544, 0.9503]\n",
      "2025-12-08 11:12:58.187120: Epoch time: 138.3 s\n",
      "2025-12-08 11:12:59.084806: \n",
      "2025-12-08 11:12:59.085807: Epoch 600\n",
      "2025-12-08 11:12:59.090566: Current learning rate: 0.00438\n",
      "2025-12-08 11:15:17.406298: train_loss -0.853\n",
      "2025-12-08 11:15:17.406298: val_loss -0.8797\n",
      "2025-12-08 11:15:17.415494: Pseudo dice [0.9254, 0.9604, 0.9476]\n",
      "2025-12-08 11:15:17.421541: Epoch time: 138.32 s\n",
      "2025-12-08 11:15:18.071161: \n",
      "2025-12-08 11:15:18.071161: Epoch 601\n",
      "2025-12-08 11:15:18.075681: Current learning rate: 0.00437\n",
      "2025-12-08 11:17:36.300196: train_loss -0.8521\n",
      "2025-12-08 11:17:36.300196: val_loss -0.8828\n",
      "2025-12-08 11:17:36.308204: Pseudo dice [0.9303, 0.961, 0.9459]\n",
      "2025-12-08 11:17:36.313950: Epoch time: 138.23 s\n",
      "2025-12-08 11:17:36.319957: Yayy! New best EMA pseudo Dice: 0.9418\n",
      "2025-12-08 11:17:37.389695: \n",
      "2025-12-08 11:17:37.389695: Epoch 602\n",
      "2025-12-08 11:17:37.405398: Current learning rate: 0.00436\n",
      "2025-12-08 11:19:55.645820: train_loss -0.8494\n",
      "2025-12-08 11:19:55.645820: val_loss -0.8765\n",
      "2025-12-08 11:19:55.653833: Pseudo dice [0.9259, 0.9538, 0.9489]\n",
      "2025-12-08 11:19:55.658716: Epoch time: 138.26 s\n",
      "2025-12-08 11:19:55.663717: Yayy! New best EMA pseudo Dice: 0.9419\n",
      "2025-12-08 11:19:56.799054: \n",
      "2025-12-08 11:19:56.799054: Epoch 603\n",
      "2025-12-08 11:19:56.799054: Current learning rate: 0.00435\n",
      "2025-12-08 11:22:15.030008: train_loss -0.848\n",
      "2025-12-08 11:22:15.045730: val_loss -0.88\n",
      "2025-12-08 11:22:15.045730: Pseudo dice [0.9304, 0.953, 0.9444]\n",
      "2025-12-08 11:22:15.045730: Epoch time: 138.23 s\n",
      "2025-12-08 11:22:15.045730: Yayy! New best EMA pseudo Dice: 0.942\n",
      "2025-12-08 11:22:15.960745: \n",
      "2025-12-08 11:22:15.960745: Epoch 604\n",
      "2025-12-08 11:22:15.960745: Current learning rate: 0.00434\n",
      "2025-12-08 11:24:34.213158: train_loss -0.8511\n",
      "2025-12-08 11:24:34.213158: val_loss -0.8815\n",
      "2025-12-08 11:24:34.218165: Pseudo dice [0.9276, 0.9613, 0.9505]\n",
      "2025-12-08 11:24:34.222170: Epoch time: 138.25 s\n",
      "2025-12-08 11:24:34.228178: Yayy! New best EMA pseudo Dice: 0.9424\n",
      "2025-12-08 11:24:35.218259: \n",
      "2025-12-08 11:24:35.218259: Epoch 605\n",
      "2025-12-08 11:24:35.218259: Current learning rate: 0.00433\n",
      "2025-12-08 11:26:53.442507: train_loss -0.8567\n",
      "2025-12-08 11:26:53.442507: val_loss -0.8681\n",
      "2025-12-08 11:26:53.448514: Pseudo dice [0.9155, 0.951, 0.9507]\n",
      "2025-12-08 11:26:53.452256: Epoch time: 138.22 s\n",
      "2025-12-08 11:26:54.092959: \n",
      "2025-12-08 11:26:54.092959: Epoch 606\n",
      "2025-12-08 11:26:54.092959: Current learning rate: 0.00432\n",
      "2025-12-08 11:29:12.498939: train_loss -0.8538\n",
      "2025-12-08 11:29:12.498939: val_loss -0.8762\n",
      "2025-12-08 11:29:12.504945: Pseudo dice [0.9261, 0.9541, 0.9506]\n",
      "2025-12-08 11:29:12.508949: Epoch time: 138.41 s\n",
      "2025-12-08 11:29:13.155500: \n",
      "2025-12-08 11:29:13.155500: Epoch 607\n",
      "2025-12-08 11:29:13.155500: Current learning rate: 0.00431\n",
      "2025-12-08 11:31:31.405615: train_loss -0.8496\n",
      "2025-12-08 11:31:31.405615: val_loss -0.8689\n",
      "2025-12-08 11:31:31.410798: Pseudo dice [0.9201, 0.9512, 0.9483]\n",
      "2025-12-08 11:31:31.415800: Epoch time: 138.25 s\n",
      "2025-12-08 11:31:32.149358: \n",
      "2025-12-08 11:31:32.149358: Epoch 608\n",
      "2025-12-08 11:31:32.149358: Current learning rate: 0.0043\n",
      "2025-12-08 11:33:50.372574: train_loss -0.8567\n",
      "2025-12-08 11:33:50.372574: val_loss -0.8746\n",
      "2025-12-08 11:33:50.378319: Pseudo dice [0.923, 0.9542, 0.9453]\n",
      "2025-12-08 11:33:50.382323: Epoch time: 138.22 s\n",
      "2025-12-08 11:33:51.202030: \n",
      "2025-12-08 11:33:51.202030: Epoch 609\n",
      "2025-12-08 11:33:51.202030: Current learning rate: 0.00429\n",
      "2025-12-08 11:36:09.369503: train_loss -0.8505\n",
      "2025-12-08 11:36:09.370504: val_loss -0.8751\n",
      "2025-12-08 11:36:09.375585: Pseudo dice [0.9249, 0.955, 0.9463]\n",
      "2025-12-08 11:36:09.379595: Epoch time: 138.17 s\n",
      "2025-12-08 11:36:10.014740: \n",
      "2025-12-08 11:36:10.014740: Epoch 610\n",
      "2025-12-08 11:36:10.014740: Current learning rate: 0.00429\n",
      "2025-12-08 11:38:28.210448: train_loss -0.8535\n",
      "2025-12-08 11:38:28.212450: val_loss -0.8797\n",
      "2025-12-08 11:38:28.217956: Pseudo dice [0.9259, 0.9536, 0.9475]\n",
      "2025-12-08 11:38:28.221831: Epoch time: 138.2 s\n",
      "2025-12-08 11:38:28.967634: \n",
      "2025-12-08 11:38:28.967634: Epoch 611\n",
      "2025-12-08 11:38:28.967634: Current learning rate: 0.00428\n",
      "2025-12-08 11:40:47.357665: train_loss -0.8515\n",
      "2025-12-08 11:40:47.365667: val_loss -0.8643\n",
      "2025-12-08 11:40:47.372669: Pseudo dice [0.9208, 0.9481, 0.9379]\n",
      "2025-12-08 11:40:47.377718: Epoch time: 138.39 s\n",
      "2025-12-08 11:40:48.061474: \n",
      "2025-12-08 11:40:48.061474: Epoch 612\n",
      "2025-12-08 11:40:48.061474: Current learning rate: 0.00427\n",
      "2025-12-08 11:43:06.157527: train_loss -0.8461\n",
      "2025-12-08 11:43:06.157527: val_loss -0.8812\n",
      "2025-12-08 11:43:06.163271: Pseudo dice [0.9301, 0.9566, 0.9473]\n",
      "2025-12-08 11:43:06.171264: Epoch time: 138.1 s\n",
      "2025-12-08 11:43:06.824162: \n",
      "2025-12-08 11:43:06.826164: Epoch 613\n",
      "2025-12-08 11:43:06.826164: Current learning rate: 0.00426\n",
      "2025-12-08 11:45:25.061231: train_loss -0.8476\n",
      "2025-12-08 11:45:25.061231: val_loss -0.8658\n",
      "2025-12-08 11:45:25.061231: Pseudo dice [0.9197, 0.9497, 0.9478]\n",
      "2025-12-08 11:45:25.061231: Epoch time: 138.24 s\n",
      "2025-12-08 11:45:25.786443: \n",
      "2025-12-08 11:45:25.787443: Epoch 614\n",
      "2025-12-08 11:45:25.792643: Current learning rate: 0.00425\n",
      "2025-12-08 11:47:43.687563: train_loss -0.8536\n",
      "2025-12-08 11:47:43.687563: val_loss -0.8762\n",
      "2025-12-08 11:47:43.693070: Pseudo dice [0.9253, 0.9549, 0.9466]\n",
      "2025-12-08 11:47:43.698550: Epoch time: 137.9 s\n",
      "2025-12-08 11:47:44.512382: \n",
      "2025-12-08 11:47:44.513387: Epoch 615\n",
      "2025-12-08 11:47:44.517420: Current learning rate: 0.00424\n",
      "2025-12-08 11:50:02.546664: train_loss -0.8489\n",
      "2025-12-08 11:50:02.546664: val_loss -0.8593\n",
      "2025-12-08 11:50:02.562541: Pseudo dice [0.9153, 0.9482, 0.9395]\n",
      "2025-12-08 11:50:02.562541: Epoch time: 138.04 s\n",
      "2025-12-08 11:50:03.201932: \n",
      "2025-12-08 11:50:03.201932: Epoch 616\n",
      "2025-12-08 11:50:03.201932: Current learning rate: 0.00423\n",
      "2025-12-08 11:52:21.484701: train_loss -0.8529\n",
      "2025-12-08 11:52:21.485701: val_loss -0.876\n",
      "2025-12-08 11:52:21.491930: Pseudo dice [0.9263, 0.955, 0.9422]\n",
      "2025-12-08 11:52:21.495931: Epoch time: 138.28 s\n",
      "2025-12-08 11:52:22.305726: \n",
      "2025-12-08 11:52:22.305726: Epoch 617\n",
      "2025-12-08 11:52:22.305726: Current learning rate: 0.00422\n",
      "2025-12-08 11:54:40.500879: train_loss -0.8566\n",
      "2025-12-08 11:54:40.500879: val_loss -0.8682\n",
      "2025-12-08 11:54:40.508887: Pseudo dice [0.9223, 0.9517, 0.943]\n",
      "2025-12-08 11:54:40.510889: Epoch time: 138.2 s\n",
      "2025-12-08 11:54:41.155396: \n",
      "2025-12-08 11:54:41.155396: Epoch 618\n",
      "2025-12-08 11:54:41.155396: Current learning rate: 0.00421\n",
      "2025-12-08 11:56:59.420792: train_loss -0.8489\n",
      "2025-12-08 11:56:59.420792: val_loss -0.8719\n",
      "2025-12-08 11:56:59.426293: Pseudo dice [0.9223, 0.9521, 0.9448]\n",
      "2025-12-08 11:56:59.430297: Epoch time: 138.27 s\n",
      "2025-12-08 11:57:00.081447: \n",
      "2025-12-08 11:57:00.083450: Epoch 619\n",
      "2025-12-08 11:57:00.083450: Current learning rate: 0.0042\n",
      "2025-12-08 11:59:18.498548: train_loss -0.8464\n",
      "2025-12-08 11:59:18.498548: val_loss -0.8766\n",
      "2025-12-08 11:59:18.511791: Pseudo dice [0.9255, 0.9569, 0.9496]\n",
      "2025-12-08 11:59:18.513794: Epoch time: 138.42 s\n",
      "2025-12-08 11:59:19.358364: \n",
      "2025-12-08 11:59:19.358364: Epoch 620\n",
      "2025-12-08 11:59:19.370532: Current learning rate: 0.00419\n",
      "2025-12-08 12:01:37.622609: train_loss -0.853\n",
      "2025-12-08 12:01:37.622609: val_loss -0.8695\n",
      "2025-12-08 12:01:37.630357: Pseudo dice [0.9193, 0.9518, 0.951]\n",
      "2025-12-08 12:01:37.636363: Epoch time: 138.26 s\n",
      "2025-12-08 12:01:38.466359: \n",
      "2025-12-08 12:01:38.467361: Epoch 621\n",
      "2025-12-08 12:01:38.468364: Current learning rate: 0.00418\n",
      "2025-12-08 12:03:56.608423: train_loss -0.8483\n",
      "2025-12-08 12:03:56.608423: val_loss -0.8784\n",
      "2025-12-08 12:03:56.608423: Pseudo dice [0.9259, 0.9558, 0.9461]\n",
      "2025-12-08 12:03:56.608423: Epoch time: 138.14 s\n",
      "2025-12-08 12:03:57.295645: \n",
      "2025-12-08 12:03:57.299144: Epoch 622\n",
      "2025-12-08 12:03:57.299144: Current learning rate: 0.00417\n",
      "2025-12-08 12:06:15.187115: train_loss -0.8468\n",
      "2025-12-08 12:06:15.187115: val_loss -0.8822\n",
      "2025-12-08 12:06:15.206437: Pseudo dice [0.9315, 0.9595, 0.9436]\n",
      "2025-12-08 12:06:15.210437: Epoch time: 137.89 s\n",
      "2025-12-08 12:06:15.930506: \n",
      "2025-12-08 12:06:15.930506: Epoch 623\n",
      "2025-12-08 12:06:15.936160: Current learning rate: 0.00416\n",
      "2025-12-08 12:08:34.107363: train_loss -0.8467\n",
      "2025-12-08 12:08:34.107363: val_loss -0.8829\n",
      "2025-12-08 12:08:34.114259: Pseudo dice [0.9279, 0.9582, 0.9516]\n",
      "2025-12-08 12:08:34.118259: Epoch time: 138.18 s\n",
      "2025-12-08 12:08:34.763387: \n",
      "2025-12-08 12:08:34.763387: Epoch 624\n",
      "2025-12-08 12:08:34.768507: Current learning rate: 0.00415\n",
      "2025-12-08 12:10:52.787512: train_loss -0.8506\n",
      "2025-12-08 12:10:52.789515: val_loss -0.8746\n",
      "2025-12-08 12:10:52.794888: Pseudo dice [0.923, 0.9542, 0.9492]\n",
      "2025-12-08 12:10:52.799895: Epoch time: 138.03 s\n",
      "2025-12-08 12:10:53.496061: \n",
      "2025-12-08 12:10:53.497066: Epoch 625\n",
      "2025-12-08 12:10:53.502118: Current learning rate: 0.00414\n",
      "2025-12-08 12:13:11.530668: train_loss -0.8549\n",
      "2025-12-08 12:13:11.530668: val_loss -0.8735\n",
      "2025-12-08 12:13:11.534675: Pseudo dice [0.9218, 0.9529, 0.9462]\n",
      "2025-12-08 12:13:11.538681: Epoch time: 138.04 s\n",
      "2025-12-08 12:13:12.256603: \n",
      "2025-12-08 12:13:12.257605: Epoch 626\n",
      "2025-12-08 12:13:12.262615: Current learning rate: 0.00413\n",
      "2025-12-08 12:15:30.682299: train_loss -0.8499\n",
      "2025-12-08 12:15:30.683300: val_loss -0.8703\n",
      "2025-12-08 12:15:30.690130: Pseudo dice [0.9204, 0.9533, 0.9473]\n",
      "2025-12-08 12:15:30.696136: Epoch time: 138.43 s\n",
      "2025-12-08 12:15:31.390268: \n",
      "2025-12-08 12:15:31.390268: Epoch 627\n",
      "2025-12-08 12:15:31.390268: Current learning rate: 0.00412\n",
      "2025-12-08 12:17:49.390009: train_loss -0.8502\n",
      "2025-12-08 12:17:49.390009: val_loss -0.8609\n",
      "2025-12-08 12:17:49.390009: Pseudo dice [0.9157, 0.9459, 0.9449]\n",
      "2025-12-08 12:17:49.405677: Epoch time: 138.0 s\n",
      "2025-12-08 12:17:50.218261: \n",
      "2025-12-08 12:17:50.218261: Epoch 628\n",
      "2025-12-08 12:17:50.234213: Current learning rate: 0.00411\n",
      "2025-12-08 12:20:08.339077: train_loss -0.8468\n",
      "2025-12-08 12:20:08.339077: val_loss -0.8728\n",
      "2025-12-08 12:20:08.345078: Pseudo dice [0.9238, 0.9495, 0.9431]\n",
      "2025-12-08 12:20:08.350079: Epoch time: 138.12 s\n",
      "2025-12-08 12:20:09.104075: \n",
      "2025-12-08 12:20:09.104075: Epoch 629\n",
      "2025-12-08 12:20:09.109162: Current learning rate: 0.0041\n",
      "2025-12-08 12:22:27.361389: train_loss -0.8479\n",
      "2025-12-08 12:22:27.362390: val_loss -0.8784\n",
      "2025-12-08 12:22:27.368390: Pseudo dice [0.9241, 0.9594, 0.9469]\n",
      "2025-12-08 12:22:27.373391: Epoch time: 138.26 s\n",
      "2025-12-08 12:22:28.015139: \n",
      "2025-12-08 12:22:28.015139: Epoch 630\n",
      "2025-12-08 12:22:28.030999: Current learning rate: 0.00409\n",
      "2025-12-08 12:24:46.295503: train_loss -0.8451\n",
      "2025-12-08 12:24:46.295503: val_loss -0.8629\n",
      "2025-12-08 12:24:46.315543: Pseudo dice [0.92, 0.9529, 0.9399]\n",
      "2025-12-08 12:24:46.321549: Epoch time: 138.28 s\n",
      "2025-12-08 12:24:46.952246: \n",
      "2025-12-08 12:24:46.952246: Epoch 631\n",
      "2025-12-08 12:24:46.968308: Current learning rate: 0.00408\n",
      "2025-12-08 12:27:05.041759: train_loss -0.847\n",
      "2025-12-08 12:27:05.041759: val_loss -0.874\n",
      "2025-12-08 12:27:05.045763: Pseudo dice [0.9228, 0.9557, 0.9474]\n",
      "2025-12-08 12:27:05.052637: Epoch time: 138.09 s\n",
      "2025-12-08 12:27:05.858378: \n",
      "2025-12-08 12:27:05.858378: Epoch 632\n",
      "2025-12-08 12:27:05.863616: Current learning rate: 0.00407\n",
      "2025-12-08 12:29:24.063162: train_loss -0.8465\n",
      "2025-12-08 12:29:24.065165: val_loss -0.8755\n",
      "2025-12-08 12:29:24.071172: Pseudo dice [0.9219, 0.9567, 0.9515]\n",
      "2025-12-08 12:29:24.075176: Epoch time: 138.2 s\n",
      "2025-12-08 12:29:24.702618: \n",
      "2025-12-08 12:29:24.702618: Epoch 633\n",
      "2025-12-08 12:29:24.718655: Current learning rate: 0.00406\n",
      "2025-12-08 12:31:42.971310: train_loss -0.8462\n",
      "2025-12-08 12:31:42.971310: val_loss -0.8716\n",
      "2025-12-08 12:31:42.976793: Pseudo dice [0.9229, 0.9559, 0.9501]\n",
      "2025-12-08 12:31:42.980803: Epoch time: 138.27 s\n",
      "2025-12-08 12:31:43.621922: \n",
      "2025-12-08 12:31:43.621922: Epoch 634\n",
      "2025-12-08 12:31:43.629534: Current learning rate: 0.00405\n",
      "2025-12-08 12:34:01.682093: train_loss -0.8533\n",
      "2025-12-08 12:34:01.682093: val_loss -0.8695\n",
      "2025-12-08 12:34:01.685836: Pseudo dice [0.9213, 0.9488, 0.9485]\n",
      "2025-12-08 12:34:01.693342: Epoch time: 138.06 s\n",
      "2025-12-08 12:34:02.479089: \n",
      "2025-12-08 12:34:02.479089: Epoch 635\n",
      "2025-12-08 12:34:02.484558: Current learning rate: 0.00404\n",
      "2025-12-08 12:36:20.505781: train_loss -0.8467\n",
      "2025-12-08 12:36:20.505781: val_loss -0.8792\n",
      "2025-12-08 12:36:20.511789: Pseudo dice [0.9274, 0.9588, 0.9447]\n",
      "2025-12-08 12:36:20.517536: Epoch time: 138.03 s\n",
      "2025-12-08 12:36:21.161222: \n",
      "2025-12-08 12:36:21.161222: Epoch 636\n",
      "2025-12-08 12:36:21.166617: Current learning rate: 0.00403\n",
      "2025-12-08 12:38:39.384805: train_loss -0.8521\n",
      "2025-12-08 12:38:39.386807: val_loss -0.8772\n",
      "2025-12-08 12:38:39.392551: Pseudo dice [0.9242, 0.9548, 0.949]\n",
      "2025-12-08 12:38:39.398557: Epoch time: 138.22 s\n",
      "2025-12-08 12:38:40.071875: \n",
      "2025-12-08 12:38:40.071875: Epoch 637\n",
      "2025-12-08 12:38:40.076886: Current learning rate: 0.00402\n",
      "2025-12-08 12:40:58.201819: train_loss -0.8494\n",
      "2025-12-08 12:40:58.202852: val_loss -0.8639\n",
      "2025-12-08 12:40:58.209970: Pseudo dice [0.9165, 0.9454, 0.9463]\n",
      "2025-12-08 12:40:58.213978: Epoch time: 138.13 s\n",
      "2025-12-08 12:40:58.952604: \n",
      "2025-12-08 12:40:58.952604: Epoch 638\n",
      "2025-12-08 12:40:58.968668: Current learning rate: 0.00401\n",
      "2025-12-08 12:43:17.202959: train_loss -0.8538\n",
      "2025-12-08 12:43:17.202959: val_loss -0.8772\n",
      "2025-12-08 12:43:17.202959: Pseudo dice [0.9259, 0.958, 0.948]\n",
      "2025-12-08 12:43:17.218887: Epoch time: 138.25 s\n",
      "2025-12-08 12:43:18.061367: \n",
      "2025-12-08 12:43:18.061367: Epoch 639\n",
      "2025-12-08 12:43:18.061367: Current learning rate: 0.004\n",
      "2025-12-08 12:45:36.186646: train_loss -0.8497\n",
      "2025-12-08 12:45:36.186646: val_loss -0.8796\n",
      "2025-12-08 12:45:36.197527: Pseudo dice [0.9307, 0.9575, 0.9476]\n",
      "2025-12-08 12:45:36.204708: Epoch time: 138.13 s\n",
      "2025-12-08 12:45:36.836503: \n",
      "2025-12-08 12:45:36.837781: Epoch 640\n",
      "2025-12-08 12:45:36.842793: Current learning rate: 0.00399\n",
      "2025-12-08 12:47:55.055684: train_loss -0.8514\n",
      "2025-12-08 12:47:55.055684: val_loss -0.8796\n",
      "2025-12-08 12:47:55.061689: Pseudo dice [0.9272, 0.9547, 0.9491]\n",
      "2025-12-08 12:47:55.066866: Epoch time: 138.22 s\n",
      "2025-12-08 12:47:55.732917: \n",
      "2025-12-08 12:47:55.732917: Epoch 641\n",
      "2025-12-08 12:47:55.732917: Current learning rate: 0.00398\n",
      "2025-12-08 12:50:13.796073: train_loss -0.8495\n",
      "2025-12-08 12:50:13.796073: val_loss -0.8814\n",
      "2025-12-08 12:50:13.796073: Pseudo dice [0.9294, 0.9582, 0.9482]\n",
      "2025-12-08 12:50:13.811918: Epoch time: 138.06 s\n",
      "2025-12-08 12:50:14.451980: \n",
      "2025-12-08 12:50:14.451980: Epoch 642\n",
      "2025-12-08 12:50:14.451980: Current learning rate: 0.00397\n",
      "2025-12-08 12:52:32.466749: train_loss -0.8481\n",
      "2025-12-08 12:52:32.467750: val_loss -0.8626\n",
      "2025-12-08 12:52:32.473797: Pseudo dice [0.9166, 0.9483, 0.9405]\n",
      "2025-12-08 12:52:32.479798: Epoch time: 138.01 s\n",
      "2025-12-08 12:52:33.130265: \n",
      "2025-12-08 12:52:33.130265: Epoch 643\n",
      "2025-12-08 12:52:33.135283: Current learning rate: 0.00396\n",
      "2025-12-08 12:54:51.125425: train_loss -0.8495\n",
      "2025-12-08 12:54:51.125425: val_loss -0.8736\n",
      "2025-12-08 12:54:51.141081: Pseudo dice [0.9265, 0.9588, 0.9412]\n",
      "2025-12-08 12:54:51.141081: Epoch time: 138.0 s\n",
      "2025-12-08 12:54:51.780529: \n",
      "2025-12-08 12:54:51.780529: Epoch 644\n",
      "2025-12-08 12:54:51.780529: Current learning rate: 0.00395\n",
      "2025-12-08 12:57:09.844167: train_loss -0.8462\n",
      "2025-12-08 12:57:09.844167: val_loss -0.8747\n",
      "2025-12-08 12:57:09.851676: Pseudo dice [0.9247, 0.9585, 0.9422]\n",
      "2025-12-08 12:57:09.851676: Epoch time: 138.06 s\n",
      "2025-12-08 12:57:10.554338: \n",
      "2025-12-08 12:57:10.554338: Epoch 645\n",
      "2025-12-08 12:57:10.559352: Current learning rate: 0.00394\n",
      "2025-12-08 12:59:28.682719: train_loss -0.8491\n",
      "2025-12-08 12:59:28.682719: val_loss -0.8805\n",
      "2025-12-08 12:59:28.686916: Pseudo dice [0.9306, 0.9603, 0.9405]\n",
      "2025-12-08 12:59:28.692058: Epoch time: 138.13 s\n",
      "2025-12-08 12:59:29.511787: \n",
      "2025-12-08 12:59:29.511787: Epoch 646\n",
      "2025-12-08 12:59:29.515794: Current learning rate: 0.00393\n",
      "2025-12-08 13:01:47.577433: train_loss -0.8503\n",
      "2025-12-08 13:01:47.577433: val_loss -0.8708\n",
      "2025-12-08 13:01:47.595304: Pseudo dice [0.9182, 0.9563, 0.9436]\n",
      "2025-12-08 13:01:47.601310: Epoch time: 138.07 s\n",
      "2025-12-08 13:01:48.246366: \n",
      "2025-12-08 13:01:48.246366: Epoch 647\n",
      "2025-12-08 13:01:48.248692: Current learning rate: 0.00392\n",
      "2025-12-08 13:04:06.263833: train_loss -0.8515\n",
      "2025-12-08 13:04:06.263833: val_loss -0.8723\n",
      "2025-12-08 13:04:06.270432: Pseudo dice [0.921, 0.954, 0.9481]\n",
      "2025-12-08 13:04:06.274436: Epoch time: 138.02 s\n",
      "2025-12-08 13:04:06.923144: \n",
      "2025-12-08 13:04:06.923144: Epoch 648\n",
      "2025-12-08 13:04:06.923144: Current learning rate: 0.00391\n",
      "2025-12-08 13:06:25.076009: train_loss -0.8512\n",
      "2025-12-08 13:06:25.076009: val_loss -0.8826\n",
      "2025-12-08 13:06:25.081069: Pseudo dice [0.927, 0.9625, 0.9515]\n",
      "2025-12-08 13:06:25.086075: Epoch time: 138.15 s\n",
      "2025-12-08 13:06:25.736226: \n",
      "2025-12-08 13:06:25.736226: Epoch 649\n",
      "2025-12-08 13:06:25.741611: Current learning rate: 0.0039\n",
      "2025-12-08 13:08:43.779920: train_loss -0.8528\n",
      "2025-12-08 13:08:43.795563: val_loss -0.8786\n",
      "2025-12-08 13:08:43.799568: Pseudo dice [0.9274, 0.956, 0.9487]\n",
      "2025-12-08 13:08:43.803572: Epoch time: 138.04 s\n",
      "2025-12-08 13:08:44.805733: \n",
      "2025-12-08 13:08:44.805733: Epoch 650\n",
      "2025-12-08 13:08:44.811478: Current learning rate: 0.00389\n",
      "2025-12-08 13:11:03.074291: train_loss -0.8493\n",
      "2025-12-08 13:11:03.075291: val_loss -0.8761\n",
      "2025-12-08 13:11:03.081501: Pseudo dice [0.9271, 0.9593, 0.9403]\n",
      "2025-12-08 13:11:03.087502: Epoch time: 138.27 s\n",
      "2025-12-08 13:11:03.737605: \n",
      "2025-12-08 13:11:03.739607: Epoch 651\n",
      "2025-12-08 13:11:03.739607: Current learning rate: 0.00388\n",
      "2025-12-08 13:13:22.036629: train_loss -0.8576\n",
      "2025-12-08 13:13:22.036629: val_loss -0.876\n",
      "2025-12-08 13:13:22.041649: Pseudo dice [0.9249, 0.9596, 0.9437]\n",
      "2025-12-08 13:13:22.045661: Epoch time: 138.3 s\n",
      "2025-12-08 13:13:22.687684: \n",
      "2025-12-08 13:13:22.688865: Epoch 652\n",
      "2025-12-08 13:13:22.692799: Current learning rate: 0.00387\n",
      "2025-12-08 13:15:40.876158: train_loss -0.8509\n",
      "2025-12-08 13:15:40.878163: val_loss -0.873\n",
      "2025-12-08 13:15:40.884170: Pseudo dice [0.923, 0.9541, 0.9468]\n",
      "2025-12-08 13:15:40.889914: Epoch time: 138.2 s\n",
      "2025-12-08 13:15:41.530931: \n",
      "2025-12-08 13:15:41.530931: Epoch 653\n",
      "2025-12-08 13:15:41.546314: Current learning rate: 0.00386\n",
      "2025-12-08 13:17:59.666878: train_loss -0.8518\n",
      "2025-12-08 13:17:59.666878: val_loss -0.874\n",
      "2025-12-08 13:17:59.670907: Pseudo dice [0.9242, 0.9545, 0.9501]\n",
      "2025-12-08 13:17:59.670907: Epoch time: 138.14 s\n",
      "2025-12-08 13:18:00.317849: \n",
      "2025-12-08 13:18:00.318849: Epoch 654\n",
      "2025-12-08 13:18:00.323081: Current learning rate: 0.00385\n",
      "2025-12-08 13:20:18.284275: train_loss -0.852\n",
      "2025-12-08 13:20:18.284275: val_loss -0.8768\n",
      "2025-12-08 13:20:18.290285: Pseudo dice [0.924, 0.9546, 0.9491]\n",
      "2025-12-08 13:20:18.294290: Epoch time: 137.97 s\n",
      "2025-12-08 13:20:18.957392: \n",
      "2025-12-08 13:20:18.957392: Epoch 655\n",
      "2025-12-08 13:20:18.957392: Current learning rate: 0.00384\n",
      "2025-12-08 13:22:37.208921: train_loss -0.8519\n",
      "2025-12-08 13:22:37.210923: val_loss -0.8775\n",
      "2025-12-08 13:22:37.214926: Pseudo dice [0.9246, 0.9557, 0.9454]\n",
      "2025-12-08 13:22:37.220747: Epoch time: 138.25 s\n",
      "2025-12-08 13:22:37.920427: \n",
      "2025-12-08 13:22:37.920427: Epoch 656\n",
      "2025-12-08 13:22:37.936234: Current learning rate: 0.00383\n",
      "2025-12-08 13:24:56.140154: train_loss -0.8506\n",
      "2025-12-08 13:24:56.140154: val_loss -0.8784\n",
      "2025-12-08 13:24:56.148244: Pseudo dice [0.9237, 0.955, 0.952]\n",
      "2025-12-08 13:24:56.154905: Epoch time: 138.22 s\n",
      "2025-12-08 13:24:56.952526: \n",
      "2025-12-08 13:24:56.952526: Epoch 657\n",
      "2025-12-08 13:24:56.968234: Current learning rate: 0.00382\n",
      "2025-12-08 13:27:15.185112: train_loss -0.851\n",
      "2025-12-08 13:27:15.186114: val_loss -0.8845\n",
      "2025-12-08 13:27:15.186114: Pseudo dice [0.9315, 0.9533, 0.9538]\n",
      "2025-12-08 13:27:15.195626: Epoch time: 138.23 s\n",
      "2025-12-08 13:27:15.199631: Yayy! New best EMA pseudo Dice: 0.9428\n",
      "2025-12-08 13:27:16.156150: \n",
      "2025-12-08 13:27:16.156150: Epoch 658\n",
      "2025-12-08 13:27:16.156150: Current learning rate: 0.00381\n",
      "2025-12-08 13:29:34.123275: train_loss -0.8551\n",
      "2025-12-08 13:29:34.125064: val_loss -0.8738\n",
      "2025-12-08 13:29:34.127066: Pseudo dice [0.9215, 0.9526, 0.95]\n",
      "2025-12-08 13:29:34.134416: Epoch time: 137.97 s\n",
      "2025-12-08 13:29:34.779781: \n",
      "2025-12-08 13:29:34.779781: Epoch 659\n",
      "2025-12-08 13:29:34.779781: Current learning rate: 0.0038\n",
      "2025-12-08 13:31:52.920936: train_loss -0.8375\n",
      "2025-12-08 13:31:52.920936: val_loss -0.8666\n",
      "2025-12-08 13:31:52.920936: Pseudo dice [0.9132, 0.9472, 0.953]\n",
      "2025-12-08 13:31:52.920936: Epoch time: 138.14 s\n",
      "2025-12-08 13:31:53.561152: \n",
      "2025-12-08 13:31:53.561152: Epoch 660\n",
      "2025-12-08 13:31:53.561152: Current learning rate: 0.00379\n",
      "2025-12-08 13:34:11.655358: train_loss -0.8367\n",
      "2025-12-08 13:34:11.655358: val_loss -0.8659\n",
      "2025-12-08 13:34:11.655358: Pseudo dice [0.921, 0.9563, 0.9431]\n",
      "2025-12-08 13:34:11.655358: Epoch time: 138.09 s\n",
      "2025-12-08 13:34:12.416150: \n",
      "2025-12-08 13:34:12.417152: Epoch 661\n",
      "2025-12-08 13:34:12.422212: Current learning rate: 0.00378\n",
      "2025-12-08 13:36:30.378357: train_loss -0.8428\n",
      "2025-12-08 13:36:30.379359: val_loss -0.8678\n",
      "2025-12-08 13:36:30.384370: Pseudo dice [0.9195, 0.9521, 0.9471]\n",
      "2025-12-08 13:36:30.388379: Epoch time: 137.96 s\n",
      "2025-12-08 13:36:31.015163: \n",
      "2025-12-08 13:36:31.015163: Epoch 662\n",
      "2025-12-08 13:36:31.015163: Current learning rate: 0.00377\n",
      "2025-12-08 13:38:48.950665: train_loss -0.8474\n",
      "2025-12-08 13:38:48.952406: val_loss -0.8791\n",
      "2025-12-08 13:38:48.959559: Pseudo dice [0.927, 0.9598, 0.9507]\n",
      "2025-12-08 13:38:48.964560: Epoch time: 137.94 s\n",
      "2025-12-08 13:38:49.639631: \n",
      "2025-12-08 13:38:49.639631: Epoch 663\n",
      "2025-12-08 13:38:49.639631: Current learning rate: 0.00376\n",
      "2025-12-08 13:41:07.859092: train_loss -0.8485\n",
      "2025-12-08 13:41:07.859092: val_loss -0.8763\n",
      "2025-12-08 13:41:07.859092: Pseudo dice [0.9261, 0.9571, 0.9499]\n",
      "2025-12-08 13:41:07.859092: Epoch time: 138.22 s\n",
      "2025-12-08 13:41:08.878801: \n",
      "2025-12-08 13:41:08.879804: Epoch 664\n",
      "2025-12-08 13:41:08.885817: Current learning rate: 0.00375\n",
      "2025-12-08 13:43:26.907921: train_loss -0.8461\n",
      "2025-12-08 13:43:26.908922: val_loss -0.8768\n",
      "2025-12-08 13:43:26.915942: Pseudo dice [0.9259, 0.9593, 0.953]\n",
      "2025-12-08 13:43:26.921955: Epoch time: 138.03 s\n",
      "2025-12-08 13:43:27.562299: \n",
      "2025-12-08 13:43:27.562299: Epoch 665\n",
      "2025-12-08 13:43:27.562299: Current learning rate: 0.00374\n",
      "2025-12-08 13:45:45.720987: train_loss -0.8485\n",
      "2025-12-08 13:45:45.721988: val_loss -0.8729\n",
      "2025-12-08 13:45:45.727014: Pseudo dice [0.9237, 0.9541, 0.9462]\n",
      "2025-12-08 13:45:45.732029: Epoch time: 138.16 s\n",
      "2025-12-08 13:45:46.374135: \n",
      "2025-12-08 13:45:46.374135: Epoch 666\n",
      "2025-12-08 13:45:46.390042: Current learning rate: 0.00373\n",
      "2025-12-08 13:48:04.329099: train_loss -0.8539\n",
      "2025-12-08 13:48:04.330099: val_loss -0.8764\n",
      "2025-12-08 13:48:04.335121: Pseudo dice [0.9244, 0.9555, 0.9459]\n",
      "2025-12-08 13:48:04.339128: Epoch time: 137.95 s\n",
      "2025-12-08 13:48:05.093156: \n",
      "2025-12-08 13:48:05.093156: Epoch 667\n",
      "2025-12-08 13:48:05.093156: Current learning rate: 0.00372\n",
      "2025-12-08 13:50:23.187428: train_loss -0.8466\n",
      "2025-12-08 13:50:23.187428: val_loss -0.8746\n",
      "2025-12-08 13:50:23.191364: Pseudo dice [0.9231, 0.9572, 0.9523]\n",
      "2025-12-08 13:50:23.197370: Epoch time: 138.09 s\n",
      "2025-12-08 13:50:23.859921: \n",
      "2025-12-08 13:50:23.859921: Epoch 668\n",
      "2025-12-08 13:50:23.859921: Current learning rate: 0.00371\n",
      "2025-12-08 13:52:41.640111: train_loss -0.8551\n",
      "2025-12-08 13:52:41.640111: val_loss -0.885\n",
      "2025-12-08 13:52:41.646566: Pseudo dice [0.9316, 0.9633, 0.9467]\n",
      "2025-12-08 13:52:41.650570: Epoch time: 137.8 s\n",
      "2025-12-08 13:52:41.654574: Yayy! New best EMA pseudo Dice: 0.9431\n",
      "2025-12-08 13:52:42.733728: \n",
      "2025-12-08 13:52:42.739226: Epoch 669\n",
      "2025-12-08 13:52:42.739226: Current learning rate: 0.0037\n",
      "2025-12-08 13:55:00.776456: train_loss -0.8571\n",
      "2025-12-08 13:55:00.778459: val_loss -0.879\n",
      "2025-12-08 13:55:00.788223: Pseudo dice [0.9281, 0.9551, 0.95]\n",
      "2025-12-08 13:55:00.794233: Epoch time: 138.04 s\n",
      "2025-12-08 13:55:00.801980: Yayy! New best EMA pseudo Dice: 0.9433\n",
      "2025-12-08 13:55:01.791488: \n",
      "2025-12-08 13:55:01.791488: Epoch 670\n",
      "2025-12-08 13:55:01.798554: Current learning rate: 0.00369\n",
      "2025-12-08 13:57:19.883137: train_loss -0.8569\n",
      "2025-12-08 13:57:19.883137: val_loss -0.8703\n",
      "2025-12-08 13:57:19.889146: Pseudo dice [0.9218, 0.9539, 0.9454]\n",
      "2025-12-08 13:57:19.895155: Epoch time: 138.09 s\n",
      "2025-12-08 13:57:20.547247: \n",
      "2025-12-08 13:57:20.547247: Epoch 671\n",
      "2025-12-08 13:57:20.563160: Current learning rate: 0.00368\n",
      "2025-12-08 13:59:38.639825: train_loss -0.851\n",
      "2025-12-08 13:59:38.639825: val_loss -0.8778\n",
      "2025-12-08 13:59:38.651332: Pseudo dice [0.9268, 0.9564, 0.948]\n",
      "2025-12-08 13:59:38.657338: Epoch time: 138.09 s\n",
      "2025-12-08 13:59:39.311259: \n",
      "2025-12-08 13:59:39.311259: Epoch 672\n",
      "2025-12-08 13:59:39.311259: Current learning rate: 0.00367\n",
      "2025-12-08 14:01:57.503399: train_loss -0.8495\n",
      "2025-12-08 14:01:57.505403: val_loss -0.8764\n",
      "2025-12-08 14:01:57.515160: Pseudo dice [0.9246, 0.9556, 0.9492]\n",
      "2025-12-08 14:01:57.521169: Epoch time: 138.19 s\n",
      "2025-12-08 14:01:58.226948: \n",
      "2025-12-08 14:01:58.226948: Epoch 673\n",
      "2025-12-08 14:01:58.232206: Current learning rate: 0.00366\n",
      "2025-12-08 14:04:16.421546: train_loss -0.8544\n",
      "2025-12-08 14:04:16.421546: val_loss -0.8823\n",
      "2025-12-08 14:04:16.428849: Pseudo dice [0.9272, 0.9575, 0.9527]\n",
      "2025-12-08 14:04:16.433204: Epoch time: 138.2 s\n",
      "2025-12-08 14:04:16.437432: Yayy! New best EMA pseudo Dice: 0.9433\n",
      "2025-12-08 14:04:17.368157: \n",
      "2025-12-08 14:04:17.369160: Epoch 674\n",
      "2025-12-08 14:04:17.373160: Current learning rate: 0.00365\n",
      "2025-12-08 14:06:35.379320: train_loss -0.8573\n",
      "2025-12-08 14:06:35.380321: val_loss -0.8849\n",
      "2025-12-08 14:06:35.385486: Pseudo dice [0.9324, 0.9622, 0.9481]\n",
      "2025-12-08 14:06:35.389491: Epoch time: 138.01 s\n",
      "2025-12-08 14:06:35.389491: Yayy! New best EMA pseudo Dice: 0.9438\n",
      "2025-12-08 14:06:36.522148: \n",
      "2025-12-08 14:06:36.522148: Epoch 675\n",
      "2025-12-08 14:06:36.527151: Current learning rate: 0.00364\n",
      "2025-12-08 14:08:54.505797: train_loss -0.854\n",
      "2025-12-08 14:08:54.505797: val_loss -0.8808\n",
      "2025-12-08 14:08:54.513805: Pseudo dice [0.9264, 0.9564, 0.9494]\n",
      "2025-12-08 14:08:54.519752: Epoch time: 137.98 s\n",
      "2025-12-08 14:08:54.523753: Yayy! New best EMA pseudo Dice: 0.9438\n",
      "2025-12-08 14:08:55.623903: \n",
      "2025-12-08 14:08:55.623903: Epoch 676\n",
      "2025-12-08 14:08:55.641427: Current learning rate: 0.00363\n",
      "2025-12-08 14:11:13.809203: train_loss -0.8492\n",
      "2025-12-08 14:11:13.809203: val_loss -0.8823\n",
      "2025-12-08 14:11:13.816720: Pseudo dice [0.929, 0.9605, 0.9456]\n",
      "2025-12-08 14:11:13.820724: Epoch time: 138.19 s\n",
      "2025-12-08 14:11:13.826730: Yayy! New best EMA pseudo Dice: 0.9439\n",
      "2025-12-08 14:11:14.767010: \n",
      "2025-12-08 14:11:14.767010: Epoch 677\n",
      "2025-12-08 14:11:14.767010: Current learning rate: 0.00362\n",
      "2025-12-08 14:13:32.906380: train_loss -0.8514\n",
      "2025-12-08 14:13:32.906380: val_loss -0.871\n",
      "2025-12-08 14:13:32.906380: Pseudo dice [0.9194, 0.9498, 0.9495]\n",
      "2025-12-08 14:13:32.924267: Epoch time: 138.14 s\n",
      "2025-12-08 14:13:33.577213: \n",
      "2025-12-08 14:13:33.577213: Epoch 678\n",
      "2025-12-08 14:13:33.577213: Current learning rate: 0.00361\n",
      "2025-12-08 14:15:51.697707: train_loss -0.8552\n",
      "2025-12-08 14:15:51.697707: val_loss -0.8754\n",
      "2025-12-08 14:15:51.704954: Pseudo dice [0.9245, 0.9542, 0.9522]\n",
      "2025-12-08 14:15:51.706955: Epoch time: 138.12 s\n",
      "2025-12-08 14:15:52.390827: \n",
      "2025-12-08 14:15:52.390827: Epoch 679\n",
      "2025-12-08 14:15:52.409650: Current learning rate: 0.0036\n",
      "2025-12-08 14:18:10.436878: train_loss -0.8514\n",
      "2025-12-08 14:18:10.436878: val_loss -0.8764\n",
      "2025-12-08 14:18:10.455440: Pseudo dice [0.9282, 0.9522, 0.94]\n",
      "2025-12-08 14:18:10.461185: Epoch time: 138.05 s\n",
      "2025-12-08 14:18:11.107991: \n",
      "2025-12-08 14:18:11.109029: Epoch 680\n",
      "2025-12-08 14:18:11.113734: Current learning rate: 0.00359\n",
      "2025-12-08 14:20:29.183959: train_loss -0.8499\n",
      "2025-12-08 14:20:29.183959: val_loss -0.8855\n",
      "2025-12-08 14:20:29.186929: Pseudo dice [0.931, 0.9633, 0.9524]\n",
      "2025-12-08 14:20:29.194530: Epoch time: 138.08 s\n",
      "2025-12-08 14:20:29.876988: \n",
      "2025-12-08 14:20:29.876988: Epoch 681\n",
      "2025-12-08 14:20:29.882746: Current learning rate: 0.00358\n",
      "2025-12-08 14:22:47.878353: train_loss -0.855\n",
      "2025-12-08 14:22:47.880356: val_loss -0.8866\n",
      "2025-12-08 14:22:47.886361: Pseudo dice [0.9307, 0.9586, 0.9464]\n",
      "2025-12-08 14:22:47.893872: Epoch time: 138.0 s\n",
      "2025-12-08 14:22:48.545602: \n",
      "2025-12-08 14:22:48.545602: Epoch 682\n",
      "2025-12-08 14:22:48.545602: Current learning rate: 0.00357\n",
      "2025-12-08 14:25:06.606699: train_loss -0.8555\n",
      "2025-12-08 14:25:06.608439: val_loss -0.8752\n",
      "2025-12-08 14:25:06.613319: Pseudo dice [0.9195, 0.9583, 0.952]\n",
      "2025-12-08 14:25:06.617322: Epoch time: 138.06 s\n",
      "2025-12-08 14:25:07.312084: \n",
      "2025-12-08 14:25:07.312084: Epoch 683\n",
      "2025-12-08 14:25:07.312084: Current learning rate: 0.00356\n",
      "2025-12-08 14:27:25.361518: train_loss -0.8478\n",
      "2025-12-08 14:27:25.363520: val_loss -0.8766\n",
      "2025-12-08 14:27:25.369264: Pseudo dice [0.9272, 0.9574, 0.9429]\n",
      "2025-12-08 14:27:25.373268: Epoch time: 138.05 s\n",
      "2025-12-08 14:27:26.014367: \n",
      "2025-12-08 14:27:26.014367: Epoch 684\n",
      "2025-12-08 14:27:26.030026: Current learning rate: 0.00355\n",
      "2025-12-08 14:29:45.748864: train_loss -0.8596\n",
      "2025-12-08 14:29:45.748864: val_loss -0.8849\n",
      "2025-12-08 14:29:45.756612: Pseudo dice [0.9335, 0.963, 0.9442]\n",
      "2025-12-08 14:29:45.764621: Epoch time: 139.73 s\n",
      "2025-12-08 14:29:45.770629: Yayy! New best EMA pseudo Dice: 0.944\n",
      "2025-12-08 14:29:46.799966: \n",
      "2025-12-08 14:29:46.799966: Epoch 685\n",
      "2025-12-08 14:29:46.811840: Current learning rate: 0.00354\n",
      "2025-12-08 14:32:05.424462: train_loss -0.8565\n",
      "2025-12-08 14:32:05.426464: val_loss -0.8789\n",
      "2025-12-08 14:32:05.430469: Pseudo dice [0.9272, 0.9539, 0.949]\n",
      "2025-12-08 14:32:05.438172: Epoch time: 138.62 s\n",
      "2025-12-08 14:32:06.265668: \n",
      "2025-12-08 14:32:06.265668: Epoch 686\n",
      "2025-12-08 14:32:06.272129: Current learning rate: 0.00353\n",
      "2025-12-08 14:34:24.760601: train_loss -0.8516\n",
      "2025-12-08 14:34:24.760601: val_loss -0.8749\n",
      "2025-12-08 14:34:24.768873: Pseudo dice [0.9254, 0.9566, 0.9446]\n",
      "2025-12-08 14:34:24.774616: Epoch time: 138.49 s\n",
      "2025-12-08 14:34:25.433776: \n",
      "2025-12-08 14:34:25.434779: Epoch 687\n",
      "2025-12-08 14:34:25.439952: Current learning rate: 0.00352\n",
      "2025-12-08 14:36:44.061196: train_loss -0.8514\n",
      "2025-12-08 14:36:44.061196: val_loss -0.8857\n",
      "2025-12-08 14:36:44.077057: Pseudo dice [0.9297, 0.961, 0.9521]\n",
      "2025-12-08 14:36:44.077057: Epoch time: 138.63 s\n",
      "2025-12-08 14:36:44.077057: Yayy! New best EMA pseudo Dice: 0.9442\n",
      "2025-12-08 14:36:45.045638: \n",
      "2025-12-08 14:36:45.045638: Epoch 688\n",
      "2025-12-08 14:36:45.061318: Current learning rate: 0.00351\n",
      "2025-12-08 14:39:03.537052: train_loss -0.8495\n",
      "2025-12-08 14:39:03.543052: val_loss -0.8803\n",
      "2025-12-08 14:39:03.548106: Pseudo dice [0.928, 0.9555, 0.9497]\n",
      "2025-12-08 14:39:03.553108: Epoch time: 138.49 s\n",
      "2025-12-08 14:39:03.557119: Yayy! New best EMA pseudo Dice: 0.9442\n",
      "2025-12-08 14:39:04.748383: \n",
      "2025-12-08 14:39:04.748383: Epoch 689\n",
      "2025-12-08 14:39:04.748383: Current learning rate: 0.0035\n",
      "2025-12-08 14:41:23.075969: train_loss -0.8569\n",
      "2025-12-08 14:41:23.078872: val_loss -0.884\n",
      "2025-12-08 14:41:23.087086: Pseudo dice [0.9301, 0.9611, 0.9493]\n",
      "2025-12-08 14:41:23.095119: Epoch time: 138.33 s\n",
      "2025-12-08 14:41:23.100863: Yayy! New best EMA pseudo Dice: 0.9444\n",
      "2025-12-08 14:41:24.030869: \n",
      "2025-12-08 14:41:24.030869: Epoch 690\n",
      "2025-12-08 14:41:24.046531: Current learning rate: 0.00349\n",
      "2025-12-08 14:43:42.276862: train_loss -0.8538\n",
      "2025-12-08 14:43:42.278863: val_loss -0.8714\n",
      "2025-12-08 14:43:42.280604: Pseudo dice [0.9199, 0.9554, 0.9489]\n",
      "2025-12-08 14:43:42.280604: Epoch time: 138.25 s\n",
      "2025-12-08 14:43:43.157835: \n",
      "2025-12-08 14:43:43.157835: Epoch 691\n",
      "2025-12-08 14:43:43.157835: Current learning rate: 0.00348\n",
      "2025-12-08 14:46:01.244288: train_loss -0.8508\n",
      "2025-12-08 14:46:01.246290: val_loss -0.8931\n",
      "2025-12-08 14:46:01.251446: Pseudo dice [0.9371, 0.9627, 0.9511]\n",
      "2025-12-08 14:46:01.256457: Epoch time: 138.09 s\n",
      "2025-12-08 14:46:01.260458: Yayy! New best EMA pseudo Dice: 0.9448\n",
      "2025-12-08 14:46:02.187280: \n",
      "2025-12-08 14:46:02.187280: Epoch 692\n",
      "2025-12-08 14:46:02.187280: Current learning rate: 0.00346\n",
      "2025-12-08 14:48:20.374037: train_loss -0.8538\n",
      "2025-12-08 14:48:20.374037: val_loss -0.8776\n",
      "2025-12-08 14:48:20.374037: Pseudo dice [0.9246, 0.9564, 0.9469]\n",
      "2025-12-08 14:48:20.389688: Epoch time: 138.19 s\n",
      "2025-12-08 14:48:21.033115: \n",
      "2025-12-08 14:48:21.034119: Epoch 693\n",
      "2025-12-08 14:48:21.034119: Current learning rate: 0.00345\n",
      "2025-12-08 14:50:39.214491: train_loss -0.8524\n",
      "2025-12-08 14:50:39.216493: val_loss -0.8894\n",
      "2025-12-08 14:50:39.222499: Pseudo dice [0.9321, 0.9608, 0.9519]\n",
      "2025-12-08 14:50:39.226503: Epoch time: 138.18 s\n",
      "2025-12-08 14:50:39.230507: Yayy! New best EMA pseudo Dice: 0.9449\n",
      "2025-12-08 14:50:40.170980: \n",
      "2025-12-08 14:50:40.170980: Epoch 694\n",
      "2025-12-08 14:50:40.186890: Current learning rate: 0.00344\n",
      "2025-12-08 14:52:58.237664: train_loss -0.8514\n",
      "2025-12-08 14:52:58.237664: val_loss -0.8758\n",
      "2025-12-08 14:52:58.237664: Pseudo dice [0.9237, 0.9585, 0.9446]\n",
      "2025-12-08 14:52:58.249638: Epoch time: 138.07 s\n",
      "2025-12-08 14:52:58.889695: \n",
      "2025-12-08 14:52:58.889695: Epoch 695\n",
      "2025-12-08 14:52:58.905593: Current learning rate: 0.00343\n",
      "2025-12-08 14:55:16.933279: train_loss -0.8574\n",
      "2025-12-08 14:55:16.933279: val_loss -0.8717\n",
      "2025-12-08 14:55:16.939348: Pseudo dice [0.9204, 0.9502, 0.9472]\n",
      "2025-12-08 14:55:16.944362: Epoch time: 138.04 s\n",
      "2025-12-08 14:55:17.592209: \n",
      "2025-12-08 14:55:17.592209: Epoch 696\n",
      "2025-12-08 14:55:17.592209: Current learning rate: 0.00342\n",
      "2025-12-08 14:57:35.687188: train_loss -0.853\n",
      "2025-12-08 14:57:35.687188: val_loss -0.8822\n",
      "2025-12-08 14:57:35.703288: Pseudo dice [0.9273, 0.9569, 0.9478]\n",
      "2025-12-08 14:57:35.703288: Epoch time: 138.09 s\n",
      "2025-12-08 14:57:36.530059: \n",
      "2025-12-08 14:57:36.530059: Epoch 697\n",
      "2025-12-08 14:57:36.530059: Current learning rate: 0.00341\n",
      "2025-12-08 14:59:54.594146: train_loss -0.8521\n",
      "2025-12-08 14:59:54.596151: val_loss -0.8788\n",
      "2025-12-08 14:59:54.604166: Pseudo dice [0.9252, 0.9563, 0.9469]\n",
      "2025-12-08 14:59:54.609910: Epoch time: 138.06 s\n",
      "2025-12-08 14:59:55.265494: \n",
      "2025-12-08 14:59:55.265494: Epoch 698\n",
      "2025-12-08 14:59:55.265494: Current learning rate: 0.0034\n",
      "2025-12-08 15:02:13.295466: train_loss -0.8512\n",
      "2025-12-08 15:02:13.295466: val_loss -0.8742\n",
      "2025-12-08 15:02:13.298509: Pseudo dice [0.9231, 0.9529, 0.9486]\n",
      "2025-12-08 15:02:13.308017: Epoch time: 138.03 s\n",
      "2025-12-08 15:02:13.984174: \n",
      "2025-12-08 15:02:13.984174: Epoch 699\n",
      "2025-12-08 15:02:13.984174: Current learning rate: 0.00339\n",
      "2025-12-08 15:04:32.133641: train_loss -0.8561\n",
      "2025-12-08 15:04:32.134644: val_loss -0.8834\n",
      "2025-12-08 15:04:32.139721: Pseudo dice [0.9295, 0.958, 0.9492]\n",
      "2025-12-08 15:04:32.144183: Epoch time: 138.15 s\n",
      "2025-12-08 15:04:33.060843: \n",
      "2025-12-08 15:04:33.060843: Epoch 700\n",
      "2025-12-08 15:04:33.060843: Current learning rate: 0.00338\n",
      "2025-12-08 15:06:51.173586: train_loss -0.8585\n",
      "2025-12-08 15:06:51.174588: val_loss -0.8767\n",
      "2025-12-08 15:06:51.182713: Pseudo dice [0.925, 0.9527, 0.9491]\n",
      "2025-12-08 15:06:51.186730: Epoch time: 138.11 s\n",
      "2025-12-08 15:06:51.842302: \n",
      "2025-12-08 15:06:51.842302: Epoch 701\n",
      "2025-12-08 15:06:51.858160: Current learning rate: 0.00337\n",
      "2025-12-08 15:09:10.185847: train_loss -0.8553\n",
      "2025-12-08 15:09:10.187885: val_loss -0.8771\n",
      "2025-12-08 15:09:10.195895: Pseudo dice [0.9245, 0.9546, 0.9494]\n",
      "2025-12-08 15:09:10.201640: Epoch time: 138.34 s\n",
      "2025-12-08 15:09:10.862355: \n",
      "2025-12-08 15:09:10.862355: Epoch 702\n",
      "2025-12-08 15:09:10.868383: Current learning rate: 0.00336\n",
      "2025-12-08 15:11:28.861055: train_loss -0.8542\n",
      "2025-12-08 15:11:28.861055: val_loss -0.8747\n",
      "2025-12-08 15:11:28.869066: Pseudo dice [0.9218, 0.955, 0.9458]\n",
      "2025-12-08 15:11:28.876076: Epoch time: 138.0 s\n",
      "2025-12-08 15:11:29.764257: \n",
      "2025-12-08 15:11:29.764257: Epoch 703\n",
      "2025-12-08 15:11:29.764257: Current learning rate: 0.00335\n",
      "2025-12-08 15:13:47.735636: train_loss -0.8498\n",
      "2025-12-08 15:13:47.737639: val_loss -0.8784\n",
      "2025-12-08 15:13:47.739642: Pseudo dice [0.9266, 0.9552, 0.9504]\n",
      "2025-12-08 15:13:47.749339: Epoch time: 137.97 s\n",
      "2025-12-08 15:13:48.394916: \n",
      "2025-12-08 15:13:48.394916: Epoch 704\n",
      "2025-12-08 15:13:48.405310: Current learning rate: 0.00334\n",
      "2025-12-08 15:16:06.511687: train_loss -0.852\n",
      "2025-12-08 15:16:06.512689: val_loss -0.8772\n",
      "2025-12-08 15:16:06.518733: Pseudo dice [0.9224, 0.9592, 0.9519]\n",
      "2025-12-08 15:16:06.523748: Epoch time: 138.12 s\n",
      "2025-12-08 15:16:07.171134: \n",
      "2025-12-08 15:16:07.171134: Epoch 705\n",
      "2025-12-08 15:16:07.187208: Current learning rate: 0.00333\n",
      "2025-12-08 15:18:25.311920: train_loss -0.8554\n",
      "2025-12-08 15:18:25.311920: val_loss -0.8725\n",
      "2025-12-08 15:18:25.311920: Pseudo dice [0.9208, 0.9474, 0.9516]\n",
      "2025-12-08 15:18:25.311920: Epoch time: 138.14 s\n",
      "2025-12-08 15:18:26.077391: \n",
      "2025-12-08 15:18:26.093311: Epoch 706\n",
      "2025-12-08 15:18:26.096830: Current learning rate: 0.00332\n",
      "2025-12-08 15:20:43.994702: train_loss -0.8558\n",
      "2025-12-08 15:20:43.994702: val_loss -0.8742\n",
      "2025-12-08 15:20:43.998703: Pseudo dice [0.923, 0.9553, 0.943]\n",
      "2025-12-08 15:20:44.007097: Epoch time: 137.92 s\n",
      "2025-12-08 15:20:44.654953: \n",
      "2025-12-08 15:20:44.654953: Epoch 707\n",
      "2025-12-08 15:20:44.670748: Current learning rate: 0.00331\n",
      "2025-12-08 15:23:02.835400: train_loss -0.8521\n",
      "2025-12-08 15:23:02.836401: val_loss -0.8735\n",
      "2025-12-08 15:23:02.842675: Pseudo dice [0.9225, 0.956, 0.9477]\n",
      "2025-12-08 15:23:02.847931: Epoch time: 138.18 s\n",
      "2025-12-08 15:23:03.499043: \n",
      "2025-12-08 15:23:03.499043: Epoch 708\n",
      "2025-12-08 15:23:03.499043: Current learning rate: 0.0033\n",
      "2025-12-08 15:25:21.555880: train_loss -0.8567\n",
      "2025-12-08 15:25:21.555880: val_loss -0.8848\n",
      "2025-12-08 15:25:21.561628: Pseudo dice [0.9306, 0.9597, 0.9505]\n",
      "2025-12-08 15:25:21.566023: Epoch time: 138.06 s\n",
      "2025-12-08 15:25:22.484155: \n",
      "2025-12-08 15:25:22.484155: Epoch 709\n",
      "2025-12-08 15:25:22.504245: Current learning rate: 0.00329\n",
      "2025-12-08 15:27:40.560529: train_loss -0.8537\n",
      "2025-12-08 15:27:40.560529: val_loss -0.884\n",
      "2025-12-08 15:27:40.565109: Pseudo dice [0.9298, 0.9567, 0.9552]\n",
      "2025-12-08 15:27:40.570751: Epoch time: 138.08 s\n",
      "2025-12-08 15:27:41.220109: \n",
      "2025-12-08 15:27:41.234573: Epoch 710\n",
      "2025-12-08 15:27:41.234573: Current learning rate: 0.00328\n",
      "2025-12-08 15:29:59.371914: train_loss -0.8539\n",
      "2025-12-08 15:29:59.372916: val_loss -0.882\n",
      "2025-12-08 15:29:59.373924: Pseudo dice [0.9301, 0.9606, 0.9455]\n",
      "2025-12-08 15:29:59.373924: Epoch time: 138.15 s\n",
      "2025-12-08 15:30:00.030276: \n",
      "2025-12-08 15:30:00.030276: Epoch 711\n",
      "2025-12-08 15:30:00.030276: Current learning rate: 0.00327\n",
      "2025-12-08 15:32:18.297550: train_loss -0.8539\n",
      "2025-12-08 15:32:18.297550: val_loss -0.8782\n",
      "2025-12-08 15:32:18.297550: Pseudo dice [0.9284, 0.9569, 0.9488]\n",
      "2025-12-08 15:32:18.317600: Epoch time: 138.27 s\n",
      "2025-12-08 15:32:19.092685: \n",
      "2025-12-08 15:32:19.092685: Epoch 712\n",
      "2025-12-08 15:32:19.108673: Current learning rate: 0.00326\n",
      "2025-12-08 15:34:37.249512: train_loss -0.8508\n",
      "2025-12-08 15:34:37.249512: val_loss -0.8741\n",
      "2025-12-08 15:34:37.249512: Pseudo dice [0.9206, 0.9496, 0.9506]\n",
      "2025-12-08 15:34:37.263076: Epoch time: 138.16 s\n",
      "2025-12-08 15:34:37.906699: \n",
      "2025-12-08 15:34:37.906699: Epoch 713\n",
      "2025-12-08 15:34:37.906699: Current learning rate: 0.00325\n",
      "2025-12-08 15:36:55.968328: train_loss -0.8538\n",
      "2025-12-08 15:36:55.968328: val_loss -0.8734\n",
      "2025-12-08 15:36:55.983577: Pseudo dice [0.9196, 0.9496, 0.9496]\n",
      "2025-12-08 15:36:55.987581: Epoch time: 138.06 s\n",
      "2025-12-08 15:36:56.642704: \n",
      "2025-12-08 15:36:56.642704: Epoch 714\n",
      "2025-12-08 15:36:56.642704: Current learning rate: 0.00324\n",
      "2025-12-08 15:39:14.765977: train_loss -0.8568\n",
      "2025-12-08 15:39:14.765977: val_loss -0.8758\n",
      "2025-12-08 15:39:14.772019: Pseudo dice [0.9245, 0.9564, 0.9502]\n",
      "2025-12-08 15:39:14.777043: Epoch time: 138.12 s\n",
      "2025-12-08 15:39:15.763789: \n",
      "2025-12-08 15:39:15.763789: Epoch 715\n",
      "2025-12-08 15:39:15.770974: Current learning rate: 0.00323\n",
      "2025-12-08 15:41:34.061836: train_loss -0.8515\n",
      "2025-12-08 15:41:34.061836: val_loss -0.8789\n",
      "2025-12-08 15:41:34.077581: Pseudo dice [0.9279, 0.9584, 0.9528]\n",
      "2025-12-08 15:41:34.077581: Epoch time: 138.3 s\n",
      "2025-12-08 15:41:34.733670: \n",
      "2025-12-08 15:41:34.733670: Epoch 716\n",
      "2025-12-08 15:41:34.733670: Current learning rate: 0.00322\n",
      "2025-12-08 15:43:52.670922: train_loss -0.859\n",
      "2025-12-08 15:43:52.670922: val_loss -0.8717\n",
      "2025-12-08 15:43:52.686578: Pseudo dice [0.92, 0.9524, 0.9521]\n",
      "2025-12-08 15:43:52.686578: Epoch time: 137.95 s\n",
      "2025-12-08 15:43:53.326801: \n",
      "2025-12-08 15:43:53.326801: Epoch 717\n",
      "2025-12-08 15:43:53.342566: Current learning rate: 0.00321\n",
      "2025-12-08 15:46:11.600828: train_loss -0.857\n",
      "2025-12-08 15:46:11.600828: val_loss -0.8657\n",
      "2025-12-08 15:46:11.604832: Pseudo dice [0.9172, 0.9487, 0.9437]\n",
      "2025-12-08 15:46:11.610838: Epoch time: 138.27 s\n",
      "2025-12-08 15:46:12.358357: \n",
      "2025-12-08 15:46:12.358357: Epoch 718\n",
      "2025-12-08 15:46:12.374274: Current learning rate: 0.0032\n",
      "2025-12-08 15:48:30.355407: train_loss -0.8506\n",
      "2025-12-08 15:48:30.357409: val_loss -0.8806\n",
      "2025-12-08 15:48:30.363153: Pseudo dice [0.9254, 0.957, 0.9509]\n",
      "2025-12-08 15:48:30.369159: Epoch time: 138.0 s\n",
      "2025-12-08 15:48:31.030609: \n",
      "2025-12-08 15:48:31.030609: Epoch 719\n",
      "2025-12-08 15:48:31.030609: Current learning rate: 0.00319\n",
      "2025-12-08 15:50:49.069439: train_loss -0.8579\n",
      "2025-12-08 15:50:49.070441: val_loss -0.878\n",
      "2025-12-08 15:50:49.075457: Pseudo dice [0.9242, 0.9577, 0.9509]\n",
      "2025-12-08 15:50:49.076459: Epoch time: 138.04 s\n",
      "2025-12-08 15:50:49.731134: \n",
      "2025-12-08 15:50:49.731134: Epoch 720\n",
      "2025-12-08 15:50:49.737223: Current learning rate: 0.00318\n",
      "2025-12-08 15:53:07.853988: train_loss -0.8553\n",
      "2025-12-08 15:53:07.853988: val_loss -0.8811\n",
      "2025-12-08 15:53:07.858010: Pseudo dice [0.9274, 0.9573, 0.949]\n",
      "2025-12-08 15:53:07.858010: Epoch time: 138.12 s\n",
      "2025-12-08 15:53:08.842216: \n",
      "2025-12-08 15:53:08.842216: Epoch 721\n",
      "2025-12-08 15:53:08.842216: Current learning rate: 0.00317\n",
      "2025-12-08 15:55:26.796894: train_loss -0.8569\n",
      "2025-12-08 15:55:26.796894: val_loss -0.8779\n",
      "2025-12-08 15:55:26.812653: Pseudo dice [0.9265, 0.9556, 0.9524]\n",
      "2025-12-08 15:55:26.812653: Epoch time: 137.96 s\n",
      "2025-12-08 15:55:27.468003: \n",
      "2025-12-08 15:55:27.468003: Epoch 722\n",
      "2025-12-08 15:55:27.468003: Current learning rate: 0.00316\n",
      "2025-12-08 15:57:45.437224: train_loss -0.8562\n",
      "2025-12-08 15:57:45.437224: val_loss -0.8776\n",
      "2025-12-08 15:57:45.437224: Pseudo dice [0.9257, 0.9533, 0.9447]\n",
      "2025-12-08 15:57:45.437224: Epoch time: 137.97 s\n",
      "2025-12-08 15:57:46.092521: \n",
      "2025-12-08 15:57:46.092521: Epoch 723\n",
      "2025-12-08 15:57:46.092521: Current learning rate: 0.00315\n",
      "2025-12-08 16:00:04.237739: train_loss -0.8545\n",
      "2025-12-08 16:00:04.239741: val_loss -0.8695\n",
      "2025-12-08 16:00:04.249504: Pseudo dice [0.9209, 0.9524, 0.9505]\n",
      "2025-12-08 16:00:04.252740: Epoch time: 138.15 s\n",
      "2025-12-08 16:00:05.014384: \n",
      "2025-12-08 16:00:05.016387: Epoch 724\n",
      "2025-12-08 16:00:05.018391: Current learning rate: 0.00314\n",
      "2025-12-08 16:02:23.216543: train_loss -0.8496\n",
      "2025-12-08 16:02:23.216543: val_loss -0.8692\n",
      "2025-12-08 16:02:23.222549: Pseudo dice [0.9173, 0.9505, 0.9507]\n",
      "2025-12-08 16:02:23.228292: Epoch time: 138.2 s\n",
      "2025-12-08 16:02:23.873899: \n",
      "2025-12-08 16:02:23.873899: Epoch 725\n",
      "2025-12-08 16:02:23.889968: Current learning rate: 0.00313\n",
      "2025-12-08 16:04:41.911976: train_loss -0.8565\n",
      "2025-12-08 16:04:41.911976: val_loss -0.8796\n",
      "2025-12-08 16:04:41.921727: Pseudo dice [0.9274, 0.9563, 0.9506]\n",
      "2025-12-08 16:04:41.927736: Epoch time: 138.04 s\n",
      "2025-12-08 16:04:42.577304: \n",
      "2025-12-08 16:04:42.577304: Epoch 726\n",
      "2025-12-08 16:04:42.593094: Current learning rate: 0.00312\n",
      "2025-12-08 16:07:00.827195: train_loss -0.8585\n",
      "2025-12-08 16:07:00.827195: val_loss -0.8738\n",
      "2025-12-08 16:07:00.834531: Pseudo dice [0.921, 0.9533, 0.9515]\n",
      "2025-12-08 16:07:00.840537: Epoch time: 138.25 s\n",
      "2025-12-08 16:07:01.783680: \n",
      "2025-12-08 16:07:01.784683: Epoch 727\n",
      "2025-12-08 16:07:01.790703: Current learning rate: 0.00311\n",
      "2025-12-08 16:09:19.836418: train_loss -0.8551\n",
      "2025-12-08 16:09:19.836418: val_loss -0.8833\n",
      "2025-12-08 16:09:19.842504: Pseudo dice [0.9295, 0.9613, 0.9428]\n",
      "2025-12-08 16:09:19.848091: Epoch time: 138.05 s\n",
      "2025-12-08 16:09:20.548191: \n",
      "2025-12-08 16:09:20.548191: Epoch 728\n",
      "2025-12-08 16:09:20.553205: Current learning rate: 0.0031\n",
      "2025-12-08 16:11:38.705555: train_loss -0.8547\n",
      "2025-12-08 16:11:38.707558: val_loss -0.8867\n",
      "2025-12-08 16:11:38.713565: Pseudo dice [0.933, 0.9584, 0.9507]\n",
      "2025-12-08 16:11:38.717569: Epoch time: 138.17 s\n",
      "2025-12-08 16:11:39.374153: \n",
      "2025-12-08 16:11:39.374153: Epoch 729\n",
      "2025-12-08 16:11:39.374153: Current learning rate: 0.00309\n",
      "2025-12-08 16:13:57.492908: train_loss -0.8544\n",
      "2025-12-08 16:13:57.493910: val_loss -0.8826\n",
      "2025-12-08 16:13:57.498925: Pseudo dice [0.9316, 0.956, 0.9509]\n",
      "2025-12-08 16:13:57.505085: Epoch time: 138.12 s\n",
      "2025-12-08 16:13:58.280923: \n",
      "2025-12-08 16:13:58.280923: Epoch 730\n",
      "2025-12-08 16:13:58.296801: Current learning rate: 0.00308\n",
      "2025-12-08 16:16:16.356356: train_loss -0.8573\n",
      "2025-12-08 16:16:16.356356: val_loss -0.8693\n",
      "2025-12-08 16:16:16.358096: Pseudo dice [0.9209, 0.9515, 0.941]\n",
      "2025-12-08 16:16:16.358096: Epoch time: 138.08 s\n",
      "2025-12-08 16:16:17.030430: \n",
      "2025-12-08 16:16:17.030430: Epoch 731\n",
      "2025-12-08 16:16:17.030430: Current learning rate: 0.00307\n",
      "2025-12-08 16:18:35.155793: train_loss -0.8526\n",
      "2025-12-08 16:18:35.155793: val_loss -0.8743\n",
      "2025-12-08 16:18:35.155793: Pseudo dice [0.9222, 0.9545, 0.9486]\n",
      "2025-12-08 16:18:35.155793: Epoch time: 138.14 s\n",
      "2025-12-08 16:18:35.843461: \n",
      "2025-12-08 16:18:35.843461: Epoch 732\n",
      "2025-12-08 16:18:35.858978: Current learning rate: 0.00306\n",
      "2025-12-08 16:20:54.015244: train_loss -0.854\n",
      "2025-12-08 16:20:54.015244: val_loss -0.8866\n",
      "2025-12-08 16:20:54.035081: Pseudo dice [0.9321, 0.9578, 0.9483]\n",
      "2025-12-08 16:20:54.039448: Epoch time: 138.17 s\n",
      "2025-12-08 16:20:54.686515: \n",
      "2025-12-08 16:20:54.686515: Epoch 733\n",
      "2025-12-08 16:20:54.694112: Current learning rate: 0.00305\n",
      "2025-12-08 16:23:12.841616: train_loss -0.8571\n",
      "2025-12-08 16:23:12.842620: val_loss -0.8839\n",
      "2025-12-08 16:23:12.848621: Pseudo dice [0.9305, 0.9571, 0.9482]\n",
      "2025-12-08 16:23:12.852570: Epoch time: 138.16 s\n",
      "2025-12-08 16:23:13.540438: \n",
      "2025-12-08 16:23:13.540438: Epoch 734\n",
      "2025-12-08 16:23:13.549816: Current learning rate: 0.00304\n",
      "2025-12-08 16:25:31.499775: train_loss -0.8519\n",
      "2025-12-08 16:25:31.499775: val_loss -0.8679\n",
      "2025-12-08 16:25:31.507985: Pseudo dice [0.916, 0.9514, 0.9461]\n",
      "2025-12-08 16:25:31.513629: Epoch time: 137.96 s\n",
      "2025-12-08 16:25:32.156291: \n",
      "2025-12-08 16:25:32.156291: Epoch 735\n",
      "2025-12-08 16:25:32.162977: Current learning rate: 0.00303\n",
      "2025-12-08 16:27:50.219733: train_loss -0.8563\n",
      "2025-12-08 16:27:50.219733: val_loss -0.8811\n",
      "2025-12-08 16:27:50.230608: Pseudo dice [0.9235, 0.9591, 0.9477]\n",
      "2025-12-08 16:27:50.237345: Epoch time: 138.06 s\n",
      "2025-12-08 16:27:50.952289: \n",
      "2025-12-08 16:27:50.968227: Epoch 736\n",
      "2025-12-08 16:27:50.968227: Current learning rate: 0.00302\n",
      "2025-12-08 16:30:09.076736: train_loss -0.851\n",
      "2025-12-08 16:30:09.076736: val_loss -0.8865\n",
      "2025-12-08 16:30:09.086309: Pseudo dice [0.9326, 0.9608, 0.9481]\n",
      "2025-12-08 16:30:09.090828: Epoch time: 138.12 s\n",
      "2025-12-08 16:30:09.749058: \n",
      "2025-12-08 16:30:09.749058: Epoch 737\n",
      "2025-12-08 16:30:09.749058: Current learning rate: 0.00301\n",
      "2025-12-08 16:32:27.873856: train_loss -0.8532\n",
      "2025-12-08 16:32:27.875858: val_loss -0.8699\n",
      "2025-12-08 16:32:27.881864: Pseudo dice [0.9196, 0.9534, 0.9483]\n",
      "2025-12-08 16:32:27.885868: Epoch time: 138.12 s\n",
      "2025-12-08 16:32:28.551021: \n",
      "2025-12-08 16:32:28.551021: Epoch 738\n",
      "2025-12-08 16:32:28.557045: Current learning rate: 0.003\n",
      "2025-12-08 16:34:46.699082: train_loss -0.8559\n",
      "2025-12-08 16:34:46.699082: val_loss -0.8763\n",
      "2025-12-08 16:34:46.706011: Pseudo dice [0.9256, 0.9546, 0.9472]\n",
      "2025-12-08 16:34:46.706011: Epoch time: 138.15 s\n",
      "2025-12-08 16:34:47.515096: \n",
      "2025-12-08 16:34:47.515096: Epoch 739\n",
      "2025-12-08 16:34:47.530977: Current learning rate: 0.00299\n",
      "2025-12-08 16:37:06.613407: train_loss -0.8493\n",
      "2025-12-08 16:37:06.614411: val_loss -0.8827\n",
      "2025-12-08 16:37:06.619438: Pseudo dice [0.9308, 0.9597, 0.9492]\n",
      "2025-12-08 16:37:06.623824: Epoch time: 139.1 s\n",
      "2025-12-08 16:37:07.280059: \n",
      "2025-12-08 16:37:07.280059: Epoch 740\n",
      "2025-12-08 16:37:07.280059: Current learning rate: 0.00297\n",
      "2025-12-08 16:39:25.779861: train_loss -0.8539\n",
      "2025-12-08 16:39:25.779861: val_loss -0.886\n",
      "2025-12-08 16:39:25.779861: Pseudo dice [0.9303, 0.96, 0.9502]\n",
      "2025-12-08 16:39:25.779861: Epoch time: 138.5 s\n",
      "2025-12-08 16:39:26.464347: \n",
      "2025-12-08 16:39:26.465352: Epoch 741\n",
      "2025-12-08 16:39:26.471559: Current learning rate: 0.00296\n",
      "2025-12-08 16:41:45.323546: train_loss -0.8542\n",
      "2025-12-08 16:41:45.323546: val_loss -0.8789\n",
      "2025-12-08 16:41:45.331293: Pseudo dice [0.9276, 0.9578, 0.948]\n",
      "2025-12-08 16:41:45.335297: Epoch time: 138.86 s\n",
      "2025-12-08 16:41:46.171733: \n",
      "2025-12-08 16:41:46.171733: Epoch 742\n",
      "2025-12-08 16:41:46.186994: Current learning rate: 0.00295\n",
      "2025-12-08 16:44:04.825731: train_loss -0.8509\n",
      "2025-12-08 16:44:04.827472: val_loss -0.87\n",
      "2025-12-08 16:44:04.833478: Pseudo dice [0.9225, 0.9566, 0.9429]\n",
      "2025-12-08 16:44:04.839483: Epoch time: 138.65 s\n",
      "2025-12-08 16:44:05.499713: \n",
      "2025-12-08 16:44:05.499713: Epoch 743\n",
      "2025-12-08 16:44:05.499713: Current learning rate: 0.00294\n",
      "2025-12-08 16:46:23.992178: train_loss -0.8496\n",
      "2025-12-08 16:46:23.992178: val_loss -0.8859\n",
      "2025-12-08 16:46:23.998200: Pseudo dice [0.9317, 0.9574, 0.949]\n",
      "2025-12-08 16:46:24.003062: Epoch time: 138.49 s\n",
      "2025-12-08 16:46:24.655903: \n",
      "2025-12-08 16:46:24.655903: Epoch 744\n",
      "2025-12-08 16:46:24.655903: Current learning rate: 0.00293\n",
      "2025-12-08 16:48:42.784578: train_loss -0.8462\n",
      "2025-12-08 16:48:42.784578: val_loss -0.8757\n",
      "2025-12-08 16:48:42.792586: Pseudo dice [0.9253, 0.9542, 0.9501]\n",
      "2025-12-08 16:48:42.798912: Epoch time: 138.13 s\n",
      "2025-12-08 16:48:43.796178: \n",
      "2025-12-08 16:48:43.796178: Epoch 745\n",
      "2025-12-08 16:48:43.812204: Current learning rate: 0.00292\n",
      "2025-12-08 16:51:01.784065: train_loss -0.8556\n",
      "2025-12-08 16:51:01.784065: val_loss -0.8789\n",
      "2025-12-08 16:51:01.790068: Pseudo dice [0.9273, 0.9582, 0.9425]\n",
      "2025-12-08 16:51:01.794395: Epoch time: 137.99 s\n",
      "2025-12-08 16:51:02.467763: \n",
      "2025-12-08 16:51:02.467763: Epoch 746\n",
      "2025-12-08 16:51:02.467763: Current learning rate: 0.00291\n",
      "2025-12-08 16:53:20.542840: train_loss -0.8506\n",
      "2025-12-08 16:53:20.544844: val_loss -0.8683\n",
      "2025-12-08 16:53:20.551853: Pseudo dice [0.9177, 0.9542, 0.9482]\n",
      "2025-12-08 16:53:20.557859: Epoch time: 138.08 s\n",
      "2025-12-08 16:53:21.250190: \n",
      "2025-12-08 16:53:21.250190: Epoch 747\n",
      "2025-12-08 16:53:21.257366: Current learning rate: 0.0029\n",
      "2025-12-08 16:55:39.219191: train_loss -0.8484\n",
      "2025-12-08 16:55:39.219191: val_loss -0.8795\n",
      "2025-12-08 16:55:39.221194: Pseudo dice [0.9267, 0.9576, 0.9473]\n",
      "2025-12-08 16:55:39.229772: Epoch time: 137.98 s\n",
      "2025-12-08 16:55:39.983727: \n",
      "2025-12-08 16:55:39.983727: Epoch 748\n",
      "2025-12-08 16:55:39.994255: Current learning rate: 0.00289\n",
      "2025-12-08 16:57:58.207278: train_loss -0.8521\n",
      "2025-12-08 16:57:58.208280: val_loss -0.8824\n",
      "2025-12-08 16:57:58.213281: Pseudo dice [0.9296, 0.9584, 0.9465]\n",
      "2025-12-08 16:57:58.217702: Epoch time: 138.22 s\n",
      "2025-12-08 16:57:58.926049: \n",
      "2025-12-08 16:57:58.926049: Epoch 749\n",
      "2025-12-08 16:57:58.932053: Current learning rate: 0.00288\n",
      "2025-12-08 17:00:16.935865: train_loss -0.8499\n",
      "2025-12-08 17:00:16.935865: val_loss -0.8687\n",
      "2025-12-08 17:00:16.941366: Pseudo dice [0.9183, 0.9514, 0.9466]\n",
      "2025-12-08 17:00:16.945370: Epoch time: 138.01 s\n",
      "2025-12-08 17:00:17.858153: \n",
      "2025-12-08 17:00:17.858153: Epoch 750\n",
      "2025-12-08 17:00:17.873908: Current learning rate: 0.00287\n",
      "2025-12-08 17:02:36.045734: train_loss -0.8539\n",
      "2025-12-08 17:02:36.045734: val_loss -0.8804\n",
      "2025-12-08 17:02:36.045734: Pseudo dice [0.9276, 0.9529, 0.9543]\n",
      "2025-12-08 17:02:36.045734: Epoch time: 138.19 s\n",
      "2025-12-08 17:02:37.061607: \n",
      "2025-12-08 17:02:37.061607: Epoch 751\n",
      "2025-12-08 17:02:37.061607: Current learning rate: 0.00286\n",
      "2025-12-08 17:04:55.101326: train_loss -0.859\n",
      "2025-12-08 17:04:55.103330: val_loss -0.8748\n",
      "2025-12-08 17:04:55.112329: Pseudo dice [0.9246, 0.9551, 0.9455]\n",
      "2025-12-08 17:04:55.118330: Epoch time: 138.04 s\n",
      "2025-12-08 17:04:55.843242: \n",
      "2025-12-08 17:04:55.843242: Epoch 752\n",
      "2025-12-08 17:04:55.843242: Current learning rate: 0.00285\n",
      "2025-12-08 17:07:14.030074: train_loss -0.8571\n",
      "2025-12-08 17:07:14.031887: val_loss -0.8714\n",
      "2025-12-08 17:07:14.037893: Pseudo dice [0.9231, 0.9548, 0.9437]\n",
      "2025-12-08 17:07:14.043638: Epoch time: 138.19 s\n",
      "2025-12-08 17:07:14.697573: \n",
      "2025-12-08 17:07:14.698585: Epoch 753\n",
      "2025-12-08 17:07:14.703943: Current learning rate: 0.00284\n",
      "2025-12-08 17:09:32.779102: train_loss -0.8483\n",
      "2025-12-08 17:09:32.780103: val_loss -0.8731\n",
      "2025-12-08 17:09:32.788105: Pseudo dice [0.9219, 0.9529, 0.9531]\n",
      "2025-12-08 17:09:32.794106: Epoch time: 138.08 s\n",
      "2025-12-08 17:09:33.592262: \n",
      "2025-12-08 17:09:33.592262: Epoch 754\n",
      "2025-12-08 17:09:33.608342: Current learning rate: 0.00283\n",
      "2025-12-08 17:11:51.722028: train_loss -0.8516\n",
      "2025-12-08 17:11:51.724029: val_loss -0.8864\n",
      "2025-12-08 17:11:51.725770: Pseudo dice [0.9279, 0.9586, 0.9558]\n",
      "2025-12-08 17:11:51.733921: Epoch time: 138.13 s\n",
      "2025-12-08 17:11:52.436433: \n",
      "2025-12-08 17:11:52.436433: Epoch 755\n",
      "2025-12-08 17:11:52.452170: Current learning rate: 0.00282\n",
      "2025-12-08 17:14:10.565205: train_loss -0.8547\n",
      "2025-12-08 17:14:10.565205: val_loss -0.8768\n",
      "2025-12-08 17:14:10.572953: Pseudo dice [0.9228, 0.9575, 0.9461]\n",
      "2025-12-08 17:14:10.576957: Epoch time: 138.13 s\n",
      "2025-12-08 17:14:11.241823: \n",
      "2025-12-08 17:14:11.241823: Epoch 756\n",
      "2025-12-08 17:14:11.247529: Current learning rate: 0.00281\n",
      "2025-12-08 17:16:29.382347: train_loss -0.8536\n",
      "2025-12-08 17:16:29.389982: val_loss -0.869\n",
      "2025-12-08 17:16:29.393926: Pseudo dice [0.9213, 0.9525, 0.9442]\n",
      "2025-12-08 17:16:29.399932: Epoch time: 138.14 s\n",
      "2025-12-08 17:16:30.358339: \n",
      "2025-12-08 17:16:30.358339: Epoch 757\n",
      "2025-12-08 17:16:30.375442: Current learning rate: 0.0028\n",
      "2025-12-08 17:18:48.580821: train_loss -0.847\n",
      "2025-12-08 17:18:48.580821: val_loss -0.8746\n",
      "2025-12-08 17:18:48.589859: Pseudo dice [0.9246, 0.9586, 0.9421]\n",
      "2025-12-08 17:18:48.592870: Epoch time: 138.22 s\n",
      "2025-12-08 17:18:49.248597: \n",
      "2025-12-08 17:18:49.248597: Epoch 758\n",
      "2025-12-08 17:18:49.248597: Current learning rate: 0.00279\n",
      "2025-12-08 17:21:07.275146: train_loss -0.8554\n",
      "2025-12-08 17:21:07.275146: val_loss -0.8774\n",
      "2025-12-08 17:21:07.284902: Pseudo dice [0.9282, 0.9531, 0.9452]\n",
      "2025-12-08 17:21:07.290908: Epoch time: 138.03 s\n",
      "2025-12-08 17:21:07.948934: \n",
      "2025-12-08 17:21:07.948934: Epoch 759\n",
      "2025-12-08 17:21:07.954946: Current learning rate: 0.00278\n",
      "2025-12-08 17:23:26.061362: train_loss -0.8597\n",
      "2025-12-08 17:23:26.061362: val_loss -0.882\n",
      "2025-12-08 17:23:26.070410: Pseudo dice [0.9282, 0.96, 0.9506]\n",
      "2025-12-08 17:23:26.076131: Epoch time: 138.11 s\n",
      "2025-12-08 17:23:26.756088: \n",
      "2025-12-08 17:23:26.756088: Epoch 760\n",
      "2025-12-08 17:23:26.761101: Current learning rate: 0.00277\n",
      "2025-12-08 17:25:44.873870: train_loss -0.8518\n",
      "2025-12-08 17:25:44.874980: val_loss -0.8777\n",
      "2025-12-08 17:25:44.882015: Pseudo dice [0.9292, 0.9554, 0.9409]\n",
      "2025-12-08 17:25:44.888018: Epoch time: 138.12 s\n",
      "2025-12-08 17:25:45.531067: \n",
      "2025-12-08 17:25:45.531067: Epoch 761\n",
      "2025-12-08 17:25:45.543736: Current learning rate: 0.00276\n",
      "2025-12-08 17:28:03.593118: train_loss -0.8519\n",
      "2025-12-08 17:28:03.593118: val_loss -0.8843\n",
      "2025-12-08 17:28:03.608939: Pseudo dice [0.9289, 0.9555, 0.9504]\n",
      "2025-12-08 17:28:03.613745: Epoch time: 138.06 s\n",
      "2025-12-08 17:28:04.325301: \n",
      "2025-12-08 17:28:04.326306: Epoch 762\n",
      "2025-12-08 17:28:04.331353: Current learning rate: 0.00275\n",
      "2025-12-08 17:30:22.344583: train_loss -0.8541\n",
      "2025-12-08 17:30:22.344583: val_loss -0.8805\n",
      "2025-12-08 17:30:22.350589: Pseudo dice [0.9281, 0.9565, 0.9495]\n",
      "2025-12-08 17:30:22.356595: Epoch time: 138.02 s\n",
      "2025-12-08 17:30:23.187927: \n",
      "2025-12-08 17:30:23.187927: Epoch 763\n",
      "2025-12-08 17:30:23.187927: Current learning rate: 0.00274\n",
      "2025-12-08 17:32:41.344105: train_loss -0.8537\n",
      "2025-12-08 17:32:41.345115: val_loss -0.8706\n",
      "2025-12-08 17:32:41.351178: Pseudo dice [0.9182, 0.9544, 0.9444]\n",
      "2025-12-08 17:32:41.355402: Epoch time: 138.16 s\n",
      "2025-12-08 17:32:42.071291: \n",
      "2025-12-08 17:32:42.071291: Epoch 764\n",
      "2025-12-08 17:32:42.077787: Current learning rate: 0.00273\n",
      "2025-12-08 17:35:00.234174: train_loss -0.855\n",
      "2025-12-08 17:35:00.234174: val_loss -0.8775\n",
      "2025-12-08 17:35:00.234174: Pseudo dice [0.9237, 0.953, 0.9516]\n",
      "2025-12-08 17:35:00.249863: Epoch time: 138.16 s\n",
      "2025-12-08 17:35:00.910100: \n",
      "2025-12-08 17:35:00.910100: Epoch 765\n",
      "2025-12-08 17:35:00.916125: Current learning rate: 0.00272\n",
      "2025-12-08 17:37:19.046699: train_loss -0.8531\n",
      "2025-12-08 17:37:19.062497: val_loss -0.8738\n",
      "2025-12-08 17:37:19.062497: Pseudo dice [0.9253, 0.9538, 0.9493]\n",
      "2025-12-08 17:37:19.062497: Epoch time: 138.14 s\n",
      "2025-12-08 17:37:19.717976: \n",
      "2025-12-08 17:37:19.717976: Epoch 766\n",
      "2025-12-08 17:37:19.733917: Current learning rate: 0.00271\n",
      "2025-12-08 17:39:38.108955: train_loss -0.8552\n",
      "2025-12-08 17:39:38.124943: val_loss -0.8774\n",
      "2025-12-08 17:39:38.124943: Pseudo dice [0.9243, 0.9573, 0.9469]\n",
      "2025-12-08 17:39:38.124943: Epoch time: 138.39 s\n",
      "2025-12-08 17:39:38.780824: \n",
      "2025-12-08 17:39:38.780824: Epoch 767\n",
      "2025-12-08 17:39:38.780824: Current learning rate: 0.0027\n",
      "2025-12-08 17:41:57.091994: train_loss -0.8428\n",
      "2025-12-08 17:41:57.092994: val_loss -0.8786\n",
      "2025-12-08 17:41:57.094779: Pseudo dice [0.927, 0.9584, 0.9484]\n",
      "2025-12-08 17:41:57.102768: Epoch time: 138.31 s\n",
      "2025-12-08 17:41:57.780260: \n",
      "2025-12-08 17:41:57.780260: Epoch 768\n",
      "2025-12-08 17:41:57.784266: Current learning rate: 0.00268\n",
      "2025-12-08 17:44:15.907614: train_loss -0.8485\n",
      "2025-12-08 17:44:15.907614: val_loss -0.8759\n",
      "2025-12-08 17:44:15.907614: Pseudo dice [0.9246, 0.9563, 0.9478]\n",
      "2025-12-08 17:44:15.907614: Epoch time: 138.13 s\n",
      "2025-12-08 17:44:16.753181: \n",
      "2025-12-08 17:44:16.754184: Epoch 769\n",
      "2025-12-08 17:44:16.759426: Current learning rate: 0.00267\n",
      "2025-12-08 17:46:34.939950: train_loss -0.8519\n",
      "2025-12-08 17:46:34.940952: val_loss -0.8789\n",
      "2025-12-08 17:46:34.946960: Pseudo dice [0.9276, 0.9562, 0.948]\n",
      "2025-12-08 17:46:34.952224: Epoch time: 138.19 s\n",
      "2025-12-08 17:46:35.608722: \n",
      "2025-12-08 17:46:35.608722: Epoch 770\n",
      "2025-12-08 17:46:35.620910: Current learning rate: 0.00266\n",
      "2025-12-08 17:48:53.624125: train_loss -0.8519\n",
      "2025-12-08 17:48:53.625933: val_loss -0.8875\n",
      "2025-12-08 17:48:53.631137: Pseudo dice [0.9341, 0.9578, 0.9467]\n",
      "2025-12-08 17:48:53.635141: Epoch time: 138.02 s\n",
      "2025-12-08 17:48:54.327237: \n",
      "2025-12-08 17:48:54.329239: Epoch 771\n",
      "2025-12-08 17:48:54.329239: Current learning rate: 0.00265\n",
      "2025-12-08 17:51:12.543224: train_loss -0.8549\n",
      "2025-12-08 17:51:12.543224: val_loss -0.8813\n",
      "2025-12-08 17:51:12.550232: Pseudo dice [0.929, 0.9565, 0.9477]\n",
      "2025-12-08 17:51:12.556238: Epoch time: 138.22 s\n",
      "2025-12-08 17:51:13.215364: \n",
      "2025-12-08 17:51:13.217366: Epoch 772\n",
      "2025-12-08 17:51:13.217366: Current learning rate: 0.00264\n",
      "2025-12-08 17:53:31.240237: train_loss -0.8515\n",
      "2025-12-08 17:53:31.242238: val_loss -0.8825\n",
      "2025-12-08 17:53:31.251465: Pseudo dice [0.9276, 0.9596, 0.9489]\n",
      "2025-12-08 17:53:31.263483: Epoch time: 138.02 s\n",
      "2025-12-08 17:53:31.982326: \n",
      "2025-12-08 17:53:31.982326: Epoch 773\n",
      "2025-12-08 17:53:31.987885: Current learning rate: 0.00263\n",
      "2025-12-08 17:55:50.096360: train_loss -0.8487\n",
      "2025-12-08 17:55:50.096360: val_loss -0.8709\n",
      "2025-12-08 17:55:50.102367: Pseudo dice [0.9218, 0.9517, 0.9475]\n",
      "2025-12-08 17:55:50.110277: Epoch time: 138.12 s\n",
      "2025-12-08 17:55:50.859495: \n",
      "2025-12-08 17:55:50.859495: Epoch 774\n",
      "2025-12-08 17:55:50.875237: Current learning rate: 0.00262\n",
      "2025-12-08 17:58:08.932349: train_loss -0.8512\n",
      "2025-12-08 17:58:08.932349: val_loss -0.8817\n",
      "2025-12-08 17:58:08.936353: Pseudo dice [0.9285, 0.9581, 0.9474]\n",
      "2025-12-08 17:58:08.945596: Epoch time: 138.07 s\n",
      "2025-12-08 17:58:09.811704: \n",
      "2025-12-08 17:58:09.811704: Epoch 775\n",
      "2025-12-08 17:58:09.827582: Current learning rate: 0.00261\n",
      "2025-12-08 18:00:27.850961: train_loss -0.8564\n",
      "2025-12-08 18:00:27.850961: val_loss -0.8749\n",
      "2025-12-08 18:00:27.856967: Pseudo dice [0.9264, 0.9528, 0.9496]\n",
      "2025-12-08 18:00:27.858969: Epoch time: 138.04 s\n",
      "2025-12-08 18:00:28.514806: \n",
      "2025-12-08 18:00:28.514806: Epoch 776\n",
      "2025-12-08 18:00:28.530729: Current learning rate: 0.0026\n",
      "2025-12-08 18:02:46.358360: train_loss -0.8563\n",
      "2025-12-08 18:02:46.358360: val_loss -0.8878\n",
      "2025-12-08 18:02:46.365446: Pseudo dice [0.9332, 0.961, 0.9487]\n",
      "2025-12-08 18:02:46.371452: Epoch time: 137.84 s\n",
      "2025-12-08 18:02:47.124254: \n",
      "2025-12-08 18:02:47.124254: Epoch 777\n",
      "2025-12-08 18:02:47.140063: Current learning rate: 0.00259\n",
      "2025-12-08 18:05:05.179268: train_loss -0.8529\n",
      "2025-12-08 18:05:05.179268: val_loss -0.8797\n",
      "2025-12-08 18:05:05.190872: Pseudo dice [0.9261, 0.9577, 0.9464]\n",
      "2025-12-08 18:05:05.196878: Epoch time: 138.06 s\n",
      "2025-12-08 18:05:05.914452: \n",
      "2025-12-08 18:05:05.915457: Epoch 778\n",
      "2025-12-08 18:05:05.921471: Current learning rate: 0.00258\n",
      "2025-12-08 18:07:23.995018: train_loss -0.8516\n",
      "2025-12-08 18:07:23.995018: val_loss -0.8762\n",
      "2025-12-08 18:07:24.002979: Pseudo dice [0.9255, 0.9571, 0.9467]\n",
      "2025-12-08 18:07:24.006982: Epoch time: 138.08 s\n",
      "2025-12-08 18:07:24.725975: \n",
      "2025-12-08 18:07:24.726977: Epoch 779\n",
      "2025-12-08 18:07:24.732988: Current learning rate: 0.00257\n",
      "2025-12-08 18:09:42.667406: train_loss -0.8552\n",
      "2025-12-08 18:09:42.669408: val_loss -0.879\n",
      "2025-12-08 18:09:42.675205: Pseudo dice [0.9265, 0.9567, 0.9508]\n",
      "2025-12-08 18:09:42.681211: Epoch time: 137.94 s\n",
      "2025-12-08 18:09:43.484574: \n",
      "2025-12-08 18:09:43.484574: Epoch 780\n",
      "2025-12-08 18:09:43.484574: Current learning rate: 0.00256\n",
      "2025-12-08 18:12:01.529047: train_loss -0.8551\n",
      "2025-12-08 18:12:01.529047: val_loss -0.8816\n",
      "2025-12-08 18:12:01.538178: Pseudo dice [0.9271, 0.9566, 0.9495]\n",
      "2025-12-08 18:12:01.544180: Epoch time: 138.04 s\n",
      "2025-12-08 18:12:02.444346: \n",
      "2025-12-08 18:12:02.444346: Epoch 781\n",
      "2025-12-08 18:12:02.444346: Current learning rate: 0.00255\n",
      "2025-12-08 18:14:20.592723: train_loss -0.8545\n",
      "2025-12-08 18:14:20.592723: val_loss -0.8806\n",
      "2025-12-08 18:14:20.615051: Pseudo dice [0.9284, 0.9569, 0.9516]\n",
      "2025-12-08 18:14:20.619055: Epoch time: 138.15 s\n",
      "2025-12-08 18:14:21.334038: \n",
      "2025-12-08 18:14:21.334038: Epoch 782\n",
      "2025-12-08 18:14:21.340299: Current learning rate: 0.00254\n",
      "2025-12-08 18:16:39.378533: train_loss -0.8568\n",
      "2025-12-08 18:16:39.379534: val_loss -0.8792\n",
      "2025-12-08 18:16:39.385535: Pseudo dice [0.926, 0.9567, 0.9525]\n",
      "2025-12-08 18:16:39.390582: Epoch time: 138.05 s\n",
      "2025-12-08 18:16:40.138682: \n",
      "2025-12-08 18:16:40.142182: Epoch 783\n",
      "2025-12-08 18:16:40.142182: Current learning rate: 0.00253\n",
      "2025-12-08 18:18:58.218020: train_loss -0.8569\n",
      "2025-12-08 18:18:58.218020: val_loss -0.8778\n",
      "2025-12-08 18:18:58.218020: Pseudo dice [0.9218, 0.9569, 0.949]\n",
      "2025-12-08 18:18:58.233988: Epoch time: 138.08 s\n",
      "2025-12-08 18:18:58.874449: \n",
      "2025-12-08 18:18:58.874449: Epoch 784\n",
      "2025-12-08 18:18:58.890095: Current learning rate: 0.00252\n",
      "2025-12-08 18:21:18.236622: train_loss -0.8571\n",
      "2025-12-08 18:21:18.237624: val_loss -0.8816\n",
      "2025-12-08 18:21:18.242642: Pseudo dice [0.9253, 0.9548, 0.9522]\n",
      "2025-12-08 18:21:18.247658: Epoch time: 139.36 s\n",
      "2025-12-08 18:21:18.904968: \n",
      "2025-12-08 18:21:18.904968: Epoch 785\n",
      "2025-12-08 18:21:18.904968: Current learning rate: 0.00251\n",
      "2025-12-08 18:23:37.267286: train_loss -0.8613\n",
      "2025-12-08 18:23:37.267286: val_loss -0.8769\n",
      "2025-12-08 18:23:37.277035: Pseudo dice [0.9227, 0.9547, 0.9509]\n",
      "2025-12-08 18:23:37.283042: Epoch time: 138.36 s\n",
      "2025-12-08 18:23:37.958024: \n",
      "2025-12-08 18:23:37.960026: Epoch 786\n",
      "2025-12-08 18:23:37.960026: Current learning rate: 0.0025\n",
      "2025-12-08 18:25:56.529596: train_loss -0.8605\n",
      "2025-12-08 18:25:56.529596: val_loss -0.8749\n",
      "2025-12-08 18:25:56.529596: Pseudo dice [0.9209, 0.9601, 0.9488]\n",
      "2025-12-08 18:25:56.529596: Epoch time: 138.57 s\n",
      "2025-12-08 18:25:57.420424: \n",
      "2025-12-08 18:25:57.420424: Epoch 787\n",
      "2025-12-08 18:25:57.420424: Current learning rate: 0.00249\n",
      "2025-12-08 18:28:15.919500: train_loss -0.8554\n",
      "2025-12-08 18:28:15.921241: val_loss -0.8781\n",
      "2025-12-08 18:28:15.927110: Pseudo dice [0.9269, 0.9575, 0.9466]\n",
      "2025-12-08 18:28:15.931114: Epoch time: 138.5 s\n",
      "2025-12-08 18:28:16.608731: \n",
      "2025-12-08 18:28:16.608731: Epoch 788\n",
      "2025-12-08 18:28:16.608731: Current learning rate: 0.00248\n",
      "2025-12-08 18:30:35.154491: train_loss -0.861\n",
      "2025-12-08 18:30:35.154491: val_loss -0.8818\n",
      "2025-12-08 18:30:35.155493: Pseudo dice [0.9244, 0.9575, 0.9553]\n",
      "2025-12-08 18:30:35.155493: Epoch time: 138.55 s\n",
      "2025-12-08 18:30:35.843200: \n",
      "2025-12-08 18:30:35.858974: Epoch 789\n",
      "2025-12-08 18:30:35.858974: Current learning rate: 0.00247\n",
      "2025-12-08 18:32:53.984088: train_loss -0.8531\n",
      "2025-12-08 18:32:53.984088: val_loss -0.8613\n",
      "2025-12-08 18:32:53.984088: Pseudo dice [0.915, 0.947, 0.9488]\n",
      "2025-12-08 18:32:53.995827: Epoch time: 138.14 s\n",
      "2025-12-08 18:32:54.765583: \n",
      "2025-12-08 18:32:54.765583: Epoch 790\n",
      "2025-12-08 18:32:54.765583: Current learning rate: 0.00245\n",
      "2025-12-08 18:35:12.765426: train_loss -0.8562\n",
      "2025-12-08 18:35:12.765426: val_loss -0.8705\n",
      "2025-12-08 18:35:12.781476: Pseudo dice [0.9206, 0.9497, 0.9482]\n",
      "2025-12-08 18:35:12.781476: Epoch time: 138.02 s\n",
      "2025-12-08 18:35:13.436925: \n",
      "2025-12-08 18:35:13.436925: Epoch 791\n",
      "2025-12-08 18:35:13.436925: Current learning rate: 0.00244\n",
      "2025-12-08 18:37:31.389206: train_loss -0.8547\n",
      "2025-12-08 18:37:31.389206: val_loss -0.8822\n",
      "2025-12-08 18:37:31.389206: Pseudo dice [0.9283, 0.9618, 0.9488]\n",
      "2025-12-08 18:37:31.389206: Epoch time: 137.95 s\n",
      "2025-12-08 18:37:32.045753: \n",
      "2025-12-08 18:37:32.045753: Epoch 792\n",
      "2025-12-08 18:37:32.045753: Current learning rate: 0.00243\n",
      "2025-12-08 18:39:50.186106: train_loss -0.8547\n",
      "2025-12-08 18:39:50.186106: val_loss -0.8846\n",
      "2025-12-08 18:39:50.186106: Pseudo dice [0.9292, 0.9605, 0.9482]\n",
      "2025-12-08 18:39:50.186106: Epoch time: 138.14 s\n",
      "2025-12-08 18:39:51.155434: \n",
      "2025-12-08 18:39:51.155434: Epoch 793\n",
      "2025-12-08 18:39:51.170934: Current learning rate: 0.00242\n",
      "2025-12-08 18:42:09.201605: train_loss -0.8574\n",
      "2025-12-08 18:42:09.201605: val_loss -0.877\n",
      "2025-12-08 18:42:09.214620: Pseudo dice [0.9229, 0.9539, 0.9476]\n",
      "2025-12-08 18:42:09.217625: Epoch time: 138.05 s\n",
      "2025-12-08 18:42:09.885997: \n",
      "2025-12-08 18:42:09.885997: Epoch 794\n",
      "2025-12-08 18:42:09.889933: Current learning rate: 0.00241\n",
      "2025-12-08 18:44:28.041142: train_loss -0.8555\n",
      "2025-12-08 18:44:28.041142: val_loss -0.8762\n",
      "2025-12-08 18:44:28.046873: Pseudo dice [0.9229, 0.9579, 0.9486]\n",
      "2025-12-08 18:44:28.048730: Epoch time: 138.16 s\n",
      "2025-12-08 18:44:28.704955: \n",
      "2025-12-08 18:44:28.704955: Epoch 795\n",
      "2025-12-08 18:44:28.718768: Current learning rate: 0.0024\n",
      "2025-12-08 18:46:46.733986: train_loss -0.8572\n",
      "2025-12-08 18:46:46.733986: val_loss -0.8724\n",
      "2025-12-08 18:46:46.750016: Pseudo dice [0.9203, 0.9551, 0.943]\n",
      "2025-12-08 18:46:46.754516: Epoch time: 138.03 s\n",
      "2025-12-08 18:46:47.562033: \n",
      "2025-12-08 18:46:47.562033: Epoch 796\n",
      "2025-12-08 18:46:47.578003: Current learning rate: 0.00239\n",
      "2025-12-08 18:49:05.655196: train_loss -0.853\n",
      "2025-12-08 18:49:05.655196: val_loss -0.8818\n",
      "2025-12-08 18:49:05.671305: Pseudo dice [0.931, 0.9586, 0.9485]\n",
      "2025-12-08 18:49:05.671305: Epoch time: 138.09 s\n",
      "2025-12-08 18:49:06.327442: \n",
      "2025-12-08 18:49:06.327442: Epoch 797\n",
      "2025-12-08 18:49:06.327442: Current learning rate: 0.00238\n",
      "2025-12-08 18:51:24.657383: train_loss -0.86\n",
      "2025-12-08 18:51:24.658382: val_loss -0.8785\n",
      "2025-12-08 18:51:24.668852: Pseudo dice [0.9265, 0.9549, 0.9475]\n",
      "2025-12-08 18:51:24.674781: Epoch time: 138.33 s\n",
      "2025-12-08 18:51:25.342435: \n",
      "2025-12-08 18:51:25.342435: Epoch 798\n",
      "2025-12-08 18:51:25.342435: Current learning rate: 0.00237\n",
      "2025-12-08 18:53:43.420893: train_loss -0.8549\n",
      "2025-12-08 18:53:43.420893: val_loss -0.8881\n",
      "2025-12-08 18:53:43.420893: Pseudo dice [0.9343, 0.9605, 0.9461]\n",
      "2025-12-08 18:53:43.432140: Epoch time: 138.09 s\n",
      "2025-12-08 18:53:44.406447: \n",
      "2025-12-08 18:53:44.406447: Epoch 799\n",
      "2025-12-08 18:53:44.413548: Current learning rate: 0.00236\n",
      "2025-12-08 18:56:02.453576: train_loss -0.8515\n",
      "2025-12-08 18:56:02.453576: val_loss -0.8693\n",
      "2025-12-08 18:56:02.453576: Pseudo dice [0.9198, 0.9544, 0.9476]\n",
      "2025-12-08 18:56:02.469539: Epoch time: 138.05 s\n",
      "2025-12-08 18:56:03.411101: \n",
      "2025-12-08 18:56:03.411101: Epoch 800\n",
      "2025-12-08 18:56:03.411101: Current learning rate: 0.00235\n",
      "2025-12-08 18:58:21.290149: train_loss -0.8578\n",
      "2025-12-08 18:58:21.292151: val_loss -0.8794\n",
      "2025-12-08 18:58:21.297896: Pseudo dice [0.9241, 0.9607, 0.9532]\n",
      "2025-12-08 18:58:21.303901: Epoch time: 137.88 s\n",
      "2025-12-08 18:58:21.967714: \n",
      "2025-12-08 18:58:21.967714: Epoch 801\n",
      "2025-12-08 18:58:21.967714: Current learning rate: 0.00234\n",
      "2025-12-08 19:00:39.870021: train_loss -0.8628\n",
      "2025-12-08 19:00:39.872024: val_loss -0.8782\n",
      "2025-12-08 19:00:39.873766: Pseudo dice [0.9262, 0.9566, 0.9464]\n",
      "2025-12-08 19:00:39.873766: Epoch time: 137.9 s\n",
      "2025-12-08 19:00:40.593046: \n",
      "2025-12-08 19:00:40.593046: Epoch 802\n",
      "2025-12-08 19:00:40.600563: Current learning rate: 0.00233\n",
      "2025-12-08 19:02:58.573763: train_loss -0.8585\n",
      "2025-12-08 19:02:58.573763: val_loss -0.8741\n",
      "2025-12-08 19:02:58.581004: Pseudo dice [0.9217, 0.952, 0.9528]\n",
      "2025-12-08 19:02:58.585966: Epoch time: 138.0 s\n",
      "2025-12-08 19:02:59.311864: \n",
      "2025-12-08 19:02:59.327599: Epoch 803\n",
      "2025-12-08 19:02:59.331664: Current learning rate: 0.00232\n",
      "2025-12-08 19:05:17.248889: train_loss -0.8562\n",
      "2025-12-08 19:05:17.248889: val_loss -0.8736\n",
      "2025-12-08 19:05:17.268758: Pseudo dice [0.9231, 0.955, 0.9449]\n",
      "2025-12-08 19:05:17.272762: Epoch time: 137.94 s\n",
      "2025-12-08 19:05:17.986116: \n",
      "2025-12-08 19:05:17.986116: Epoch 804\n",
      "2025-12-08 19:05:17.992145: Current learning rate: 0.00231\n",
      "2025-12-08 19:07:35.967156: train_loss -0.8542\n",
      "2025-12-08 19:07:35.967156: val_loss -0.8822\n",
      "2025-12-08 19:07:35.982802: Pseudo dice [0.9265, 0.9583, 0.9514]\n",
      "2025-12-08 19:07:35.982802: Epoch time: 137.98 s\n",
      "2025-12-08 19:07:36.835065: \n",
      "2025-12-08 19:07:36.835065: Epoch 805\n",
      "2025-12-08 19:07:36.841093: Current learning rate: 0.0023\n",
      "2025-12-08 19:09:54.837471: train_loss -0.8581\n",
      "2025-12-08 19:09:54.839473: val_loss -0.8812\n",
      "2025-12-08 19:09:54.851233: Pseudo dice [0.9281, 0.9591, 0.9506]\n",
      "2025-12-08 19:09:54.860997: Epoch time: 138.0 s\n",
      "2025-12-08 19:09:55.515122: \n",
      "2025-12-08 19:09:55.515122: Epoch 806\n",
      "2025-12-08 19:09:55.533233: Current learning rate: 0.00229\n",
      "2025-12-08 19:12:13.488602: train_loss -0.855\n",
      "2025-12-08 19:12:13.489602: val_loss -0.887\n",
      "2025-12-08 19:12:13.498604: Pseudo dice [0.9313, 0.9601, 0.9503]\n",
      "2025-12-08 19:12:13.504680: Epoch time: 137.97 s\n",
      "2025-12-08 19:12:14.233557: \n",
      "2025-12-08 19:12:14.233557: Epoch 807\n",
      "2025-12-08 19:12:14.233557: Current learning rate: 0.00228\n",
      "2025-12-08 19:14:32.229224: train_loss -0.8567\n",
      "2025-12-08 19:14:32.229224: val_loss -0.8858\n",
      "2025-12-08 19:14:32.236920: Pseudo dice [0.9301, 0.9575, 0.9474]\n",
      "2025-12-08 19:14:32.241268: Epoch time: 138.01 s\n",
      "2025-12-08 19:14:32.901841: \n",
      "2025-12-08 19:14:32.901841: Epoch 808\n",
      "2025-12-08 19:14:32.904853: Current learning rate: 0.00226\n",
      "2025-12-08 19:16:50.946065: train_loss -0.8552\n",
      "2025-12-08 19:16:50.948067: val_loss -0.8841\n",
      "2025-12-08 19:16:50.956075: Pseudo dice [0.9271, 0.9575, 0.9506]\n",
      "2025-12-08 19:16:50.962081: Epoch time: 138.05 s\n",
      "2025-12-08 19:16:51.639742: \n",
      "2025-12-08 19:16:51.639742: Epoch 809\n",
      "2025-12-08 19:16:51.639742: Current learning rate: 0.00225\n",
      "2025-12-08 19:19:09.685284: train_loss -0.8575\n",
      "2025-12-08 19:19:09.686284: val_loss -0.8829\n",
      "2025-12-08 19:19:09.691285: Pseudo dice [0.9277, 0.957, 0.9508]\n",
      "2025-12-08 19:19:09.697287: Epoch time: 138.05 s\n",
      "2025-12-08 19:19:10.577804: \n",
      "2025-12-08 19:19:10.577804: Epoch 810\n",
      "2025-12-08 19:19:10.577804: Current learning rate: 0.00224\n",
      "2025-12-08 19:21:28.538909: train_loss -0.8557\n",
      "2025-12-08 19:21:28.538909: val_loss -0.8815\n",
      "2025-12-08 19:21:28.544916: Pseudo dice [0.9237, 0.9586, 0.9543]\n",
      "2025-12-08 19:21:28.550661: Epoch time: 137.96 s\n",
      "2025-12-08 19:21:29.296805: \n",
      "2025-12-08 19:21:29.296805: Epoch 811\n",
      "2025-12-08 19:21:29.312562: Current learning rate: 0.00223\n",
      "2025-12-08 19:23:47.386775: train_loss -0.855\n",
      "2025-12-08 19:23:47.388777: val_loss -0.8829\n",
      "2025-12-08 19:23:47.395020: Pseudo dice [0.9265, 0.9589, 0.9522]\n",
      "2025-12-08 19:23:47.401026: Epoch time: 138.09 s\n",
      "2025-12-08 19:23:48.072648: \n",
      "2025-12-08 19:23:48.072648: Epoch 812\n",
      "2025-12-08 19:23:48.078875: Current learning rate: 0.00222\n",
      "2025-12-08 19:26:06.366960: train_loss -0.8575\n",
      "2025-12-08 19:26:06.368962: val_loss -0.8844\n",
      "2025-12-08 19:26:06.378475: Pseudo dice [0.9312, 0.9599, 0.9504]\n",
      "2025-12-08 19:26:06.384481: Epoch time: 138.3 s\n",
      "2025-12-08 19:26:07.046837: \n",
      "2025-12-08 19:26:07.046837: Epoch 813\n",
      "2025-12-08 19:26:07.046837: Current learning rate: 0.00221\n",
      "2025-12-08 19:28:25.103093: train_loss -0.8618\n",
      "2025-12-08 19:28:25.105095: val_loss -0.8786\n",
      "2025-12-08 19:28:25.111225: Pseudo dice [0.924, 0.9558, 0.9501]\n",
      "2025-12-08 19:28:25.116226: Epoch time: 138.06 s\n",
      "2025-12-08 19:28:25.780141: \n",
      "2025-12-08 19:28:25.780141: Epoch 814\n",
      "2025-12-08 19:28:25.780141: Current learning rate: 0.0022\n",
      "2025-12-08 19:30:43.970251: train_loss -0.8568\n",
      "2025-12-08 19:30:43.970251: val_loss -0.8811\n",
      "2025-12-08 19:30:43.983253: Pseudo dice [0.9311, 0.9547, 0.9478]\n",
      "2025-12-08 19:30:43.989281: Epoch time: 138.19 s\n",
      "2025-12-08 19:30:44.717874: \n",
      "2025-12-08 19:30:44.717874: Epoch 815\n",
      "2025-12-08 19:30:44.723909: Current learning rate: 0.00219\n",
      "2025-12-08 19:33:02.874304: train_loss -0.8612\n",
      "2025-12-08 19:33:02.890149: val_loss -0.8867\n",
      "2025-12-08 19:33:02.893602: Pseudo dice [0.9312, 0.9588, 0.9503]\n",
      "2025-12-08 19:33:02.893602: Epoch time: 138.16 s\n",
      "2025-12-08 19:33:03.732724: \n",
      "2025-12-08 19:33:03.732724: Epoch 816\n",
      "2025-12-08 19:33:03.732724: Current learning rate: 0.00218\n",
      "2025-12-08 19:35:21.878896: train_loss -0.8551\n",
      "2025-12-08 19:35:21.878896: val_loss -0.8781\n",
      "2025-12-08 19:35:21.886904: Pseudo dice [0.9248, 0.9566, 0.9486]\n",
      "2025-12-08 19:35:21.894794: Epoch time: 138.15 s\n",
      "2025-12-08 19:35:22.631432: \n",
      "2025-12-08 19:35:22.631432: Epoch 817\n",
      "2025-12-08 19:35:22.638453: Current learning rate: 0.00217\n",
      "2025-12-08 19:37:40.792414: train_loss -0.8566\n",
      "2025-12-08 19:37:40.792414: val_loss -0.8795\n",
      "2025-12-08 19:37:40.799288: Pseudo dice [0.926, 0.9564, 0.9542]\n",
      "2025-12-08 19:37:40.801291: Epoch time: 138.16 s\n",
      "2025-12-08 19:37:41.476904: \n",
      "2025-12-08 19:37:41.476904: Epoch 818\n",
      "2025-12-08 19:37:41.482933: Current learning rate: 0.00216\n",
      "2025-12-08 19:39:59.741018: train_loss -0.8564\n",
      "2025-12-08 19:39:59.741018: val_loss -0.8831\n",
      "2025-12-08 19:39:59.747183: Pseudo dice [0.9274, 0.9598, 0.95]\n",
      "2025-12-08 19:39:59.753190: Epoch time: 138.27 s\n",
      "2025-12-08 19:40:00.508635: \n",
      "2025-12-08 19:40:00.509640: Epoch 819\n",
      "2025-12-08 19:40:00.514655: Current learning rate: 0.00215\n",
      "2025-12-08 19:42:18.545534: train_loss -0.8555\n",
      "2025-12-08 19:42:18.545534: val_loss -0.8755\n",
      "2025-12-08 19:42:18.564286: Pseudo dice [0.9208, 0.9538, 0.9517]\n",
      "2025-12-08 19:42:18.570292: Epoch time: 138.04 s\n",
      "2025-12-08 19:42:19.212644: \n",
      "2025-12-08 19:42:19.212644: Epoch 820\n",
      "2025-12-08 19:42:19.218464: Current learning rate: 0.00214\n",
      "2025-12-08 19:44:37.373867: train_loss -0.8565\n",
      "2025-12-08 19:44:37.375871: val_loss -0.8765\n",
      "2025-12-08 19:44:37.385870: Pseudo dice [0.9228, 0.9559, 0.9527]\n",
      "2025-12-08 19:44:37.395963: Epoch time: 138.16 s\n",
      "2025-12-08 19:44:38.015818: \n",
      "2025-12-08 19:44:38.015818: Epoch 821\n",
      "2025-12-08 19:44:38.031682: Current learning rate: 0.00213\n",
      "2025-12-08 19:46:56.190500: train_loss -0.8566\n",
      "2025-12-08 19:46:56.190500: val_loss -0.8667\n",
      "2025-12-08 19:46:56.198762: Pseudo dice [0.919, 0.9488, 0.9483]\n",
      "2025-12-08 19:46:56.202494: Epoch time: 138.17 s\n",
      "2025-12-08 19:46:57.154943: \n",
      "2025-12-08 19:46:57.154943: Epoch 822\n",
      "2025-12-08 19:46:57.154943: Current learning rate: 0.00212\n",
      "2025-12-08 19:49:15.203988: train_loss -0.8596\n",
      "2025-12-08 19:49:15.205991: val_loss -0.8825\n",
      "2025-12-08 19:49:15.214209: Pseudo dice [0.9243, 0.9589, 0.9553]\n",
      "2025-12-08 19:49:15.219954: Epoch time: 138.05 s\n",
      "2025-12-08 19:49:15.858793: \n",
      "2025-12-08 19:49:15.858793: Epoch 823\n",
      "2025-12-08 19:49:15.874435: Current learning rate: 0.0021\n",
      "2025-12-08 19:51:34.046142: train_loss -0.8593\n",
      "2025-12-08 19:51:34.046142: val_loss -0.8871\n",
      "2025-12-08 19:51:34.065470: Pseudo dice [0.9309, 0.9607, 0.9501]\n",
      "2025-12-08 19:51:34.069469: Epoch time: 138.19 s\n",
      "2025-12-08 19:51:34.686307: \n",
      "2025-12-08 19:51:34.686307: Epoch 824\n",
      "2025-12-08 19:51:34.702253: Current learning rate: 0.00209\n",
      "2025-12-08 19:53:52.631636: train_loss -0.8592\n",
      "2025-12-08 19:53:52.631636: val_loss -0.8909\n",
      "2025-12-08 19:53:52.637643: Pseudo dice [0.932, 0.9642, 0.9519]\n",
      "2025-12-08 19:53:52.643388: Epoch time: 137.95 s\n",
      "2025-12-08 19:53:52.649393: Yayy! New best EMA pseudo Dice: 0.945\n",
      "2025-12-08 19:53:53.660229: \n",
      "2025-12-08 19:53:53.661233: Epoch 825\n",
      "2025-12-08 19:53:53.667261: Current learning rate: 0.00208\n",
      "2025-12-08 19:56:11.577435: train_loss -0.8591\n",
      "2025-12-08 19:56:11.577435: val_loss -0.8753\n",
      "2025-12-08 19:56:11.583827: Pseudo dice [0.9256, 0.9542, 0.9467]\n",
      "2025-12-08 19:56:11.589834: Epoch time: 137.92 s\n",
      "2025-12-08 19:56:12.217752: \n",
      "2025-12-08 19:56:12.217752: Epoch 826\n",
      "2025-12-08 19:56:12.233420: Current learning rate: 0.00207\n",
      "2025-12-08 19:58:30.307872: train_loss -0.8563\n",
      "2025-12-08 19:58:30.309875: val_loss -0.8838\n",
      "2025-12-08 19:58:30.317624: Pseudo dice [0.9291, 0.9575, 0.9549]\n",
      "2025-12-08 19:58:30.323368: Epoch time: 138.09 s\n",
      "2025-12-08 19:58:31.019893: \n",
      "2025-12-08 19:58:31.019893: Epoch 827\n",
      "2025-12-08 19:58:31.026908: Current learning rate: 0.00206\n",
      "2025-12-08 20:00:49.189579: train_loss -0.8601\n",
      "2025-12-08 20:00:49.189579: val_loss -0.878\n",
      "2025-12-08 20:00:49.197075: Pseudo dice [0.9276, 0.9548, 0.9442]\n",
      "2025-12-08 20:00:49.201079: Epoch time: 138.17 s\n",
      "2025-12-08 20:00:49.920916: \n",
      "2025-12-08 20:00:49.920916: Epoch 828\n",
      "2025-12-08 20:00:49.920916: Current learning rate: 0.00205\n",
      "2025-12-08 20:03:08.014956: train_loss -0.858\n",
      "2025-12-08 20:03:08.014956: val_loss -0.8756\n",
      "2025-12-08 20:03:08.014956: Pseudo dice [0.9222, 0.9536, 0.9557]\n",
      "2025-12-08 20:03:08.030941: Epoch time: 138.09 s\n",
      "2025-12-08 20:03:08.844064: \n",
      "2025-12-08 20:03:08.844064: Epoch 829\n",
      "2025-12-08 20:03:08.844064: Current learning rate: 0.00204\n",
      "2025-12-08 20:05:26.963784: train_loss -0.8537\n",
      "2025-12-08 20:05:26.965786: val_loss -0.8708\n",
      "2025-12-08 20:05:26.971930: Pseudo dice [0.9202, 0.951, 0.9517]\n",
      "2025-12-08 20:05:26.976949: Epoch time: 138.12 s\n",
      "2025-12-08 20:05:27.670239: \n",
      "2025-12-08 20:05:27.670239: Epoch 830\n",
      "2025-12-08 20:05:27.685898: Current learning rate: 0.00203\n",
      "2025-12-08 20:07:45.759203: train_loss -0.8546\n",
      "2025-12-08 20:07:45.761205: val_loss -0.8773\n",
      "2025-12-08 20:07:45.766949: Pseudo dice [0.9256, 0.9574, 0.9459]\n",
      "2025-12-08 20:07:45.766949: Epoch time: 138.09 s\n",
      "2025-12-08 20:07:46.471664: \n",
      "2025-12-08 20:07:46.471664: Epoch 831\n",
      "2025-12-08 20:07:46.478684: Current learning rate: 0.00202\n",
      "2025-12-08 20:10:04.654881: train_loss -0.8578\n",
      "2025-12-08 20:10:04.654881: val_loss -0.8734\n",
      "2025-12-08 20:10:04.654881: Pseudo dice [0.9214, 0.9533, 0.9481]\n",
      "2025-12-08 20:10:04.670535: Epoch time: 138.18 s\n",
      "2025-12-08 20:10:05.295662: \n",
      "2025-12-08 20:10:05.295662: Epoch 832\n",
      "2025-12-08 20:10:05.295662: Current learning rate: 0.00201\n",
      "2025-12-08 20:12:23.362376: train_loss -0.8535\n",
      "2025-12-08 20:12:23.362376: val_loss -0.8737\n",
      "2025-12-08 20:12:23.364377: Pseudo dice [0.9232, 0.9536, 0.947]\n",
      "2025-12-08 20:12:23.374270: Epoch time: 138.07 s\n",
      "2025-12-08 20:12:24.014679: \n",
      "2025-12-08 20:12:24.014679: Epoch 833\n",
      "2025-12-08 20:12:24.014679: Current learning rate: 0.002\n",
      "2025-12-08 20:14:42.233750: train_loss -0.8587\n",
      "2025-12-08 20:14:42.233750: val_loss -0.8798\n",
      "2025-12-08 20:14:42.239757: Pseudo dice [0.9266, 0.9543, 0.9503]\n",
      "2025-12-08 20:14:42.245763: Epoch time: 138.22 s\n",
      "2025-12-08 20:14:42.883367: \n",
      "2025-12-08 20:14:42.885369: Epoch 834\n",
      "2025-12-08 20:14:42.889818: Current learning rate: 0.00199\n",
      "2025-12-08 20:17:01.014964: train_loss -0.8632\n",
      "2025-12-08 20:17:01.014964: val_loss -0.8852\n",
      "2025-12-08 20:17:01.035213: Pseudo dice [0.9287, 0.9602, 0.9501]\n",
      "2025-12-08 20:17:01.040236: Epoch time: 138.13 s\n",
      "2025-12-08 20:17:01.826707: \n",
      "2025-12-08 20:17:01.826707: Epoch 835\n",
      "2025-12-08 20:17:01.842831: Current learning rate: 0.00198\n",
      "2025-12-08 20:19:20.014034: train_loss -0.8602\n",
      "2025-12-08 20:19:20.014034: val_loss -0.8887\n",
      "2025-12-08 20:19:20.014034: Pseudo dice [0.9332, 0.9615, 0.9526]\n",
      "2025-12-08 20:19:20.014034: Epoch time: 138.19 s\n",
      "2025-12-08 20:19:20.718606: \n",
      "2025-12-08 20:19:20.718606: Epoch 836\n",
      "2025-12-08 20:19:20.718606: Current learning rate: 0.00196\n",
      "2025-12-08 20:21:38.971270: train_loss -0.8625\n",
      "2025-12-08 20:21:38.971270: val_loss -0.8737\n",
      "2025-12-08 20:21:38.980756: Pseudo dice [0.9199, 0.9551, 0.95]\n",
      "2025-12-08 20:21:38.986763: Epoch time: 138.25 s\n",
      "2025-12-08 20:21:39.686005: \n",
      "2025-12-08 20:21:39.686005: Epoch 837\n",
      "2025-12-08 20:21:39.686005: Current learning rate: 0.00195\n",
      "2025-12-08 20:23:57.736133: train_loss -0.8513\n",
      "2025-12-08 20:23:57.736133: val_loss -0.8845\n",
      "2025-12-08 20:23:57.736133: Pseudo dice [0.9287, 0.9604, 0.9523]\n",
      "2025-12-08 20:23:57.751778: Epoch time: 138.05 s\n",
      "2025-12-08 20:23:58.467860: \n",
      "2025-12-08 20:23:58.467860: Epoch 838\n",
      "2025-12-08 20:23:58.467860: Current learning rate: 0.00194\n",
      "2025-12-08 20:26:16.561401: train_loss -0.8596\n",
      "2025-12-08 20:26:16.561401: val_loss -0.8673\n",
      "2025-12-08 20:26:16.561401: Pseudo dice [0.9161, 0.9475, 0.9477]\n",
      "2025-12-08 20:26:16.561401: Epoch time: 138.09 s\n",
      "2025-12-08 20:26:17.217859: \n",
      "2025-12-08 20:26:17.217859: Epoch 839\n",
      "2025-12-08 20:26:17.217859: Current learning rate: 0.00193\n",
      "2025-12-08 20:28:35.380162: train_loss -0.8555\n",
      "2025-12-08 20:28:35.381164: val_loss -0.8805\n",
      "2025-12-08 20:28:35.387178: Pseudo dice [0.9263, 0.9524, 0.9493]\n",
      "2025-12-08 20:28:35.389186: Epoch time: 138.16 s\n",
      "2025-12-08 20:28:36.077481: \n",
      "2025-12-08 20:28:36.077481: Epoch 840\n",
      "2025-12-08 20:28:36.093191: Current learning rate: 0.00192\n",
      "2025-12-08 20:30:54.314471: train_loss -0.8562\n",
      "2025-12-08 20:30:54.314471: val_loss -0.8889\n",
      "2025-12-08 20:30:54.321669: Pseudo dice [0.933, 0.9555, 0.955]\n",
      "2025-12-08 20:30:54.327664: Epoch time: 138.24 s\n",
      "2025-12-08 20:30:55.124665: \n",
      "2025-12-08 20:30:55.124665: Epoch 841\n",
      "2025-12-08 20:30:55.124665: Current learning rate: 0.00191\n",
      "2025-12-08 20:33:13.278963: train_loss -0.8593\n",
      "2025-12-08 20:33:13.280703: val_loss -0.8821\n",
      "2025-12-08 20:33:13.286712: Pseudo dice [0.9275, 0.9558, 0.951]\n",
      "2025-12-08 20:33:13.294420: Epoch time: 138.15 s\n",
      "2025-12-08 20:33:13.984424: \n",
      "2025-12-08 20:33:13.984424: Epoch 842\n",
      "2025-12-08 20:33:13.984424: Current learning rate: 0.0019\n",
      "2025-12-08 20:35:32.043719: train_loss -0.8603\n",
      "2025-12-08 20:35:32.043719: val_loss -0.8814\n",
      "2025-12-08 20:35:32.053228: Pseudo dice [0.9243, 0.9537, 0.9521]\n",
      "2025-12-08 20:35:32.060974: Epoch time: 138.06 s\n",
      "2025-12-08 20:35:32.701906: \n",
      "2025-12-08 20:35:32.701906: Epoch 843\n",
      "2025-12-08 20:35:32.701906: Current learning rate: 0.00189\n",
      "2025-12-08 20:37:50.873913: train_loss -0.858\n",
      "2025-12-08 20:37:50.873913: val_loss -0.884\n",
      "2025-12-08 20:37:50.881185: Pseudo dice [0.9272, 0.9554, 0.9561]\n",
      "2025-12-08 20:37:50.887192: Epoch time: 138.19 s\n",
      "2025-12-08 20:37:51.700486: \n",
      "2025-12-08 20:37:51.700486: Epoch 844\n",
      "2025-12-08 20:37:51.702228: Current learning rate: 0.00188\n",
      "2025-12-08 20:40:09.827553: train_loss -0.8563\n",
      "2025-12-08 20:40:09.827553: val_loss -0.8759\n",
      "2025-12-08 20:40:09.845301: Pseudo dice [0.9238, 0.9503, 0.953]\n",
      "2025-12-08 20:40:09.845301: Epoch time: 138.13 s\n",
      "2025-12-08 20:40:10.484773: \n",
      "2025-12-08 20:40:10.484773: Epoch 845\n",
      "2025-12-08 20:40:10.484773: Current learning rate: 0.00187\n",
      "2025-12-08 20:42:28.462438: train_loss -0.8628\n",
      "2025-12-08 20:42:28.462438: val_loss -0.8854\n",
      "2025-12-08 20:42:28.470188: Pseudo dice [0.93, 0.9579, 0.948]\n",
      "2025-12-08 20:42:28.478196: Epoch time: 137.98 s\n",
      "2025-12-08 20:42:29.170966: \n",
      "2025-12-08 20:42:29.170966: Epoch 846\n",
      "2025-12-08 20:42:29.170966: Current learning rate: 0.00186\n",
      "2025-12-08 20:44:47.204086: train_loss -0.861\n",
      "2025-12-08 20:44:47.204086: val_loss -0.8729\n",
      "2025-12-08 20:44:47.211810: Pseudo dice [0.9201, 0.955, 0.9525]\n",
      "2025-12-08 20:44:47.217554: Epoch time: 138.03 s\n",
      "2025-12-08 20:44:48.076614: \n",
      "2025-12-08 20:44:48.092267: Epoch 847\n",
      "2025-12-08 20:44:48.092267: Current learning rate: 0.00185\n",
      "2025-12-08 20:47:05.996790: train_loss -0.8612\n",
      "2025-12-08 20:47:05.996790: val_loss -0.891\n",
      "2025-12-08 20:47:06.000708: Pseudo dice [0.9325, 0.9597, 0.9503]\n",
      "2025-12-08 20:47:06.008218: Epoch time: 137.92 s\n",
      "2025-12-08 20:47:06.763971: \n",
      "2025-12-08 20:47:06.763971: Epoch 848\n",
      "2025-12-08 20:47:06.783162: Current learning rate: 0.00184\n",
      "2025-12-08 20:49:24.874495: train_loss -0.8621\n",
      "2025-12-08 20:49:24.874495: val_loss -0.8761\n",
      "2025-12-08 20:49:24.874495: Pseudo dice [0.9235, 0.9538, 0.9484]\n",
      "2025-12-08 20:49:24.890402: Epoch time: 138.11 s\n",
      "2025-12-08 20:49:25.514901: \n",
      "2025-12-08 20:49:25.514901: Epoch 849\n",
      "2025-12-08 20:49:25.514901: Current learning rate: 0.00182\n",
      "2025-12-08 20:51:43.609506: train_loss -0.8558\n",
      "2025-12-08 20:51:43.610506: val_loss -0.8798\n",
      "2025-12-08 20:51:43.616956: Pseudo dice [0.924, 0.9554, 0.9568]\n",
      "2025-12-08 20:51:43.622957: Epoch time: 138.09 s\n",
      "2025-12-08 20:51:44.528773: \n",
      "2025-12-08 20:51:44.528773: Epoch 850\n",
      "2025-12-08 20:51:44.529777: Current learning rate: 0.00181\n",
      "2025-12-08 20:54:02.690138: train_loss -0.8524\n",
      "2025-12-08 20:54:02.692140: val_loss -0.8446\n",
      "2025-12-08 20:54:02.692140: Pseudo dice [0.9128, 0.9415, 0.9414]\n",
      "2025-12-08 20:54:02.705089: Epoch time: 138.16 s\n",
      "2025-12-08 20:54:03.436871: \n",
      "2025-12-08 20:54:03.436871: Epoch 851\n",
      "2025-12-08 20:54:03.436871: Current learning rate: 0.0018\n",
      "2025-12-08 20:56:21.548438: train_loss -0.8302\n",
      "2025-12-08 20:56:21.548438: val_loss -0.8582\n",
      "2025-12-08 20:56:21.554444: Pseudo dice [0.9151, 0.9502, 0.9469]\n",
      "2025-12-08 20:56:21.560450: Epoch time: 138.11 s\n",
      "2025-12-08 20:56:22.200755: \n",
      "2025-12-08 20:56:22.201755: Epoch 852\n",
      "2025-12-08 20:56:22.203681: Current learning rate: 0.00179\n",
      "2025-12-08 20:58:41.119598: train_loss -0.8453\n",
      "2025-12-08 20:58:41.119598: val_loss -0.886\n",
      "2025-12-08 20:58:41.129493: Pseudo dice [0.9305, 0.9579, 0.9532]\n",
      "2025-12-08 20:58:41.138668: Epoch time: 138.92 s\n",
      "2025-12-08 20:58:42.050856: \n",
      "2025-12-08 20:58:42.050856: Epoch 853\n",
      "2025-12-08 20:58:42.063916: Current learning rate: 0.00178\n",
      "2025-12-08 21:01:01.642681: train_loss -0.848\n",
      "2025-12-08 21:01:01.643681: val_loss -0.8755\n",
      "2025-12-08 21:01:01.648371: Pseudo dice [0.9246, 0.957, 0.9475]\n",
      "2025-12-08 21:01:01.654559: Epoch time: 139.59 s\n",
      "2025-12-08 21:01:02.292306: \n",
      "2025-12-08 21:01:02.292306: Epoch 854\n",
      "2025-12-08 21:01:02.306144: Current learning rate: 0.00177\n",
      "2025-12-08 21:03:20.985643: train_loss -0.8533\n",
      "2025-12-08 21:03:20.987646: val_loss -0.8835\n",
      "2025-12-08 21:03:20.995830: Pseudo dice [0.9276, 0.9543, 0.9545]\n",
      "2025-12-08 21:03:21.003596: Epoch time: 138.69 s\n",
      "2025-12-08 21:03:21.784941: \n",
      "2025-12-08 21:03:21.784941: Epoch 855\n",
      "2025-12-08 21:03:21.796477: Current learning rate: 0.00176\n",
      "2025-12-08 21:05:40.503781: train_loss -0.8538\n",
      "2025-12-08 21:05:40.503781: val_loss -0.8887\n",
      "2025-12-08 21:05:40.503781: Pseudo dice [0.9308, 0.958, 0.9536]\n",
      "2025-12-08 21:05:40.514116: Epoch time: 138.72 s\n",
      "2025-12-08 21:05:41.155556: \n",
      "2025-12-08 21:05:41.155556: Epoch 856\n",
      "2025-12-08 21:05:41.155556: Current learning rate: 0.00175\n",
      "2025-12-08 21:08:00.077375: train_loss -0.8537\n",
      "2025-12-08 21:08:00.077375: val_loss -0.8702\n",
      "2025-12-08 21:08:00.097130: Pseudo dice [0.9173, 0.9503, 0.9521]\n",
      "2025-12-08 21:08:00.103135: Epoch time: 138.92 s\n",
      "2025-12-08 21:08:00.780620: \n",
      "2025-12-08 21:08:00.780620: Epoch 857\n",
      "2025-12-08 21:08:00.796667: Current learning rate: 0.00174\n",
      "2025-12-08 21:10:19.943634: train_loss -0.8575\n",
      "2025-12-08 21:10:19.950293: val_loss -0.883\n",
      "2025-12-08 21:10:19.955333: Pseudo dice [0.9303, 0.9583, 0.9499]\n",
      "2025-12-08 21:10:19.961608: Epoch time: 139.16 s\n",
      "2025-12-08 21:10:20.865422: \n",
      "2025-12-08 21:10:20.865422: Epoch 858\n",
      "2025-12-08 21:10:20.871698: Current learning rate: 0.00173\n",
      "2025-12-08 21:12:41.916282: train_loss -0.8581\n",
      "2025-12-08 21:12:41.917288: val_loss -0.8786\n",
      "2025-12-08 21:12:41.923925: Pseudo dice [0.9289, 0.9533, 0.951]\n",
      "2025-12-08 21:12:41.929927: Epoch time: 141.06 s\n",
      "2025-12-08 21:12:42.702041: \n",
      "2025-12-08 21:12:42.702041: Epoch 859\n",
      "2025-12-08 21:12:42.710253: Current learning rate: 0.00172\n",
      "2025-12-08 21:15:03.118529: train_loss -0.8599\n",
      "2025-12-08 21:15:03.119529: val_loss -0.8761\n",
      "2025-12-08 21:15:03.128207: Pseudo dice [0.9212, 0.955, 0.9516]\n",
      "2025-12-08 21:15:03.135149: Epoch time: 140.42 s\n",
      "2025-12-08 21:15:04.085816: \n",
      "2025-12-08 21:15:04.085816: Epoch 860\n",
      "2025-12-08 21:15:04.088742: Current learning rate: 0.0017\n",
      "2025-12-08 21:17:24.331582: train_loss -0.8577\n",
      "2025-12-08 21:17:24.332582: val_loss -0.8793\n",
      "2025-12-08 21:17:24.341471: Pseudo dice [0.9261, 0.9546, 0.9498]\n",
      "2025-12-08 21:17:24.347914: Epoch time: 140.25 s\n",
      "2025-12-08 21:17:25.018127: \n",
      "2025-12-08 21:17:25.018127: Epoch 861\n",
      "2025-12-08 21:17:25.026038: Current learning rate: 0.00169\n",
      "2025-12-08 21:19:45.673506: train_loss -0.8546\n",
      "2025-12-08 21:19:45.673506: val_loss -0.8708\n",
      "2025-12-08 21:19:45.679512: Pseudo dice [0.9191, 0.9523, 0.9491]\n",
      "2025-12-08 21:19:45.685518: Epoch time: 140.66 s\n",
      "2025-12-08 21:19:46.339595: \n",
      "2025-12-08 21:19:46.339595: Epoch 862\n",
      "2025-12-08 21:19:46.346446: Current learning rate: 0.00168\n",
      "2025-12-08 21:22:06.889476: train_loss -0.8525\n",
      "2025-12-08 21:22:06.890523: val_loss -0.8894\n",
      "2025-12-08 21:22:06.896544: Pseudo dice [0.9341, 0.96, 0.9458]\n",
      "2025-12-08 21:22:06.902570: Epoch time: 140.55 s\n",
      "2025-12-08 21:22:07.554304: \n",
      "2025-12-08 21:22:07.555304: Epoch 863\n",
      "2025-12-08 21:22:07.561334: Current learning rate: 0.00167\n",
      "2025-12-08 21:24:27.262975: train_loss -0.8541\n",
      "2025-12-08 21:24:27.263976: val_loss -0.8802\n",
      "2025-12-08 21:24:27.270982: Pseudo dice [0.9268, 0.9564, 0.9506]\n",
      "2025-12-08 21:24:27.277278: Epoch time: 139.71 s\n",
      "2025-12-08 21:24:28.105320: \n",
      "2025-12-08 21:24:28.105320: Epoch 864\n",
      "2025-12-08 21:24:28.112329: Current learning rate: 0.00166\n",
      "2025-12-08 21:26:48.230374: train_loss -0.8571\n",
      "2025-12-08 21:26:48.230374: val_loss -0.8724\n",
      "2025-12-08 21:26:48.238349: Pseudo dice [0.9232, 0.9493, 0.9506]\n",
      "2025-12-08 21:26:48.243042: Epoch time: 140.13 s\n",
      "2025-12-08 21:26:48.873294: \n",
      "2025-12-08 21:26:48.873294: Epoch 865\n",
      "2025-12-08 21:26:48.877955: Current learning rate: 0.00165\n",
      "2025-12-08 21:29:10.475772: train_loss -0.855\n",
      "2025-12-08 21:29:10.475772: val_loss -0.8629\n",
      "2025-12-08 21:29:10.483131: Pseudo dice [0.9171, 0.9469, 0.941]\n",
      "2025-12-08 21:29:10.489356: Epoch time: 141.6 s\n",
      "2025-12-08 21:29:11.312366: \n",
      "2025-12-08 21:29:11.312366: Epoch 866\n",
      "2025-12-08 21:29:11.328161: Current learning rate: 0.00164\n",
      "2025-12-08 21:31:32.805335: train_loss -0.8595\n",
      "2025-12-08 21:31:32.805335: val_loss -0.8812\n",
      "2025-12-08 21:31:32.811336: Pseudo dice [0.9255, 0.9605, 0.951]\n",
      "2025-12-08 21:31:32.817176: Epoch time: 141.49 s\n",
      "2025-12-08 21:31:33.457348: \n",
      "2025-12-08 21:31:33.457348: Epoch 867\n",
      "2025-12-08 21:31:33.464374: Current learning rate: 0.00163\n",
      "2025-12-08 21:33:54.321192: train_loss -0.859\n",
      "2025-12-08 21:33:54.322195: val_loss -0.8866\n",
      "2025-12-08 21:33:54.327221: Pseudo dice [0.9311, 0.9589, 0.9502]\n",
      "2025-12-08 21:33:54.334760: Epoch time: 140.86 s\n",
      "2025-12-08 21:33:55.003225: \n",
      "2025-12-08 21:33:55.005228: Epoch 868\n",
      "2025-12-08 21:33:55.005228: Current learning rate: 0.00162\n",
      "2025-12-08 21:36:13.499388: train_loss -0.8625\n",
      "2025-12-08 21:36:13.506887: val_loss -0.8785\n",
      "2025-12-08 21:36:13.512893: Pseudo dice [0.9255, 0.953, 0.9545]\n",
      "2025-12-08 21:36:13.518517: Epoch time: 138.5 s\n",
      "2025-12-08 21:36:14.202589: \n",
      "2025-12-08 21:36:14.202589: Epoch 869\n",
      "2025-12-08 21:36:14.206095: Current learning rate: 0.00161\n",
      "2025-12-08 21:38:32.749557: train_loss -0.8629\n",
      "2025-12-08 21:38:32.749557: val_loss -0.8825\n",
      "2025-12-08 21:38:32.765322: Pseudo dice [0.9282, 0.9544, 0.9518]\n",
      "2025-12-08 21:38:32.765322: Epoch time: 138.55 s\n",
      "2025-12-08 21:38:33.421762: \n",
      "2025-12-08 21:38:33.421762: Epoch 870\n",
      "2025-12-08 21:38:33.441218: Current learning rate: 0.00159\n",
      "2025-12-08 21:40:52.140175: train_loss -0.8615\n",
      "2025-12-08 21:40:52.155946: val_loss -0.8763\n",
      "2025-12-08 21:40:52.164306: Pseudo dice [0.9238, 0.9542, 0.9492]\n",
      "2025-12-08 21:40:52.169307: Epoch time: 138.72 s\n",
      "2025-12-08 21:40:52.874351: \n",
      "2025-12-08 21:40:52.874351: Epoch 871\n",
      "2025-12-08 21:40:52.874351: Current learning rate: 0.00158\n",
      "2025-12-08 21:43:11.198414: train_loss -0.8636\n",
      "2025-12-08 21:43:11.199415: val_loss -0.889\n",
      "2025-12-08 21:43:11.207117: Pseudo dice [0.9325, 0.9595, 0.9505]\n",
      "2025-12-08 21:43:11.213122: Epoch time: 138.32 s\n",
      "2025-12-08 21:43:11.848518: \n",
      "2025-12-08 21:43:11.848518: Epoch 872\n",
      "2025-12-08 21:43:11.854541: Current learning rate: 0.00157\n",
      "2025-12-08 21:45:30.452594: train_loss -0.8607\n",
      "2025-12-08 21:45:30.452594: val_loss -0.8753\n",
      "2025-12-08 21:45:30.458177: Pseudo dice [0.9231, 0.9532, 0.9475]\n",
      "2025-12-08 21:45:30.464183: Epoch time: 138.61 s\n",
      "2025-12-08 21:45:31.406051: \n",
      "2025-12-08 21:45:31.406051: Epoch 873\n",
      "2025-12-08 21:45:31.406051: Current learning rate: 0.00156\n",
      "2025-12-08 21:47:49.483148: train_loss -0.861\n",
      "2025-12-08 21:47:49.498960: val_loss -0.8881\n",
      "2025-12-08 21:47:49.498960: Pseudo dice [0.9297, 0.9632, 0.9494]\n",
      "2025-12-08 21:47:49.498960: Epoch time: 138.08 s\n",
      "2025-12-08 21:47:50.139557: \n",
      "2025-12-08 21:47:50.140624: Epoch 874\n",
      "2025-12-08 21:47:50.146776: Current learning rate: 0.00155\n",
      "2025-12-08 21:50:07.967714: train_loss -0.8636\n",
      "2025-12-08 21:50:07.967714: val_loss -0.8847\n",
      "2025-12-08 21:50:07.983816: Pseudo dice [0.9278, 0.9603, 0.953]\n",
      "2025-12-08 21:50:07.983816: Epoch time: 137.83 s\n",
      "2025-12-08 21:50:08.614652: \n",
      "2025-12-08 21:50:08.614652: Epoch 875\n",
      "2025-12-08 21:50:08.620672: Current learning rate: 0.00154\n",
      "2025-12-08 21:52:26.723044: train_loss -0.8579\n",
      "2025-12-08 21:52:26.723044: val_loss -0.8813\n",
      "2025-12-08 21:52:26.723044: Pseudo dice [0.9246, 0.9565, 0.9518]\n",
      "2025-12-08 21:52:26.733360: Epoch time: 138.11 s\n",
      "2025-12-08 21:52:27.356089: \n",
      "2025-12-08 21:52:27.356089: Epoch 876\n",
      "2025-12-08 21:52:27.357830: Current learning rate: 0.00153\n",
      "2025-12-08 21:54:45.450783: train_loss -0.8595\n",
      "2025-12-08 21:54:45.450783: val_loss -0.8794\n",
      "2025-12-08 21:54:45.458277: Pseudo dice [0.9254, 0.9556, 0.9504]\n",
      "2025-12-08 21:54:45.462283: Epoch time: 138.09 s\n",
      "2025-12-08 21:54:46.158461: \n",
      "2025-12-08 21:54:46.158461: Epoch 877\n",
      "2025-12-08 21:54:46.165479: Current learning rate: 0.00152\n",
      "2025-12-08 21:57:04.170472: train_loss -0.864\n",
      "2025-12-08 21:57:04.172474: val_loss -0.8712\n",
      "2025-12-08 21:57:04.178480: Pseudo dice [0.9191, 0.9507, 0.9546]\n",
      "2025-12-08 21:57:04.184486: Epoch time: 138.01 s\n",
      "2025-12-08 21:57:04.884621: \n",
      "2025-12-08 21:57:04.886361: Epoch 878\n",
      "2025-12-08 21:57:04.889124: Current learning rate: 0.00151\n",
      "2025-12-08 21:59:23.076955: train_loss -0.8618\n",
      "2025-12-08 21:59:23.078959: val_loss -0.8692\n",
      "2025-12-08 21:59:23.088970: Pseudo dice [0.9169, 0.9521, 0.9463]\n",
      "2025-12-08 21:59:23.094714: Epoch time: 138.19 s\n",
      "2025-12-08 21:59:23.889063: \n",
      "2025-12-08 21:59:23.889063: Epoch 879\n",
      "2025-12-08 21:59:23.904813: Current learning rate: 0.00149\n",
      "2025-12-08 22:01:41.844756: train_loss -0.8625\n",
      "2025-12-08 22:01:41.846759: val_loss -0.8849\n",
      "2025-12-08 22:01:41.854505: Pseudo dice [0.9326, 0.9594, 0.9481]\n",
      "2025-12-08 22:01:41.863594: Epoch time: 137.96 s\n",
      "2025-12-08 22:01:42.501500: \n",
      "2025-12-08 22:01:42.501500: Epoch 880\n",
      "2025-12-08 22:01:42.501500: Current learning rate: 0.00148\n",
      "2025-12-08 22:04:00.426143: train_loss -0.8583\n",
      "2025-12-08 22:04:00.426143: val_loss -0.8804\n",
      "2025-12-08 22:04:00.436153: Pseudo dice [0.926, 0.9559, 0.9498]\n",
      "2025-12-08 22:04:00.443901: Epoch time: 137.93 s\n",
      "2025-12-08 22:04:01.191188: \n",
      "2025-12-08 22:04:01.192188: Epoch 881\n",
      "2025-12-08 22:04:01.197696: Current learning rate: 0.00147\n",
      "2025-12-08 22:06:19.326842: train_loss -0.8625\n",
      "2025-12-08 22:06:19.326842: val_loss -0.8805\n",
      "2025-12-08 22:06:19.337438: Pseudo dice [0.9249, 0.9559, 0.9506]\n",
      "2025-12-08 22:06:19.342943: Epoch time: 138.14 s\n",
      "2025-12-08 22:06:20.014690: \n",
      "2025-12-08 22:06:20.014690: Epoch 882\n",
      "2025-12-08 22:06:20.014690: Current learning rate: 0.00146\n",
      "2025-12-08 22:08:38.093192: train_loss -0.8574\n",
      "2025-12-08 22:08:38.093192: val_loss -0.8873\n",
      "2025-12-08 22:08:38.104438: Pseudo dice [0.9313, 0.9634, 0.9529]\n",
      "2025-12-08 22:08:38.108442: Epoch time: 138.08 s\n",
      "2025-12-08 22:08:38.810440: \n",
      "2025-12-08 22:08:38.811445: Epoch 883\n",
      "2025-12-08 22:08:38.811445: Current learning rate: 0.00145\n",
      "2025-12-08 22:10:56.702685: train_loss -0.8604\n",
      "2025-12-08 22:10:56.702685: val_loss -0.8678\n",
      "2025-12-08 22:10:56.710687: Pseudo dice [0.9167, 0.9519, 0.9478]\n",
      "2025-12-08 22:10:56.715696: Epoch time: 137.89 s\n",
      "2025-12-08 22:10:57.467975: \n",
      "2025-12-08 22:10:57.468980: Epoch 884\n",
      "2025-12-08 22:10:57.476024: Current learning rate: 0.00144\n",
      "2025-12-08 22:13:15.498992: train_loss -0.8595\n",
      "2025-12-08 22:13:15.498992: val_loss -0.8835\n",
      "2025-12-08 22:13:15.498992: Pseudo dice [0.9268, 0.9584, 0.953]\n",
      "2025-12-08 22:13:15.498992: Epoch time: 138.03 s\n",
      "2025-12-08 22:13:16.125149: \n",
      "2025-12-08 22:13:16.125149: Epoch 885\n",
      "2025-12-08 22:13:16.125149: Current learning rate: 0.00143\n",
      "2025-12-08 22:15:34.179533: train_loss -0.8609\n",
      "2025-12-08 22:15:34.181535: val_loss -0.8883\n",
      "2025-12-08 22:15:34.187541: Pseudo dice [0.9312, 0.9626, 0.9537]\n",
      "2025-12-08 22:15:34.192547: Epoch time: 138.05 s\n",
      "2025-12-08 22:15:35.062373: \n",
      "2025-12-08 22:15:35.078081: Epoch 886\n",
      "2025-12-08 22:15:35.078081: Current learning rate: 0.00142\n",
      "2025-12-08 22:17:53.236816: train_loss -0.8613\n",
      "2025-12-08 22:17:53.237818: val_loss -0.8897\n",
      "2025-12-08 22:17:53.244820: Pseudo dice [0.9315, 0.9597, 0.954]\n",
      "2025-12-08 22:17:53.250865: Epoch time: 138.17 s\n",
      "2025-12-08 22:17:53.952148: \n",
      "2025-12-08 22:17:53.952148: Epoch 887\n",
      "2025-12-08 22:17:53.967852: Current learning rate: 0.00141\n",
      "2025-12-08 22:20:11.998894: train_loss -0.86\n",
      "2025-12-08 22:20:11.998894: val_loss -0.8767\n",
      "2025-12-08 22:20:12.014554: Pseudo dice [0.9242, 0.9562, 0.9515]\n",
      "2025-12-08 22:20:12.014554: Epoch time: 138.05 s\n",
      "2025-12-08 22:20:12.639304: \n",
      "2025-12-08 22:20:12.639304: Epoch 888\n",
      "2025-12-08 22:20:12.639304: Current learning rate: 0.00139\n",
      "2025-12-08 22:22:30.558100: train_loss -0.8604\n",
      "2025-12-08 22:22:30.558100: val_loss -0.8739\n",
      "2025-12-08 22:22:30.561843: Pseudo dice [0.9195, 0.9534, 0.9565]\n",
      "2025-12-08 22:22:30.567724: Epoch time: 137.92 s\n",
      "2025-12-08 22:22:31.235452: \n",
      "2025-12-08 22:22:31.235452: Epoch 889\n",
      "2025-12-08 22:22:31.251581: Current learning rate: 0.00138\n",
      "2025-12-08 22:24:49.188931: train_loss -0.8659\n",
      "2025-12-08 22:24:49.188931: val_loss -0.8789\n",
      "2025-12-08 22:24:49.202699: Pseudo dice [0.9251, 0.9549, 0.9517]\n",
      "2025-12-08 22:24:49.208707: Epoch time: 137.95 s\n",
      "2025-12-08 22:24:49.859715: \n",
      "2025-12-08 22:24:49.859715: Epoch 890\n",
      "2025-12-08 22:24:49.875382: Current learning rate: 0.00137\n",
      "2025-12-08 22:27:08.209919: train_loss -0.8569\n",
      "2025-12-08 22:27:08.209919: val_loss -0.8911\n",
      "2025-12-08 22:27:08.219671: Pseudo dice [0.9322, 0.9656, 0.9523]\n",
      "2025-12-08 22:27:08.229685: Epoch time: 138.35 s\n",
      "2025-12-08 22:27:08.237433: Yayy! New best EMA pseudo Dice: 0.9451\n",
      "2025-12-08 22:27:09.221732: \n",
      "2025-12-08 22:27:09.221732: Epoch 891\n",
      "2025-12-08 22:27:09.221732: Current learning rate: 0.00136\n",
      "2025-12-08 22:29:27.490243: train_loss -0.8578\n",
      "2025-12-08 22:29:27.490243: val_loss -0.8854\n",
      "2025-12-08 22:29:27.496248: Pseudo dice [0.9305, 0.9593, 0.9496]\n",
      "2025-12-08 22:29:27.500252: Epoch time: 138.27 s\n",
      "2025-12-08 22:29:27.506665: Yayy! New best EMA pseudo Dice: 0.9452\n",
      "2025-12-08 22:29:28.729039: \n",
      "2025-12-08 22:29:28.730057: Epoch 892\n",
      "2025-12-08 22:29:28.736173: Current learning rate: 0.00135\n",
      "2025-12-08 22:31:46.659729: train_loss -0.8598\n",
      "2025-12-08 22:31:46.659729: val_loss -0.8869\n",
      "2025-12-08 22:31:46.668731: Pseudo dice [0.9314, 0.9628, 0.9513]\n",
      "2025-12-08 22:31:46.676522: Epoch time: 137.93 s\n",
      "2025-12-08 22:31:46.682527: Yayy! New best EMA pseudo Dice: 0.9456\n",
      "2025-12-08 22:31:47.770421: \n",
      "2025-12-08 22:31:47.770421: Epoch 893\n",
      "2025-12-08 22:31:47.777450: Current learning rate: 0.00134\n",
      "2025-12-08 22:34:05.751729: train_loss -0.8589\n",
      "2025-12-08 22:34:05.751729: val_loss -0.8813\n",
      "2025-12-08 22:34:05.761739: Pseudo dice [0.9243, 0.9593, 0.9536]\n",
      "2025-12-08 22:34:05.767484: Epoch time: 137.98 s\n",
      "2025-12-08 22:34:05.773489: Yayy! New best EMA pseudo Dice: 0.9456\n",
      "2025-12-08 22:34:06.718661: \n",
      "2025-12-08 22:34:06.718661: Epoch 894\n",
      "2025-12-08 22:34:06.734374: Current learning rate: 0.00133\n",
      "2025-12-08 22:36:24.640201: train_loss -0.8666\n",
      "2025-12-08 22:36:24.640201: val_loss -0.8818\n",
      "2025-12-08 22:36:24.655457: Pseudo dice [0.9265, 0.9564, 0.9516]\n",
      "2025-12-08 22:36:24.661721: Epoch time: 137.92 s\n",
      "2025-12-08 22:36:25.295728: \n",
      "2025-12-08 22:36:25.295728: Epoch 895\n",
      "2025-12-08 22:36:25.295728: Current learning rate: 0.00132\n",
      "2025-12-08 22:38:43.231928: train_loss -0.8609\n",
      "2025-12-08 22:38:43.233668: val_loss -0.8941\n",
      "2025-12-08 22:38:43.241679: Pseudo dice [0.9325, 0.9667, 0.9542]\n",
      "2025-12-08 22:38:43.249425: Epoch time: 137.94 s\n",
      "2025-12-08 22:38:43.256501: Yayy! New best EMA pseudo Dice: 0.9461\n",
      "2025-12-08 22:38:44.180835: \n",
      "2025-12-08 22:38:44.180835: Epoch 896\n",
      "2025-12-08 22:38:44.186863: Current learning rate: 0.0013\n",
      "2025-12-08 22:41:02.190971: train_loss -0.8609\n",
      "2025-12-08 22:41:02.200574: val_loss -0.8717\n",
      "2025-12-08 22:41:02.208055: Pseudo dice [0.9185, 0.9529, 0.952]\n",
      "2025-12-08 22:41:02.215539: Epoch time: 138.02 s\n",
      "2025-12-08 22:41:02.842445: \n",
      "2025-12-08 22:41:02.842445: Epoch 897\n",
      "2025-12-08 22:41:02.842445: Current learning rate: 0.00129\n",
      "2025-12-08 22:43:20.907752: train_loss -0.8627\n",
      "2025-12-08 22:43:20.907752: val_loss -0.8887\n",
      "2025-12-08 22:43:20.915497: Pseudo dice [0.9305, 0.9604, 0.9505]\n",
      "2025-12-08 22:43:20.921241: Epoch time: 138.07 s\n",
      "2025-12-08 22:43:21.715903: \n",
      "2025-12-08 22:43:21.715903: Epoch 898\n",
      "2025-12-08 22:43:21.723046: Current learning rate: 0.00128\n",
      "2025-12-08 22:45:39.736147: train_loss -0.8588\n",
      "2025-12-08 22:45:39.736147: val_loss -0.8792\n",
      "2025-12-08 22:45:39.751767: Pseudo dice [0.9294, 0.9563, 0.9402]\n",
      "2025-12-08 22:45:39.751767: Epoch time: 138.02 s\n",
      "2025-12-08 22:45:40.376297: \n",
      "2025-12-08 22:45:40.376297: Epoch 899\n",
      "2025-12-08 22:45:40.390313: Current learning rate: 0.00127\n",
      "2025-12-08 22:47:58.344433: train_loss -0.8601\n",
      "2025-12-08 22:47:58.344433: val_loss -0.8874\n",
      "2025-12-08 22:47:58.360085: Pseudo dice [0.9322, 0.96, 0.953]\n",
      "2025-12-08 22:47:58.360085: Epoch time: 137.97 s\n",
      "2025-12-08 22:47:59.289320: \n",
      "2025-12-08 22:47:59.290323: Epoch 900\n",
      "2025-12-08 22:47:59.296319: Current learning rate: 0.00126\n",
      "2025-12-08 22:50:17.514726: train_loss -0.8584\n",
      "2025-12-08 22:50:17.514726: val_loss -0.8851\n",
      "2025-12-08 22:50:17.522221: Pseudo dice [0.9273, 0.9536, 0.9546]\n",
      "2025-12-08 22:50:17.528227: Epoch time: 138.23 s\n",
      "2025-12-08 22:50:18.165370: \n",
      "2025-12-08 22:50:18.165370: Epoch 901\n",
      "2025-12-08 22:50:18.171767: Current learning rate: 0.00125\n",
      "2025-12-08 22:52:36.173424: train_loss -0.8587\n",
      "2025-12-08 22:52:36.175427: val_loss -0.8747\n",
      "2025-12-08 22:52:36.180800: Pseudo dice [0.9201, 0.9509, 0.9505]\n",
      "2025-12-08 22:52:36.186806: Epoch time: 138.01 s\n",
      "2025-12-08 22:52:36.889045: \n",
      "2025-12-08 22:52:36.889045: Epoch 902\n",
      "2025-12-08 22:52:36.905158: Current learning rate: 0.00124\n",
      "2025-12-08 22:54:54.796586: train_loss -0.8591\n",
      "2025-12-08 22:54:54.796586: val_loss -0.8848\n",
      "2025-12-08 22:54:54.807936: Pseudo dice [0.9287, 0.9556, 0.9522]\n",
      "2025-12-08 22:54:54.815944: Epoch time: 137.91 s\n",
      "2025-12-08 22:54:55.437339: \n",
      "2025-12-08 22:54:55.437339: Epoch 903\n",
      "2025-12-08 22:54:55.453102: Current learning rate: 0.00122\n",
      "2025-12-08 22:57:13.640755: train_loss -0.8582\n",
      "2025-12-08 22:57:13.640755: val_loss -0.8817\n",
      "2025-12-08 22:57:13.650427: Pseudo dice [0.9277, 0.9578, 0.9521]\n",
      "2025-12-08 22:57:13.656440: Epoch time: 138.2 s\n",
      "2025-12-08 22:57:14.422769: \n",
      "2025-12-08 22:57:14.422769: Epoch 904\n",
      "2025-12-08 22:57:14.441635: Current learning rate: 0.00121\n",
      "2025-12-08 22:59:32.436190: train_loss -0.8566\n",
      "2025-12-08 22:59:32.436190: val_loss -0.8837\n",
      "2025-12-08 22:59:32.443238: Pseudo dice [0.9274, 0.9602, 0.9494]\n",
      "2025-12-08 22:59:32.449259: Epoch time: 138.01 s\n",
      "2025-12-08 22:59:33.249120: \n",
      "2025-12-08 22:59:33.249120: Epoch 905\n",
      "2025-12-08 22:59:33.249120: Current learning rate: 0.0012\n",
      "2025-12-08 23:01:51.368309: train_loss -0.8617\n",
      "2025-12-08 23:01:51.368309: val_loss -0.8718\n",
      "2025-12-08 23:01:51.380071: Pseudo dice [0.9181, 0.9512, 0.9537]\n",
      "2025-12-08 23:01:51.388081: Epoch time: 138.12 s\n",
      "2025-12-08 23:01:52.014668: \n",
      "2025-12-08 23:01:52.014668: Epoch 906\n",
      "2025-12-08 23:01:52.014668: Current learning rate: 0.00119\n",
      "2025-12-08 23:04:10.116211: train_loss -0.861\n",
      "2025-12-08 23:04:10.116211: val_loss -0.877\n",
      "2025-12-08 23:04:10.122217: Pseudo dice [0.921, 0.9551, 0.951]\n",
      "2025-12-08 23:04:10.129313: Epoch time: 138.1 s\n",
      "2025-12-08 23:04:10.811831: \n",
      "2025-12-08 23:04:10.827578: Epoch 907\n",
      "2025-12-08 23:04:10.827578: Current learning rate: 0.00118\n",
      "2025-12-08 23:06:28.966268: train_loss -0.8582\n",
      "2025-12-08 23:06:28.968009: val_loss -0.8812\n",
      "2025-12-08 23:06:28.974829: Pseudo dice [0.9264, 0.9615, 0.9494]\n",
      "2025-12-08 23:06:28.978833: Epoch time: 138.15 s\n",
      "2025-12-08 23:06:29.608177: \n",
      "2025-12-08 23:06:29.608177: Epoch 908\n",
      "2025-12-08 23:06:29.608177: Current learning rate: 0.00117\n",
      "2025-12-08 23:08:47.782441: train_loss -0.8607\n",
      "2025-12-08 23:08:47.782441: val_loss -0.8907\n",
      "2025-12-08 23:08:47.800372: Pseudo dice [0.9317, 0.959, 0.9561]\n",
      "2025-12-08 23:08:47.805387: Epoch time: 138.17 s\n",
      "2025-12-08 23:08:48.467901: \n",
      "2025-12-08 23:08:48.467901: Epoch 909\n",
      "2025-12-08 23:08:48.467901: Current learning rate: 0.00116\n",
      "2025-12-08 23:11:06.564006: train_loss -0.8595\n",
      "2025-12-08 23:11:06.565008: val_loss -0.8835\n",
      "2025-12-08 23:11:06.571022: Pseudo dice [0.9289, 0.9574, 0.9554]\n",
      "2025-12-08 23:11:06.577047: Epoch time: 138.1 s\n",
      "2025-12-08 23:11:07.265009: \n",
      "2025-12-08 23:11:07.265009: Epoch 910\n",
      "2025-12-08 23:11:07.265009: Current learning rate: 0.00115\n",
      "2025-12-08 23:13:25.530153: train_loss -0.858\n",
      "2025-12-08 23:13:25.530153: val_loss -0.8858\n",
      "2025-12-08 23:13:25.547807: Pseudo dice [0.9306, 0.9628, 0.9497]\n",
      "2025-12-08 23:13:25.547807: Epoch time: 138.27 s\n",
      "2025-12-08 23:13:26.344623: \n",
      "2025-12-08 23:13:26.344623: Epoch 911\n",
      "2025-12-08 23:13:26.344623: Current learning rate: 0.00113\n",
      "2025-12-08 23:15:44.509355: train_loss -0.8583\n",
      "2025-12-08 23:15:44.509355: val_loss -0.8912\n",
      "2025-12-08 23:15:44.517368: Pseudo dice [0.9337, 0.961, 0.9473]\n",
      "2025-12-08 23:15:44.523370: Epoch time: 138.16 s\n",
      "2025-12-08 23:15:45.209562: \n",
      "2025-12-08 23:15:45.209562: Epoch 912\n",
      "2025-12-08 23:15:45.209562: Current learning rate: 0.00112\n",
      "2025-12-08 23:18:03.229896: train_loss -0.8647\n",
      "2025-12-08 23:18:03.231898: val_loss -0.8835\n",
      "2025-12-08 23:18:03.239645: Pseudo dice [0.9281, 0.9571, 0.949]\n",
      "2025-12-08 23:18:03.243648: Epoch time: 138.02 s\n",
      "2025-12-08 23:18:03.983905: \n",
      "2025-12-08 23:18:03.983905: Epoch 913\n",
      "2025-12-08 23:18:04.001696: Current learning rate: 0.00111\n",
      "2025-12-08 23:20:22.108779: train_loss -0.8608\n",
      "2025-12-08 23:20:22.108779: val_loss -0.8902\n",
      "2025-12-08 23:20:22.118290: Pseudo dice [0.9302, 0.9596, 0.9555]\n",
      "2025-12-08 23:20:22.122294: Epoch time: 138.12 s\n",
      "2025-12-08 23:20:22.758726: \n",
      "2025-12-08 23:20:22.759738: Epoch 914\n",
      "2025-12-08 23:20:22.766513: Current learning rate: 0.0011\n",
      "2025-12-08 23:22:40.584112: train_loss -0.8621\n",
      "2025-12-08 23:22:40.586115: val_loss -0.8874\n",
      "2025-12-08 23:22:40.593861: Pseudo dice [0.9276, 0.9589, 0.9537]\n",
      "2025-12-08 23:22:40.599867: Epoch time: 137.83 s\n",
      "2025-12-08 23:22:41.232179: \n",
      "2025-12-08 23:22:41.232179: Epoch 915\n",
      "2025-12-08 23:22:41.233184: Current learning rate: 0.00109\n",
      "2025-12-08 23:24:59.337909: train_loss -0.86\n",
      "2025-12-08 23:24:59.339912: val_loss -0.8854\n",
      "2025-12-08 23:24:59.345658: Pseudo dice [0.9305, 0.9569, 0.9514]\n",
      "2025-12-08 23:24:59.351664: Epoch time: 138.11 s\n",
      "2025-12-08 23:24:59.968017: \n",
      "2025-12-08 23:24:59.968017: Epoch 916\n",
      "2025-12-08 23:24:59.983812: Current learning rate: 0.00108\n",
      "2025-12-08 23:27:18.297062: train_loss -0.8564\n",
      "2025-12-08 23:27:18.297062: val_loss -0.8716\n",
      "2025-12-08 23:27:18.304075: Pseudo dice [0.9211, 0.9512, 0.9451]\n",
      "2025-12-08 23:27:18.310076: Epoch time: 138.33 s\n",
      "2025-12-08 23:27:19.108407: \n",
      "2025-12-08 23:27:19.108407: Epoch 917\n",
      "2025-12-08 23:27:19.108407: Current learning rate: 0.00106\n",
      "2025-12-08 23:29:37.317205: train_loss -0.8618\n",
      "2025-12-08 23:29:37.317205: val_loss -0.8847\n",
      "2025-12-08 23:29:37.325213: Pseudo dice [0.931, 0.958, 0.9469]\n",
      "2025-12-08 23:29:37.330716: Epoch time: 138.21 s\n",
      "2025-12-08 23:29:38.015276: \n",
      "2025-12-08 23:29:38.015276: Epoch 918\n",
      "2025-12-08 23:29:38.022293: Current learning rate: 0.00105\n",
      "2025-12-08 23:31:55.987942: train_loss -0.8651\n",
      "2025-12-08 23:31:55.987942: val_loss -0.8799\n",
      "2025-12-08 23:31:55.999693: Pseudo dice [0.9269, 0.9578, 0.9473]\n",
      "2025-12-08 23:31:56.011711: Epoch time: 137.97 s\n",
      "2025-12-08 23:31:56.648056: \n",
      "2025-12-08 23:31:56.648056: Epoch 919\n",
      "2025-12-08 23:31:56.655580: Current learning rate: 0.00104\n",
      "2025-12-08 23:34:14.614665: train_loss -0.8643\n",
      "2025-12-08 23:34:14.614665: val_loss -0.8836\n",
      "2025-12-08 23:34:14.622673: Pseudo dice [0.9287, 0.956, 0.9518]\n",
      "2025-12-08 23:34:14.628290: Epoch time: 137.97 s\n",
      "2025-12-08 23:34:15.252282: \n",
      "2025-12-08 23:34:15.253282: Epoch 920\n",
      "2025-12-08 23:34:15.258828: Current learning rate: 0.00103\n",
      "2025-12-08 23:36:33.295356: train_loss -0.8654\n",
      "2025-12-08 23:36:33.295356: val_loss -0.8828\n",
      "2025-12-08 23:36:33.304069: Pseudo dice [0.9267, 0.9576, 0.954]\n",
      "2025-12-08 23:36:33.308873: Epoch time: 138.04 s\n",
      "2025-12-08 23:36:33.967107: \n",
      "2025-12-08 23:36:33.967107: Epoch 921\n",
      "2025-12-08 23:36:33.982939: Current learning rate: 0.00102\n",
      "2025-12-08 23:38:52.155399: train_loss -0.859\n",
      "2025-12-08 23:38:52.155399: val_loss -0.8824\n",
      "2025-12-08 23:38:52.167423: Pseudo dice [0.927, 0.9572, 0.951]\n",
      "2025-12-08 23:38:52.175170: Epoch time: 138.19 s\n",
      "2025-12-08 23:38:52.795760: \n",
      "2025-12-08 23:38:52.795760: Epoch 922\n",
      "2025-12-08 23:38:52.811668: Current learning rate: 0.00101\n",
      "2025-12-08 23:41:12.357891: train_loss -0.8637\n",
      "2025-12-08 23:41:12.357891: val_loss -0.8775\n",
      "2025-12-08 23:41:12.364308: Pseudo dice [0.925, 0.9566, 0.9495]\n",
      "2025-12-08 23:41:12.368819: Epoch time: 139.56 s\n",
      "2025-12-08 23:41:12.991977: \n",
      "2025-12-08 23:41:12.991977: Epoch 923\n",
      "2025-12-08 23:41:12.997108: Current learning rate: 0.001\n",
      "2025-12-08 23:43:31.474879: train_loss -0.8646\n",
      "2025-12-08 23:43:31.474879: val_loss -0.8865\n",
      "2025-12-08 23:43:31.484891: Pseudo dice [0.931, 0.96, 0.9514]\n",
      "2025-12-08 23:43:31.490636: Epoch time: 138.5 s\n",
      "2025-12-08 23:43:32.295345: \n",
      "2025-12-08 23:43:32.295345: Epoch 924\n",
      "2025-12-08 23:43:32.295345: Current learning rate: 0.00098\n",
      "2025-12-08 23:45:50.639628: train_loss -0.866\n",
      "2025-12-08 23:45:50.639628: val_loss -0.8926\n",
      "2025-12-08 23:45:50.647376: Pseudo dice [0.934, 0.9597, 0.9518]\n",
      "2025-12-08 23:45:50.653839: Epoch time: 138.34 s\n",
      "2025-12-08 23:45:51.396692: \n",
      "2025-12-08 23:45:51.396692: Epoch 925\n",
      "2025-12-08 23:45:51.404704: Current learning rate: 0.00097\n",
      "2025-12-08 23:48:09.797849: train_loss -0.8641\n",
      "2025-12-08 23:48:09.797849: val_loss -0.8894\n",
      "2025-12-08 23:48:09.797849: Pseudo dice [0.9326, 0.9621, 0.9511]\n",
      "2025-12-08 23:48:09.807821: Epoch time: 138.4 s\n",
      "2025-12-08 23:48:10.433750: \n",
      "2025-12-08 23:48:10.433750: Epoch 926\n",
      "2025-12-08 23:48:10.433750: Current learning rate: 0.00096\n",
      "2025-12-08 23:50:28.875886: train_loss -0.8646\n",
      "2025-12-08 23:50:28.875886: val_loss -0.8941\n",
      "2025-12-08 23:50:28.881891: Pseudo dice [0.9339, 0.9636, 0.9516]\n",
      "2025-12-08 23:50:28.889260: Epoch time: 138.44 s\n",
      "2025-12-08 23:50:28.895266: Yayy! New best EMA pseudo Dice: 0.9463\n",
      "2025-12-08 23:50:29.798206: \n",
      "2025-12-08 23:50:29.798206: Epoch 927\n",
      "2025-12-08 23:50:29.804480: Current learning rate: 0.00095\n",
      "2025-12-08 23:52:47.961868: train_loss -0.8594\n",
      "2025-12-08 23:52:47.961868: val_loss -0.8813\n",
      "2025-12-08 23:52:47.977778: Pseudo dice [0.9236, 0.9567, 0.9502]\n",
      "2025-12-08 23:52:47.977778: Epoch time: 138.17 s\n",
      "2025-12-08 23:52:48.838809: \n",
      "2025-12-08 23:52:48.838809: Epoch 928\n",
      "2025-12-08 23:52:48.838809: Current learning rate: 0.00094\n",
      "2025-12-08 23:55:06.899708: train_loss -0.8655\n",
      "2025-12-08 23:55:06.899708: val_loss -0.887\n",
      "2025-12-08 23:55:06.899708: Pseudo dice [0.9289, 0.9589, 0.9506]\n",
      "2025-12-08 23:55:06.915591: Epoch time: 138.06 s\n",
      "2025-12-08 23:55:07.532890: \n",
      "2025-12-08 23:55:07.532890: Epoch 929\n",
      "2025-12-08 23:55:07.532890: Current learning rate: 0.00092\n",
      "2025-12-08 23:57:25.828827: train_loss -0.8551\n",
      "2025-12-08 23:57:25.828827: val_loss -0.8833\n",
      "2025-12-08 23:57:25.838576: Pseudo dice [0.929, 0.9584, 0.9497]\n",
      "2025-12-08 23:57:25.842580: Epoch time: 138.3 s\n",
      "2025-12-08 23:57:26.470056: \n",
      "2025-12-08 23:57:26.470056: Epoch 930\n",
      "2025-12-08 23:57:26.470056: Current learning rate: 0.00091\n",
      "2025-12-08 23:59:44.581079: train_loss -0.8614\n",
      "2025-12-08 23:59:44.581079: val_loss -0.8825\n",
      "2025-12-08 23:59:44.588994: Pseudo dice [0.9282, 0.9602, 0.952]\n",
      "2025-12-08 23:59:44.595001: Epoch time: 138.11 s\n",
      "2025-12-08 23:59:45.569155: \n",
      "2025-12-08 23:59:45.569155: Epoch 931\n",
      "2025-12-08 23:59:45.586920: Current learning rate: 0.0009\n",
      "2025-12-09 00:02:03.747560: train_loss -0.8663\n",
      "2025-12-09 00:02:03.747560: val_loss -0.8894\n",
      "2025-12-09 00:02:03.763267: Pseudo dice [0.931, 0.9641, 0.9516]\n",
      "2025-12-09 00:02:03.763267: Epoch time: 138.18 s\n",
      "2025-12-09 00:02:03.763267: Yayy! New best EMA pseudo Dice: 0.9464\n",
      "2025-12-09 00:02:04.761514: \n",
      "2025-12-09 00:02:04.761514: Epoch 932\n",
      "2025-12-09 00:02:04.761514: Current learning rate: 0.00089\n",
      "2025-12-09 00:04:22.904267: train_loss -0.8637\n",
      "2025-12-09 00:04:22.906269: val_loss -0.8758\n",
      "2025-12-09 00:04:22.906269: Pseudo dice [0.9248, 0.9537, 0.9526]\n",
      "2025-12-09 00:04:22.916707: Epoch time: 138.16 s\n",
      "2025-12-09 00:04:23.540768: \n",
      "2025-12-09 00:04:23.540768: Epoch 933\n",
      "2025-12-09 00:04:23.540768: Current learning rate: 0.00088\n",
      "2025-12-09 00:06:41.677257: train_loss -0.863\n",
      "2025-12-09 00:06:41.677257: val_loss -0.884\n",
      "2025-12-09 00:06:41.685265: Pseudo dice [0.9287, 0.959, 0.9534]\n",
      "2025-12-09 00:06:41.691009: Epoch time: 138.14 s\n",
      "2025-12-09 00:06:42.495827: \n",
      "2025-12-09 00:06:42.495827: Epoch 934\n",
      "2025-12-09 00:06:42.495827: Current learning rate: 0.00087\n",
      "2025-12-09 00:09:00.511299: train_loss -0.8664\n",
      "2025-12-09 00:09:00.511299: val_loss -0.8878\n",
      "2025-12-09 00:09:00.529116: Pseudo dice [0.9308, 0.9612, 0.9498]\n",
      "2025-12-09 00:09:00.535122: Epoch time: 138.02 s\n",
      "2025-12-09 00:09:01.253980: \n",
      "2025-12-09 00:09:01.253980: Epoch 935\n",
      "2025-12-09 00:09:01.270051: Current learning rate: 0.00085\n",
      "2025-12-09 00:11:19.373273: train_loss -0.8651\n",
      "2025-12-09 00:11:19.375276: val_loss -0.8794\n",
      "2025-12-09 00:11:19.381283: Pseudo dice [0.9217, 0.957, 0.9553]\n",
      "2025-12-09 00:11:19.385287: Epoch time: 138.12 s\n",
      "2025-12-09 00:11:20.020909: \n",
      "2025-12-09 00:11:20.021912: Epoch 936\n",
      "2025-12-09 00:11:20.021912: Current learning rate: 0.00084\n",
      "2025-12-09 00:13:38.175034: train_loss -0.8598\n",
      "2025-12-09 00:13:38.175034: val_loss -0.8856\n",
      "2025-12-09 00:13:38.191150: Pseudo dice [0.9302, 0.9595, 0.9489]\n",
      "2025-12-09 00:13:38.191150: Epoch time: 138.16 s\n",
      "2025-12-09 00:13:39.204703: \n",
      "2025-12-09 00:13:39.204703: Epoch 937\n",
      "2025-12-09 00:13:39.204703: Current learning rate: 0.00083\n",
      "2025-12-09 00:15:57.221310: train_loss -0.8633\n",
      "2025-12-09 00:15:57.221310: val_loss -0.8824\n",
      "2025-12-09 00:15:57.221310: Pseudo dice [0.9272, 0.9573, 0.9496]\n",
      "2025-12-09 00:15:57.221310: Epoch time: 138.02 s\n",
      "2025-12-09 00:15:57.863778: \n",
      "2025-12-09 00:15:57.863778: Epoch 938\n",
      "2025-12-09 00:15:57.868090: Current learning rate: 0.00082\n",
      "2025-12-09 00:18:16.194149: train_loss -0.8671\n",
      "2025-12-09 00:18:16.194149: val_loss -0.8855\n",
      "2025-12-09 00:18:16.207900: Pseudo dice [0.932, 0.9599, 0.9442]\n",
      "2025-12-09 00:18:16.207900: Epoch time: 138.33 s\n",
      "2025-12-09 00:18:16.842175: \n",
      "2025-12-09 00:18:16.842175: Epoch 939\n",
      "2025-12-09 00:18:16.842175: Current learning rate: 0.00081\n",
      "2025-12-09 00:20:35.744829: train_loss -0.864\n",
      "2025-12-09 00:20:35.744829: val_loss -0.8803\n",
      "2025-12-09 00:20:35.744829: Pseudo dice [0.927, 0.953, 0.9521]\n",
      "2025-12-09 00:20:35.760588: Epoch time: 138.9 s\n",
      "2025-12-09 00:20:36.552299: \n",
      "2025-12-09 00:20:36.552299: Epoch 940\n",
      "2025-12-09 00:20:36.552299: Current learning rate: 0.00079\n",
      "2025-12-09 00:22:55.125354: train_loss -0.8628\n",
      "2025-12-09 00:22:55.125354: val_loss -0.8739\n",
      "2025-12-09 00:22:55.133100: Pseudo dice [0.9185, 0.953, 0.9553]\n",
      "2025-12-09 00:22:55.138898: Epoch time: 138.59 s\n",
      "2025-12-09 00:22:55.831434: \n",
      "2025-12-09 00:22:55.831434: Epoch 941\n",
      "2025-12-09 00:22:55.846762: Current learning rate: 0.00078\n",
      "2025-12-09 00:25:14.514328: train_loss -0.8619\n",
      "2025-12-09 00:25:14.514328: val_loss -0.8742\n",
      "2025-12-09 00:25:14.521967: Pseudo dice [0.9191, 0.9534, 0.951]\n",
      "2025-12-09 00:25:14.529975: Epoch time: 138.68 s\n",
      "2025-12-09 00:25:15.229328: \n",
      "2025-12-09 00:25:15.229328: Epoch 942\n",
      "2025-12-09 00:25:15.229328: Current learning rate: 0.00077\n",
      "2025-12-09 00:27:33.639635: train_loss -0.8634\n",
      "2025-12-09 00:27:33.639635: val_loss -0.8848\n",
      "2025-12-09 00:27:33.639635: Pseudo dice [0.9255, 0.9591, 0.9521]\n",
      "2025-12-09 00:27:33.659510: Epoch time: 138.41 s\n",
      "2025-12-09 00:27:34.492669: \n",
      "2025-12-09 00:27:34.508582: Epoch 943\n",
      "2025-12-09 00:27:34.514591: Current learning rate: 0.00076\n",
      "2025-12-09 00:29:52.860782: train_loss -0.8605\n",
      "2025-12-09 00:29:52.860782: val_loss -0.8753\n",
      "2025-12-09 00:29:52.876546: Pseudo dice [0.9225, 0.956, 0.947]\n",
      "2025-12-09 00:29:52.876546: Epoch time: 138.37 s\n",
      "2025-12-09 00:29:53.515744: \n",
      "2025-12-09 00:29:53.515744: Epoch 944\n",
      "2025-12-09 00:29:53.515744: Current learning rate: 0.00075\n",
      "2025-12-09 00:32:11.615592: train_loss -0.8606\n",
      "2025-12-09 00:32:11.615592: val_loss -0.8898\n",
      "2025-12-09 00:32:11.615592: Pseudo dice [0.9296, 0.9614, 0.958]\n",
      "2025-12-09 00:32:11.631644: Epoch time: 138.1 s\n",
      "2025-12-09 00:32:12.262644: \n",
      "2025-12-09 00:32:12.262644: Epoch 945\n",
      "2025-12-09 00:32:12.262644: Current learning rate: 0.00074\n",
      "2025-12-09 00:34:30.539233: train_loss -0.8648\n",
      "2025-12-09 00:34:30.539233: val_loss -0.8897\n",
      "2025-12-09 00:34:30.546257: Pseudo dice [0.9316, 0.9555, 0.9553]\n",
      "2025-12-09 00:34:30.550475: Epoch time: 138.28 s\n",
      "2025-12-09 00:34:31.180989: \n",
      "2025-12-09 00:34:31.182991: Epoch 946\n",
      "2025-12-09 00:34:31.187928: Current learning rate: 0.00072\n",
      "2025-12-09 00:36:49.151145: train_loss -0.8601\n",
      "2025-12-09 00:36:49.151145: val_loss -0.8814\n",
      "2025-12-09 00:36:49.170823: Pseudo dice [0.9291, 0.9557, 0.9525]\n",
      "2025-12-09 00:36:49.178832: Epoch time: 137.97 s\n",
      "2025-12-09 00:36:49.809514: \n",
      "2025-12-09 00:36:49.811516: Epoch 947\n",
      "2025-12-09 00:36:49.817551: Current learning rate: 0.00071\n",
      "2025-12-09 00:39:07.989095: train_loss -0.8619\n",
      "2025-12-09 00:39:07.989095: val_loss -0.8903\n",
      "2025-12-09 00:39:07.989095: Pseudo dice [0.9319, 0.9603, 0.9518]\n",
      "2025-12-09 00:39:08.004878: Epoch time: 138.18 s\n",
      "2025-12-09 00:39:08.628475: \n",
      "2025-12-09 00:39:08.628475: Epoch 948\n",
      "2025-12-09 00:39:08.628475: Current learning rate: 0.0007\n",
      "2025-12-09 00:41:26.635899: train_loss -0.8655\n",
      "2025-12-09 00:41:26.635899: val_loss -0.8798\n",
      "2025-12-09 00:41:26.643907: Pseudo dice [0.9246, 0.9542, 0.9531]\n",
      "2025-12-09 00:41:26.649913: Epoch time: 138.01 s\n",
      "2025-12-09 00:41:27.295447: \n",
      "2025-12-09 00:41:27.297450: Epoch 949\n",
      "2025-12-09 00:41:27.297450: Current learning rate: 0.00069\n",
      "2025-12-09 00:43:45.249300: train_loss -0.8662\n",
      "2025-12-09 00:43:45.249300: val_loss -0.8795\n",
      "2025-12-09 00:43:45.259310: Pseudo dice [0.9253, 0.9543, 0.9488]\n",
      "2025-12-09 00:43:45.265055: Epoch time: 137.95 s\n",
      "2025-12-09 00:43:46.405988: \n",
      "2025-12-09 00:43:46.407990: Epoch 950\n",
      "2025-12-09 00:43:46.414438: Current learning rate: 0.00067\n",
      "2025-12-09 00:46:04.558399: train_loss -0.8659\n",
      "2025-12-09 00:46:04.558399: val_loss -0.8762\n",
      "2025-12-09 00:46:04.566364: Pseudo dice [0.921, 0.9525, 0.951]\n",
      "2025-12-09 00:46:04.572370: Epoch time: 138.15 s\n",
      "2025-12-09 00:46:05.240777: \n",
      "2025-12-09 00:46:05.240777: Epoch 951\n",
      "2025-12-09 00:46:05.240777: Current learning rate: 0.00066\n",
      "2025-12-09 00:48:23.652069: train_loss -0.863\n",
      "2025-12-09 00:48:23.654071: val_loss -0.881\n",
      "2025-12-09 00:48:23.659577: Pseudo dice [0.9242, 0.9544, 0.9544]\n",
      "2025-12-09 00:48:23.659577: Epoch time: 138.41 s\n",
      "2025-12-09 00:48:24.307471: \n",
      "2025-12-09 00:48:24.307471: Epoch 952\n",
      "2025-12-09 00:48:24.323316: Current learning rate: 0.00065\n",
      "2025-12-09 00:50:42.407940: train_loss -0.8602\n",
      "2025-12-09 00:50:42.409942: val_loss -0.8828\n",
      "2025-12-09 00:50:42.415507: Pseudo dice [0.9266, 0.9568, 0.9534]\n",
      "2025-12-09 00:50:42.421512: Epoch time: 138.1 s\n",
      "2025-12-09 00:50:43.084547: \n",
      "2025-12-09 00:50:43.084547: Epoch 953\n",
      "2025-12-09 00:50:43.096294: Current learning rate: 0.00064\n",
      "2025-12-09 00:53:01.278290: train_loss -0.8608\n",
      "2025-12-09 00:53:01.278290: val_loss -0.8875\n",
      "2025-12-09 00:53:01.288038: Pseudo dice [0.9301, 0.9565, 0.9592]\n",
      "2025-12-09 00:53:01.292042: Epoch time: 138.19 s\n",
      "2025-12-09 00:53:02.010962: \n",
      "2025-12-09 00:53:02.010962: Epoch 954\n",
      "2025-12-09 00:53:02.026595: Current learning rate: 0.00063\n",
      "2025-12-09 00:55:20.077012: train_loss -0.8644\n",
      "2025-12-09 00:55:20.077012: val_loss -0.8822\n",
      "2025-12-09 00:55:20.092705: Pseudo dice [0.9266, 0.9582, 0.9539]\n",
      "2025-12-09 00:55:20.092705: Epoch time: 138.07 s\n",
      "2025-12-09 00:55:20.798968: \n",
      "2025-12-09 00:55:20.798968: Epoch 955\n",
      "2025-12-09 00:55:20.803137: Current learning rate: 0.00061\n",
      "2025-12-09 00:57:38.984826: train_loss -0.8656\n",
      "2025-12-09 00:57:38.984826: val_loss -0.8819\n",
      "2025-12-09 00:57:38.994449: Pseudo dice [0.9287, 0.9571, 0.9491]\n",
      "2025-12-09 00:57:39.002457: Epoch time: 138.19 s\n",
      "2025-12-09 00:57:39.892729: \n",
      "2025-12-09 00:57:39.892729: Epoch 956\n",
      "2025-12-09 00:57:39.892729: Current learning rate: 0.0006\n",
      "2025-12-09 00:59:58.071404: train_loss -0.8603\n",
      "2025-12-09 00:59:58.071404: val_loss -0.8862\n",
      "2025-12-09 00:59:58.085416: Pseudo dice [0.9279, 0.9565, 0.9524]\n",
      "2025-12-09 00:59:58.091020: Epoch time: 138.18 s\n",
      "2025-12-09 00:59:58.719701: \n",
      "2025-12-09 00:59:58.719701: Epoch 957\n",
      "2025-12-09 00:59:58.735532: Current learning rate: 0.00059\n",
      "2025-12-09 01:02:16.957267: train_loss -0.8643\n",
      "2025-12-09 01:02:16.972902: val_loss -0.885\n",
      "2025-12-09 01:02:16.978910: Pseudo dice [0.9301, 0.9585, 0.9503]\n",
      "2025-12-09 01:02:16.984916: Epoch time: 138.24 s\n",
      "2025-12-09 01:02:17.619654: \n",
      "2025-12-09 01:02:17.619654: Epoch 958\n",
      "2025-12-09 01:02:17.619654: Current learning rate: 0.00058\n",
      "2025-12-09 01:04:35.755251: train_loss -0.8652\n",
      "2025-12-09 01:04:35.757253: val_loss -0.8845\n",
      "2025-12-09 01:04:35.764808: Pseudo dice [0.9273, 0.9579, 0.9528]\n",
      "2025-12-09 01:04:35.770814: Epoch time: 138.14 s\n",
      "2025-12-09 01:04:36.498950: \n",
      "2025-12-09 01:04:36.498950: Epoch 959\n",
      "2025-12-09 01:04:36.505442: Current learning rate: 0.00056\n",
      "2025-12-09 01:06:54.695382: train_loss -0.8645\n",
      "2025-12-09 01:06:54.695382: val_loss -0.8847\n",
      "2025-12-09 01:06:54.695382: Pseudo dice [0.9288, 0.9594, 0.9541]\n",
      "2025-12-09 01:06:54.707583: Epoch time: 138.2 s\n",
      "2025-12-09 01:06:55.337204: \n",
      "2025-12-09 01:06:55.337204: Epoch 960\n",
      "2025-12-09 01:06:55.337204: Current learning rate: 0.00055\n",
      "2025-12-09 01:09:13.858275: train_loss -0.8634\n",
      "2025-12-09 01:09:13.860277: val_loss -0.8915\n",
      "2025-12-09 01:09:13.862279: Pseudo dice [0.9342, 0.9598, 0.9548]\n",
      "2025-12-09 01:09:13.862279: Epoch time: 138.52 s\n",
      "2025-12-09 01:09:14.621254: \n",
      "2025-12-09 01:09:14.621254: Epoch 961\n",
      "2025-12-09 01:09:14.621254: Current learning rate: 0.00054\n",
      "2025-12-09 01:11:32.862940: train_loss -0.8607\n",
      "2025-12-09 01:11:32.862940: val_loss -0.8803\n",
      "2025-12-09 01:11:32.862940: Pseudo dice [0.9255, 0.9584, 0.9511]\n",
      "2025-12-09 01:11:32.878955: Epoch time: 138.24 s\n",
      "2025-12-09 01:11:33.512436: \n",
      "2025-12-09 01:11:33.512436: Epoch 962\n",
      "2025-12-09 01:11:33.512436: Current learning rate: 0.00053\n",
      "2025-12-09 01:13:51.594019: train_loss -0.8654\n",
      "2025-12-09 01:13:51.594019: val_loss -0.8851\n",
      "2025-12-09 01:13:51.609855: Pseudo dice [0.9314, 0.9564, 0.9477]\n",
      "2025-12-09 01:13:51.609855: Epoch time: 138.08 s\n",
      "2025-12-09 01:13:52.416860: \n",
      "2025-12-09 01:13:52.416860: Epoch 963\n",
      "2025-12-09 01:13:52.432760: Current learning rate: 0.00051\n",
      "2025-12-09 01:16:10.649755: train_loss -0.864\n",
      "2025-12-09 01:16:10.649755: val_loss -0.8841\n",
      "2025-12-09 01:16:10.665553: Pseudo dice [0.9272, 0.9602, 0.9503]\n",
      "2025-12-09 01:16:10.669557: Epoch time: 138.23 s\n",
      "2025-12-09 01:16:11.305384: \n",
      "2025-12-09 01:16:11.307389: Epoch 964\n",
      "2025-12-09 01:16:11.314831: Current learning rate: 0.0005\n",
      "2025-12-09 01:18:29.386487: train_loss -0.8616\n",
      "2025-12-09 01:18:29.388228: val_loss -0.8833\n",
      "2025-12-09 01:18:29.388228: Pseudo dice [0.93, 0.9581, 0.9526]\n",
      "2025-12-09 01:18:29.401474: Epoch time: 138.08 s\n",
      "2025-12-09 01:18:30.097550: \n",
      "2025-12-09 01:18:30.097550: Epoch 965\n",
      "2025-12-09 01:18:30.097550: Current learning rate: 0.00049\n",
      "2025-12-09 01:20:48.279490: train_loss -0.8656\n",
      "2025-12-09 01:20:48.279490: val_loss -0.883\n",
      "2025-12-09 01:20:48.293246: Pseudo dice [0.9264, 0.9606, 0.9478]\n",
      "2025-12-09 01:20:48.300995: Epoch time: 138.18 s\n",
      "2025-12-09 01:20:48.931604: \n",
      "2025-12-09 01:20:48.931604: Epoch 966\n",
      "2025-12-09 01:20:48.947270: Current learning rate: 0.00048\n",
      "2025-12-09 01:23:07.049186: train_loss -0.8659\n",
      "2025-12-09 01:23:07.049186: val_loss -0.8913\n",
      "2025-12-09 01:23:07.056933: Pseudo dice [0.9331, 0.9608, 0.9506]\n",
      "2025-12-09 01:23:07.064479: Epoch time: 138.12 s\n",
      "2025-12-09 01:23:07.802176: \n",
      "2025-12-09 01:23:07.802176: Epoch 967\n",
      "2025-12-09 01:23:07.802176: Current learning rate: 0.00046\n",
      "2025-12-09 01:25:25.862397: train_loss -0.8677\n",
      "2025-12-09 01:25:25.864398: val_loss -0.8843\n",
      "2025-12-09 01:25:25.870404: Pseudo dice [0.9261, 0.9565, 0.9521]\n",
      "2025-12-09 01:25:25.877412: Epoch time: 138.06 s\n",
      "2025-12-09 01:25:26.567099: \n",
      "2025-12-09 01:25:26.567099: Epoch 968\n",
      "2025-12-09 01:25:26.567099: Current learning rate: 0.00045\n",
      "2025-12-09 01:27:44.668184: train_loss -0.8645\n",
      "2025-12-09 01:27:44.670187: val_loss -0.8844\n",
      "2025-12-09 01:27:44.677935: Pseudo dice [0.9284, 0.9582, 0.9485]\n",
      "2025-12-09 01:27:44.682157: Epoch time: 138.11 s\n",
      "2025-12-09 01:27:45.532036: \n",
      "2025-12-09 01:27:45.532036: Epoch 969\n",
      "2025-12-09 01:27:45.532036: Current learning rate: 0.00044\n",
      "2025-12-09 01:30:03.694713: train_loss -0.8636\n",
      "2025-12-09 01:30:03.694713: val_loss -0.8784\n",
      "2025-12-09 01:30:03.694713: Pseudo dice [0.922, 0.9575, 0.9535]\n",
      "2025-12-09 01:30:03.710473: Epoch time: 138.18 s\n",
      "2025-12-09 01:30:04.392874: \n",
      "2025-12-09 01:30:04.392874: Epoch 970\n",
      "2025-12-09 01:30:04.408733: Current learning rate: 0.00043\n",
      "2025-12-09 01:32:22.729078: train_loss -0.8597\n",
      "2025-12-09 01:32:22.729078: val_loss -0.8875\n",
      "2025-12-09 01:32:22.739089: Pseudo dice [0.93, 0.9588, 0.955]\n",
      "2025-12-09 01:32:22.741091: Epoch time: 138.34 s\n",
      "2025-12-09 01:32:23.476565: \n",
      "2025-12-09 01:32:23.476565: Epoch 971\n",
      "2025-12-09 01:32:23.476565: Current learning rate: 0.00041\n",
      "2025-12-09 01:34:41.579323: train_loss -0.8634\n",
      "2025-12-09 01:34:41.579323: val_loss -0.8851\n",
      "2025-12-09 01:34:41.587331: Pseudo dice [0.929, 0.9595, 0.9517]\n",
      "2025-12-09 01:34:41.592813: Epoch time: 138.1 s\n",
      "2025-12-09 01:34:42.288915: \n",
      "2025-12-09 01:34:42.288915: Epoch 972\n",
      "2025-12-09 01:34:42.306998: Current learning rate: 0.0004\n",
      "2025-12-09 01:37:00.478932: train_loss -0.8634\n",
      "2025-12-09 01:37:00.478932: val_loss -0.8842\n",
      "2025-12-09 01:37:00.494721: Pseudo dice [0.9262, 0.9599, 0.9526]\n",
      "2025-12-09 01:37:00.494721: Epoch time: 138.19 s\n",
      "2025-12-09 01:37:01.207094: \n",
      "2025-12-09 01:37:01.207094: Epoch 973\n",
      "2025-12-09 01:37:01.223079: Current learning rate: 0.00039\n",
      "2025-12-09 01:39:19.627141: train_loss -0.8657\n",
      "2025-12-09 01:39:19.634888: val_loss -0.8892\n",
      "2025-12-09 01:39:19.640894: Pseudo dice [0.9302, 0.9607, 0.9543]\n",
      "2025-12-09 01:39:19.646900: Epoch time: 138.42 s\n",
      "2025-12-09 01:39:20.271695: \n",
      "2025-12-09 01:39:20.271695: Epoch 974\n",
      "2025-12-09 01:39:20.287609: Current learning rate: 0.00037\n",
      "2025-12-09 01:41:38.321416: train_loss -0.8645\n",
      "2025-12-09 01:41:38.321416: val_loss -0.8838\n",
      "2025-12-09 01:41:38.337166: Pseudo dice [0.9291, 0.9596, 0.9488]\n",
      "2025-12-09 01:41:38.337166: Epoch time: 138.05 s\n",
      "2025-12-09 01:41:39.129686: \n",
      "2025-12-09 01:41:39.145750: Epoch 975\n",
      "2025-12-09 01:41:39.145750: Current learning rate: 0.00036\n",
      "2025-12-09 01:43:57.420134: train_loss -0.8616\n",
      "2025-12-09 01:43:57.420134: val_loss -0.8872\n",
      "2025-12-09 01:43:57.435943: Pseudo dice [0.9289, 0.9576, 0.953]\n",
      "2025-12-09 01:43:57.440474: Epoch time: 138.29 s\n",
      "2025-12-09 01:43:58.150448: \n",
      "2025-12-09 01:43:58.150448: Epoch 976\n",
      "2025-12-09 01:43:58.166267: Current learning rate: 0.00035\n",
      "2025-12-09 01:46:16.406887: train_loss -0.8606\n",
      "2025-12-09 01:46:16.406887: val_loss -0.8971\n",
      "2025-12-09 01:46:16.422758: Pseudo dice [0.9366, 0.9638, 0.9549]\n",
      "2025-12-09 01:46:16.424760: Epoch time: 138.26 s\n",
      "2025-12-09 01:46:16.430976: Yayy! New best EMA pseudo Dice: 0.9468\n",
      "2025-12-09 01:46:17.390983: \n",
      "2025-12-09 01:46:17.390983: Epoch 977\n",
      "2025-12-09 01:46:17.406730: Current learning rate: 0.00034\n",
      "2025-12-09 01:48:35.605344: train_loss -0.8655\n",
      "2025-12-09 01:48:35.605344: val_loss -0.8892\n",
      "2025-12-09 01:48:35.613089: Pseudo dice [0.9327, 0.9629, 0.9482]\n",
      "2025-12-09 01:48:35.613089: Epoch time: 138.21 s\n",
      "2025-12-09 01:48:35.621944: Yayy! New best EMA pseudo Dice: 0.9469\n",
      "2025-12-09 01:48:36.623237: \n",
      "2025-12-09 01:48:36.623237: Epoch 978\n",
      "2025-12-09 01:48:36.639202: Current learning rate: 0.00032\n",
      "2025-12-09 01:50:54.869846: train_loss -0.8614\n",
      "2025-12-09 01:50:54.871848: val_loss -0.8886\n",
      "2025-12-09 01:50:54.879596: Pseudo dice [0.9313, 0.9583, 0.9514]\n",
      "2025-12-09 01:50:54.887605: Epoch time: 138.25 s\n",
      "2025-12-09 01:50:54.893351: Yayy! New best EMA pseudo Dice: 0.9469\n",
      "2025-12-09 01:50:55.823470: \n",
      "2025-12-09 01:50:55.823470: Epoch 979\n",
      "2025-12-09 01:50:55.841502: Current learning rate: 0.00031\n",
      "2025-12-09 01:53:14.038652: train_loss -0.8684\n",
      "2025-12-09 01:53:14.038652: val_loss -0.8924\n",
      "2025-12-09 01:53:14.042658: Pseudo dice [0.9344, 0.9618, 0.9537]\n",
      "2025-12-09 01:53:14.049202: Epoch time: 138.22 s\n",
      "2025-12-09 01:53:14.058419: Yayy! New best EMA pseudo Dice: 0.9472\n",
      "2025-12-09 01:53:15.230993: \n",
      "2025-12-09 01:53:15.230993: Epoch 980\n",
      "2025-12-09 01:53:15.230993: Current learning rate: 0.0003\n",
      "2025-12-09 01:55:33.594475: train_loss -0.8619\n",
      "2025-12-09 01:55:33.594475: val_loss -0.8923\n",
      "2025-12-09 01:55:33.612240: Pseudo dice [0.9336, 0.9598, 0.9545]\n",
      "2025-12-09 01:55:33.612240: Epoch time: 138.36 s\n",
      "2025-12-09 01:55:33.626110: Yayy! New best EMA pseudo Dice: 0.9474\n",
      "2025-12-09 01:55:34.547577: \n",
      "2025-12-09 01:55:34.547577: Epoch 981\n",
      "2025-12-09 01:55:34.547577: Current learning rate: 0.00028\n",
      "2025-12-09 01:57:52.868595: train_loss -0.8649\n",
      "2025-12-09 01:57:52.868595: val_loss -0.8866\n",
      "2025-12-09 01:57:52.884368: Pseudo dice [0.928, 0.9586, 0.9529]\n",
      "2025-12-09 01:57:52.884368: Epoch time: 138.32 s\n",
      "2025-12-09 01:57:53.567270: \n",
      "2025-12-09 01:57:53.567270: Epoch 982\n",
      "2025-12-09 01:57:53.567270: Current learning rate: 0.00027\n",
      "2025-12-09 02:00:11.860437: train_loss -0.8639\n",
      "2025-12-09 02:00:11.862439: val_loss -0.8811\n",
      "2025-12-09 02:00:11.867587: Pseudo dice [0.9248, 0.9565, 0.9526]\n",
      "2025-12-09 02:00:11.872750: Epoch time: 138.29 s\n",
      "2025-12-09 02:00:12.609561: \n",
      "2025-12-09 02:00:12.609561: Epoch 983\n",
      "2025-12-09 02:00:12.625496: Current learning rate: 0.00026\n",
      "2025-12-09 02:02:30.793948: train_loss -0.8661\n",
      "2025-12-09 02:02:30.793948: val_loss -0.885\n",
      "2025-12-09 02:02:30.793948: Pseudo dice [0.9284, 0.962, 0.9508]\n",
      "2025-12-09 02:02:30.809686: Epoch time: 138.18 s\n",
      "2025-12-09 02:02:31.475857: \n",
      "2025-12-09 02:02:31.475857: Epoch 984\n",
      "2025-12-09 02:02:31.491630: Current learning rate: 0.00024\n",
      "2025-12-09 02:04:49.883636: train_loss -0.8562\n",
      "2025-12-09 02:04:49.883636: val_loss -0.8827\n",
      "2025-12-09 02:04:49.899409: Pseudo dice [0.9242, 0.9534, 0.9555]\n",
      "2025-12-09 02:04:49.899409: Epoch time: 138.41 s\n",
      "2025-12-09 02:04:50.616789: \n",
      "2025-12-09 02:04:50.616789: Epoch 985\n",
      "2025-12-09 02:04:50.616789: Current learning rate: 0.00023\n",
      "2025-12-09 02:07:08.865412: train_loss -0.866\n",
      "2025-12-09 02:07:08.865412: val_loss -0.8907\n",
      "2025-12-09 02:07:08.873179: Pseudo dice [0.9312, 0.9588, 0.9569]\n",
      "2025-12-09 02:07:08.881446: Epoch time: 138.25 s\n",
      "2025-12-09 02:07:09.567440: \n",
      "2025-12-09 02:07:09.567440: Epoch 986\n",
      "2025-12-09 02:07:09.585062: Current learning rate: 0.00021\n",
      "2025-12-09 02:09:27.925254: train_loss -0.8646\n",
      "2025-12-09 02:09:27.925254: val_loss -0.8779\n",
      "2025-12-09 02:09:27.927256: Pseudo dice [0.923, 0.9559, 0.9507]\n",
      "2025-12-09 02:09:27.935135: Epoch time: 138.36 s\n",
      "2025-12-09 02:09:28.844705: \n",
      "2025-12-09 02:09:28.844705: Epoch 987\n",
      "2025-12-09 02:09:28.860594: Current learning rate: 0.0002\n",
      "2025-12-09 02:11:47.059606: train_loss -0.8686\n",
      "2025-12-09 02:11:47.059606: val_loss -0.8871\n",
      "2025-12-09 02:11:47.065350: Pseudo dice [0.9316, 0.9597, 0.9486]\n",
      "2025-12-09 02:11:47.069172: Epoch time: 138.21 s\n",
      "2025-12-09 02:11:47.694001: \n",
      "2025-12-09 02:11:47.694001: Epoch 988\n",
      "2025-12-09 02:11:47.709911: Current learning rate: 0.00019\n",
      "2025-12-09 02:14:05.961917: train_loss -0.8617\n",
      "2025-12-09 02:14:05.961917: val_loss -0.8821\n",
      "2025-12-09 02:14:05.977945: Pseudo dice [0.9267, 0.959, 0.9509]\n",
      "2025-12-09 02:14:05.977945: Epoch time: 138.27 s\n",
      "2025-12-09 02:14:06.770595: \n",
      "2025-12-09 02:14:06.770595: Epoch 989\n",
      "2025-12-09 02:14:06.770595: Current learning rate: 0.00017\n",
      "2025-12-09 02:16:24.882771: train_loss -0.8636\n",
      "2025-12-09 02:16:24.882771: val_loss -0.883\n",
      "2025-12-09 02:16:24.898553: Pseudo dice [0.9285, 0.9587, 0.9478]\n",
      "2025-12-09 02:16:24.902525: Epoch time: 138.11 s\n",
      "2025-12-09 02:16:25.545879: \n",
      "2025-12-09 02:16:25.545879: Epoch 990\n",
      "2025-12-09 02:16:25.545879: Current learning rate: 0.00016\n",
      "2025-12-09 02:18:43.854167: train_loss -0.8622\n",
      "2025-12-09 02:18:43.854167: val_loss -0.8911\n",
      "2025-12-09 02:18:43.869926: Pseudo dice [0.934, 0.9606, 0.9545]\n",
      "2025-12-09 02:18:43.869926: Epoch time: 138.31 s\n",
      "2025-12-09 02:18:44.595631: \n",
      "2025-12-09 02:18:44.595631: Epoch 991\n",
      "2025-12-09 02:18:44.598637: Current learning rate: 0.00014\n",
      "2025-12-09 02:21:02.719771: train_loss -0.8668\n",
      "2025-12-09 02:21:02.719771: val_loss -0.8909\n",
      "2025-12-09 02:21:02.719771: Pseudo dice [0.9321, 0.9624, 0.9555]\n",
      "2025-12-09 02:21:02.735515: Epoch time: 138.13 s\n",
      "2025-12-09 02:21:03.467211: \n",
      "2025-12-09 02:21:03.467211: Epoch 992\n",
      "2025-12-09 02:21:03.481219: Current learning rate: 0.00013\n",
      "2025-12-09 02:23:21.878032: train_loss -0.8651\n",
      "2025-12-09 02:23:21.878032: val_loss -0.8922\n",
      "2025-12-09 02:23:21.889797: Pseudo dice [0.9334, 0.9602, 0.9488]\n",
      "2025-12-09 02:23:21.898426: Epoch time: 138.41 s\n",
      "2025-12-09 02:23:22.697762: \n",
      "2025-12-09 02:23:22.697762: Epoch 993\n",
      "2025-12-09 02:23:22.697762: Current learning rate: 0.00011\n",
      "2025-12-09 02:25:40.839727: train_loss -0.8682\n",
      "2025-12-09 02:25:40.839727: val_loss -0.8969\n",
      "2025-12-09 02:25:40.855485: Pseudo dice [0.9369, 0.9623, 0.9564]\n",
      "2025-12-09 02:25:40.855485: Epoch time: 138.14 s\n",
      "2025-12-09 02:25:40.871516: Yayy! New best EMA pseudo Dice: 0.9476\n",
      "2025-12-09 02:25:41.760688: \n",
      "2025-12-09 02:25:41.760688: Epoch 994\n",
      "2025-12-09 02:25:41.777892: Current learning rate: 0.0001\n",
      "2025-12-09 02:28:00.041387: train_loss -0.8618\n",
      "2025-12-09 02:28:00.041387: val_loss -0.8913\n",
      "2025-12-09 02:28:00.041387: Pseudo dice [0.9336, 0.9615, 0.954]\n",
      "2025-12-09 02:28:00.055396: Epoch time: 138.28 s\n",
      "2025-12-09 02:28:00.055396: Yayy! New best EMA pseudo Dice: 0.9478\n",
      "2025-12-09 02:28:01.196252: \n",
      "2025-12-09 02:28:01.196252: Epoch 995\n",
      "2025-12-09 02:28:01.204263: Current learning rate: 8e-05\n",
      "2025-12-09 02:30:19.399535: train_loss -0.8672\n",
      "2025-12-09 02:30:19.399535: val_loss -0.8823\n",
      "2025-12-09 02:30:19.407468: Pseudo dice [0.9281, 0.9583, 0.9505]\n",
      "2025-12-09 02:30:19.407468: Epoch time: 138.21 s\n",
      "2025-12-09 02:30:20.145894: \n",
      "2025-12-09 02:30:20.145894: Epoch 996\n",
      "2025-12-09 02:30:20.152395: Current learning rate: 7e-05\n",
      "2025-12-09 02:32:38.283765: train_loss -0.8677\n",
      "2025-12-09 02:32:38.283765: val_loss -0.8786\n",
      "2025-12-09 02:32:38.291015: Pseudo dice [0.9223, 0.9523, 0.9543]\n",
      "2025-12-09 02:32:38.298761: Epoch time: 138.14 s\n",
      "2025-12-09 02:32:39.008296: \n",
      "2025-12-09 02:32:39.008296: Epoch 997\n",
      "2025-12-09 02:32:39.008296: Current learning rate: 5e-05\n",
      "2025-12-09 02:34:57.088151: train_loss -0.8654\n",
      "2025-12-09 02:34:57.088151: val_loss -0.8792\n",
      "2025-12-09 02:34:57.096159: Pseudo dice [0.9234, 0.9565, 0.9514]\n",
      "2025-12-09 02:34:57.103905: Epoch time: 138.08 s\n",
      "2025-12-09 02:34:57.893020: \n",
      "2025-12-09 02:34:57.893020: Epoch 998\n",
      "2025-12-09 02:34:57.909064: Current learning rate: 4e-05\n",
      "2025-12-09 02:37:16.282831: train_loss -0.8659\n",
      "2025-12-09 02:37:16.282831: val_loss -0.8798\n",
      "2025-12-09 02:37:16.293190: Pseudo dice [0.925, 0.9536, 0.9517]\n",
      "2025-12-09 02:37:16.298934: Epoch time: 138.39 s\n",
      "2025-12-09 02:37:17.027145: \n",
      "2025-12-09 02:37:17.027145: Epoch 999\n",
      "2025-12-09 02:37:17.027145: Current learning rate: 2e-05\n",
      "2025-12-09 02:39:35.339341: train_loss -0.8659\n",
      "2025-12-09 02:39:35.341343: val_loss -0.8836\n",
      "2025-12-09 02:39:35.347397: Pseudo dice [0.9251, 0.958, 0.9542]\n",
      "2025-12-09 02:39:35.347397: Epoch time: 138.31 s\n",
      "2025-12-09 02:39:36.421947: Training done.\n",
      "2025-12-09 02:39:36.707715: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-09 02:39:36.713725: The split file contains 5 splits.\n",
      "2025-12-09 02:39:36.752995: Desired fold for training: 0\n",
      "2025-12-09 02:39:36.786520: This split has 400 training and 100 validation cases.\n",
      "2025-12-09 02:39:36.804286: predicting OAS30014_MR_d0196_9\n",
      "2025-12-09 02:39:36.958697: OAS30014_MR_d0196_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:39:58.746167: predicting OAS30017_MR_d0054_1\n",
      "2025-12-09 02:39:58.746167: OAS30017_MR_d0054_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:40:16.451824: predicting OAS30017_MR_d0054_10\n",
      "2025-12-09 02:40:16.467570: OAS30017_MR_d0054_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:40:34.147182: predicting OAS30017_MR_d0054_9\n",
      "2025-12-09 02:40:34.169172: OAS30017_MR_d0054_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:40:51.941931: predicting OAS30025_MR_d0210_7\n",
      "2025-12-09 02:40:51.952480: OAS30025_MR_d0210_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:41:09.690891: predicting OAS30025_MR_d0210_8\n",
      "2025-12-09 02:41:09.710557: OAS30025_MR_d0210_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:41:27.399452: predicting OAS30036_MR_d0059_3\n",
      "2025-12-09 02:41:27.423056: OAS30036_MR_d0059_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:41:45.127167: predicting OAS30039_MR_d1203_1\n",
      "2025-12-09 02:41:45.142930: OAS30039_MR_d1203_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:02.818247: predicting OAS30039_MR_d1203_10\n",
      "2025-12-09 02:42:02.840054: OAS30039_MR_d1203_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:20.569011: predicting OAS30039_MR_d1203_8\n",
      "2025-12-09 02:42:20.578493: OAS30039_MR_d1203_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:38.287601: predicting OAS30052_MR_d0693_2\n",
      "2025-12-09 02:42:38.299641: OAS30052_MR_d0693_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:55.996156: predicting OAS30052_MR_d0693_5\n",
      "2025-12-09 02:42:56.016531: OAS30052_MR_d0693_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:43:13.673009: predicting OAS30052_MR_d0693_6\n",
      "2025-12-09 02:43:13.695706: OAS30052_MR_d0693_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:43:31.414482: predicting OAS30078_MR_d0210_6\n",
      "2025-12-09 02:43:31.436475: OAS30078_MR_d0210_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:43:49.117414: predicting OAS30078_MR_d0210_8\n",
      "2025-12-09 02:43:49.127428: OAS30078_MR_d0210_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:06.836894: predicting OAS30083_MR_d0465_1\n",
      "2025-12-09 02:44:06.848567: OAS30083_MR_d0465_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:24.511685: predicting OAS30087_MR_d0260_5\n",
      "2025-12-09 02:44:24.523513: OAS30087_MR_d0260_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:42.184021: predicting OAS30099_MR_d0032_1\n",
      "2025-12-09 02:44:42.193348: OAS30099_MR_d0032_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:59.885038: predicting OAS30099_MR_d0032_7\n",
      "2025-12-09 02:44:59.905079: OAS30099_MR_d0032_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:45:17.593966: predicting OAS30099_MR_d0032_9\n",
      "2025-12-09 02:45:17.609995: OAS30099_MR_d0032_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:45:35.275320: predicting OAS30102_MR_d0024_1\n",
      "2025-12-09 02:45:35.284915: OAS30102_MR_d0024_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:45:52.958901: predicting OAS30102_MR_d0024_10\n",
      "2025-12-09 02:45:52.968498: OAS30102_MR_d0024_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:46:10.679825: predicting OAS30102_MR_d0024_3\n",
      "2025-12-09 02:46:10.690679: OAS30102_MR_d0024_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:46:28.415468: predicting OAS30104_MR_d0328_1\n",
      "2025-12-09 02:46:28.425479: OAS30104_MR_d0328_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:46:46.099731: predicting OAS30104_MR_d0328_5\n",
      "2025-12-09 02:46:46.110172: OAS30104_MR_d0328_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:03.823339: predicting OAS30107_MR_d0387_3\n",
      "2025-12-09 02:47:03.832535: OAS30107_MR_d0387_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:21.562269: predicting OAS30125_MR_d0201_1\n",
      "2025-12-09 02:47:21.572635: OAS30125_MR_d0201_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:39.250517: predicting OAS30125_MR_d0201_4\n",
      "2025-12-09 02:47:39.259403: OAS30125_MR_d0201_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:56.933176: predicting OAS30125_MR_d0201_5\n",
      "2025-12-09 02:47:56.952409: OAS30125_MR_d0201_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:48:14.668004: predicting OAS30127_MR_d0098_4\n",
      "2025-12-09 02:48:14.679119: OAS30127_MR_d0098_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:48:32.346652: predicting OAS30127_MR_d0098_8\n",
      "2025-12-09 02:48:32.357560: OAS30127_MR_d0098_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:48:50.073815: predicting OAS30134_MR_d0080_1\n",
      "2025-12-09 02:48:50.083683: OAS30134_MR_d0080_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:49:07.801308: predicting OAS30134_MR_d0080_4\n",
      "2025-12-09 02:49:07.814556: OAS30134_MR_d0080_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:49:25.501059: predicting OAS30134_MR_d0080_9\n",
      "2025-12-09 02:49:25.516108: OAS30134_MR_d0080_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:49:43.227242: predicting OAS30140_MR_d0172_3\n",
      "2025-12-09 02:49:43.239622: OAS30140_MR_d0172_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:00.957093: predicting OAS30147_MR_d0048_5\n",
      "2025-12-09 02:50:00.978920: OAS30147_MR_d0048_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:18.674383: predicting OAS30165_MR_d1763_1\n",
      "2025-12-09 02:50:18.692338: OAS30165_MR_d1763_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:36.349209: predicting OAS30165_MR_d1763_3\n",
      "2025-12-09 02:50:36.373179: OAS30165_MR_d1763_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:54.071485: predicting OAS30165_MR_d1763_4\n",
      "2025-12-09 02:50:54.083168: OAS30165_MR_d1763_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:51:11.778953: predicting OAS30165_MR_d1763_8\n",
      "2025-12-09 02:51:11.796936: OAS30165_MR_d1763_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:51:29.480866: predicting OAS30167_MR_d0111_10\n",
      "2025-12-09 02:51:29.496012: OAS30167_MR_d0111_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:51:47.201452: predicting OAS30167_MR_d0111_6\n",
      "2025-12-09 02:51:47.223147: OAS30167_MR_d0111_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:04.960795: predicting OAS30176_MR_d0000_3\n",
      "2025-12-09 02:52:04.971182: OAS30176_MR_d0000_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:22.647094: predicting OAS30195_MR_d1596_6\n",
      "2025-12-09 02:52:22.659155: OAS30195_MR_d1596_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:40.319597: predicting OAS30226_MR_d0183_1\n",
      "2025-12-09 02:52:40.342119: OAS30226_MR_d0183_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:58.008246: predicting OAS30226_MR_d0183_10\n",
      "2025-12-09 02:52:58.024188: OAS30226_MR_d0183_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:53:15.725749: predicting OAS30238_MR_d0037_1\n",
      "2025-12-09 02:53:15.738021: OAS30238_MR_d0037_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:53:33.424500: predicting OAS30238_MR_d0037_10\n",
      "2025-12-09 02:53:33.436773: OAS30238_MR_d0037_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:53:51.119259: predicting OAS30238_MR_d0037_4\n",
      "2025-12-09 02:53:51.144859: OAS30238_MR_d0037_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:54:08.852646: predicting OAS30250_MR_d0389_5\n",
      "2025-12-09 02:54:08.875051: OAS30250_MR_d0389_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:54:26.566837: predicting OAS30274_MR_d3332_10\n",
      "2025-12-09 02:54:26.586530: OAS30274_MR_d3332_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:54:44.272654: predicting OAS30274_MR_d3332_2\n",
      "2025-12-09 02:54:44.296249: OAS30274_MR_d3332_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:01.984726: predicting OAS30274_MR_d3332_4\n",
      "2025-12-09 02:55:01.994468: OAS30274_MR_d3332_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:19.661209: predicting OAS30292_MR_d0165_10\n",
      "2025-12-09 02:55:19.677269: OAS30292_MR_d0165_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:37.352799: predicting OAS30292_MR_d0165_3\n",
      "2025-12-09 02:55:37.368347: OAS30292_MR_d0165_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:55.033191: predicting OAS30292_MR_d0165_9\n",
      "2025-12-09 02:55:55.042764: OAS30292_MR_d0165_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:56:12.708555: predicting OAS30297_MR_d1712_2\n",
      "2025-12-09 02:56:12.717768: OAS30297_MR_d1712_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:56:30.434258: predicting OAS30297_MR_d1712_5\n",
      "2025-12-09 02:56:30.443326: OAS30297_MR_d1712_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:56:48.114841: predicting OAS30297_MR_d1712_8\n",
      "2025-12-09 02:56:48.124856: OAS30297_MR_d1712_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:05.801759: predicting OAS30297_MR_d1712_9\n",
      "2025-12-09 02:57:05.816847: OAS30297_MR_d1712_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:23.525357: predicting OAS30300_MR_d0100_3\n",
      "2025-12-09 02:57:23.537528: OAS30300_MR_d0100_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:41.264589: predicting OAS30300_MR_d0100_4\n",
      "2025-12-09 02:57:41.274297: OAS30300_MR_d0100_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:58.972664: predicting OAS30302_MR_d0262_1\n",
      "2025-12-09 02:57:58.980417: OAS30302_MR_d0262_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:58:16.658774: predicting OAS30302_MR_d0262_3\n",
      "2025-12-09 02:58:16.669575: OAS30302_MR_d0262_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:58:34.324687: predicting OAS30306_MR_d0028_2\n",
      "2025-12-09 02:58:34.342397: OAS30306_MR_d0028_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:58:52.040785: predicting OAS30321_MR_d3003_2\n",
      "2025-12-09 02:58:52.050913: OAS30321_MR_d3003_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:59:09.753227: predicting OAS30325_MR_d0032_2\n",
      "2025-12-09 02:59:09.774864: OAS30325_MR_d0032_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:59:27.504108: predicting OAS30343_MR_d4178_2\n",
      "2025-12-09 02:59:27.523606: OAS30343_MR_d4178_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:59:45.213475: predicting OAS30343_MR_d4178_6\n",
      "2025-12-09 02:59:45.233810: OAS30343_MR_d4178_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:02.922451: predicting OAS30349_MR_d0699_10\n",
      "2025-12-09 03:00:02.937356: OAS30349_MR_d0699_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:20.635502: predicting OAS30349_MR_d0699_2\n",
      "2025-12-09 03:00:20.657226: OAS30349_MR_d0699_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:38.363477: predicting OAS30349_MR_d0699_3\n",
      "2025-12-09 03:00:38.374452: OAS30349_MR_d0699_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:56.074534: predicting OAS30349_MR_d0699_5\n",
      "2025-12-09 03:00:56.080540: OAS30349_MR_d0699_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:01:13.807286: predicting OAS30349_MR_d0699_7\n",
      "2025-12-09 03:01:13.821149: OAS30349_MR_d0699_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:01:31.540524: predicting OAS30350_MR_d0018_2\n",
      "2025-12-09 03:01:31.550373: OAS30350_MR_d0018_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:01:49.260570: predicting OAS30352_MR_d0099_10\n",
      "2025-12-09 03:01:49.280241: OAS30352_MR_d0099_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:02:06.984650: predicting OAS30354_MR_d0056_10\n",
      "2025-12-09 03:02:07.004657: OAS30354_MR_d0056_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:02:24.729280: predicting OAS30354_MR_d0056_3\n",
      "2025-12-09 03:02:24.734508: OAS30354_MR_d0056_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:02:42.397912: predicting OAS30354_MR_d0056_5\n",
      "2025-12-09 03:02:42.408187: OAS30354_MR_d0056_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:00.097838: predicting OAS30355_MR_d0048_1\n",
      "2025-12-09 03:03:00.107090: OAS30355_MR_d0048_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:17.825193: predicting OAS30355_MR_d0048_7\n",
      "2025-12-09 03:03:17.835208: OAS30355_MR_d0048_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:35.542667: predicting OAS30355_MR_d0048_8\n",
      "2025-12-09 03:03:35.554377: OAS30355_MR_d0048_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:53.239689: predicting OAS30361_MR_d1457_1\n",
      "2025-12-09 03:03:53.261682: OAS30361_MR_d1457_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:04:10.946823: predicting OAS30361_MR_d1457_4\n",
      "2025-12-09 03:04:10.966559: OAS30361_MR_d1457_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:04:28.656093: predicting OAS30361_MR_d1457_8\n",
      "2025-12-09 03:04:28.670427: OAS30361_MR_d1457_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:04:46.365697: predicting OAS30369_MR_d4058_1\n",
      "2025-12-09 03:04:46.376242: OAS30369_MR_d4058_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:04.079599: predicting OAS30371_MR_d0338_6\n",
      "2025-12-09 03:05:04.087613: OAS30371_MR_d0338_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:21.779699: predicting OAS30373_MR_d1211_10\n",
      "2025-12-09 03:05:21.795347: OAS30373_MR_d1211_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:39.535772: predicting OAS30373_MR_d1211_2\n",
      "2025-12-09 03:05:39.548678: OAS30373_MR_d1211_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:57.254274: predicting OAS30373_MR_d1211_9\n",
      "2025-12-09 03:05:57.278089: OAS30373_MR_d1211_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:06:14.990406: predicting OAS30379_MR_d2106_1\n",
      "2025-12-09 03:06:15.008123: OAS30379_MR_d2106_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:06:32.709626: predicting OAS30379_MR_d2106_10\n",
      "2025-12-09 03:06:32.727318: OAS30379_MR_d2106_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:06:50.425219: predicting OAS30379_MR_d2106_4\n",
      "2025-12-09 03:06:50.440092: OAS30379_MR_d2106_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:07:08.142959: predicting OAS30379_MR_d2106_6\n",
      "2025-12-09 03:07:08.158762: OAS30379_MR_d2106_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:07:25.858183: predicting OAS30379_MR_d2106_9\n",
      "2025-12-09 03:07:25.871461: OAS30379_MR_d2106_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:07:43.561249: predicting OAS30380_MR_d3446_4\n",
      "2025-12-09 03:07:43.570802: OAS30380_MR_d3446_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:01.243198: predicting OAS30383_MR_d0134_1\n",
      "2025-12-09 03:08:01.260804: OAS30383_MR_d0134_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:18.927143: predicting OAS30383_MR_d0134_10\n",
      "2025-12-09 03:08:18.939142: OAS30383_MR_d0134_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:36.650287: predicting OAS30388_MR_d0073_6\n",
      "2025-12-09 03:08:36.662050: OAS30388_MR_d0073_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:54.386204: predicting OAS30388_MR_d0073_7\n",
      "2025-12-09 03:08:54.399407: OAS30388_MR_d0073_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:09:37.678043: Validation complete\n",
      "2025-12-09 03:09:37.678043: Mean Validation Dice:  0.9385947886337376\n"
     ]
    }
   ],
   "source": [
    "# Train nnU-net on fold 0\n",
    "!nnUNetv2_train 500 3d_lowres 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40b40f3e-32ff-439a-a854-def07ea2bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-12-16 02:53:46.199408: do_dummy_2d_data_aug: False\n",
      "2025-12-16 02:53:46.199408: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-16 02:53:46.199408: The split file contains 5 splits.\n",
      "2025-12-16 02:53:46.199408: Desired fold for training: 1\n",
      "2025-12-16 02:53:46.199408: This split has 400 training and 100 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2025-12-16 02:54:18.777400: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_lowres\n",
      " {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [202, 202, 202], 'spacing': [1.2667700813876164, 1.2667700813876164, 1.2667700813876164], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset500_MRI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [256, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.422696590423584, 'median': 0.4194243550300598, 'min': 0.0027002037968486547, 'percentile_00_5': 0.05628390982747078, 'percentile_99_5': 0.8565635681152344, 'std': 0.19347868859767914}}} \n",
      "\n",
      "2025-12-16 02:54:18.793147: unpacking dataset...\n",
      "2025-12-16 02:54:19.337305: unpacking done...\n",
      "2025-12-16 02:54:19.343313: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-12-16 02:54:19.378188: \n",
      "2025-12-16 02:54:19.378188: Epoch 0\n",
      "2025-12-16 02:54:19.378188: Current learning rate: 0.01\n",
      "2025-12-16 02:56:46.864900: train_loss 0.1324\n",
      "2025-12-16 02:56:46.864900: val_loss 0.0009\n",
      "2025-12-16 02:56:46.866903: Pseudo dice [0.521, 0.5617, 0.2047]\n",
      "2025-12-16 02:56:46.866903: Epoch time: 147.5 s\n",
      "2025-12-16 02:56:46.866903: Yayy! New best EMA pseudo Dice: 0.4291\n",
      "2025-12-16 02:56:47.881575: \n",
      "2025-12-16 02:56:47.881575: Epoch 1\n",
      "2025-12-16 02:56:47.881575: Current learning rate: 0.00999\n",
      "2025-12-16 02:59:05.779346: train_loss -0.0663\n",
      "2025-12-16 02:59:05.781348: val_loss -0.1588\n",
      "2025-12-16 02:59:05.781348: Pseudo dice [0.5946, 0.6073, 0.3849]\n",
      "2025-12-16 02:59:05.783351: Epoch time: 137.9 s\n",
      "2025-12-16 02:59:05.783351: Yayy! New best EMA pseudo Dice: 0.4391\n",
      "2025-12-16 02:59:06.818069: \n",
      "2025-12-16 02:59:06.818069: Epoch 2\n",
      "2025-12-16 02:59:06.818069: Current learning rate: 0.00998\n",
      "2025-12-16 03:01:24.552033: train_loss -0.1533\n",
      "2025-12-16 03:01:24.552033: val_loss -0.2177\n",
      "2025-12-16 03:01:24.552033: Pseudo dice [0.6011, 0.632, 0.4591]\n",
      "2025-12-16 03:01:24.552033: Epoch time: 137.74 s\n",
      "2025-12-16 03:01:24.552033: Yayy! New best EMA pseudo Dice: 0.4516\n",
      "2025-12-16 03:01:25.494250: \n",
      "2025-12-16 03:01:25.494250: Epoch 3\n",
      "2025-12-16 03:01:25.494250: Current learning rate: 0.00997\n",
      "2025-12-16 03:03:43.397291: train_loss -0.2238\n",
      "2025-12-16 03:03:43.397291: val_loss -0.2634\n",
      "2025-12-16 03:03:43.399293: Pseudo dice [0.6319, 0.6585, 0.5158]\n",
      "2025-12-16 03:03:43.399293: Epoch time: 137.91 s\n",
      "2025-12-16 03:03:43.399293: Yayy! New best EMA pseudo Dice: 0.4667\n",
      "2025-12-16 03:03:44.470751: \n",
      "2025-12-16 03:03:44.470751: Epoch 4\n",
      "2025-12-16 03:03:44.470751: Current learning rate: 0.00996\n",
      "2025-12-16 03:06:02.161813: train_loss -0.2643\n",
      "2025-12-16 03:06:02.161813: val_loss -0.3039\n",
      "2025-12-16 03:06:02.161813: Pseudo dice [0.6369, 0.7034, 0.5541]\n",
      "2025-12-16 03:06:02.161813: Epoch time: 137.69 s\n",
      "2025-12-16 03:06:02.161813: Yayy! New best EMA pseudo Dice: 0.4831\n",
      "2025-12-16 03:06:03.091055: \n",
      "2025-12-16 03:06:03.091055: Epoch 5\n",
      "2025-12-16 03:06:03.091055: Current learning rate: 0.00995\n",
      "2025-12-16 03:08:20.935762: train_loss -0.3389\n",
      "2025-12-16 03:08:20.935762: val_loss -0.3868\n",
      "2025-12-16 03:08:20.935762: Pseudo dice [0.6862, 0.7456, 0.593]\n",
      "2025-12-16 03:08:20.935762: Epoch time: 137.85 s\n",
      "2025-12-16 03:08:20.935762: Yayy! New best EMA pseudo Dice: 0.5023\n",
      "2025-12-16 03:08:21.812753: \n",
      "2025-12-16 03:08:21.812753: Epoch 6\n",
      "2025-12-16 03:08:21.812753: Current learning rate: 0.00995\n",
      "2025-12-16 03:10:40.415144: train_loss -0.3965\n",
      "2025-12-16 03:10:40.417147: val_loss -0.3925\n",
      "2025-12-16 03:10:40.419150: Pseudo dice [0.6982, 0.7601, 0.6057]\n",
      "2025-12-16 03:10:40.419150: Epoch time: 138.6 s\n",
      "2025-12-16 03:10:40.419150: Yayy! New best EMA pseudo Dice: 0.5209\n",
      "2025-12-16 03:10:41.485706: \n",
      "2025-12-16 03:10:41.485706: Epoch 7\n",
      "2025-12-16 03:10:41.485706: Current learning rate: 0.00994\n",
      "2025-12-16 03:12:59.310319: train_loss -0.4342\n",
      "2025-12-16 03:12:59.325987: val_loss -0.4591\n",
      "2025-12-16 03:12:59.327849: Pseudo dice [0.7297, 0.7986, 0.6494]\n",
      "2025-12-16 03:12:59.327849: Epoch time: 137.83 s\n",
      "2025-12-16 03:12:59.327849: Yayy! New best EMA pseudo Dice: 0.5414\n",
      "2025-12-16 03:13:00.398220: \n",
      "2025-12-16 03:13:00.398220: Epoch 8\n",
      "2025-12-16 03:13:00.398220: Current learning rate: 0.00993\n",
      "2025-12-16 03:15:18.300085: train_loss -0.4829\n",
      "2025-12-16 03:15:18.300085: val_loss -0.5115\n",
      "2025-12-16 03:15:18.300085: Pseudo dice [0.7729, 0.8152, 0.6881]\n",
      "2025-12-16 03:15:18.300085: Epoch time: 137.9 s\n",
      "2025-12-16 03:15:18.315936: Yayy! New best EMA pseudo Dice: 0.5631\n",
      "2025-12-16 03:15:19.223984: \n",
      "2025-12-16 03:15:19.223984: Epoch 9\n",
      "2025-12-16 03:15:19.223984: Current learning rate: 0.00992\n",
      "2025-12-16 03:17:36.990010: train_loss -0.5154\n",
      "2025-12-16 03:17:36.990010: val_loss -0.5261\n",
      "2025-12-16 03:17:36.991750: Pseudo dice [0.7709, 0.8353, 0.7021]\n",
      "2025-12-16 03:17:36.991750: Epoch time: 137.77 s\n",
      "2025-12-16 03:17:36.991750: Yayy! New best EMA pseudo Dice: 0.5838\n",
      "2025-12-16 03:17:38.045633: \n",
      "2025-12-16 03:17:38.045633: Epoch 10\n",
      "2025-12-16 03:17:38.049297: Current learning rate: 0.00991\n",
      "2025-12-16 03:19:55.929363: train_loss -0.5444\n",
      "2025-12-16 03:19:55.929363: val_loss -0.5362\n",
      "2025-12-16 03:19:55.940712: Pseudo dice [0.7612, 0.8415, 0.6963]\n",
      "2025-12-16 03:19:55.940712: Epoch time: 137.88 s\n",
      "2025-12-16 03:19:55.940712: Yayy! New best EMA pseudo Dice: 0.602\n",
      "2025-12-16 03:19:56.835324: \n",
      "2025-12-16 03:19:56.837326: Epoch 11\n",
      "2025-12-16 03:19:56.837326: Current learning rate: 0.0099\n",
      "2025-12-16 03:22:14.797796: train_loss -0.5535\n",
      "2025-12-16 03:22:14.797796: val_loss -0.5881\n",
      "2025-12-16 03:22:14.797796: Pseudo dice [0.8059, 0.8606, 0.7283]\n",
      "2025-12-16 03:22:14.802252: Epoch time: 137.96 s\n",
      "2025-12-16 03:22:14.802252: Yayy! New best EMA pseudo Dice: 0.6216\n",
      "2025-12-16 03:22:15.705709: \n",
      "2025-12-16 03:22:15.707450: Epoch 12\n",
      "2025-12-16 03:22:15.707450: Current learning rate: 0.00989\n",
      "2025-12-16 03:24:33.587930: train_loss -0.581\n",
      "2025-12-16 03:24:33.587930: val_loss -0.5923\n",
      "2025-12-16 03:24:33.587930: Pseudo dice [0.8074, 0.8679, 0.7414]\n",
      "2025-12-16 03:24:33.598868: Epoch time: 137.88 s\n",
      "2025-12-16 03:24:33.598868: Yayy! New best EMA pseudo Dice: 0.64\n",
      "2025-12-16 03:24:34.690760: \n",
      "2025-12-16 03:24:34.690760: Epoch 13\n",
      "2025-12-16 03:24:34.690760: Current learning rate: 0.00988\n",
      "2025-12-16 03:26:52.416308: train_loss -0.5643\n",
      "2025-12-16 03:26:52.416308: val_loss -0.5939\n",
      "2025-12-16 03:26:52.416308: Pseudo dice [0.8027, 0.8618, 0.7472]\n",
      "2025-12-16 03:26:52.416308: Epoch time: 137.73 s\n",
      "2025-12-16 03:26:52.425809: Yayy! New best EMA pseudo Dice: 0.6564\n",
      "2025-12-16 03:26:53.500653: \n",
      "2025-12-16 03:26:53.500653: Epoch 14\n",
      "2025-12-16 03:26:53.500653: Current learning rate: 0.00987\n",
      "2025-12-16 03:29:11.491453: train_loss -0.5793\n",
      "2025-12-16 03:29:11.493455: val_loss -0.6072\n",
      "2025-12-16 03:29:11.493455: Pseudo dice [0.8182, 0.8768, 0.7396]\n",
      "2025-12-16 03:29:11.493455: Epoch time: 137.99 s\n",
      "2025-12-16 03:29:11.493455: Yayy! New best EMA pseudo Dice: 0.6719\n",
      "2025-12-16 03:29:12.411987: \n",
      "2025-12-16 03:29:12.411987: Epoch 15\n",
      "2025-12-16 03:29:12.411987: Current learning rate: 0.00986\n",
      "2025-12-16 03:31:30.202300: train_loss -0.6047\n",
      "2025-12-16 03:31:30.202300: val_loss -0.6177\n",
      "2025-12-16 03:31:30.204302: Pseudo dice [0.8045, 0.858, 0.7974]\n",
      "2025-12-16 03:31:30.204302: Epoch time: 137.79 s\n",
      "2025-12-16 03:31:30.206304: Yayy! New best EMA pseudo Dice: 0.6867\n",
      "2025-12-16 03:31:31.270834: \n",
      "2025-12-16 03:31:31.270834: Epoch 16\n",
      "2025-12-16 03:31:31.270834: Current learning rate: 0.00986\n",
      "2025-12-16 03:33:49.123510: train_loss -0.6234\n",
      "2025-12-16 03:33:49.123510: val_loss -0.6317\n",
      "2025-12-16 03:33:49.123510: Pseudo dice [0.8177, 0.8732, 0.7781]\n",
      "2025-12-16 03:33:49.123510: Epoch time: 137.85 s\n",
      "2025-12-16 03:33:49.123510: Yayy! New best EMA pseudo Dice: 0.7004\n",
      "2025-12-16 03:33:50.055535: \n",
      "2025-12-16 03:33:50.055535: Epoch 17\n",
      "2025-12-16 03:33:50.055535: Current learning rate: 0.00985\n",
      "2025-12-16 03:36:07.962632: train_loss -0.6334\n",
      "2025-12-16 03:36:07.964634: val_loss -0.6217\n",
      "2025-12-16 03:36:07.964634: Pseudo dice [0.8075, 0.8651, 0.7991]\n",
      "2025-12-16 03:36:07.966637: Epoch time: 137.91 s\n",
      "2025-12-16 03:36:07.966637: Yayy! New best EMA pseudo Dice: 0.7127\n",
      "2025-12-16 03:36:08.889738: \n",
      "2025-12-16 03:36:08.891740: Epoch 18\n",
      "2025-12-16 03:36:08.891740: Current learning rate: 0.00984\n",
      "2025-12-16 03:38:26.814190: train_loss -0.6479\n",
      "2025-12-16 03:38:26.814190: val_loss -0.6543\n",
      "2025-12-16 03:38:26.816193: Pseudo dice [0.8334, 0.8905, 0.7991]\n",
      "2025-12-16 03:38:26.816193: Epoch time: 137.92 s\n",
      "2025-12-16 03:38:26.818195: Yayy! New best EMA pseudo Dice: 0.7255\n",
      "2025-12-16 03:38:28.006310: \n",
      "2025-12-16 03:38:28.006310: Epoch 19\n",
      "2025-12-16 03:38:28.019678: Current learning rate: 0.00983\n",
      "2025-12-16 03:40:45.784899: train_loss -0.6506\n",
      "2025-12-16 03:40:45.784899: val_loss -0.6687\n",
      "2025-12-16 03:40:45.784899: Pseudo dice [0.8387, 0.8889, 0.8048]\n",
      "2025-12-16 03:40:45.784899: Epoch time: 137.78 s\n",
      "2025-12-16 03:40:45.784899: Yayy! New best EMA pseudo Dice: 0.7374\n",
      "2025-12-16 03:40:46.715650: \n",
      "2025-12-16 03:40:46.715650: Epoch 20\n",
      "2025-12-16 03:40:46.715650: Current learning rate: 0.00982\n",
      "2025-12-16 03:43:04.752820: train_loss -0.6656\n",
      "2025-12-16 03:43:04.752820: val_loss -0.6827\n",
      "2025-12-16 03:43:04.752820: Pseudo dice [0.8536, 0.8966, 0.8117]\n",
      "2025-12-16 03:43:04.752820: Epoch time: 138.04 s\n",
      "2025-12-16 03:43:04.752820: Yayy! New best EMA pseudo Dice: 0.7491\n",
      "2025-12-16 03:43:05.671366: \n",
      "2025-12-16 03:43:05.671366: Epoch 21\n",
      "2025-12-16 03:43:05.671366: Current learning rate: 0.00981\n",
      "2025-12-16 03:45:23.753887: train_loss -0.6708\n",
      "2025-12-16 03:45:23.753887: val_loss -0.6754\n",
      "2025-12-16 03:45:23.753887: Pseudo dice [0.8415, 0.895, 0.8319]\n",
      "2025-12-16 03:45:23.753887: Epoch time: 138.08 s\n",
      "2025-12-16 03:45:23.753887: Yayy! New best EMA pseudo Dice: 0.7598\n",
      "2025-12-16 03:45:24.835936: \n",
      "2025-12-16 03:45:24.835936: Epoch 22\n",
      "2025-12-16 03:45:24.835936: Current learning rate: 0.0098\n",
      "2025-12-16 03:47:42.818131: train_loss -0.6618\n",
      "2025-12-16 03:47:42.820133: val_loss -0.6791\n",
      "2025-12-16 03:47:42.820133: Pseudo dice [0.8495, 0.9016, 0.814]\n",
      "2025-12-16 03:47:42.820133: Epoch time: 137.98 s\n",
      "2025-12-16 03:47:42.820133: Yayy! New best EMA pseudo Dice: 0.7693\n",
      "2025-12-16 03:47:43.707495: \n",
      "2025-12-16 03:47:43.707495: Epoch 23\n",
      "2025-12-16 03:47:43.707495: Current learning rate: 0.00979\n",
      "2025-12-16 03:50:01.600580: train_loss -0.6753\n",
      "2025-12-16 03:50:01.600580: val_loss -0.6878\n",
      "2025-12-16 03:50:01.602583: Pseudo dice [0.8453, 0.8927, 0.8341]\n",
      "2025-12-16 03:50:01.602583: Epoch time: 137.89 s\n",
      "2025-12-16 03:50:01.602583: Yayy! New best EMA pseudo Dice: 0.7781\n",
      "2025-12-16 03:50:02.465817: \n",
      "2025-12-16 03:50:02.465817: Epoch 24\n",
      "2025-12-16 03:50:02.465817: Current learning rate: 0.00978\n",
      "2025-12-16 03:52:20.395366: train_loss -0.6849\n",
      "2025-12-16 03:52:20.395366: val_loss -0.7024\n",
      "2025-12-16 03:52:20.395366: Pseudo dice [0.8578, 0.9041, 0.8431]\n",
      "2025-12-16 03:52:20.395366: Epoch time: 137.93 s\n",
      "2025-12-16 03:52:20.398922: Yayy! New best EMA pseudo Dice: 0.7871\n",
      "2025-12-16 03:52:21.555968: \n",
      "2025-12-16 03:52:21.555968: Epoch 25\n",
      "2025-12-16 03:52:21.555968: Current learning rate: 0.00977\n",
      "2025-12-16 03:54:39.530111: train_loss -0.6904\n",
      "2025-12-16 03:54:39.530111: val_loss -0.6975\n",
      "2025-12-16 03:54:39.530111: Pseudo dice [0.8452, 0.9023, 0.8351]\n",
      "2025-12-16 03:54:39.530111: Epoch time: 137.97 s\n",
      "2025-12-16 03:54:39.545796: Yayy! New best EMA pseudo Dice: 0.7945\n",
      "2025-12-16 03:54:40.403024: \n",
      "2025-12-16 03:54:40.403024: Epoch 26\n",
      "2025-12-16 03:54:40.403024: Current learning rate: 0.00977\n",
      "2025-12-16 03:56:58.289611: train_loss -0.699\n",
      "2025-12-16 03:56:58.291613: val_loss -0.7162\n",
      "2025-12-16 03:56:58.293615: Pseudo dice [0.8618, 0.9112, 0.85]\n",
      "2025-12-16 03:56:58.293615: Epoch time: 137.89 s\n",
      "2025-12-16 03:56:58.295618: Yayy! New best EMA pseudo Dice: 0.8025\n",
      "2025-12-16 03:56:59.165680: \n",
      "2025-12-16 03:56:59.167683: Epoch 27\n",
      "2025-12-16 03:56:59.167683: Current learning rate: 0.00976\n",
      "2025-12-16 03:59:17.105994: train_loss -0.6938\n",
      "2025-12-16 03:59:17.105994: val_loss -0.7138\n",
      "2025-12-16 03:59:17.105994: Pseudo dice [0.8601, 0.9084, 0.8349]\n",
      "2025-12-16 03:59:17.105994: Epoch time: 137.94 s\n",
      "2025-12-16 03:59:17.105994: Yayy! New best EMA pseudo Dice: 0.809\n",
      "2025-12-16 03:59:18.084361: \n",
      "2025-12-16 03:59:18.084361: Epoch 28\n",
      "2025-12-16 03:59:18.084361: Current learning rate: 0.00975\n",
      "2025-12-16 04:01:36.120235: train_loss -0.7018\n",
      "2025-12-16 04:01:36.120235: val_loss -0.703\n",
      "2025-12-16 04:01:36.120235: Pseudo dice [0.8504, 0.8997, 0.8399]\n",
      "2025-12-16 04:01:36.120235: Epoch time: 138.04 s\n",
      "2025-12-16 04:01:36.120235: Yayy! New best EMA pseudo Dice: 0.8144\n",
      "2025-12-16 04:01:37.008708: \n",
      "2025-12-16 04:01:37.010449: Epoch 29\n",
      "2025-12-16 04:01:37.010449: Current learning rate: 0.00974\n",
      "2025-12-16 04:03:54.887595: train_loss -0.7032\n",
      "2025-12-16 04:03:54.887595: val_loss -0.711\n",
      "2025-12-16 04:03:54.889598: Pseudo dice [0.8412, 0.9053, 0.8561]\n",
      "2025-12-16 04:03:54.891602: Epoch time: 137.88 s\n",
      "2025-12-16 04:03:54.891602: Yayy! New best EMA pseudo Dice: 0.8197\n",
      "2025-12-16 04:03:55.781666: \n",
      "2025-12-16 04:03:55.781666: Epoch 30\n",
      "2025-12-16 04:03:55.781666: Current learning rate: 0.00973\n",
      "2025-12-16 04:06:13.671409: train_loss -0.7186\n",
      "2025-12-16 04:06:13.671409: val_loss -0.7308\n",
      "2025-12-16 04:06:13.671409: Pseudo dice [0.8642, 0.9175, 0.8524]\n",
      "2025-12-16 04:06:13.671409: Epoch time: 137.89 s\n",
      "2025-12-16 04:06:13.671409: Yayy! New best EMA pseudo Dice: 0.8256\n",
      "2025-12-16 04:06:14.933607: \n",
      "2025-12-16 04:06:14.933607: Epoch 31\n",
      "2025-12-16 04:06:14.933607: Current learning rate: 0.00972\n",
      "2025-12-16 04:08:32.813566: train_loss -0.7202\n",
      "2025-12-16 04:08:32.813566: val_loss -0.7372\n",
      "2025-12-16 04:08:32.815569: Pseudo dice [0.8618, 0.9136, 0.8692]\n",
      "2025-12-16 04:08:32.817572: Epoch time: 137.88 s\n",
      "2025-12-16 04:08:32.817572: Yayy! New best EMA pseudo Dice: 0.8312\n",
      "2025-12-16 04:08:33.711154: \n",
      "2025-12-16 04:08:33.713156: Epoch 32\n",
      "2025-12-16 04:08:33.713156: Current learning rate: 0.00971\n",
      "2025-12-16 04:10:52.530124: train_loss -0.7295\n",
      "2025-12-16 04:10:52.545881: val_loss -0.7225\n",
      "2025-12-16 04:10:52.545881: Pseudo dice [0.8664, 0.9132, 0.8362]\n",
      "2025-12-16 04:10:52.545881: Epoch time: 138.82 s\n",
      "2025-12-16 04:10:52.545881: Yayy! New best EMA pseudo Dice: 0.8352\n",
      "2025-12-16 04:10:53.439636: \n",
      "2025-12-16 04:10:53.439636: Epoch 33\n",
      "2025-12-16 04:10:53.455651: Current learning rate: 0.0097\n",
      "2025-12-16 04:13:11.298946: train_loss -0.7274\n",
      "2025-12-16 04:13:11.298946: val_loss -0.7336\n",
      "2025-12-16 04:13:11.300948: Pseudo dice [0.86, 0.908, 0.8646]\n",
      "2025-12-16 04:13:11.302951: Epoch time: 137.86 s\n",
      "2025-12-16 04:13:11.302951: Yayy! New best EMA pseudo Dice: 0.8395\n",
      "2025-12-16 04:13:12.290410: \n",
      "2025-12-16 04:13:12.290410: Epoch 34\n",
      "2025-12-16 04:13:12.306531: Current learning rate: 0.00969\n",
      "2025-12-16 04:15:30.356715: train_loss -0.7303\n",
      "2025-12-16 04:15:30.356715: val_loss -0.7401\n",
      "2025-12-16 04:15:30.358717: Pseudo dice [0.8671, 0.919, 0.8569]\n",
      "2025-12-16 04:15:30.360718: Epoch time: 138.07 s\n",
      "2025-12-16 04:15:30.360718: Yayy! New best EMA pseudo Dice: 0.8436\n",
      "2025-12-16 04:15:31.248782: \n",
      "2025-12-16 04:15:31.248782: Epoch 35\n",
      "2025-12-16 04:15:31.248782: Current learning rate: 0.00968\n",
      "2025-12-16 04:17:49.247996: train_loss -0.7374\n",
      "2025-12-16 04:17:49.247996: val_loss -0.7534\n",
      "2025-12-16 04:17:49.249736: Pseudo dice [0.8761, 0.919, 0.8606]\n",
      "2025-12-16 04:17:49.251739: Epoch time: 138.0 s\n",
      "2025-12-16 04:17:49.251739: Yayy! New best EMA pseudo Dice: 0.8478\n",
      "2025-12-16 04:17:50.295619: \n",
      "2025-12-16 04:17:50.295619: Epoch 36\n",
      "2025-12-16 04:17:50.295619: Current learning rate: 0.00968\n",
      "2025-12-16 04:20:08.368108: train_loss -0.7362\n",
      "2025-12-16 04:20:08.370110: val_loss -0.7293\n",
      "2025-12-16 04:20:08.371850: Pseudo dice [0.8638, 0.9158, 0.8576]\n",
      "2025-12-16 04:20:08.371850: Epoch time: 138.07 s\n",
      "2025-12-16 04:20:08.371850: Yayy! New best EMA pseudo Dice: 0.8509\n",
      "2025-12-16 04:20:09.290222: \n",
      "2025-12-16 04:20:09.290222: Epoch 37\n",
      "2025-12-16 04:20:09.290222: Current learning rate: 0.00967\n",
      "2025-12-16 04:22:27.392654: train_loss -0.7423\n",
      "2025-12-16 04:22:27.392654: val_loss -0.7414\n",
      "2025-12-16 04:22:27.408758: Pseudo dice [0.8725, 0.917, 0.8583]\n",
      "2025-12-16 04:22:27.408758: Epoch time: 138.1 s\n",
      "2025-12-16 04:22:27.408758: Yayy! New best EMA pseudo Dice: 0.8541\n",
      "2025-12-16 04:22:28.316015: \n",
      "2025-12-16 04:22:28.316015: Epoch 38\n",
      "2025-12-16 04:22:28.316015: Current learning rate: 0.00966\n",
      "2025-12-16 04:24:46.340600: train_loss -0.7386\n",
      "2025-12-16 04:24:46.340600: val_loss -0.7456\n",
      "2025-12-16 04:24:46.340600: Pseudo dice [0.8654, 0.9199, 0.8714]\n",
      "2025-12-16 04:24:46.340600: Epoch time: 138.03 s\n",
      "2025-12-16 04:24:46.340600: Yayy! New best EMA pseudo Dice: 0.8572\n",
      "2025-12-16 04:24:47.229631: \n",
      "2025-12-16 04:24:47.229631: Epoch 39\n",
      "2025-12-16 04:24:47.229631: Current learning rate: 0.00965\n",
      "2025-12-16 04:27:05.130792: train_loss -0.7416\n",
      "2025-12-16 04:27:05.130792: val_loss -0.7533\n",
      "2025-12-16 04:27:05.130792: Pseudo dice [0.8746, 0.9143, 0.8784]\n",
      "2025-12-16 04:27:05.130792: Epoch time: 137.9 s\n",
      "2025-12-16 04:27:05.130792: Yayy! New best EMA pseudo Dice: 0.8604\n",
      "2025-12-16 04:27:06.060648: \n",
      "2025-12-16 04:27:06.060648: Epoch 40\n",
      "2025-12-16 04:27:06.060648: Current learning rate: 0.00964\n",
      "2025-12-16 04:29:23.857627: train_loss -0.7482\n",
      "2025-12-16 04:29:23.859629: val_loss -0.7542\n",
      "2025-12-16 04:29:23.861631: Pseudo dice [0.8804, 0.9209, 0.8539]\n",
      "2025-12-16 04:29:23.861631: Epoch time: 137.8 s\n",
      "2025-12-16 04:29:23.861631: Yayy! New best EMA pseudo Dice: 0.8629\n",
      "2025-12-16 04:29:24.766719: \n",
      "2025-12-16 04:29:24.766719: Epoch 41\n",
      "2025-12-16 04:29:24.766719: Current learning rate: 0.00963\n",
      "2025-12-16 04:31:42.775482: train_loss -0.7436\n",
      "2025-12-16 04:31:42.775482: val_loss -0.7678\n",
      "2025-12-16 04:31:42.777484: Pseudo dice [0.889, 0.923, 0.865]\n",
      "2025-12-16 04:31:42.777484: Epoch time: 138.01 s\n",
      "2025-12-16 04:31:42.779224: Yayy! New best EMA pseudo Dice: 0.8658\n",
      "2025-12-16 04:31:43.840773: \n",
      "2025-12-16 04:31:43.840773: Epoch 42\n",
      "2025-12-16 04:31:43.840773: Current learning rate: 0.00962\n",
      "2025-12-16 04:34:01.823817: train_loss -0.745\n",
      "2025-12-16 04:34:01.825821: val_loss -0.7314\n",
      "2025-12-16 04:34:01.825821: Pseudo dice [0.8527, 0.9015, 0.8646]\n",
      "2025-12-16 04:34:01.825821: Epoch time: 137.99 s\n",
      "2025-12-16 04:34:01.825821: Yayy! New best EMA pseudo Dice: 0.8665\n",
      "2025-12-16 04:34:02.712060: \n",
      "2025-12-16 04:34:02.712060: Epoch 43\n",
      "2025-12-16 04:34:02.712060: Current learning rate: 0.00961\n",
      "2025-12-16 04:36:20.608635: train_loss -0.7549\n",
      "2025-12-16 04:36:20.608635: val_loss -0.7765\n",
      "2025-12-16 04:36:20.610636: Pseudo dice [0.889, 0.9269, 0.8631]\n",
      "2025-12-16 04:36:20.610636: Epoch time: 137.9 s\n",
      "2025-12-16 04:36:20.613143: Yayy! New best EMA pseudo Dice: 0.8692\n",
      "2025-12-16 04:36:21.503424: \n",
      "2025-12-16 04:36:21.505164: Epoch 44\n",
      "2025-12-16 04:36:21.505164: Current learning rate: 0.0096\n",
      "2025-12-16 04:38:39.366693: train_loss -0.7686\n",
      "2025-12-16 04:38:39.366693: val_loss -0.7654\n",
      "2025-12-16 04:38:39.368695: Pseudo dice [0.8729, 0.9201, 0.8768]\n",
      "2025-12-16 04:38:39.370698: Epoch time: 137.86 s\n",
      "2025-12-16 04:38:39.370698: Yayy! New best EMA pseudo Dice: 0.8713\n",
      "2025-12-16 04:38:40.249074: \n",
      "2025-12-16 04:38:40.249074: Epoch 45\n",
      "2025-12-16 04:38:40.249074: Current learning rate: 0.00959\n",
      "2025-12-16 04:40:58.273391: train_loss -0.7646\n",
      "2025-12-16 04:40:58.273391: val_loss -0.7761\n",
      "2025-12-16 04:40:58.273391: Pseudo dice [0.883, 0.9287, 0.8799]\n",
      "2025-12-16 04:40:58.273391: Epoch time: 138.02 s\n",
      "2025-12-16 04:40:58.273391: Yayy! New best EMA pseudo Dice: 0.8739\n",
      "2025-12-16 04:40:59.194070: \n",
      "2025-12-16 04:40:59.194070: Epoch 46\n",
      "2025-12-16 04:40:59.194070: Current learning rate: 0.00959\n",
      "2025-12-16 04:43:17.286612: train_loss -0.7598\n",
      "2025-12-16 04:43:17.286612: val_loss -0.7677\n",
      "2025-12-16 04:43:17.286612: Pseudo dice [0.8786, 0.9312, 0.8724]\n",
      "2025-12-16 04:43:17.286612: Epoch time: 138.09 s\n",
      "2025-12-16 04:43:17.286612: Yayy! New best EMA pseudo Dice: 0.8759\n",
      "2025-12-16 04:43:18.180067: \n",
      "2025-12-16 04:43:18.180067: Epoch 47\n",
      "2025-12-16 04:43:18.180067: Current learning rate: 0.00958\n",
      "2025-12-16 04:45:36.116207: train_loss -0.7636\n",
      "2025-12-16 04:45:36.116207: val_loss -0.7428\n",
      "2025-12-16 04:45:36.120212: Pseudo dice [0.8634, 0.916, 0.8578]\n",
      "2025-12-16 04:45:36.122215: Epoch time: 137.94 s\n",
      "2025-12-16 04:45:36.124218: Yayy! New best EMA pseudo Dice: 0.8762\n",
      "2025-12-16 04:45:37.170884: \n",
      "2025-12-16 04:45:37.170884: Epoch 48\n",
      "2025-12-16 04:45:37.170884: Current learning rate: 0.00957\n",
      "2025-12-16 04:47:55.032303: train_loss -0.7621\n",
      "2025-12-16 04:47:55.032303: val_loss -0.776\n",
      "2025-12-16 04:47:55.032303: Pseudo dice [0.8799, 0.9263, 0.8721]\n",
      "2025-12-16 04:47:55.036304: Epoch time: 137.86 s\n",
      "2025-12-16 04:47:55.036304: Yayy! New best EMA pseudo Dice: 0.8779\n",
      "2025-12-16 04:47:55.959119: \n",
      "2025-12-16 04:47:55.959119: Epoch 49\n",
      "2025-12-16 04:47:55.959119: Current learning rate: 0.00956\n",
      "2025-12-16 04:50:13.932590: train_loss -0.7672\n",
      "2025-12-16 04:50:13.932590: val_loss -0.777\n",
      "2025-12-16 04:50:13.948369: Pseudo dice [0.8891, 0.9322, 0.8656]\n",
      "2025-12-16 04:50:13.948369: Epoch time: 137.98 s\n",
      "2025-12-16 04:50:14.185639: Yayy! New best EMA pseudo Dice: 0.8796\n",
      "2025-12-16 04:50:15.076454: \n",
      "2025-12-16 04:50:15.076454: Epoch 50\n",
      "2025-12-16 04:50:15.078456: Current learning rate: 0.00955\n",
      "2025-12-16 04:52:32.914865: train_loss -0.7728\n",
      "2025-12-16 04:52:32.930785: val_loss -0.7653\n",
      "2025-12-16 04:52:32.930785: Pseudo dice [0.8743, 0.9183, 0.8715]\n",
      "2025-12-16 04:52:32.930785: Epoch time: 137.84 s\n",
      "2025-12-16 04:52:32.930785: Yayy! New best EMA pseudo Dice: 0.8805\n",
      "2025-12-16 04:52:33.818604: \n",
      "2025-12-16 04:52:33.818604: Epoch 51\n",
      "2025-12-16 04:52:33.818604: Current learning rate: 0.00954\n",
      "2025-12-16 04:54:51.851911: train_loss -0.7752\n",
      "2025-12-16 04:54:51.853915: val_loss -0.7793\n",
      "2025-12-16 04:54:51.853915: Pseudo dice [0.8811, 0.9299, 0.8787]\n",
      "2025-12-16 04:54:51.855655: Epoch time: 138.03 s\n",
      "2025-12-16 04:54:51.855655: Yayy! New best EMA pseudo Dice: 0.8821\n",
      "2025-12-16 04:54:52.767600: \n",
      "2025-12-16 04:54:52.767600: Epoch 52\n",
      "2025-12-16 04:54:52.767600: Current learning rate: 0.00953\n",
      "2025-12-16 04:57:10.906337: train_loss -0.7768\n",
      "2025-12-16 04:57:10.906337: val_loss -0.7705\n",
      "2025-12-16 04:57:10.906337: Pseudo dice [0.8712, 0.9223, 0.8894]\n",
      "2025-12-16 04:57:10.906337: Epoch time: 138.14 s\n",
      "2025-12-16 04:57:10.906337: Yayy! New best EMA pseudo Dice: 0.8833\n",
      "2025-12-16 04:57:11.795083: \n",
      "2025-12-16 04:57:11.795083: Epoch 53\n",
      "2025-12-16 04:57:11.795083: Current learning rate: 0.00952\n",
      "2025-12-16 04:59:29.760453: train_loss -0.765\n",
      "2025-12-16 04:59:29.760453: val_loss -0.7934\n",
      "2025-12-16 04:59:29.762194: Pseudo dice [0.8962, 0.9345, 0.8786]\n",
      "2025-12-16 04:59:29.762194: Epoch time: 137.97 s\n",
      "2025-12-16 04:59:29.762194: Yayy! New best EMA pseudo Dice: 0.8853\n",
      "2025-12-16 04:59:30.651361: \n",
      "2025-12-16 04:59:30.651361: Epoch 54\n",
      "2025-12-16 04:59:30.651361: Current learning rate: 0.00951\n",
      "2025-12-16 05:01:48.532446: train_loss -0.7774\n",
      "2025-12-16 05:01:48.535434: val_loss -0.7806\n",
      "2025-12-16 05:01:48.537438: Pseudo dice [0.8888, 0.9241, 0.8832]\n",
      "2025-12-16 05:01:48.539441: Epoch time: 137.88 s\n",
      "2025-12-16 05:01:48.539441: Yayy! New best EMA pseudo Dice: 0.8866\n",
      "2025-12-16 05:01:49.439443: \n",
      "2025-12-16 05:01:49.439443: Epoch 55\n",
      "2025-12-16 05:01:49.439443: Current learning rate: 0.0095\n",
      "2025-12-16 05:04:07.328993: train_loss -0.7722\n",
      "2025-12-16 05:04:07.330996: val_loss -0.7728\n",
      "2025-12-16 05:04:07.332999: Pseudo dice [0.8777, 0.9205, 0.8747]\n",
      "2025-12-16 05:04:07.335002: Epoch time: 137.89 s\n",
      "2025-12-16 05:04:07.336746: Yayy! New best EMA pseudo Dice: 0.8871\n",
      "2025-12-16 05:04:08.284328: \n",
      "2025-12-16 05:04:08.284328: Epoch 56\n",
      "2025-12-16 05:04:08.284328: Current learning rate: 0.00949\n",
      "2025-12-16 05:06:26.151141: train_loss -0.7626\n",
      "2025-12-16 05:06:26.153143: val_loss -0.7826\n",
      "2025-12-16 05:06:26.153143: Pseudo dice [0.8828, 0.9328, 0.8879]\n",
      "2025-12-16 05:06:26.153143: Epoch time: 137.87 s\n",
      "2025-12-16 05:06:26.153143: Yayy! New best EMA pseudo Dice: 0.8885\n",
      "2025-12-16 05:06:27.033497: \n",
      "2025-12-16 05:06:27.033497: Epoch 57\n",
      "2025-12-16 05:06:27.033497: Current learning rate: 0.00949\n",
      "2025-12-16 05:08:45.082974: train_loss -0.7691\n",
      "2025-12-16 05:08:45.082974: val_loss -0.7669\n",
      "2025-12-16 05:08:45.082974: Pseudo dice [0.8761, 0.9255, 0.8787]\n",
      "2025-12-16 05:08:45.082974: Epoch time: 138.05 s\n",
      "2025-12-16 05:08:45.092657: Yayy! New best EMA pseudo Dice: 0.889\n",
      "2025-12-16 05:08:46.014063: \n",
      "2025-12-16 05:08:46.014063: Epoch 58\n",
      "2025-12-16 05:08:46.014063: Current learning rate: 0.00948\n",
      "2025-12-16 05:11:04.729937: train_loss -0.7634\n",
      "2025-12-16 05:11:04.729937: val_loss -0.7728\n",
      "2025-12-16 05:11:04.729937: Pseudo dice [0.8758, 0.925, 0.8808]\n",
      "2025-12-16 05:11:04.734283: Epoch time: 138.72 s\n",
      "2025-12-16 05:11:04.734283: Yayy! New best EMA pseudo Dice: 0.8895\n",
      "2025-12-16 05:11:05.621921: \n",
      "2025-12-16 05:11:05.621921: Epoch 59\n",
      "2025-12-16 05:11:05.624739: Current learning rate: 0.00947\n",
      "2025-12-16 05:13:23.432786: train_loss -0.7366\n",
      "2025-12-16 05:13:23.432786: val_loss -0.7537\n",
      "2025-12-16 05:13:23.434788: Pseudo dice [0.8714, 0.916, 0.8686]\n",
      "2025-12-16 05:13:23.434788: Epoch time: 137.81 s\n",
      "2025-12-16 05:13:24.231301: \n",
      "2025-12-16 05:13:24.231301: Epoch 60\n",
      "2025-12-16 05:13:24.231301: Current learning rate: 0.00946\n",
      "2025-12-16 05:15:42.284155: train_loss -0.7513\n",
      "2025-12-16 05:15:42.286157: val_loss -0.7756\n",
      "2025-12-16 05:15:42.286157: Pseudo dice [0.886, 0.9296, 0.8684]\n",
      "2025-12-16 05:15:42.286157: Epoch time: 138.05 s\n",
      "2025-12-16 05:15:42.289479: Yayy! New best EMA pseudo Dice: 0.8896\n",
      "2025-12-16 05:15:43.183263: \n",
      "2025-12-16 05:15:43.183263: Epoch 61\n",
      "2025-12-16 05:15:43.183263: Current learning rate: 0.00945\n",
      "2025-12-16 05:18:00.962498: train_loss -0.7645\n",
      "2025-12-16 05:18:00.962498: val_loss -0.7774\n",
      "2025-12-16 05:18:00.962498: Pseudo dice [0.8868, 0.9289, 0.8732]\n",
      "2025-12-16 05:18:00.962498: Epoch time: 137.78 s\n",
      "2025-12-16 05:18:00.973812: Yayy! New best EMA pseudo Dice: 0.8903\n",
      "2025-12-16 05:18:01.860464: \n",
      "2025-12-16 05:18:01.860464: Epoch 62\n",
      "2025-12-16 05:18:01.860464: Current learning rate: 0.00944\n",
      "2025-12-16 05:20:19.722506: train_loss -0.7801\n",
      "2025-12-16 05:20:19.722506: val_loss -0.7951\n",
      "2025-12-16 05:20:19.722506: Pseudo dice [0.8929, 0.9319, 0.882]\n",
      "2025-12-16 05:20:19.730125: Epoch time: 137.86 s\n",
      "2025-12-16 05:20:19.730125: Yayy! New best EMA pseudo Dice: 0.8915\n",
      "2025-12-16 05:20:20.628532: \n",
      "2025-12-16 05:20:20.628532: Epoch 63\n",
      "2025-12-16 05:20:20.628532: Current learning rate: 0.00943\n",
      "2025-12-16 05:22:38.657552: train_loss -0.7775\n",
      "2025-12-16 05:22:38.657552: val_loss -0.7924\n",
      "2025-12-16 05:22:38.657552: Pseudo dice [0.8883, 0.9296, 0.8926]\n",
      "2025-12-16 05:22:38.657552: Epoch time: 138.03 s\n",
      "2025-12-16 05:22:38.669829: Yayy! New best EMA pseudo Dice: 0.8927\n",
      "2025-12-16 05:22:39.579349: \n",
      "2025-12-16 05:22:39.579349: Epoch 64\n",
      "2025-12-16 05:22:39.579349: Current learning rate: 0.00942\n",
      "2025-12-16 05:24:57.483163: train_loss -0.7835\n",
      "2025-12-16 05:24:57.483163: val_loss -0.7945\n",
      "2025-12-16 05:24:57.483163: Pseudo dice [0.8913, 0.9341, 0.8872]\n",
      "2025-12-16 05:24:57.483163: Epoch time: 137.91 s\n",
      "2025-12-16 05:24:57.483163: Yayy! New best EMA pseudo Dice: 0.8938\n",
      "2025-12-16 05:24:58.384354: \n",
      "2025-12-16 05:24:58.386357: Epoch 65\n",
      "2025-12-16 05:24:58.386357: Current learning rate: 0.00941\n",
      "2025-12-16 05:27:16.457556: train_loss -0.7752\n",
      "2025-12-16 05:27:16.457556: val_loss -0.7664\n",
      "2025-12-16 05:27:16.459296: Pseudo dice [0.8713, 0.9148, 0.8832]\n",
      "2025-12-16 05:27:16.461299: Epoch time: 138.07 s\n",
      "2025-12-16 05:27:17.275554: \n",
      "2025-12-16 05:27:17.275554: Epoch 66\n",
      "2025-12-16 05:27:17.275554: Current learning rate: 0.0094\n",
      "2025-12-16 05:29:35.075014: train_loss -0.7774\n",
      "2025-12-16 05:29:35.075014: val_loss -0.7821\n",
      "2025-12-16 05:29:35.075014: Pseudo dice [0.8762, 0.9247, 0.8903]\n",
      "2025-12-16 05:29:35.075014: Epoch time: 137.8 s\n",
      "2025-12-16 05:29:35.729534: \n",
      "2025-12-16 05:29:35.731536: Epoch 67\n",
      "2025-12-16 05:29:35.731536: Current learning rate: 0.00939\n",
      "2025-12-16 05:31:53.748152: train_loss -0.7845\n",
      "2025-12-16 05:31:53.748152: val_loss -0.8126\n",
      "2025-12-16 05:31:53.748152: Pseudo dice [0.9003, 0.939, 0.8885]\n",
      "2025-12-16 05:31:53.748152: Epoch time: 138.02 s\n",
      "2025-12-16 05:31:53.748152: Yayy! New best EMA pseudo Dice: 0.8953\n",
      "2025-12-16 05:31:54.619756: \n",
      "2025-12-16 05:31:54.619756: Epoch 68\n",
      "2025-12-16 05:31:54.619756: Current learning rate: 0.00939\n",
      "2025-12-16 05:34:12.466573: train_loss -0.7946\n",
      "2025-12-16 05:34:12.466573: val_loss -0.7996\n",
      "2025-12-16 05:34:12.466573: Pseudo dice [0.8951, 0.9342, 0.8846]\n",
      "2025-12-16 05:34:12.466573: Epoch time: 137.85 s\n",
      "2025-12-16 05:34:12.466573: Yayy! New best EMA pseudo Dice: 0.8963\n",
      "2025-12-16 05:34:13.368963: \n",
      "2025-12-16 05:34:13.370966: Epoch 69\n",
      "2025-12-16 05:34:13.370966: Current learning rate: 0.00938\n",
      "2025-12-16 05:36:31.215481: train_loss -0.7943\n",
      "2025-12-16 05:36:31.215481: val_loss -0.8137\n",
      "2025-12-16 05:36:31.217483: Pseudo dice [0.9016, 0.9408, 0.901]\n",
      "2025-12-16 05:36:31.219485: Epoch time: 137.85 s\n",
      "2025-12-16 05:36:31.219485: Yayy! New best EMA pseudo Dice: 0.8981\n",
      "2025-12-16 05:36:32.156308: \n",
      "2025-12-16 05:36:32.159922: Epoch 70\n",
      "2025-12-16 05:36:32.159922: Current learning rate: 0.00937\n",
      "2025-12-16 05:38:49.918176: train_loss -0.7952\n",
      "2025-12-16 05:38:49.918176: val_loss -0.8062\n",
      "2025-12-16 05:38:49.918176: Pseudo dice [0.9006, 0.9398, 0.9007]\n",
      "2025-12-16 05:38:49.928228: Epoch time: 137.76 s\n",
      "2025-12-16 05:38:49.930231: Yayy! New best EMA pseudo Dice: 0.8996\n",
      "2025-12-16 05:38:50.818488: \n",
      "2025-12-16 05:38:50.820896: Epoch 71\n",
      "2025-12-16 05:38:50.820896: Current learning rate: 0.00936\n",
      "2025-12-16 05:41:08.745566: train_loss -0.8005\n",
      "2025-12-16 05:41:08.747567: val_loss -0.7917\n",
      "2025-12-16 05:41:08.749387: Pseudo dice [0.8867, 0.9277, 0.8864]\n",
      "2025-12-16 05:41:08.749387: Epoch time: 137.93 s\n",
      "2025-12-16 05:41:08.751390: Yayy! New best EMA pseudo Dice: 0.8997\n",
      "2025-12-16 05:41:09.838476: \n",
      "2025-12-16 05:41:09.838476: Epoch 72\n",
      "2025-12-16 05:41:09.838476: Current learning rate: 0.00935\n",
      "2025-12-16 05:43:27.766435: train_loss -0.7981\n",
      "2025-12-16 05:43:27.766435: val_loss -0.7956\n",
      "2025-12-16 05:43:27.768437: Pseudo dice [0.8844, 0.9313, 0.9025]\n",
      "2025-12-16 05:43:27.770439: Epoch time: 137.93 s\n",
      "2025-12-16 05:43:27.772180: Yayy! New best EMA pseudo Dice: 0.9003\n",
      "2025-12-16 05:43:28.683681: \n",
      "2025-12-16 05:43:28.683681: Epoch 73\n",
      "2025-12-16 05:43:28.683681: Current learning rate: 0.00934\n",
      "2025-12-16 05:45:46.932826: train_loss -0.7862\n",
      "2025-12-16 05:45:46.934829: val_loss -0.8106\n",
      "2025-12-16 05:45:46.934829: Pseudo dice [0.8985, 0.9389, 0.9022]\n",
      "2025-12-16 05:45:46.934829: Epoch time: 138.25 s\n",
      "2025-12-16 05:45:46.940061: Yayy! New best EMA pseudo Dice: 0.9016\n",
      "2025-12-16 05:45:47.845033: \n",
      "2025-12-16 05:45:47.845033: Epoch 74\n",
      "2025-12-16 05:45:47.845033: Current learning rate: 0.00933\n",
      "2025-12-16 05:48:05.784727: train_loss -0.7865\n",
      "2025-12-16 05:48:05.784727: val_loss -0.8153\n",
      "2025-12-16 05:48:05.787260: Pseudo dice [0.9022, 0.9426, 0.9029]\n",
      "2025-12-16 05:48:05.787260: Epoch time: 137.94 s\n",
      "2025-12-16 05:48:05.787260: Yayy! New best EMA pseudo Dice: 0.9031\n",
      "2025-12-16 05:48:06.683159: \n",
      "2025-12-16 05:48:06.683159: Epoch 75\n",
      "2025-12-16 05:48:06.685671: Current learning rate: 0.00932\n",
      "2025-12-16 05:50:24.540187: train_loss -0.79\n",
      "2025-12-16 05:50:24.540187: val_loss -0.7972\n",
      "2025-12-16 05:50:24.542189: Pseudo dice [0.8903, 0.9299, 0.8807]\n",
      "2025-12-16 05:50:24.542189: Epoch time: 137.86 s\n",
      "2025-12-16 05:50:25.284940: \n",
      "2025-12-16 05:50:25.284940: Epoch 76\n",
      "2025-12-16 05:50:25.284940: Current learning rate: 0.00931\n",
      "2025-12-16 05:52:43.447507: train_loss -0.7972\n",
      "2025-12-16 05:52:43.449248: val_loss -0.8065\n",
      "2025-12-16 05:52:43.451251: Pseudo dice [0.8941, 0.9358, 0.8993]\n",
      "2025-12-16 05:52:43.451251: Epoch time: 138.16 s\n",
      "2025-12-16 05:52:43.453253: Yayy! New best EMA pseudo Dice: 0.9035\n",
      "2025-12-16 05:52:44.354001: \n",
      "2025-12-16 05:52:44.354001: Epoch 77\n",
      "2025-12-16 05:52:44.354001: Current learning rate: 0.0093\n",
      "2025-12-16 05:55:02.313652: train_loss -0.7957\n",
      "2025-12-16 05:55:02.313652: val_loss -0.7985\n",
      "2025-12-16 05:55:02.317572: Pseudo dice [0.8881, 0.9278, 0.9028]\n",
      "2025-12-16 05:55:02.317572: Epoch time: 137.96 s\n",
      "2025-12-16 05:55:02.317572: Yayy! New best EMA pseudo Dice: 0.9038\n",
      "2025-12-16 05:55:03.377017: \n",
      "2025-12-16 05:55:03.377017: Epoch 78\n",
      "2025-12-16 05:55:03.393023: Current learning rate: 0.0093\n",
      "2025-12-16 05:57:21.332628: train_loss -0.7933\n",
      "2025-12-16 05:57:21.332628: val_loss -0.83\n",
      "2025-12-16 05:57:21.335219: Pseudo dice [0.9154, 0.9463, 0.8987]\n",
      "2025-12-16 05:57:21.335219: Epoch time: 137.96 s\n",
      "2025-12-16 05:57:21.337221: Yayy! New best EMA pseudo Dice: 0.9054\n",
      "2025-12-16 05:57:22.314346: \n",
      "2025-12-16 05:57:22.314346: Epoch 79\n",
      "2025-12-16 05:57:22.314346: Current learning rate: 0.00929\n",
      "2025-12-16 05:59:40.372844: train_loss -0.7905\n",
      "2025-12-16 05:59:40.372844: val_loss -0.7922\n",
      "2025-12-16 05:59:40.372844: Pseudo dice [0.8891, 0.9254, 0.8833]\n",
      "2025-12-16 05:59:40.372844: Epoch time: 138.06 s\n",
      "2025-12-16 05:59:41.023784: \n",
      "2025-12-16 05:59:41.037812: Epoch 80\n",
      "2025-12-16 05:59:41.037812: Current learning rate: 0.00928\n",
      "2025-12-16 06:01:58.858962: train_loss -0.793\n",
      "2025-12-16 06:01:58.858962: val_loss -0.8111\n",
      "2025-12-16 06:01:58.860964: Pseudo dice [0.9022, 0.9404, 0.895]\n",
      "2025-12-16 06:01:58.862966: Epoch time: 137.84 s\n",
      "2025-12-16 06:01:58.862966: Yayy! New best EMA pseudo Dice: 0.9056\n",
      "2025-12-16 06:01:59.777862: \n",
      "2025-12-16 06:01:59.777862: Epoch 81\n",
      "2025-12-16 06:01:59.777862: Current learning rate: 0.00927\n",
      "2025-12-16 06:04:17.763553: train_loss -0.7992\n",
      "2025-12-16 06:04:17.763553: val_loss -0.8107\n",
      "2025-12-16 06:04:17.765555: Pseudo dice [0.8955, 0.9357, 0.8961]\n",
      "2025-12-16 06:04:17.767557: Epoch time: 137.99 s\n",
      "2025-12-16 06:04:17.769559: Yayy! New best EMA pseudo Dice: 0.9059\n",
      "2025-12-16 06:04:18.747092: \n",
      "2025-12-16 06:04:18.763211: Epoch 82\n",
      "2025-12-16 06:04:18.763211: Current learning rate: 0.00926\n",
      "2025-12-16 06:06:36.783101: train_loss -0.8024\n",
      "2025-12-16 06:06:36.783101: val_loss -0.8026\n",
      "2025-12-16 06:06:36.783101: Pseudo dice [0.8917, 0.9368, 0.891]\n",
      "2025-12-16 06:06:36.783101: Epoch time: 138.04 s\n",
      "2025-12-16 06:06:36.783101: Yayy! New best EMA pseudo Dice: 0.906\n",
      "2025-12-16 06:06:37.658737: \n",
      "2025-12-16 06:06:37.658737: Epoch 83\n",
      "2025-12-16 06:06:37.658737: Current learning rate: 0.00925\n",
      "2025-12-16 06:08:55.658219: train_loss -0.8023\n",
      "2025-12-16 06:08:55.658219: val_loss -0.8165\n",
      "2025-12-16 06:08:55.662224: Pseudo dice [0.8991, 0.9384, 0.9027]\n",
      "2025-12-16 06:08:55.662224: Epoch time: 138.0 s\n",
      "2025-12-16 06:08:55.664227: Yayy! New best EMA pseudo Dice: 0.9067\n",
      "2025-12-16 06:08:56.724125: \n",
      "2025-12-16 06:08:56.726500: Epoch 84\n",
      "2025-12-16 06:08:56.726500: Current learning rate: 0.00924\n",
      "2025-12-16 06:11:15.405712: train_loss -0.7946\n",
      "2025-12-16 06:11:15.405712: val_loss -0.8038\n",
      "2025-12-16 06:11:15.405712: Pseudo dice [0.8946, 0.935, 0.8999]\n",
      "2025-12-16 06:11:15.405712: Epoch time: 138.68 s\n",
      "2025-12-16 06:11:15.405712: Yayy! New best EMA pseudo Dice: 0.907\n",
      "2025-12-16 06:11:16.304983: \n",
      "2025-12-16 06:11:16.304983: Epoch 85\n",
      "2025-12-16 06:11:16.304983: Current learning rate: 0.00923\n",
      "2025-12-16 06:13:34.316675: train_loss -0.8011\n",
      "2025-12-16 06:13:34.318416: val_loss -0.8233\n",
      "2025-12-16 06:13:34.318416: Pseudo dice [0.9086, 0.9446, 0.901]\n",
      "2025-12-16 06:13:34.318416: Epoch time: 138.01 s\n",
      "2025-12-16 06:13:34.318416: Yayy! New best EMA pseudo Dice: 0.9081\n",
      "2025-12-16 06:13:35.205317: \n",
      "2025-12-16 06:13:35.205317: Epoch 86\n",
      "2025-12-16 06:13:35.221072: Current learning rate: 0.00922\n",
      "2025-12-16 06:15:53.075864: train_loss -0.801\n",
      "2025-12-16 06:15:53.075864: val_loss -0.8168\n",
      "2025-12-16 06:15:53.075864: Pseudo dice [0.8979, 0.9389, 0.9081]\n",
      "2025-12-16 06:15:53.088405: Epoch time: 137.87 s\n",
      "2025-12-16 06:15:53.090408: Yayy! New best EMA pseudo Dice: 0.9088\n",
      "2025-12-16 06:15:53.958930: \n",
      "2025-12-16 06:15:53.958930: Epoch 87\n",
      "2025-12-16 06:15:53.973459: Current learning rate: 0.00921\n",
      "2025-12-16 06:18:11.879557: train_loss -0.8049\n",
      "2025-12-16 06:18:11.881562: val_loss -0.8222\n",
      "2025-12-16 06:18:11.883564: Pseudo dice [0.9027, 0.9398, 0.8975]\n",
      "2025-12-16 06:18:11.883564: Epoch time: 137.92 s\n",
      "2025-12-16 06:18:11.883564: Yayy! New best EMA pseudo Dice: 0.9093\n",
      "2025-12-16 06:18:12.791945: \n",
      "2025-12-16 06:18:12.791945: Epoch 88\n",
      "2025-12-16 06:18:12.791945: Current learning rate: 0.0092\n",
      "2025-12-16 06:20:30.768612: train_loss -0.8016\n",
      "2025-12-16 06:20:30.768612: val_loss -0.8195\n",
      "2025-12-16 06:20:30.768612: Pseudo dice [0.8976, 0.9372, 0.905]\n",
      "2025-12-16 06:20:30.768612: Epoch time: 137.99 s\n",
      "2025-12-16 06:20:30.768612: Yayy! New best EMA pseudo Dice: 0.9097\n",
      "2025-12-16 06:20:31.640877: \n",
      "2025-12-16 06:20:31.640877: Epoch 89\n",
      "2025-12-16 06:20:31.640877: Current learning rate: 0.0092\n",
      "2025-12-16 06:22:49.693232: train_loss -0.7986\n",
      "2025-12-16 06:22:49.693232: val_loss -0.8104\n",
      "2025-12-16 06:22:49.695234: Pseudo dice [0.9028, 0.9382, 0.893]\n",
      "2025-12-16 06:22:49.697236: Epoch time: 138.05 s\n",
      "2025-12-16 06:22:49.698976: Yayy! New best EMA pseudo Dice: 0.9098\n",
      "2025-12-16 06:22:50.756917: \n",
      "2025-12-16 06:22:50.756917: Epoch 90\n",
      "2025-12-16 06:22:50.756917: Current learning rate: 0.00919\n",
      "2025-12-16 06:25:08.601519: train_loss -0.7966\n",
      "2025-12-16 06:25:08.601519: val_loss -0.7974\n",
      "2025-12-16 06:25:08.603521: Pseudo dice [0.8832, 0.9302, 0.9052]\n",
      "2025-12-16 06:25:08.603521: Epoch time: 137.84 s\n",
      "2025-12-16 06:25:09.233560: \n",
      "2025-12-16 06:25:09.233560: Epoch 91\n",
      "2025-12-16 06:25:09.233560: Current learning rate: 0.00918\n",
      "2025-12-16 06:27:27.380934: train_loss -0.7996\n",
      "2025-12-16 06:27:27.380934: val_loss -0.8089\n",
      "2025-12-16 06:27:27.380934: Pseudo dice [0.8974, 0.9375, 0.8939]\n",
      "2025-12-16 06:27:27.396945: Epoch time: 138.15 s\n",
      "2025-12-16 06:27:27.999871: \n",
      "2025-12-16 06:27:27.999871: Epoch 92\n",
      "2025-12-16 06:27:27.999871: Current learning rate: 0.00917\n",
      "2025-12-16 06:29:45.873798: train_loss -0.7939\n",
      "2025-12-16 06:29:45.873798: val_loss -0.8236\n",
      "2025-12-16 06:29:45.877803: Pseudo dice [0.9026, 0.9402, 0.913]\n",
      "2025-12-16 06:29:45.877803: Epoch time: 137.87 s\n",
      "2025-12-16 06:29:45.879804: Yayy! New best EMA pseudo Dice: 0.9104\n",
      "2025-12-16 06:29:46.722537: \n",
      "2025-12-16 06:29:46.722537: Epoch 93\n",
      "2025-12-16 06:29:46.722537: Current learning rate: 0.00916\n",
      "2025-12-16 06:32:04.729845: train_loss -0.7969\n",
      "2025-12-16 06:32:04.729845: val_loss -0.8183\n",
      "2025-12-16 06:32:04.729845: Pseudo dice [0.903, 0.9411, 0.9053]\n",
      "2025-12-16 06:32:04.729845: Epoch time: 138.01 s\n",
      "2025-12-16 06:32:04.729845: Yayy! New best EMA pseudo Dice: 0.911\n",
      "2025-12-16 06:32:05.695252: \n",
      "2025-12-16 06:32:05.695252: Epoch 94\n",
      "2025-12-16 06:32:05.695252: Current learning rate: 0.00915\n",
      "2025-12-16 06:34:23.549925: train_loss -0.7943\n",
      "2025-12-16 06:34:23.549925: val_loss -0.8041\n",
      "2025-12-16 06:34:23.549925: Pseudo dice [0.893, 0.9309, 0.8977]\n",
      "2025-12-16 06:34:23.549925: Epoch time: 137.85 s\n",
      "2025-12-16 06:34:24.168117: \n",
      "2025-12-16 06:34:24.168117: Epoch 95\n",
      "2025-12-16 06:34:24.168117: Current learning rate: 0.00914\n",
      "2025-12-16 06:36:42.114124: train_loss -0.8082\n",
      "2025-12-16 06:36:42.116126: val_loss -0.8081\n",
      "2025-12-16 06:36:42.117866: Pseudo dice [0.8935, 0.9322, 0.9055]\n",
      "2025-12-16 06:36:42.117866: Epoch time: 137.95 s\n",
      "2025-12-16 06:36:42.904596: \n",
      "2025-12-16 06:36:42.904596: Epoch 96\n",
      "2025-12-16 06:36:42.904596: Current learning rate: 0.00913\n",
      "2025-12-16 06:39:00.841005: train_loss -0.8077\n",
      "2025-12-16 06:39:00.841005: val_loss -0.8038\n",
      "2025-12-16 06:39:00.841005: Pseudo dice [0.8903, 0.9335, 0.9013]\n",
      "2025-12-16 06:39:00.856769: Epoch time: 137.94 s\n",
      "2025-12-16 06:39:01.587850: \n",
      "2025-12-16 06:39:01.587850: Epoch 97\n",
      "2025-12-16 06:39:01.587850: Current learning rate: 0.00912\n",
      "2025-12-16 06:41:19.696202: train_loss -0.809\n",
      "2025-12-16 06:41:19.696202: val_loss -0.8321\n",
      "2025-12-16 06:41:19.698205: Pseudo dice [0.9099, 0.9499, 0.9036]\n",
      "2025-12-16 06:41:19.698205: Epoch time: 138.11 s\n",
      "2025-12-16 06:41:19.698205: Yayy! New best EMA pseudo Dice: 0.9114\n",
      "2025-12-16 06:41:20.559130: \n",
      "2025-12-16 06:41:20.559130: Epoch 98\n",
      "2025-12-16 06:41:20.559130: Current learning rate: 0.00911\n",
      "2025-12-16 06:43:38.588167: train_loss -0.8028\n",
      "2025-12-16 06:43:38.588167: val_loss -0.8205\n",
      "2025-12-16 06:43:38.588167: Pseudo dice [0.9072, 0.9423, 0.9062]\n",
      "2025-12-16 06:43:38.588167: Epoch time: 138.03 s\n",
      "2025-12-16 06:43:38.588167: Yayy! New best EMA pseudo Dice: 0.9122\n",
      "2025-12-16 06:43:39.473575: \n",
      "2025-12-16 06:43:39.475395: Epoch 99\n",
      "2025-12-16 06:43:39.475395: Current learning rate: 0.0091\n",
      "2025-12-16 06:45:57.572548: train_loss -0.8016\n",
      "2025-12-16 06:45:57.572548: val_loss -0.827\n",
      "2025-12-16 06:45:57.574552: Pseudo dice [0.9134, 0.9432, 0.8954]\n",
      "2025-12-16 06:45:57.576555: Epoch time: 138.1 s\n",
      "2025-12-16 06:45:57.834612: Yayy! New best EMA pseudo Dice: 0.9127\n",
      "2025-12-16 06:45:58.838291: \n",
      "2025-12-16 06:45:58.850825: Epoch 100\n",
      "2025-12-16 06:45:58.850825: Current learning rate: 0.0091\n",
      "2025-12-16 06:48:16.792234: train_loss -0.8058\n",
      "2025-12-16 06:48:16.794237: val_loss -0.8197\n",
      "2025-12-16 06:48:16.796072: Pseudo dice [0.8997, 0.9423, 0.8992]\n",
      "2025-12-16 06:48:16.796072: Epoch time: 137.95 s\n",
      "2025-12-16 06:48:16.798075: Yayy! New best EMA pseudo Dice: 0.9128\n",
      "2025-12-16 06:48:17.677306: \n",
      "2025-12-16 06:48:17.679117: Epoch 101\n",
      "2025-12-16 06:48:17.680859: Current learning rate: 0.00909\n",
      "2025-12-16 06:50:35.817964: train_loss -0.811\n",
      "2025-12-16 06:50:35.817964: val_loss -0.823\n",
      "2025-12-16 06:50:35.819967: Pseudo dice [0.9038, 0.9424, 0.9027]\n",
      "2025-12-16 06:50:35.821968: Epoch time: 138.14 s\n",
      "2025-12-16 06:50:35.821968: Yayy! New best EMA pseudo Dice: 0.9131\n",
      "2025-12-16 06:50:36.715034: \n",
      "2025-12-16 06:50:36.715034: Epoch 102\n",
      "2025-12-16 06:50:36.715034: Current learning rate: 0.00908\n",
      "2025-12-16 06:52:54.601068: train_loss -0.8065\n",
      "2025-12-16 06:52:54.601068: val_loss -0.831\n",
      "2025-12-16 06:52:54.617066: Pseudo dice [0.9102, 0.9436, 0.9018]\n",
      "2025-12-16 06:52:54.617066: Epoch time: 137.89 s\n",
      "2025-12-16 06:52:54.617066: Yayy! New best EMA pseudo Dice: 0.9137\n",
      "2025-12-16 06:52:55.756529: \n",
      "2025-12-16 06:52:55.758392: Epoch 103\n",
      "2025-12-16 06:52:55.758392: Current learning rate: 0.00907\n",
      "2025-12-16 06:55:13.708247: train_loss -0.8058\n",
      "2025-12-16 06:55:13.708247: val_loss -0.8217\n",
      "2025-12-16 06:55:13.710251: Pseudo dice [0.9053, 0.9436, 0.8982]\n",
      "2025-12-16 06:55:13.712254: Epoch time: 137.95 s\n",
      "2025-12-16 06:55:13.712254: Yayy! New best EMA pseudo Dice: 0.9139\n",
      "2025-12-16 06:55:14.613018: \n",
      "2025-12-16 06:55:14.613018: Epoch 104\n",
      "2025-12-16 06:55:14.613018: Current learning rate: 0.00906\n",
      "2025-12-16 06:57:32.670336: train_loss -0.8033\n",
      "2025-12-16 06:57:32.670336: val_loss -0.8251\n",
      "2025-12-16 06:57:32.674340: Pseudo dice [0.9046, 0.9413, 0.9037]\n",
      "2025-12-16 06:57:32.674340: Epoch time: 138.06 s\n",
      "2025-12-16 06:57:32.676342: Yayy! New best EMA pseudo Dice: 0.9141\n",
      "2025-12-16 06:57:33.575412: \n",
      "2025-12-16 06:57:33.575412: Epoch 105\n",
      "2025-12-16 06:57:33.575412: Current learning rate: 0.00905\n",
      "2025-12-16 06:59:51.438244: train_loss -0.8161\n",
      "2025-12-16 06:59:51.438244: val_loss -0.8184\n",
      "2025-12-16 06:59:51.438244: Pseudo dice [0.8967, 0.9375, 0.9066]\n",
      "2025-12-16 06:59:51.443752: Epoch time: 137.86 s\n",
      "2025-12-16 06:59:52.228300: \n",
      "2025-12-16 06:59:52.228300: Epoch 106\n",
      "2025-12-16 06:59:52.228300: Current learning rate: 0.00904\n",
      "2025-12-16 07:02:10.282017: train_loss -0.8084\n",
      "2025-12-16 07:02:10.282017: val_loss -0.8112\n",
      "2025-12-16 07:02:10.284020: Pseudo dice [0.8961, 0.9329, 0.9068]\n",
      "2025-12-16 07:02:10.286021: Epoch time: 138.07 s\n",
      "2025-12-16 07:02:10.925228: \n",
      "2025-12-16 07:02:10.925228: Epoch 107\n",
      "2025-12-16 07:02:10.925228: Current learning rate: 0.00903\n",
      "2025-12-16 07:04:28.899063: train_loss -0.8097\n",
      "2025-12-16 07:04:28.899063: val_loss -0.831\n",
      "2025-12-16 07:04:28.899063: Pseudo dice [0.9094, 0.9498, 0.9041]\n",
      "2025-12-16 07:04:28.914800: Epoch time: 137.97 s\n",
      "2025-12-16 07:04:28.914800: Yayy! New best EMA pseudo Dice: 0.9146\n",
      "2025-12-16 07:04:29.771517: \n",
      "2025-12-16 07:04:29.771517: Epoch 108\n",
      "2025-12-16 07:04:29.771517: Current learning rate: 0.00902\n",
      "2025-12-16 07:06:47.723268: train_loss -0.8118\n",
      "2025-12-16 07:06:47.723268: val_loss -0.821\n",
      "2025-12-16 07:06:47.723268: Pseudo dice [0.9034, 0.9406, 0.9067]\n",
      "2025-12-16 07:06:47.736795: Epoch time: 137.97 s\n",
      "2025-12-16 07:06:47.738799: Yayy! New best EMA pseudo Dice: 0.9148\n",
      "2025-12-16 07:06:48.966311: \n",
      "2025-12-16 07:06:48.968314: Epoch 109\n",
      "2025-12-16 07:06:48.970318: Current learning rate: 0.00901\n",
      "2025-12-16 07:09:06.990718: train_loss -0.8096\n",
      "2025-12-16 07:09:06.990718: val_loss -0.8326\n",
      "2025-12-16 07:09:06.990718: Pseudo dice [0.9052, 0.9431, 0.9142]\n",
      "2025-12-16 07:09:06.990718: Epoch time: 138.02 s\n",
      "2025-12-16 07:09:06.990718: Yayy! New best EMA pseudo Dice: 0.9154\n",
      "2025-12-16 07:09:07.861807: \n",
      "2025-12-16 07:09:07.861807: Epoch 110\n",
      "2025-12-16 07:09:07.877668: Current learning rate: 0.009\n",
      "2025-12-16 07:11:26.476675: train_loss -0.8163\n",
      "2025-12-16 07:11:26.476675: val_loss -0.8232\n",
      "2025-12-16 07:11:26.480679: Pseudo dice [0.9037, 0.9408, 0.897]\n",
      "2025-12-16 07:11:26.482681: Epoch time: 138.61 s\n",
      "2025-12-16 07:11:27.123350: \n",
      "2025-12-16 07:11:27.123350: Epoch 111\n",
      "2025-12-16 07:11:27.123350: Current learning rate: 0.009\n",
      "2025-12-16 07:13:45.053591: train_loss -0.8124\n",
      "2025-12-16 07:13:45.053591: val_loss -0.82\n",
      "2025-12-16 07:13:45.055593: Pseudo dice [0.9019, 0.9404, 0.8946]\n",
      "2025-12-16 07:13:45.057595: Epoch time: 137.93 s\n",
      "2025-12-16 07:13:45.770086: \n",
      "2025-12-16 07:13:45.770086: Epoch 112\n",
      "2025-12-16 07:13:45.770086: Current learning rate: 0.00899\n",
      "2025-12-16 07:16:03.878836: train_loss -0.8139\n",
      "2025-12-16 07:16:03.894652: val_loss -0.8194\n",
      "2025-12-16 07:16:03.896442: Pseudo dice [0.9016, 0.9414, 0.9043]\n",
      "2025-12-16 07:16:03.898445: Epoch time: 138.11 s\n",
      "2025-12-16 07:16:04.526295: \n",
      "2025-12-16 07:16:04.526295: Epoch 113\n",
      "2025-12-16 07:16:04.526295: Current learning rate: 0.00898\n",
      "2025-12-16 07:18:22.493658: train_loss -0.8136\n",
      "2025-12-16 07:18:22.493658: val_loss -0.8242\n",
      "2025-12-16 07:18:22.503704: Pseudo dice [0.905, 0.9417, 0.9019]\n",
      "2025-12-16 07:18:22.503704: Epoch time: 137.97 s\n",
      "2025-12-16 07:18:23.123390: \n",
      "2025-12-16 07:18:23.123390: Epoch 114\n",
      "2025-12-16 07:18:23.123390: Current learning rate: 0.00897\n",
      "2025-12-16 07:20:41.032346: train_loss -0.8151\n",
      "2025-12-16 07:20:41.032346: val_loss -0.8339\n",
      "2025-12-16 07:20:41.032346: Pseudo dice [0.9064, 0.9475, 0.9095]\n",
      "2025-12-16 07:20:41.037366: Epoch time: 137.91 s\n",
      "2025-12-16 07:20:41.037366: Yayy! New best EMA pseudo Dice: 0.9158\n",
      "2025-12-16 07:20:42.252425: \n",
      "2025-12-16 07:20:42.252425: Epoch 115\n",
      "2025-12-16 07:20:42.252425: Current learning rate: 0.00896\n",
      "2025-12-16 07:23:00.158074: train_loss -0.8098\n",
      "2025-12-16 07:23:00.160077: val_loss -0.8272\n",
      "2025-12-16 07:23:00.160077: Pseudo dice [0.9106, 0.9449, 0.8999]\n",
      "2025-12-16 07:23:00.160077: Epoch time: 137.92 s\n",
      "2025-12-16 07:23:00.160077: Yayy! New best EMA pseudo Dice: 0.916\n",
      "2025-12-16 07:23:01.056574: \n",
      "2025-12-16 07:23:01.056574: Epoch 116\n",
      "2025-12-16 07:23:01.056574: Current learning rate: 0.00895\n",
      "2025-12-16 07:25:18.940677: train_loss -0.8095\n",
      "2025-12-16 07:25:18.940677: val_loss -0.8229\n",
      "2025-12-16 07:25:18.942679: Pseudo dice [0.9052, 0.9436, 0.8986]\n",
      "2025-12-16 07:25:18.944419: Epoch time: 137.89 s\n",
      "2025-12-16 07:25:19.572071: \n",
      "2025-12-16 07:25:19.572071: Epoch 117\n",
      "2025-12-16 07:25:19.572071: Current learning rate: 0.00894\n",
      "2025-12-16 07:27:37.613979: train_loss -0.8092\n",
      "2025-12-16 07:27:37.613979: val_loss -0.8239\n",
      "2025-12-16 07:27:37.613979: Pseudo dice [0.9028, 0.9413, 0.9094]\n",
      "2025-12-16 07:27:37.613979: Epoch time: 138.04 s\n",
      "2025-12-16 07:27:37.613979: Yayy! New best EMA pseudo Dice: 0.9162\n",
      "2025-12-16 07:27:38.591943: \n",
      "2025-12-16 07:27:38.591943: Epoch 118\n",
      "2025-12-16 07:27:38.591943: Current learning rate: 0.00893\n",
      "2025-12-16 07:29:56.429544: train_loss -0.8218\n",
      "2025-12-16 07:29:56.429544: val_loss -0.8395\n",
      "2025-12-16 07:29:56.429544: Pseudo dice [0.918, 0.948, 0.9088]\n",
      "2025-12-16 07:29:56.429544: Epoch time: 137.84 s\n",
      "2025-12-16 07:29:56.429544: Yayy! New best EMA pseudo Dice: 0.9171\n",
      "2025-12-16 07:29:57.318383: \n",
      "2025-12-16 07:29:57.318383: Epoch 119\n",
      "2025-12-16 07:29:57.318383: Current learning rate: 0.00892\n",
      "2025-12-16 07:32:15.538294: train_loss -0.8164\n",
      "2025-12-16 07:32:15.554067: val_loss -0.8398\n",
      "2025-12-16 07:32:15.554067: Pseudo dice [0.9143, 0.9484, 0.9057]\n",
      "2025-12-16 07:32:15.554067: Epoch time: 138.24 s\n",
      "2025-12-16 07:32:15.554067: Yayy! New best EMA pseudo Dice: 0.9176\n",
      "2025-12-16 07:32:16.456086: \n",
      "2025-12-16 07:32:16.457826: Epoch 120\n",
      "2025-12-16 07:32:16.457826: Current learning rate: 0.00891\n",
      "2025-12-16 07:34:34.463506: train_loss -0.8161\n",
      "2025-12-16 07:34:34.479173: val_loss -0.8296\n",
      "2025-12-16 07:34:34.479173: Pseudo dice [0.9074, 0.9415, 0.9035]\n",
      "2025-12-16 07:34:34.479173: Epoch time: 138.01 s\n",
      "2025-12-16 07:34:35.401632: \n",
      "2025-12-16 07:34:35.401632: Epoch 121\n",
      "2025-12-16 07:34:35.401632: Current learning rate: 0.0089\n",
      "2025-12-16 07:36:53.453721: train_loss -0.8051\n",
      "2025-12-16 07:36:53.455724: val_loss -0.8268\n",
      "2025-12-16 07:36:53.457726: Pseudo dice [0.9052, 0.9472, 0.9087]\n",
      "2025-12-16 07:36:53.459727: Epoch time: 138.05 s\n",
      "2025-12-16 07:36:53.461729: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2025-12-16 07:36:54.355407: \n",
      "2025-12-16 07:36:54.357409: Epoch 122\n",
      "2025-12-16 07:36:54.357409: Current learning rate: 0.00889\n",
      "2025-12-16 07:39:12.146041: train_loss -0.8122\n",
      "2025-12-16 07:39:12.146041: val_loss -0.8291\n",
      "2025-12-16 07:39:12.146041: Pseudo dice [0.905, 0.9407, 0.9145]\n",
      "2025-12-16 07:39:12.146041: Epoch time: 137.79 s\n",
      "2025-12-16 07:39:12.146041: Yayy! New best EMA pseudo Dice: 0.9181\n",
      "2025-12-16 07:39:13.042765: \n",
      "2025-12-16 07:39:13.042765: Epoch 123\n",
      "2025-12-16 07:39:13.042765: Current learning rate: 0.00889\n",
      "2025-12-16 07:41:31.117325: train_loss -0.8113\n",
      "2025-12-16 07:41:31.117325: val_loss -0.8091\n",
      "2025-12-16 07:41:31.117325: Pseudo dice [0.893, 0.9346, 0.9038]\n",
      "2025-12-16 07:41:31.117325: Epoch time: 138.08 s\n",
      "2025-12-16 07:41:31.858912: \n",
      "2025-12-16 07:41:31.858912: Epoch 124\n",
      "2025-12-16 07:41:31.874723: Current learning rate: 0.00888\n",
      "2025-12-16 07:43:49.915825: train_loss -0.8158\n",
      "2025-12-16 07:43:49.917827: val_loss -0.8246\n",
      "2025-12-16 07:43:49.917827: Pseudo dice [0.9048, 0.9409, 0.91]\n",
      "2025-12-16 07:43:49.917827: Epoch time: 138.06 s\n",
      "2025-12-16 07:43:50.544592: \n",
      "2025-12-16 07:43:50.544592: Epoch 125\n",
      "2025-12-16 07:43:50.544592: Current learning rate: 0.00887\n",
      "2025-12-16 07:46:08.526284: train_loss -0.8162\n",
      "2025-12-16 07:46:08.526284: val_loss -0.8419\n",
      "2025-12-16 07:46:08.526284: Pseudo dice [0.9121, 0.9468, 0.9142]\n",
      "2025-12-16 07:46:08.526284: Epoch time: 137.99 s\n",
      "2025-12-16 07:46:08.526284: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2025-12-16 07:46:09.395917: \n",
      "2025-12-16 07:46:09.395917: Epoch 126\n",
      "2025-12-16 07:46:09.395917: Current learning rate: 0.00886\n",
      "2025-12-16 07:48:27.312660: train_loss -0.8139\n",
      "2025-12-16 07:48:27.314663: val_loss -0.83\n",
      "2025-12-16 07:48:27.314663: Pseudo dice [0.9068, 0.946, 0.9037]\n",
      "2025-12-16 07:48:27.314663: Epoch time: 137.92 s\n",
      "2025-12-16 07:48:27.314663: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2025-12-16 07:48:28.527623: \n",
      "2025-12-16 07:48:28.543591: Epoch 127\n",
      "2025-12-16 07:48:28.543591: Current learning rate: 0.00885\n",
      "2025-12-16 07:50:46.399028: train_loss -0.8142\n",
      "2025-12-16 07:50:46.399028: val_loss -0.8448\n",
      "2025-12-16 07:50:46.399028: Pseudo dice [0.9169, 0.9461, 0.9106]\n",
      "2025-12-16 07:50:46.399028: Epoch time: 137.87 s\n",
      "2025-12-16 07:50:46.399028: Yayy! New best EMA pseudo Dice: 0.9189\n",
      "2025-12-16 07:50:47.284914: \n",
      "2025-12-16 07:50:47.284914: Epoch 128\n",
      "2025-12-16 07:50:47.300993: Current learning rate: 0.00884\n",
      "2025-12-16 07:53:05.236288: train_loss -0.819\n",
      "2025-12-16 07:53:05.238291: val_loss -0.8364\n",
      "2025-12-16 07:53:05.240292: Pseudo dice [0.9077, 0.9436, 0.9242]\n",
      "2025-12-16 07:53:05.242294: Epoch time: 137.95 s\n",
      "2025-12-16 07:53:05.244297: Yayy! New best EMA pseudo Dice: 0.9195\n",
      "2025-12-16 07:53:06.128640: \n",
      "2025-12-16 07:53:06.144431: Epoch 129\n",
      "2025-12-16 07:53:06.144431: Current learning rate: 0.00883\n",
      "2025-12-16 07:55:23.938346: train_loss -0.8144\n",
      "2025-12-16 07:55:23.938346: val_loss -0.8246\n",
      "2025-12-16 07:55:23.938346: Pseudo dice [0.9031, 0.9402, 0.9072]\n",
      "2025-12-16 07:55:23.938346: Epoch time: 137.81 s\n",
      "2025-12-16 07:55:24.688314: \n",
      "2025-12-16 07:55:24.688314: Epoch 130\n",
      "2025-12-16 07:55:24.690318: Current learning rate: 0.00882\n",
      "2025-12-16 07:57:42.620835: train_loss -0.8084\n",
      "2025-12-16 07:57:42.620835: val_loss -0.8145\n",
      "2025-12-16 07:57:42.620835: Pseudo dice [0.9003, 0.9407, 0.8989]\n",
      "2025-12-16 07:57:42.620835: Epoch time: 137.93 s\n",
      "2025-12-16 07:57:43.270276: \n",
      "2025-12-16 07:57:43.270276: Epoch 131\n",
      "2025-12-16 07:57:43.286261: Current learning rate: 0.00881\n",
      "2025-12-16 08:00:02.445165: train_loss -0.8148\n",
      "2025-12-16 08:00:02.445165: val_loss -0.833\n",
      "2025-12-16 08:00:02.445165: Pseudo dice [0.9065, 0.944, 0.9079]\n",
      "2025-12-16 08:00:02.460949: Epoch time: 139.17 s\n",
      "2025-12-16 08:00:03.087065: \n",
      "2025-12-16 08:00:03.087065: Epoch 132\n",
      "2025-12-16 08:00:03.087065: Current learning rate: 0.0088\n",
      "2025-12-16 08:02:21.230690: train_loss -0.8133\n",
      "2025-12-16 08:02:21.230690: val_loss -0.8285\n",
      "2025-12-16 08:02:21.230690: Pseudo dice [0.9088, 0.9455, 0.895]\n",
      "2025-12-16 08:02:21.230690: Epoch time: 138.14 s\n",
      "2025-12-16 08:02:22.220348: \n",
      "2025-12-16 08:02:22.220348: Epoch 133\n",
      "2025-12-16 08:02:22.220348: Current learning rate: 0.00879\n",
      "2025-12-16 08:04:40.420774: train_loss -0.8046\n",
      "2025-12-16 08:04:40.420774: val_loss -0.8304\n",
      "2025-12-16 08:04:40.422777: Pseudo dice [0.9138, 0.9431, 0.9035]\n",
      "2025-12-16 08:04:40.422777: Epoch time: 138.2 s\n",
      "2025-12-16 08:04:41.043418: \n",
      "2025-12-16 08:04:41.043418: Epoch 134\n",
      "2025-12-16 08:04:41.043418: Current learning rate: 0.00879\n",
      "2025-12-16 08:06:59.039310: train_loss -0.8166\n",
      "2025-12-16 08:06:59.055130: val_loss -0.8249\n",
      "2025-12-16 08:06:59.057077: Pseudo dice [0.9066, 0.9387, 0.9106]\n",
      "2025-12-16 08:06:59.057077: Epoch time: 138.0 s\n",
      "2025-12-16 08:06:59.694614: \n",
      "2025-12-16 08:06:59.694614: Epoch 135\n",
      "2025-12-16 08:06:59.694614: Current learning rate: 0.00878\n",
      "2025-12-16 08:09:17.910825: train_loss -0.8119\n",
      "2025-12-16 08:09:17.910825: val_loss -0.821\n",
      "2025-12-16 08:09:17.926459: Pseudo dice [0.8961, 0.942, 0.916]\n",
      "2025-12-16 08:09:17.928351: Epoch time: 138.22 s\n",
      "2025-12-16 08:09:18.660184: \n",
      "2025-12-16 08:09:18.660184: Epoch 136\n",
      "2025-12-16 08:09:18.660184: Current learning rate: 0.00877\n",
      "2025-12-16 08:11:36.774931: train_loss -0.8157\n",
      "2025-12-16 08:11:36.774931: val_loss -0.8306\n",
      "2025-12-16 08:11:36.774931: Pseudo dice [0.906, 0.9442, 0.8988]\n",
      "2025-12-16 08:11:36.774931: Epoch time: 138.11 s\n",
      "2025-12-16 08:11:37.440377: \n",
      "2025-12-16 08:11:37.440377: Epoch 137\n",
      "2025-12-16 08:11:37.440377: Current learning rate: 0.00876\n",
      "2025-12-16 08:13:55.415548: train_loss -0.8138\n",
      "2025-12-16 08:13:55.417551: val_loss -0.826\n",
      "2025-12-16 08:13:55.421557: Pseudo dice [0.9063, 0.9425, 0.9153]\n",
      "2025-12-16 08:13:55.423559: Epoch time: 137.98 s\n",
      "2025-12-16 08:13:56.063657: \n",
      "2025-12-16 08:13:56.063657: Epoch 138\n",
      "2025-12-16 08:13:56.063657: Current learning rate: 0.00875\n",
      "2025-12-16 08:16:14.005322: train_loss -0.8118\n",
      "2025-12-16 08:16:14.005322: val_loss -0.8417\n",
      "2025-12-16 08:16:14.005322: Pseudo dice [0.9164, 0.9432, 0.9221]\n",
      "2025-12-16 08:16:14.005322: Epoch time: 137.94 s\n",
      "2025-12-16 08:16:14.005322: Yayy! New best EMA pseudo Dice: 0.9195\n",
      "2025-12-16 08:16:15.209779: \n",
      "2025-12-16 08:16:15.209779: Epoch 139\n",
      "2025-12-16 08:16:15.209779: Current learning rate: 0.00874\n",
      "2025-12-16 08:18:33.059963: train_loss -0.7745\n",
      "2025-12-16 08:18:33.059963: val_loss -0.7778\n",
      "2025-12-16 08:18:33.059963: Pseudo dice [0.8859, 0.9272, 0.8741]\n",
      "2025-12-16 08:18:33.059963: Epoch time: 137.85 s\n",
      "2025-12-16 08:18:33.709684: \n",
      "2025-12-16 08:18:33.709684: Epoch 140\n",
      "2025-12-16 08:18:33.709684: Current learning rate: 0.00873\n",
      "2025-12-16 08:20:51.647556: train_loss -0.7958\n",
      "2025-12-16 08:20:51.649558: val_loss -0.8079\n",
      "2025-12-16 08:20:51.651560: Pseudo dice [0.8911, 0.9325, 0.9125]\n",
      "2025-12-16 08:20:51.653562: Epoch time: 137.95 s\n",
      "2025-12-16 08:20:52.292313: \n",
      "2025-12-16 08:20:52.292313: Epoch 141\n",
      "2025-12-16 08:20:52.292313: Current learning rate: 0.00872\n",
      "2025-12-16 08:23:10.292566: train_loss -0.7971\n",
      "2025-12-16 08:23:10.292566: val_loss -0.7909\n",
      "2025-12-16 08:23:10.294568: Pseudo dice [0.887, 0.9345, 0.8883]\n",
      "2025-12-16 08:23:10.294568: Epoch time: 138.0 s\n",
      "2025-12-16 08:23:11.112309: \n",
      "2025-12-16 08:23:11.112309: Epoch 142\n",
      "2025-12-16 08:23:11.112309: Current learning rate: 0.00871\n",
      "2025-12-16 08:25:28.852893: train_loss -0.8033\n",
      "2025-12-16 08:25:28.855161: val_loss -0.8299\n",
      "2025-12-16 08:25:28.857163: Pseudo dice [0.9162, 0.9479, 0.9005]\n",
      "2025-12-16 08:25:28.857163: Epoch time: 137.74 s\n",
      "2025-12-16 08:25:29.499963: \n",
      "2025-12-16 08:25:29.499963: Epoch 143\n",
      "2025-12-16 08:25:29.499963: Current learning rate: 0.0087\n",
      "2025-12-16 08:27:47.603652: train_loss -0.8068\n",
      "2025-12-16 08:27:47.603652: val_loss -0.8187\n",
      "2025-12-16 08:27:47.607660: Pseudo dice [0.9045, 0.939, 0.8956]\n",
      "2025-12-16 08:27:47.609663: Epoch time: 138.1 s\n",
      "2025-12-16 08:27:48.254272: \n",
      "2025-12-16 08:27:48.254272: Epoch 144\n",
      "2025-12-16 08:27:48.254272: Current learning rate: 0.00869\n",
      "2025-12-16 08:30:06.207587: train_loss -0.8103\n",
      "2025-12-16 08:30:06.207587: val_loss -0.8294\n",
      "2025-12-16 08:30:06.211330: Pseudo dice [0.9045, 0.943, 0.9165]\n",
      "2025-12-16 08:30:06.213171: Epoch time: 137.95 s\n",
      "2025-12-16 08:30:07.097632: \n",
      "2025-12-16 08:30:07.097632: Epoch 145\n",
      "2025-12-16 08:30:07.097632: Current learning rate: 0.00868\n",
      "2025-12-16 08:32:24.971073: train_loss -0.8124\n",
      "2025-12-16 08:32:24.973075: val_loss -0.838\n",
      "2025-12-16 08:32:24.975077: Pseudo dice [0.9181, 0.9473, 0.9138]\n",
      "2025-12-16 08:32:24.976818: Epoch time: 137.87 s\n",
      "2025-12-16 08:32:25.626074: \n",
      "2025-12-16 08:32:25.626074: Epoch 146\n",
      "2025-12-16 08:32:25.626074: Current learning rate: 0.00868\n",
      "2025-12-16 08:34:43.546978: train_loss -0.8146\n",
      "2025-12-16 08:34:43.546978: val_loss -0.8251\n",
      "2025-12-16 08:34:43.548980: Pseudo dice [0.9044, 0.9441, 0.9088]\n",
      "2025-12-16 08:34:43.548980: Epoch time: 137.92 s\n",
      "2025-12-16 08:34:44.193823: \n",
      "2025-12-16 08:34:44.193823: Epoch 147\n",
      "2025-12-16 08:34:44.193823: Current learning rate: 0.00867\n",
      "2025-12-16 08:37:02.006536: train_loss -0.8084\n",
      "2025-12-16 08:37:02.008538: val_loss -0.8267\n",
      "2025-12-16 08:37:02.012543: Pseudo dice [0.9015, 0.9428, 0.9069]\n",
      "2025-12-16 08:37:02.016553: Epoch time: 137.83 s\n",
      "2025-12-16 08:37:02.652684: \n",
      "2025-12-16 08:37:02.652684: Epoch 148\n",
      "2025-12-16 08:37:02.652684: Current learning rate: 0.00866\n",
      "2025-12-16 08:39:20.392180: train_loss -0.8124\n",
      "2025-12-16 08:39:20.392180: val_loss -0.8313\n",
      "2025-12-16 08:39:20.392180: Pseudo dice [0.9119, 0.9496, 0.9025]\n",
      "2025-12-16 08:39:20.407961: Epoch time: 137.74 s\n",
      "2025-12-16 08:39:21.040771: \n",
      "2025-12-16 08:39:21.040771: Epoch 149\n",
      "2025-12-16 08:39:21.040771: Current learning rate: 0.00865\n",
      "2025-12-16 08:41:39.086846: train_loss -0.8159\n",
      "2025-12-16 08:41:39.088848: val_loss -0.8337\n",
      "2025-12-16 08:41:39.092595: Pseudo dice [0.9125, 0.9453, 0.9078]\n",
      "2025-12-16 08:41:39.094597: Epoch time: 138.05 s\n",
      "2025-12-16 08:41:40.071059: \n",
      "2025-12-16 08:41:40.071059: Epoch 150\n",
      "2025-12-16 08:41:40.071059: Current learning rate: 0.00864\n",
      "2025-12-16 08:43:58.207414: train_loss -0.8167\n",
      "2025-12-16 08:43:58.209416: val_loss -0.8345\n",
      "2025-12-16 08:43:58.211418: Pseudo dice [0.9106, 0.9443, 0.9048]\n",
      "2025-12-16 08:43:58.211418: Epoch time: 138.14 s\n",
      "2025-12-16 08:43:59.025797: \n",
      "2025-12-16 08:43:59.025797: Epoch 151\n",
      "2025-12-16 08:43:59.025797: Current learning rate: 0.00863\n",
      "2025-12-16 08:46:17.073371: train_loss -0.8188\n",
      "2025-12-16 08:46:17.075373: val_loss -0.8369\n",
      "2025-12-16 08:46:17.077375: Pseudo dice [0.9096, 0.9462, 0.9099]\n",
      "2025-12-16 08:46:17.077375: Epoch time: 138.05 s\n",
      "2025-12-16 08:46:17.727559: \n",
      "2025-12-16 08:46:17.727559: Epoch 152\n",
      "2025-12-16 08:46:17.727559: Current learning rate: 0.00862\n",
      "2025-12-16 08:48:35.613771: train_loss -0.8188\n",
      "2025-12-16 08:48:35.615777: val_loss -0.8381\n",
      "2025-12-16 08:48:35.617518: Pseudo dice [0.9075, 0.9455, 0.9215]\n",
      "2025-12-16 08:48:35.619521: Epoch time: 137.89 s\n",
      "2025-12-16 08:48:36.441549: \n",
      "2025-12-16 08:48:36.441549: Epoch 153\n",
      "2025-12-16 08:48:36.441549: Current learning rate: 0.00861\n",
      "2025-12-16 08:50:54.463149: train_loss -0.8205\n",
      "2025-12-16 08:50:54.463149: val_loss -0.8433\n",
      "2025-12-16 08:50:54.468743: Pseudo dice [0.9191, 0.9524, 0.9025]\n",
      "2025-12-16 08:50:54.470744: Epoch time: 138.04 s\n",
      "2025-12-16 08:50:54.470744: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2025-12-16 08:50:55.347893: \n",
      "2025-12-16 08:50:55.347893: Epoch 154\n",
      "2025-12-16 08:50:55.347893: Current learning rate: 0.0086\n",
      "2025-12-16 08:53:13.145121: train_loss -0.818\n",
      "2025-12-16 08:53:13.145121: val_loss -0.8308\n",
      "2025-12-16 08:53:13.147123: Pseudo dice [0.9084, 0.936, 0.9116]\n",
      "2025-12-16 08:53:13.150359: Epoch time: 137.8 s\n",
      "2025-12-16 08:53:13.805763: \n",
      "2025-12-16 08:53:13.805763: Epoch 155\n",
      "2025-12-16 08:53:13.805763: Current learning rate: 0.00859\n",
      "2025-12-16 08:55:31.881943: train_loss -0.8201\n",
      "2025-12-16 08:55:31.881943: val_loss -0.852\n",
      "2025-12-16 08:55:31.885948: Pseudo dice [0.9224, 0.9534, 0.9166]\n",
      "2025-12-16 08:55:31.889691: Epoch time: 138.08 s\n",
      "2025-12-16 08:55:31.891693: Yayy! New best EMA pseudo Dice: 0.9208\n",
      "2025-12-16 08:55:32.998777: \n",
      "2025-12-16 08:55:32.998777: Epoch 156\n",
      "2025-12-16 08:55:32.998777: Current learning rate: 0.00858\n",
      "2025-12-16 08:57:50.879634: train_loss -0.8192\n",
      "2025-12-16 08:57:50.881376: val_loss -0.8483\n",
      "2025-12-16 08:57:50.885385: Pseudo dice [0.9172, 0.9521, 0.9183]\n",
      "2025-12-16 08:57:50.887387: Epoch time: 137.88 s\n",
      "2025-12-16 08:57:50.889389: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2025-12-16 08:57:51.979815: \n",
      "2025-12-16 08:57:51.979815: Epoch 157\n",
      "2025-12-16 08:57:51.979815: Current learning rate: 0.00858\n",
      "2025-12-16 09:00:09.865686: train_loss -0.8278\n",
      "2025-12-16 09:00:09.881608: val_loss -0.847\n",
      "2025-12-16 09:00:09.881608: Pseudo dice [0.9179, 0.9491, 0.9175]\n",
      "2025-12-16 09:00:09.881608: Epoch time: 137.89 s\n",
      "2025-12-16 09:00:09.887039: Yayy! New best EMA pseudo Dice: 0.9223\n",
      "2025-12-16 09:00:10.811819: \n",
      "2025-12-16 09:00:10.811819: Epoch 158\n",
      "2025-12-16 09:00:10.811819: Current learning rate: 0.00857\n",
      "2025-12-16 09:02:28.794671: train_loss -0.8242\n",
      "2025-12-16 09:02:28.794671: val_loss -0.8245\n",
      "2025-12-16 09:02:28.794671: Pseudo dice [0.9029, 0.9408, 0.9011]\n",
      "2025-12-16 09:02:28.808901: Epoch time: 137.98 s\n",
      "2025-12-16 09:02:29.606963: \n",
      "2025-12-16 09:02:29.606963: Epoch 159\n",
      "2025-12-16 09:02:29.606963: Current learning rate: 0.00856\n",
      "2025-12-16 09:04:47.723097: train_loss -0.8199\n",
      "2025-12-16 09:04:47.723097: val_loss -0.8334\n",
      "2025-12-16 09:04:47.725100: Pseudo dice [0.9071, 0.9459, 0.9111]\n",
      "2025-12-16 09:04:47.725100: Epoch time: 138.12 s\n",
      "2025-12-16 09:04:48.392933: \n",
      "2025-12-16 09:04:48.392933: Epoch 160\n",
      "2025-12-16 09:04:48.392933: Current learning rate: 0.00855\n",
      "2025-12-16 09:07:06.375530: train_loss -0.8246\n",
      "2025-12-16 09:07:06.391251: val_loss -0.8374\n",
      "2025-12-16 09:07:06.393306: Pseudo dice [0.9075, 0.9394, 0.918]\n",
      "2025-12-16 09:07:06.393306: Epoch time: 137.98 s\n",
      "2025-12-16 09:07:07.042664: \n",
      "2025-12-16 09:07:07.042664: Epoch 161\n",
      "2025-12-16 09:07:07.042664: Current learning rate: 0.00854\n",
      "2025-12-16 09:09:25.180270: train_loss -0.8301\n",
      "2025-12-16 09:09:25.180270: val_loss -0.8406\n",
      "2025-12-16 09:09:25.184015: Pseudo dice [0.9099, 0.9445, 0.9144]\n",
      "2025-12-16 09:09:25.186018: Epoch time: 138.14 s\n",
      "2025-12-16 09:09:26.192772: \n",
      "2025-12-16 09:09:26.192772: Epoch 162\n",
      "2025-12-16 09:09:26.196778: Current learning rate: 0.00853\n",
      "2025-12-16 09:11:44.314291: train_loss -0.8275\n",
      "2025-12-16 09:11:44.314291: val_loss -0.8367\n",
      "2025-12-16 09:11:44.314291: Pseudo dice [0.9065, 0.9474, 0.9121]\n",
      "2025-12-16 09:11:44.314291: Epoch time: 138.12 s\n",
      "2025-12-16 09:11:44.949655: \n",
      "2025-12-16 09:11:44.949655: Epoch 163\n",
      "2025-12-16 09:11:44.965695: Current learning rate: 0.00852\n",
      "2025-12-16 09:14:02.993183: train_loss -0.8249\n",
      "2025-12-16 09:14:02.993183: val_loss -0.8533\n",
      "2025-12-16 09:14:02.993183: Pseudo dice [0.9171, 0.9513, 0.9223]\n",
      "2025-12-16 09:14:02.993183: Epoch time: 138.04 s\n",
      "2025-12-16 09:14:02.993183: Yayy! New best EMA pseudo Dice: 0.9226\n",
      "2025-12-16 09:14:03.881413: \n",
      "2025-12-16 09:14:03.881413: Epoch 164\n",
      "2025-12-16 09:14:03.881413: Current learning rate: 0.00851\n",
      "2025-12-16 09:16:21.877011: train_loss -0.8256\n",
      "2025-12-16 09:16:21.877011: val_loss -0.8366\n",
      "2025-12-16 09:16:21.879015: Pseudo dice [0.908, 0.9454, 0.9271]\n",
      "2025-12-16 09:16:21.881018: Epoch time: 138.0 s\n",
      "2025-12-16 09:16:21.882759: Yayy! New best EMA pseudo Dice: 0.923\n",
      "2025-12-16 09:16:22.973123: \n",
      "2025-12-16 09:16:22.973123: Epoch 165\n",
      "2025-12-16 09:16:22.973123: Current learning rate: 0.0085\n",
      "2025-12-16 09:18:40.889887: train_loss -0.8152\n",
      "2025-12-16 09:18:40.889887: val_loss -0.8389\n",
      "2025-12-16 09:18:40.893630: Pseudo dice [0.9163, 0.9456, 0.912]\n",
      "2025-12-16 09:18:40.893630: Epoch time: 137.92 s\n",
      "2025-12-16 09:18:40.893630: Yayy! New best EMA pseudo Dice: 0.9232\n",
      "2025-12-16 09:18:41.812716: \n",
      "2025-12-16 09:18:41.812716: Epoch 166\n",
      "2025-12-16 09:18:41.812716: Current learning rate: 0.00849\n",
      "2025-12-16 09:20:59.798169: train_loss -0.821\n",
      "2025-12-16 09:20:59.800172: val_loss -0.8444\n",
      "2025-12-16 09:20:59.802173: Pseudo dice [0.9102, 0.9484, 0.9312]\n",
      "2025-12-16 09:20:59.804175: Epoch time: 137.99 s\n",
      "2025-12-16 09:20:59.806178: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2025-12-16 09:21:00.730867: \n",
      "2025-12-16 09:21:00.730867: Epoch 167\n",
      "2025-12-16 09:21:00.730867: Current learning rate: 0.00848\n",
      "2025-12-16 09:23:18.700394: train_loss -0.8227\n",
      "2025-12-16 09:23:18.700394: val_loss -0.8565\n",
      "2025-12-16 09:23:18.704398: Pseudo dice [0.9235, 0.9575, 0.9201]\n",
      "2025-12-16 09:23:18.704398: Epoch time: 137.97 s\n",
      "2025-12-16 09:23:18.704398: Yayy! New best EMA pseudo Dice: 0.9248\n",
      "2025-12-16 09:23:19.955060: \n",
      "2025-12-16 09:23:19.955060: Epoch 168\n",
      "2025-12-16 09:23:19.955060: Current learning rate: 0.00847\n",
      "2025-12-16 09:25:37.930491: train_loss -0.8211\n",
      "2025-12-16 09:25:37.930491: val_loss -0.8063\n",
      "2025-12-16 09:25:37.932492: Pseudo dice [0.8935, 0.9389, 0.9066]\n",
      "2025-12-16 09:25:37.934494: Epoch time: 137.98 s\n",
      "2025-12-16 09:25:38.588709: \n",
      "2025-12-16 09:25:38.588709: Epoch 169\n",
      "2025-12-16 09:25:38.596265: Current learning rate: 0.00847\n",
      "2025-12-16 09:27:56.490700: train_loss -0.8148\n",
      "2025-12-16 09:27:56.490700: val_loss -0.8323\n",
      "2025-12-16 09:27:56.493688: Pseudo dice [0.9098, 0.9485, 0.9132]\n",
      "2025-12-16 09:27:56.495691: Epoch time: 137.9 s\n",
      "2025-12-16 09:27:57.149137: \n",
      "2025-12-16 09:27:57.149137: Epoch 170\n",
      "2025-12-16 09:27:57.154075: Current learning rate: 0.00846\n",
      "2025-12-16 09:30:14.917004: train_loss -0.8084\n",
      "2025-12-16 09:30:14.917004: val_loss -0.8388\n",
      "2025-12-16 09:30:14.920509: Pseudo dice [0.9114, 0.9438, 0.9199]\n",
      "2025-12-16 09:30:14.922511: Epoch time: 137.77 s\n",
      "2025-12-16 09:30:15.707001: \n",
      "2025-12-16 09:30:15.707001: Epoch 171\n",
      "2025-12-16 09:30:15.712379: Current learning rate: 0.00845\n",
      "2025-12-16 09:32:33.781558: train_loss -0.8201\n",
      "2025-12-16 09:32:33.781558: val_loss -0.8394\n",
      "2025-12-16 09:32:33.797327: Pseudo dice [0.9116, 0.9487, 0.9236]\n",
      "2025-12-16 09:32:33.797327: Epoch time: 138.07 s\n",
      "2025-12-16 09:32:34.432767: \n",
      "2025-12-16 09:32:34.432767: Epoch 172\n",
      "2025-12-16 09:32:34.444613: Current learning rate: 0.00844\n",
      "2025-12-16 09:34:52.489471: train_loss -0.824\n",
      "2025-12-16 09:34:52.491212: val_loss -0.8497\n",
      "2025-12-16 09:34:52.493215: Pseudo dice [0.9213, 0.9509, 0.9123]\n",
      "2025-12-16 09:34:52.493215: Epoch time: 138.06 s\n",
      "2025-12-16 09:34:53.139836: \n",
      "2025-12-16 09:34:53.139836: Epoch 173\n",
      "2025-12-16 09:34:53.155694: Current learning rate: 0.00843\n",
      "2025-12-16 09:37:11.382997: train_loss -0.8243\n",
      "2025-12-16 09:37:11.384998: val_loss -0.8381\n",
      "2025-12-16 09:37:11.387001: Pseudo dice [0.9046, 0.9423, 0.9194]\n",
      "2025-12-16 09:37:11.389003: Epoch time: 138.24 s\n",
      "2025-12-16 09:37:12.324250: \n",
      "2025-12-16 09:37:12.324250: Epoch 174\n",
      "2025-12-16 09:37:12.324250: Current learning rate: 0.00842\n",
      "2025-12-16 09:39:30.199776: train_loss -0.8182\n",
      "2025-12-16 09:39:30.199776: val_loss -0.8371\n",
      "2025-12-16 09:39:30.199776: Pseudo dice [0.9103, 0.9471, 0.912]\n",
      "2025-12-16 09:39:30.199776: Epoch time: 137.88 s\n",
      "2025-12-16 09:39:30.863528: \n",
      "2025-12-16 09:39:30.863528: Epoch 175\n",
      "2025-12-16 09:39:30.863528: Current learning rate: 0.00841\n",
      "2025-12-16 09:41:48.960871: train_loss -0.8213\n",
      "2025-12-16 09:41:48.960871: val_loss -0.8345\n",
      "2025-12-16 09:41:48.963245: Pseudo dice [0.9051, 0.9417, 0.9235]\n",
      "2025-12-16 09:41:48.965248: Epoch time: 138.1 s\n",
      "2025-12-16 09:41:49.608570: \n",
      "2025-12-16 09:41:49.608570: Epoch 176\n",
      "2025-12-16 09:41:49.624251: Current learning rate: 0.0084\n",
      "2025-12-16 09:44:07.619451: train_loss -0.8276\n",
      "2025-12-16 09:44:07.619451: val_loss -0.8405\n",
      "2025-12-16 09:44:07.621454: Pseudo dice [0.9107, 0.9501, 0.9158]\n",
      "2025-12-16 09:44:07.623456: Epoch time: 138.01 s\n",
      "2025-12-16 09:44:08.408533: \n",
      "2025-12-16 09:44:08.408533: Epoch 177\n",
      "2025-12-16 09:44:08.424614: Current learning rate: 0.00839\n",
      "2025-12-16 09:46:26.320095: train_loss -0.8264\n",
      "2025-12-16 09:46:26.333909: val_loss -0.8323\n",
      "2025-12-16 09:46:26.335792: Pseudo dice [0.9083, 0.9424, 0.907]\n",
      "2025-12-16 09:46:26.337794: Epoch time: 137.91 s\n",
      "2025-12-16 09:46:26.984305: \n",
      "2025-12-16 09:46:26.984305: Epoch 178\n",
      "2025-12-16 09:46:26.989915: Current learning rate: 0.00838\n",
      "2025-12-16 09:48:44.852975: train_loss -0.8263\n",
      "2025-12-16 09:48:44.852975: val_loss -0.8403\n",
      "2025-12-16 09:48:44.852975: Pseudo dice [0.9131, 0.9437, 0.9246]\n",
      "2025-12-16 09:48:44.852975: Epoch time: 137.87 s\n",
      "2025-12-16 09:48:45.500901: \n",
      "2025-12-16 09:48:45.516643: Epoch 179\n",
      "2025-12-16 09:48:45.516643: Current learning rate: 0.00837\n",
      "2025-12-16 09:51:03.446881: train_loss -0.8289\n",
      "2025-12-16 09:51:03.446881: val_loss -0.8355\n",
      "2025-12-16 09:51:03.462735: Pseudo dice [0.9077, 0.9481, 0.9195]\n",
      "2025-12-16 09:51:03.462735: Epoch time: 137.95 s\n",
      "2025-12-16 09:51:04.286434: \n",
      "2025-12-16 09:51:04.286434: Epoch 180\n",
      "2025-12-16 09:51:04.286434: Current learning rate: 0.00836\n",
      "2025-12-16 09:53:22.164889: train_loss -0.8286\n",
      "2025-12-16 09:53:22.164889: val_loss -0.8426\n",
      "2025-12-16 09:53:22.166891: Pseudo dice [0.9138, 0.9502, 0.9123]\n",
      "2025-12-16 09:53:22.168893: Epoch time: 137.88 s\n",
      "2025-12-16 09:53:22.823304: \n",
      "2025-12-16 09:53:22.823304: Epoch 181\n",
      "2025-12-16 09:53:22.823304: Current learning rate: 0.00836\n",
      "2025-12-16 09:55:40.751195: train_loss -0.8331\n",
      "2025-12-16 09:55:40.751195: val_loss -0.8387\n",
      "2025-12-16 09:55:40.751195: Pseudo dice [0.9087, 0.9455, 0.923]\n",
      "2025-12-16 09:55:40.751195: Epoch time: 137.94 s\n",
      "2025-12-16 09:55:41.416953: \n",
      "2025-12-16 09:55:41.416953: Epoch 182\n",
      "2025-12-16 09:55:41.416953: Current learning rate: 0.00835\n",
      "2025-12-16 09:57:59.177516: train_loss -0.8245\n",
      "2025-12-16 09:57:59.177516: val_loss -0.8435\n",
      "2025-12-16 09:57:59.181440: Pseudo dice [0.9101, 0.9478, 0.9182]\n",
      "2025-12-16 09:57:59.183441: Epoch time: 137.76 s\n",
      "2025-12-16 09:57:59.841945: \n",
      "2025-12-16 09:57:59.841945: Epoch 183\n",
      "2025-12-16 09:57:59.841945: Current learning rate: 0.00834\n",
      "2025-12-16 10:00:17.741603: train_loss -0.8296\n",
      "2025-12-16 10:00:17.743605: val_loss -0.8375\n",
      "2025-12-16 10:00:17.747996: Pseudo dice [0.9095, 0.9445, 0.9157]\n",
      "2025-12-16 10:00:17.750002: Epoch time: 137.92 s\n",
      "2025-12-16 10:00:18.398878: \n",
      "2025-12-16 10:00:18.412897: Epoch 184\n",
      "2025-12-16 10:00:18.412897: Current learning rate: 0.00833\n",
      "2025-12-16 10:02:36.281253: train_loss -0.8301\n",
      "2025-12-16 10:02:36.281253: val_loss -0.8434\n",
      "2025-12-16 10:02:36.281253: Pseudo dice [0.909, 0.9469, 0.9258]\n",
      "2025-12-16 10:02:36.281253: Epoch time: 137.88 s\n",
      "2025-12-16 10:02:36.995447: \n",
      "2025-12-16 10:02:36.995447: Epoch 185\n",
      "2025-12-16 10:02:36.995447: Current learning rate: 0.00832\n",
      "2025-12-16 10:04:55.067536: train_loss -0.8267\n",
      "2025-12-16 10:04:55.067536: val_loss -0.8301\n",
      "2025-12-16 10:04:55.067536: Pseudo dice [0.9071, 0.9417, 0.9049]\n",
      "2025-12-16 10:04:55.067536: Epoch time: 138.07 s\n",
      "2025-12-16 10:04:55.717058: \n",
      "2025-12-16 10:04:55.717058: Epoch 186\n",
      "2025-12-16 10:04:55.717058: Current learning rate: 0.00831\n",
      "2025-12-16 10:07:13.810108: train_loss -0.8199\n",
      "2025-12-16 10:07:13.810108: val_loss -0.8465\n",
      "2025-12-16 10:07:13.810108: Pseudo dice [0.9132, 0.9518, 0.9178]\n",
      "2025-12-16 10:07:13.810108: Epoch time: 138.09 s\n",
      "2025-12-16 10:07:14.631314: \n",
      "2025-12-16 10:07:14.631314: Epoch 187\n",
      "2025-12-16 10:07:14.631314: Current learning rate: 0.0083\n",
      "2025-12-16 10:09:32.812096: train_loss -0.8216\n",
      "2025-12-16 10:09:32.814098: val_loss -0.8393\n",
      "2025-12-16 10:09:32.816100: Pseudo dice [0.904, 0.9466, 0.9116]\n",
      "2025-12-16 10:09:32.819843: Epoch time: 138.18 s\n",
      "2025-12-16 10:09:33.501024: \n",
      "2025-12-16 10:09:33.501024: Epoch 188\n",
      "2025-12-16 10:09:33.517123: Current learning rate: 0.00829\n",
      "2025-12-16 10:11:51.691409: train_loss -0.8294\n",
      "2025-12-16 10:11:51.691409: val_loss -0.8518\n",
      "2025-12-16 10:11:51.691409: Pseudo dice [0.9175, 0.9491, 0.92]\n",
      "2025-12-16 10:11:51.691409: Epoch time: 138.19 s\n",
      "2025-12-16 10:11:52.340409: \n",
      "2025-12-16 10:11:52.340409: Epoch 189\n",
      "2025-12-16 10:11:52.356097: Current learning rate: 0.00828\n",
      "2025-12-16 10:14:10.450679: train_loss -0.8353\n",
      "2025-12-16 10:14:10.450679: val_loss -0.8489\n",
      "2025-12-16 10:14:10.453793: Pseudo dice [0.9129, 0.9515, 0.9301]\n",
      "2025-12-16 10:14:10.455795: Epoch time: 138.11 s\n",
      "2025-12-16 10:14:10.459799: Yayy! New best EMA pseudo Dice: 0.9252\n",
      "2025-12-16 10:14:11.337190: \n",
      "2025-12-16 10:14:11.337190: Epoch 190\n",
      "2025-12-16 10:14:11.337190: Current learning rate: 0.00827\n",
      "2025-12-16 10:16:29.223527: train_loss -0.8324\n",
      "2025-12-16 10:16:29.223527: val_loss -0.8427\n",
      "2025-12-16 10:16:29.239261: Pseudo dice [0.9126, 0.9492, 0.9217]\n",
      "2025-12-16 10:16:29.239261: Epoch time: 137.89 s\n",
      "2025-12-16 10:16:29.239261: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2025-12-16 10:16:30.223144: \n",
      "2025-12-16 10:16:30.223144: Epoch 191\n",
      "2025-12-16 10:16:30.223144: Current learning rate: 0.00826\n",
      "2025-12-16 10:18:48.248619: train_loss -0.8313\n",
      "2025-12-16 10:18:48.248619: val_loss -0.8475\n",
      "2025-12-16 10:18:48.248619: Pseudo dice [0.9127, 0.9483, 0.9266]\n",
      "2025-12-16 10:18:48.248619: Epoch time: 138.04 s\n",
      "2025-12-16 10:18:48.248619: Yayy! New best EMA pseudo Dice: 0.9258\n",
      "2025-12-16 10:18:49.171018: \n",
      "2025-12-16 10:18:49.185028: Epoch 192\n",
      "2025-12-16 10:18:49.185028: Current learning rate: 0.00825\n",
      "2025-12-16 10:21:07.211937: train_loss -0.8305\n",
      "2025-12-16 10:21:07.211937: val_loss -0.8483\n",
      "2025-12-16 10:21:07.211937: Pseudo dice [0.917, 0.9461, 0.9214]\n",
      "2025-12-16 10:21:07.227693: Epoch time: 138.04 s\n",
      "2025-12-16 10:21:07.227693: Yayy! New best EMA pseudo Dice: 0.9261\n",
      "2025-12-16 10:21:08.322987: \n",
      "2025-12-16 10:21:08.322987: Epoch 193\n",
      "2025-12-16 10:21:08.338900: Current learning rate: 0.00824\n",
      "2025-12-16 10:23:26.473074: train_loss -0.8349\n",
      "2025-12-16 10:23:26.473074: val_loss -0.851\n",
      "2025-12-16 10:23:26.486822: Pseudo dice [0.9144, 0.9469, 0.9273]\n",
      "2025-12-16 10:23:26.486822: Epoch time: 138.15 s\n",
      "2025-12-16 10:23:26.486822: Yayy! New best EMA pseudo Dice: 0.9264\n",
      "2025-12-16 10:23:27.406866: \n",
      "2025-12-16 10:23:27.406866: Epoch 194\n",
      "2025-12-16 10:23:27.406866: Current learning rate: 0.00824\n",
      "2025-12-16 10:25:45.288596: train_loss -0.8294\n",
      "2025-12-16 10:25:45.288596: val_loss -0.8555\n",
      "2025-12-16 10:25:45.288596: Pseudo dice [0.9199, 0.9537, 0.9229]\n",
      "2025-12-16 10:25:45.288596: Epoch time: 137.88 s\n",
      "2025-12-16 10:25:45.288596: Yayy! New best EMA pseudo Dice: 0.927\n",
      "2025-12-16 10:25:46.207489: \n",
      "2025-12-16 10:25:46.207489: Epoch 195\n",
      "2025-12-16 10:25:46.207489: Current learning rate: 0.00823\n",
      "2025-12-16 10:28:04.206743: train_loss -0.8294\n",
      "2025-12-16 10:28:04.206743: val_loss -0.8407\n",
      "2025-12-16 10:28:04.210748: Pseudo dice [0.9108, 0.9441, 0.9103]\n",
      "2025-12-16 10:28:04.212750: Epoch time: 138.0 s\n",
      "2025-12-16 10:28:04.873764: \n",
      "2025-12-16 10:28:04.873764: Epoch 196\n",
      "2025-12-16 10:28:04.873764: Current learning rate: 0.00822\n",
      "2025-12-16 10:30:22.783882: train_loss -0.8307\n",
      "2025-12-16 10:30:22.783882: val_loss -0.841\n",
      "2025-12-16 10:30:22.783882: Pseudo dice [0.9093, 0.9494, 0.9236]\n",
      "2025-12-16 10:30:22.783882: Epoch time: 137.91 s\n",
      "2025-12-16 10:30:23.525552: \n",
      "2025-12-16 10:30:23.525552: Epoch 197\n",
      "2025-12-16 10:30:23.525552: Current learning rate: 0.00821\n",
      "2025-12-16 10:32:41.292562: train_loss -0.8294\n",
      "2025-12-16 10:32:41.292562: val_loss -0.8519\n",
      "2025-12-16 10:32:41.292562: Pseudo dice [0.9179, 0.9539, 0.9152]\n",
      "2025-12-16 10:32:41.292562: Epoch time: 137.77 s\n",
      "2025-12-16 10:32:42.100820: \n",
      "2025-12-16 10:32:42.100820: Epoch 198\n",
      "2025-12-16 10:32:42.116611: Current learning rate: 0.0082\n",
      "2025-12-16 10:35:00.010622: train_loss -0.8331\n",
      "2025-12-16 10:35:00.010622: val_loss -0.8551\n",
      "2025-12-16 10:35:00.010622: Pseudo dice [0.9198, 0.9522, 0.919]\n",
      "2025-12-16 10:35:00.010622: Epoch time: 137.91 s\n",
      "2025-12-16 10:35:00.010622: Yayy! New best EMA pseudo Dice: 0.9272\n",
      "2025-12-16 10:35:00.902721: \n",
      "2025-12-16 10:35:00.904723: Epoch 199\n",
      "2025-12-16 10:35:00.906849: Current learning rate: 0.00819\n",
      "2025-12-16 10:37:18.763352: train_loss -0.8338\n",
      "2025-12-16 10:37:18.765354: val_loss -0.8526\n",
      "2025-12-16 10:37:18.767356: Pseudo dice [0.919, 0.9518, 0.9217]\n",
      "2025-12-16 10:37:18.769358: Epoch time: 137.86 s\n",
      "2025-12-16 10:37:19.026463: Yayy! New best EMA pseudo Dice: 0.9275\n",
      "2025-12-16 10:37:20.127155: \n",
      "2025-12-16 10:37:20.127155: Epoch 200\n",
      "2025-12-16 10:37:20.129257: Current learning rate: 0.00818\n",
      "2025-12-16 10:39:37.963537: train_loss -0.8316\n",
      "2025-12-16 10:39:37.963537: val_loss -0.8459\n",
      "2025-12-16 10:39:37.970422: Pseudo dice [0.9124, 0.9494, 0.9172]\n",
      "2025-12-16 10:39:37.972424: Epoch time: 137.84 s\n",
      "2025-12-16 10:39:38.627895: \n",
      "2025-12-16 10:39:38.627895: Epoch 201\n",
      "2025-12-16 10:39:38.627895: Current learning rate: 0.00817\n",
      "2025-12-16 10:41:56.555402: train_loss -0.8312\n",
      "2025-12-16 10:41:56.555402: val_loss -0.8346\n",
      "2025-12-16 10:41:56.557405: Pseudo dice [0.9029, 0.9378, 0.9273]\n",
      "2025-12-16 10:41:56.562904: Epoch time: 137.93 s\n",
      "2025-12-16 10:41:57.217003: \n",
      "2025-12-16 10:41:57.217003: Epoch 202\n",
      "2025-12-16 10:41:57.217003: Current learning rate: 0.00816\n",
      "2025-12-16 10:44:16.714457: train_loss -0.8354\n",
      "2025-12-16 10:44:16.714457: val_loss -0.8519\n",
      "2025-12-16 10:44:16.718951: Pseudo dice [0.9188, 0.9533, 0.9188]\n",
      "2025-12-16 10:44:16.720954: Epoch time: 139.5 s\n",
      "2025-12-16 10:44:17.402208: \n",
      "2025-12-16 10:44:17.402208: Epoch 203\n",
      "2025-12-16 10:44:17.402208: Current learning rate: 0.00815\n",
      "2025-12-16 10:46:37.050549: train_loss -0.8299\n",
      "2025-12-16 10:46:37.050549: val_loss -0.8518\n",
      "2025-12-16 10:46:37.050549: Pseudo dice [0.9163, 0.9501, 0.9265]\n",
      "2025-12-16 10:46:37.050549: Epoch time: 139.65 s\n",
      "2025-12-16 10:46:37.061811: Yayy! New best EMA pseudo Dice: 0.9276\n",
      "2025-12-16 10:46:38.118531: \n",
      "2025-12-16 10:46:38.118531: Epoch 204\n",
      "2025-12-16 10:46:38.118531: Current learning rate: 0.00814\n",
      "2025-12-16 10:48:56.018219: train_loss -0.8343\n",
      "2025-12-16 10:48:56.020224: val_loss -0.8579\n",
      "2025-12-16 10:48:56.024237: Pseudo dice [0.9224, 0.9576, 0.9139]\n",
      "2025-12-16 10:48:56.026239: Epoch time: 137.9 s\n",
      "2025-12-16 10:48:56.027980: Yayy! New best EMA pseudo Dice: 0.928\n",
      "2025-12-16 10:48:56.943422: \n",
      "2025-12-16 10:48:56.943422: Epoch 205\n",
      "2025-12-16 10:48:56.943422: Current learning rate: 0.00813\n",
      "2025-12-16 10:51:14.909730: train_loss -0.8295\n",
      "2025-12-16 10:51:14.909730: val_loss -0.8497\n",
      "2025-12-16 10:51:14.909730: Pseudo dice [0.9146, 0.9478, 0.9198]\n",
      "2025-12-16 10:51:14.909730: Epoch time: 137.97 s\n",
      "2025-12-16 10:51:15.535568: \n",
      "2025-12-16 10:51:15.535568: Epoch 206\n",
      "2025-12-16 10:51:15.535568: Current learning rate: 0.00813\n",
      "2025-12-16 10:53:33.586471: train_loss -0.8307\n",
      "2025-12-16 10:53:33.586471: val_loss -0.861\n",
      "2025-12-16 10:53:33.588474: Pseudo dice [0.924, 0.9564, 0.9121]\n",
      "2025-12-16 10:53:33.588474: Epoch time: 138.05 s\n",
      "2025-12-16 10:53:33.594805: Yayy! New best EMA pseudo Dice: 0.9282\n",
      "2025-12-16 10:53:34.482681: \n",
      "2025-12-16 10:53:34.482681: Epoch 207\n",
      "2025-12-16 10:53:34.482681: Current learning rate: 0.00812\n",
      "2025-12-16 10:55:52.553198: train_loss -0.8333\n",
      "2025-12-16 10:55:52.555199: val_loss -0.8416\n",
      "2025-12-16 10:55:52.557201: Pseudo dice [0.916, 0.9442, 0.9138]\n",
      "2025-12-16 10:55:52.558941: Epoch time: 138.07 s\n",
      "2025-12-16 10:55:53.190536: \n",
      "2025-12-16 10:55:53.190536: Epoch 208\n",
      "2025-12-16 10:55:53.190536: Current learning rate: 0.00811\n",
      "2025-12-16 10:58:11.284456: train_loss -0.8283\n",
      "2025-12-16 10:58:11.284456: val_loss -0.8431\n",
      "2025-12-16 10:58:11.284456: Pseudo dice [0.9105, 0.9482, 0.9189]\n",
      "2025-12-16 10:58:11.288900: Epoch time: 138.09 s\n",
      "2025-12-16 10:58:11.907638: \n",
      "2025-12-16 10:58:11.907638: Epoch 209\n",
      "2025-12-16 10:58:11.907638: Current learning rate: 0.0081\n",
      "2025-12-16 11:00:29.824456: train_loss -0.8327\n",
      "2025-12-16 11:00:29.824456: val_loss -0.8541\n",
      "2025-12-16 11:00:29.831514: Pseudo dice [0.914, 0.9461, 0.9348]\n",
      "2025-12-16 11:00:29.834081: Epoch time: 137.92 s\n",
      "2025-12-16 11:00:30.626452: \n",
      "2025-12-16 11:00:30.626452: Epoch 210\n",
      "2025-12-16 11:00:30.626452: Current learning rate: 0.00809\n",
      "2025-12-16 11:02:48.619218: train_loss -0.8277\n",
      "2025-12-16 11:02:48.619218: val_loss -0.8403\n",
      "2025-12-16 11:02:48.623222: Pseudo dice [0.9082, 0.9463, 0.9173]\n",
      "2025-12-16 11:02:48.625224: Epoch time: 137.99 s\n",
      "2025-12-16 11:02:49.254083: \n",
      "2025-12-16 11:02:49.254083: Epoch 211\n",
      "2025-12-16 11:02:49.257828: Current learning rate: 0.00808\n",
      "2025-12-16 11:05:07.290141: train_loss -0.8343\n",
      "2025-12-16 11:05:07.290141: val_loss -0.8534\n",
      "2025-12-16 11:05:07.295147: Pseudo dice [0.918, 0.9532, 0.9225]\n",
      "2025-12-16 11:05:07.297149: Epoch time: 138.04 s\n",
      "2025-12-16 11:05:07.930880: \n",
      "2025-12-16 11:05:07.930880: Epoch 212\n",
      "2025-12-16 11:05:07.930880: Current learning rate: 0.00807\n",
      "2025-12-16 11:07:25.968099: train_loss -0.8202\n",
      "2025-12-16 11:07:25.968099: val_loss -0.8528\n",
      "2025-12-16 11:07:25.984046: Pseudo dice [0.9156, 0.9521, 0.9275]\n",
      "2025-12-16 11:07:25.984046: Epoch time: 138.04 s\n",
      "2025-12-16 11:07:25.984046: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2025-12-16 11:07:26.864779: \n",
      "2025-12-16 11:07:26.864779: Epoch 213\n",
      "2025-12-16 11:07:26.864779: Current learning rate: 0.00806\n",
      "2025-12-16 11:09:45.335214: train_loss -0.8311\n",
      "2025-12-16 11:09:45.350923: val_loss -0.8423\n",
      "2025-12-16 11:09:45.350923: Pseudo dice [0.9077, 0.948, 0.9186]\n",
      "2025-12-16 11:09:45.350923: Epoch time: 138.47 s\n",
      "2025-12-16 11:09:45.969582: \n",
      "2025-12-16 11:09:45.969582: Epoch 214\n",
      "2025-12-16 11:09:45.969582: Current learning rate: 0.00805\n",
      "2025-12-16 11:12:04.300150: train_loss -0.8286\n",
      "2025-12-16 11:12:04.300150: val_loss -0.8269\n",
      "2025-12-16 11:12:04.304154: Pseudo dice [0.8997, 0.9364, 0.9091]\n",
      "2025-12-16 11:12:04.306156: Epoch time: 138.33 s\n",
      "2025-12-16 11:12:04.948822: \n",
      "2025-12-16 11:12:04.948822: Epoch 215\n",
      "2025-12-16 11:12:04.948822: Current learning rate: 0.00804\n",
      "2025-12-16 11:14:22.908002: train_loss -0.8287\n",
      "2025-12-16 11:14:22.908002: val_loss -0.8261\n",
      "2025-12-16 11:14:22.908002: Pseudo dice [0.899, 0.9393, 0.906]\n",
      "2025-12-16 11:14:22.908002: Epoch time: 137.96 s\n",
      "2025-12-16 11:14:23.708010: \n",
      "2025-12-16 11:14:23.708010: Epoch 216\n",
      "2025-12-16 11:14:23.708010: Current learning rate: 0.00803\n",
      "2025-12-16 11:16:41.755986: train_loss -0.8321\n",
      "2025-12-16 11:16:41.757727: val_loss -0.8425\n",
      "2025-12-16 11:16:41.759729: Pseudo dice [0.9155, 0.9457, 0.9228]\n",
      "2025-12-16 11:16:41.763734: Epoch time: 138.05 s\n",
      "2025-12-16 11:16:42.391367: \n",
      "2025-12-16 11:16:42.391367: Epoch 217\n",
      "2025-12-16 11:16:42.391367: Current learning rate: 0.00802\n",
      "2025-12-16 11:19:00.387583: train_loss -0.8298\n",
      "2025-12-16 11:19:00.387583: val_loss -0.8421\n",
      "2025-12-16 11:19:00.387583: Pseudo dice [0.9061, 0.9468, 0.9239]\n",
      "2025-12-16 11:19:00.400463: Epoch time: 138.0 s\n",
      "2025-12-16 11:19:01.039875: \n",
      "2025-12-16 11:19:01.039875: Epoch 218\n",
      "2025-12-16 11:19:01.039875: Current learning rate: 0.00801\n",
      "2025-12-16 11:21:19.004887: train_loss -0.8339\n",
      "2025-12-16 11:21:19.004887: val_loss -0.847\n",
      "2025-12-16 11:21:19.006889: Pseudo dice [0.9112, 0.9499, 0.9269]\n",
      "2025-12-16 11:21:19.010893: Epoch time: 137.97 s\n",
      "2025-12-16 11:21:19.643876: \n",
      "2025-12-16 11:21:19.645878: Epoch 219\n",
      "2025-12-16 11:21:19.648625: Current learning rate: 0.00801\n",
      "2025-12-16 11:23:37.676148: train_loss -0.8315\n",
      "2025-12-16 11:23:37.676148: val_loss -0.8465\n",
      "2025-12-16 11:23:37.678151: Pseudo dice [0.9163, 0.949, 0.9155]\n",
      "2025-12-16 11:23:37.678151: Epoch time: 138.03 s\n",
      "2025-12-16 11:23:38.333202: \n",
      "2025-12-16 11:23:38.333202: Epoch 220\n",
      "2025-12-16 11:23:38.333202: Current learning rate: 0.008\n",
      "2025-12-16 11:25:56.492593: train_loss -0.8265\n",
      "2025-12-16 11:25:56.494595: val_loss -0.8435\n",
      "2025-12-16 11:25:56.496597: Pseudo dice [0.9101, 0.9501, 0.9226]\n",
      "2025-12-16 11:25:56.498599: Epoch time: 138.16 s\n",
      "2025-12-16 11:25:57.135141: \n",
      "2025-12-16 11:25:57.135141: Epoch 221\n",
      "2025-12-16 11:25:57.135141: Current learning rate: 0.00799\n",
      "2025-12-16 11:28:15.001089: train_loss -0.8293\n",
      "2025-12-16 11:28:15.001089: val_loss -0.841\n",
      "2025-12-16 11:28:15.001089: Pseudo dice [0.9105, 0.9443, 0.9175]\n",
      "2025-12-16 11:28:15.001089: Epoch time: 137.87 s\n",
      "2025-12-16 11:28:15.632859: \n",
      "2025-12-16 11:28:15.632859: Epoch 222\n",
      "2025-12-16 11:28:15.632859: Current learning rate: 0.00798\n",
      "2025-12-16 11:30:33.765129: train_loss -0.8286\n",
      "2025-12-16 11:30:33.765129: val_loss -0.8549\n",
      "2025-12-16 11:30:33.769133: Pseudo dice [0.9179, 0.9524, 0.9318]\n",
      "2025-12-16 11:30:33.769133: Epoch time: 138.13 s\n",
      "2025-12-16 11:30:34.578800: \n",
      "2025-12-16 11:30:34.578800: Epoch 223\n",
      "2025-12-16 11:30:34.589061: Current learning rate: 0.00797\n",
      "2025-12-16 11:32:52.403569: train_loss -0.8356\n",
      "2025-12-16 11:32:52.403569: val_loss -0.8437\n",
      "2025-12-16 11:32:52.403569: Pseudo dice [0.9115, 0.9445, 0.9207]\n",
      "2025-12-16 11:32:52.417233: Epoch time: 137.82 s\n",
      "2025-12-16 11:32:53.035771: \n",
      "2025-12-16 11:32:53.035771: Epoch 224\n",
      "2025-12-16 11:32:53.051722: Current learning rate: 0.00796\n",
      "2025-12-16 11:35:10.934595: train_loss -0.8349\n",
      "2025-12-16 11:35:10.934595: val_loss -0.8608\n",
      "2025-12-16 11:35:10.934595: Pseudo dice [0.9291, 0.9566, 0.9199]\n",
      "2025-12-16 11:35:10.934595: Epoch time: 137.9 s\n",
      "2025-12-16 11:35:11.553732: \n",
      "2025-12-16 11:35:11.553732: Epoch 225\n",
      "2025-12-16 11:35:11.571584: Current learning rate: 0.00795\n",
      "2025-12-16 11:37:29.475529: train_loss -0.8384\n",
      "2025-12-16 11:37:29.479533: val_loss -0.8455\n",
      "2025-12-16 11:37:29.479533: Pseudo dice [0.9091, 0.9408, 0.9315]\n",
      "2025-12-16 11:37:29.483021: Epoch time: 137.92 s\n",
      "2025-12-16 11:37:30.096676: \n",
      "2025-12-16 11:37:30.096676: Epoch 226\n",
      "2025-12-16 11:37:30.112606: Current learning rate: 0.00794\n",
      "2025-12-16 11:39:47.987831: train_loss -0.8322\n",
      "2025-12-16 11:39:48.001883: val_loss -0.8584\n",
      "2025-12-16 11:39:48.003886: Pseudo dice [0.9208, 0.9529, 0.9238]\n",
      "2025-12-16 11:39:48.005889: Epoch time: 137.89 s\n",
      "2025-12-16 11:39:48.808910: \n",
      "2025-12-16 11:39:48.808910: Epoch 227\n",
      "2025-12-16 11:39:48.808910: Current learning rate: 0.00793\n",
      "2025-12-16 11:42:06.835869: train_loss -0.8392\n",
      "2025-12-16 11:42:06.835869: val_loss -0.8587\n",
      "2025-12-16 11:42:06.835869: Pseudo dice [0.921, 0.9542, 0.9242]\n",
      "2025-12-16 11:42:06.851419: Epoch time: 138.03 s\n",
      "2025-12-16 11:42:06.851419: Yayy! New best EMA pseudo Dice: 0.9286\n",
      "2025-12-16 11:42:07.710111: \n",
      "2025-12-16 11:42:07.712113: Epoch 228\n",
      "2025-12-16 11:42:07.712113: Current learning rate: 0.00792\n",
      "2025-12-16 11:44:25.793665: train_loss -0.8356\n",
      "2025-12-16 11:44:25.795668: val_loss -0.8627\n",
      "2025-12-16 11:44:25.797706: Pseudo dice [0.9244, 0.9528, 0.9296]\n",
      "2025-12-16 11:44:25.799708: Epoch time: 138.08 s\n",
      "2025-12-16 11:44:25.801710: Yayy! New best EMA pseudo Dice: 0.9293\n",
      "2025-12-16 11:44:26.868684: \n",
      "2025-12-16 11:44:26.868684: Epoch 229\n",
      "2025-12-16 11:44:26.868684: Current learning rate: 0.00791\n",
      "2025-12-16 11:46:44.744298: train_loss -0.8304\n",
      "2025-12-16 11:46:44.744298: val_loss -0.8387\n",
      "2025-12-16 11:46:44.744298: Pseudo dice [0.9093, 0.9431, 0.9206]\n",
      "2025-12-16 11:46:44.744298: Epoch time: 137.88 s\n",
      "2025-12-16 11:46:45.517439: \n",
      "2025-12-16 11:46:45.517439: Epoch 230\n",
      "2025-12-16 11:46:45.517439: Current learning rate: 0.0079\n",
      "2025-12-16 11:49:03.448052: train_loss -0.8337\n",
      "2025-12-16 11:49:03.449795: val_loss -0.8493\n",
      "2025-12-16 11:49:03.455812: Pseudo dice [0.9145, 0.9512, 0.9189]\n",
      "2025-12-16 11:49:03.457814: Epoch time: 137.93 s\n",
      "2025-12-16 11:49:04.088300: \n",
      "2025-12-16 11:49:04.088300: Epoch 231\n",
      "2025-12-16 11:49:04.092306: Current learning rate: 0.00789\n",
      "2025-12-16 11:51:21.937022: train_loss -0.8336\n",
      "2025-12-16 11:51:21.937022: val_loss -0.8588\n",
      "2025-12-16 11:51:21.941028: Pseudo dice [0.9203, 0.9546, 0.9219]\n",
      "2025-12-16 11:51:21.943030: Epoch time: 137.85 s\n",
      "2025-12-16 11:51:22.562615: \n",
      "2025-12-16 11:51:22.562615: Epoch 232\n",
      "2025-12-16 11:51:22.562615: Current learning rate: 0.00789\n",
      "2025-12-16 11:53:40.678890: train_loss -0.8177\n",
      "2025-12-16 11:53:40.680893: val_loss -0.8284\n",
      "2025-12-16 11:53:40.683610: Pseudo dice [0.9081, 0.944, 0.89]\n",
      "2025-12-16 11:53:40.685488: Epoch time: 138.12 s\n",
      "2025-12-16 11:53:41.394250: \n",
      "2025-12-16 11:53:41.394250: Epoch 233\n",
      "2025-12-16 11:53:41.394250: Current learning rate: 0.00788\n",
      "2025-12-16 11:55:59.426661: train_loss -0.8198\n",
      "2025-12-16 11:55:59.426661: val_loss -0.8345\n",
      "2025-12-16 11:55:59.426661: Pseudo dice [0.9037, 0.9439, 0.92]\n",
      "2025-12-16 11:55:59.426661: Epoch time: 138.03 s\n",
      "2025-12-16 11:56:00.044883: \n",
      "2025-12-16 11:56:00.044883: Epoch 234\n",
      "2025-12-16 11:56:00.044883: Current learning rate: 0.00787\n",
      "2025-12-16 11:58:18.118201: train_loss -0.8281\n",
      "2025-12-16 11:58:18.118201: val_loss -0.8439\n",
      "2025-12-16 11:58:18.118201: Pseudo dice [0.9183, 0.9474, 0.9026]\n",
      "2025-12-16 11:58:18.118201: Epoch time: 138.09 s\n",
      "2025-12-16 11:58:18.738576: \n",
      "2025-12-16 11:58:18.738576: Epoch 235\n",
      "2025-12-16 11:58:18.738576: Current learning rate: 0.00786\n",
      "2025-12-16 12:00:36.650623: train_loss -0.8304\n",
      "2025-12-16 12:00:36.650623: val_loss -0.8455\n",
      "2025-12-16 12:00:36.650623: Pseudo dice [0.9138, 0.9472, 0.9191]\n",
      "2025-12-16 12:00:36.650623: Epoch time: 137.91 s\n",
      "2025-12-16 12:00:37.569366: \n",
      "2025-12-16 12:00:37.569366: Epoch 236\n",
      "2025-12-16 12:00:37.569366: Current learning rate: 0.00785\n",
      "2025-12-16 12:02:55.555220: train_loss -0.8339\n",
      "2025-12-16 12:02:55.557223: val_loss -0.8528\n",
      "2025-12-16 12:02:55.559225: Pseudo dice [0.9202, 0.9483, 0.9195]\n",
      "2025-12-16 12:02:55.563229: Epoch time: 137.99 s\n",
      "2025-12-16 12:02:56.211781: \n",
      "2025-12-16 12:02:56.211781: Epoch 237\n",
      "2025-12-16 12:02:56.211781: Current learning rate: 0.00784\n",
      "2025-12-16 12:05:14.197168: train_loss -0.8305\n",
      "2025-12-16 12:05:14.197168: val_loss -0.8491\n",
      "2025-12-16 12:05:14.201171: Pseudo dice [0.9136, 0.9479, 0.9255]\n",
      "2025-12-16 12:05:14.203676: Epoch time: 137.99 s\n",
      "2025-12-16 12:05:14.830482: \n",
      "2025-12-16 12:05:14.830482: Epoch 238\n",
      "2025-12-16 12:05:14.830482: Current learning rate: 0.00783\n",
      "2025-12-16 12:07:32.844026: train_loss -0.8161\n",
      "2025-12-16 12:07:32.844026: val_loss -0.8455\n",
      "2025-12-16 12:07:32.849032: Pseudo dice [0.9131, 0.9488, 0.9274]\n",
      "2025-12-16 12:07:32.851034: Epoch time: 138.01 s\n",
      "2025-12-16 12:07:33.472010: \n",
      "2025-12-16 12:07:33.472010: Epoch 239\n",
      "2025-12-16 12:07:33.475361: Current learning rate: 0.00782\n",
      "2025-12-16 12:09:51.942196: train_loss -0.823\n",
      "2025-12-16 12:09:51.944198: val_loss -0.843\n",
      "2025-12-16 12:09:51.946200: Pseudo dice [0.9158, 0.9465, 0.9096]\n",
      "2025-12-16 12:09:51.950034: Epoch time: 138.47 s\n",
      "2025-12-16 12:09:52.578795: \n",
      "2025-12-16 12:09:52.578795: Epoch 240\n",
      "2025-12-16 12:09:52.580797: Current learning rate: 0.00781\n",
      "2025-12-16 12:12:10.667344: train_loss -0.8249\n",
      "2025-12-16 12:12:10.667344: val_loss -0.8407\n",
      "2025-12-16 12:12:10.670318: Pseudo dice [0.9093, 0.9462, 0.9209]\n",
      "2025-12-16 12:12:10.673322: Epoch time: 138.09 s\n",
      "2025-12-16 12:12:11.306232: \n",
      "2025-12-16 12:12:11.306232: Epoch 241\n",
      "2025-12-16 12:12:11.306232: Current learning rate: 0.0078\n",
      "2025-12-16 12:14:29.414619: train_loss -0.8276\n",
      "2025-12-16 12:14:29.414619: val_loss -0.8476\n",
      "2025-12-16 12:14:29.418622: Pseudo dice [0.9088, 0.9457, 0.9264]\n",
      "2025-12-16 12:14:29.420624: Epoch time: 138.11 s\n",
      "2025-12-16 12:14:30.084801: \n",
      "2025-12-16 12:14:30.084801: Epoch 242\n",
      "2025-12-16 12:14:30.084801: Current learning rate: 0.00779\n",
      "2025-12-16 12:16:47.976641: train_loss -0.8337\n",
      "2025-12-16 12:16:47.976641: val_loss -0.8517\n",
      "2025-12-16 12:16:47.976641: Pseudo dice [0.9147, 0.9475, 0.9293]\n",
      "2025-12-16 12:16:47.976641: Epoch time: 137.89 s\n",
      "2025-12-16 12:16:48.780125: \n",
      "2025-12-16 12:16:48.780125: Epoch 243\n",
      "2025-12-16 12:16:48.780125: Current learning rate: 0.00778\n",
      "2025-12-16 12:19:06.576393: train_loss -0.8308\n",
      "2025-12-16 12:19:06.576393: val_loss -0.84\n",
      "2025-12-16 12:19:06.580397: Pseudo dice [0.9081, 0.9467, 0.9112]\n",
      "2025-12-16 12:19:06.582138: Epoch time: 137.8 s\n",
      "2025-12-16 12:19:07.201005: \n",
      "2025-12-16 12:19:07.201005: Epoch 244\n",
      "2025-12-16 12:19:07.201005: Current learning rate: 0.00777\n",
      "2025-12-16 12:21:25.234280: train_loss -0.8359\n",
      "2025-12-16 12:21:25.234280: val_loss -0.8568\n",
      "2025-12-16 12:21:25.236288: Pseudo dice [0.9205, 0.9533, 0.9186]\n",
      "2025-12-16 12:21:25.236288: Epoch time: 138.03 s\n",
      "2025-12-16 12:21:25.866504: \n",
      "2025-12-16 12:21:25.866504: Epoch 245\n",
      "2025-12-16 12:21:25.866504: Current learning rate: 0.00777\n",
      "2025-12-16 12:23:44.025345: train_loss -0.8362\n",
      "2025-12-16 12:23:44.025345: val_loss -0.8573\n",
      "2025-12-16 12:23:44.029089: Pseudo dice [0.9209, 0.952, 0.9159]\n",
      "2025-12-16 12:23:44.029089: Epoch time: 138.16 s\n",
      "2025-12-16 12:23:44.661607: \n",
      "2025-12-16 12:23:44.661607: Epoch 246\n",
      "2025-12-16 12:23:44.661607: Current learning rate: 0.00776\n",
      "2025-12-16 12:26:02.493044: train_loss -0.8371\n",
      "2025-12-16 12:26:02.493044: val_loss -0.8429\n",
      "2025-12-16 12:26:02.497052: Pseudo dice [0.912, 0.9471, 0.9178]\n",
      "2025-12-16 12:26:02.499685: Epoch time: 137.83 s\n",
      "2025-12-16 12:26:03.117564: \n",
      "2025-12-16 12:26:03.117564: Epoch 247\n",
      "2025-12-16 12:26:03.133669: Current learning rate: 0.00775\n",
      "2025-12-16 12:28:21.019683: train_loss -0.8378\n",
      "2025-12-16 12:28:21.021685: val_loss -0.8427\n",
      "2025-12-16 12:28:21.023687: Pseudo dice [0.9124, 0.9474, 0.9169]\n",
      "2025-12-16 12:28:21.027691: Epoch time: 137.9 s\n",
      "2025-12-16 12:28:21.652651: \n",
      "2025-12-16 12:28:21.652651: Epoch 248\n",
      "2025-12-16 12:28:21.652651: Current learning rate: 0.00774\n",
      "2025-12-16 12:30:39.533468: train_loss -0.8292\n",
      "2025-12-16 12:30:39.533468: val_loss -0.8411\n",
      "2025-12-16 12:30:39.533468: Pseudo dice [0.9086, 0.9486, 0.9096]\n",
      "2025-12-16 12:30:39.533468: Epoch time: 137.88 s\n",
      "2025-12-16 12:30:40.356647: \n",
      "2025-12-16 12:30:40.356647: Epoch 249\n",
      "2025-12-16 12:30:40.356647: Current learning rate: 0.00773\n",
      "2025-12-16 12:32:58.180074: train_loss -0.8332\n",
      "2025-12-16 12:32:58.180074: val_loss -0.8598\n",
      "2025-12-16 12:32:58.180074: Pseudo dice [0.9194, 0.9528, 0.9308]\n",
      "2025-12-16 12:32:58.180074: Epoch time: 137.82 s\n",
      "2025-12-16 12:32:59.099537: \n",
      "2025-12-16 12:32:59.099537: Epoch 250\n",
      "2025-12-16 12:32:59.099537: Current learning rate: 0.00772\n",
      "2025-12-16 12:35:17.068311: train_loss -0.8353\n",
      "2025-12-16 12:35:17.068311: val_loss -0.8608\n",
      "2025-12-16 12:35:17.074040: Pseudo dice [0.9185, 0.9519, 0.9271]\n",
      "2025-12-16 12:35:17.076042: Epoch time: 137.97 s\n",
      "2025-12-16 12:35:17.684935: \n",
      "2025-12-16 12:35:17.684935: Epoch 251\n",
      "2025-12-16 12:35:17.684935: Current learning rate: 0.00771\n",
      "2025-12-16 12:37:35.778282: train_loss -0.8229\n",
      "2025-12-16 12:37:35.778282: val_loss -0.8269\n",
      "2025-12-16 12:37:35.778282: Pseudo dice [0.9002, 0.9366, 0.9074]\n",
      "2025-12-16 12:37:35.778282: Epoch time: 138.09 s\n",
      "2025-12-16 12:37:36.395031: \n",
      "2025-12-16 12:37:36.395031: Epoch 252\n",
      "2025-12-16 12:37:36.395031: Current learning rate: 0.0077\n",
      "2025-12-16 12:39:54.487101: train_loss -0.8176\n",
      "2025-12-16 12:39:54.487101: val_loss -0.8609\n",
      "2025-12-16 12:39:54.487101: Pseudo dice [0.9228, 0.9583, 0.9271]\n",
      "2025-12-16 12:39:54.487101: Epoch time: 138.09 s\n",
      "2025-12-16 12:39:55.261377: \n",
      "2025-12-16 12:39:55.261377: Epoch 253\n",
      "2025-12-16 12:39:55.261377: Current learning rate: 0.00769\n",
      "2025-12-16 12:42:13.335876: train_loss -0.8257\n",
      "2025-12-16 12:42:13.335876: val_loss -0.845\n",
      "2025-12-16 12:42:13.339880: Pseudo dice [0.9146, 0.9512, 0.9228]\n",
      "2025-12-16 12:42:13.342884: Epoch time: 138.07 s\n",
      "2025-12-16 12:42:13.975221: \n",
      "2025-12-16 12:42:13.975221: Epoch 254\n",
      "2025-12-16 12:42:13.975221: Current learning rate: 0.00768\n",
      "2025-12-16 12:44:32.037352: train_loss -0.826\n",
      "2025-12-16 12:44:32.053103: val_loss -0.8575\n",
      "2025-12-16 12:44:32.053103: Pseudo dice [0.9207, 0.9528, 0.9292]\n",
      "2025-12-16 12:44:32.053103: Epoch time: 138.06 s\n",
      "2025-12-16 12:44:32.845178: \n",
      "2025-12-16 12:44:32.845178: Epoch 255\n",
      "2025-12-16 12:44:32.845178: Current learning rate: 0.00767\n",
      "2025-12-16 12:46:50.823747: train_loss -0.8316\n",
      "2025-12-16 12:46:50.823747: val_loss -0.8463\n",
      "2025-12-16 12:46:50.823747: Pseudo dice [0.9178, 0.95, 0.9169]\n",
      "2025-12-16 12:46:50.839385: Epoch time: 137.99 s\n",
      "2025-12-16 12:46:51.554149: \n",
      "2025-12-16 12:46:51.554149: Epoch 256\n",
      "2025-12-16 12:46:51.554149: Current learning rate: 0.00766\n",
      "2025-12-16 12:49:09.353668: train_loss -0.8314\n",
      "2025-12-16 12:49:09.355670: val_loss -0.8585\n",
      "2025-12-16 12:49:09.357672: Pseudo dice [0.9157, 0.9508, 0.9364]\n",
      "2025-12-16 12:49:09.357672: Epoch time: 137.8 s\n",
      "2025-12-16 12:49:09.984352: \n",
      "2025-12-16 12:49:09.984352: Epoch 257\n",
      "2025-12-16 12:49:10.000242: Current learning rate: 0.00765\n",
      "2025-12-16 12:51:27.952257: train_loss -0.8336\n",
      "2025-12-16 12:51:27.952257: val_loss -0.855\n",
      "2025-12-16 12:51:27.956003: Pseudo dice [0.9153, 0.9526, 0.9355]\n",
      "2025-12-16 12:51:27.958005: Epoch time: 137.97 s\n",
      "2025-12-16 12:51:27.958005: Yayy! New best EMA pseudo Dice: 0.9295\n",
      "2025-12-16 12:51:28.828033: \n",
      "2025-12-16 12:51:28.828033: Epoch 258\n",
      "2025-12-16 12:51:28.828033: Current learning rate: 0.00764\n",
      "2025-12-16 12:53:46.722689: train_loss -0.8348\n",
      "2025-12-16 12:53:46.722689: val_loss -0.8548\n",
      "2025-12-16 12:53:46.722689: Pseudo dice [0.9213, 0.95, 0.926]\n",
      "2025-12-16 12:53:46.722689: Epoch time: 137.89 s\n",
      "2025-12-16 12:53:46.722689: Yayy! New best EMA pseudo Dice: 0.9298\n",
      "2025-12-16 12:53:47.765576: \n",
      "2025-12-16 12:53:47.765576: Epoch 259\n",
      "2025-12-16 12:53:47.765576: Current learning rate: 0.00764\n",
      "2025-12-16 12:56:05.871234: train_loss -0.8369\n",
      "2025-12-16 12:56:05.873235: val_loss -0.8516\n",
      "2025-12-16 12:56:05.873235: Pseudo dice [0.9115, 0.9452, 0.9323]\n",
      "2025-12-16 12:56:05.873235: Epoch time: 138.11 s\n",
      "2025-12-16 12:56:06.491220: \n",
      "2025-12-16 12:56:06.491220: Epoch 260\n",
      "2025-12-16 12:56:06.491220: Current learning rate: 0.00763\n",
      "2025-12-16 12:58:24.459063: train_loss -0.8386\n",
      "2025-12-16 12:58:24.459063: val_loss -0.8625\n",
      "2025-12-16 12:58:24.461066: Pseudo dice [0.9227, 0.9561, 0.9224]\n",
      "2025-12-16 12:58:24.464808: Epoch time: 137.97 s\n",
      "2025-12-16 12:58:24.466810: Yayy! New best EMA pseudo Dice: 0.9302\n",
      "2025-12-16 12:58:25.532539: \n",
      "2025-12-16 12:58:25.532539: Epoch 261\n",
      "2025-12-16 12:58:25.532539: Current learning rate: 0.00762\n",
      "2025-12-16 13:00:43.540227: train_loss -0.8395\n",
      "2025-12-16 13:00:43.540227: val_loss -0.8639\n",
      "2025-12-16 13:00:43.544231: Pseudo dice [0.9238, 0.9555, 0.9246]\n",
      "2025-12-16 13:00:43.546233: Epoch time: 138.01 s\n",
      "2025-12-16 13:00:43.550237: Yayy! New best EMA pseudo Dice: 0.9306\n",
      "2025-12-16 13:00:44.584657: \n",
      "2025-12-16 13:00:44.584657: Epoch 262\n",
      "2025-12-16 13:00:44.584657: Current learning rate: 0.00761\n",
      "2025-12-16 13:03:02.493730: train_loss -0.8333\n",
      "2025-12-16 13:03:02.495732: val_loss -0.8524\n",
      "2025-12-16 13:03:02.503486: Pseudo dice [0.9188, 0.9504, 0.917]\n",
      "2025-12-16 13:03:02.505489: Epoch time: 137.91 s\n",
      "2025-12-16 13:03:03.135616: \n",
      "2025-12-16 13:03:03.135616: Epoch 263\n",
      "2025-12-16 13:03:03.135616: Current learning rate: 0.0076\n",
      "2025-12-16 13:05:21.262272: train_loss -0.8351\n",
      "2025-12-16 13:05:21.264274: val_loss -0.8484\n",
      "2025-12-16 13:05:21.264274: Pseudo dice [0.9142, 0.9489, 0.9223]\n",
      "2025-12-16 13:05:21.268426: Epoch time: 138.13 s\n",
      "2025-12-16 13:05:21.896380: \n",
      "2025-12-16 13:05:21.896380: Epoch 264\n",
      "2025-12-16 13:05:21.896380: Current learning rate: 0.00759\n",
      "2025-12-16 13:07:39.727717: train_loss -0.8082\n",
      "2025-12-16 13:07:39.729457: val_loss -0.8181\n",
      "2025-12-16 13:07:39.733462: Pseudo dice [0.9007, 0.9396, 0.9177]\n",
      "2025-12-16 13:07:39.735464: Epoch time: 137.83 s\n",
      "2025-12-16 13:07:40.363475: \n",
      "2025-12-16 13:07:40.363475: Epoch 265\n",
      "2025-12-16 13:07:40.363475: Current learning rate: 0.00758\n",
      "2025-12-16 13:09:58.588528: train_loss -0.8091\n",
      "2025-12-16 13:09:58.588528: val_loss -0.8369\n",
      "2025-12-16 13:09:58.590530: Pseudo dice [0.9088, 0.9448, 0.9219]\n",
      "2025-12-16 13:09:58.593032: Epoch time: 138.23 s\n",
      "2025-12-16 13:09:59.213868: \n",
      "2025-12-16 13:09:59.213868: Epoch 266\n",
      "2025-12-16 13:09:59.213868: Current learning rate: 0.00757\n",
      "2025-12-16 13:12:17.207084: train_loss -0.8234\n",
      "2025-12-16 13:12:17.209086: val_loss -0.8495\n",
      "2025-12-16 13:12:17.213089: Pseudo dice [0.9169, 0.9518, 0.9259]\n",
      "2025-12-16 13:12:17.215091: Epoch time: 137.99 s\n",
      "2025-12-16 13:12:17.838947: \n",
      "2025-12-16 13:12:17.838947: Epoch 267\n",
      "2025-12-16 13:12:17.838947: Current learning rate: 0.00756\n",
      "2025-12-16 13:14:35.810217: train_loss -0.8315\n",
      "2025-12-16 13:14:35.810217: val_loss -0.8434\n",
      "2025-12-16 13:14:35.815054: Pseudo dice [0.9157, 0.9471, 0.9126]\n",
      "2025-12-16 13:14:35.817056: Epoch time: 137.97 s\n",
      "2025-12-16 13:14:36.615347: \n",
      "2025-12-16 13:14:36.615347: Epoch 268\n",
      "2025-12-16 13:14:36.615347: Current learning rate: 0.00755\n",
      "2025-12-16 13:16:54.536474: train_loss -0.8249\n",
      "2025-12-16 13:16:54.536474: val_loss -0.7922\n",
      "2025-12-16 13:16:54.550294: Pseudo dice [0.8954, 0.9341, 0.8854]\n",
      "2025-12-16 13:16:54.550294: Epoch time: 137.92 s\n",
      "2025-12-16 13:16:55.181619: \n",
      "2025-12-16 13:16:55.181619: Epoch 269\n",
      "2025-12-16 13:16:55.181619: Current learning rate: 0.00754\n",
      "2025-12-16 13:19:13.120633: train_loss -0.8207\n",
      "2025-12-16 13:19:13.122636: val_loss -0.8358\n",
      "2025-12-16 13:19:13.126639: Pseudo dice [0.9069, 0.9439, 0.9104]\n",
      "2025-12-16 13:19:13.128641: Epoch time: 137.94 s\n",
      "2025-12-16 13:19:13.795821: \n",
      "2025-12-16 13:19:13.795821: Epoch 270\n",
      "2025-12-16 13:19:13.795821: Current learning rate: 0.00753\n",
      "2025-12-16 13:21:31.930283: train_loss -0.8307\n",
      "2025-12-16 13:21:31.930283: val_loss -0.8546\n",
      "2025-12-16 13:21:31.934287: Pseudo dice [0.9242, 0.9544, 0.918]\n",
      "2025-12-16 13:21:31.936289: Epoch time: 138.15 s\n",
      "2025-12-16 13:21:32.560452: \n",
      "2025-12-16 13:21:32.560452: Epoch 271\n",
      "2025-12-16 13:21:32.560452: Current learning rate: 0.00752\n",
      "2025-12-16 13:23:50.386999: train_loss -0.8312\n",
      "2025-12-16 13:23:50.386999: val_loss -0.8504\n",
      "2025-12-16 13:23:50.389001: Pseudo dice [0.9145, 0.9507, 0.9209]\n",
      "2025-12-16 13:23:50.389001: Epoch time: 137.83 s\n",
      "2025-12-16 13:23:51.011505: \n",
      "2025-12-16 13:23:51.011505: Epoch 272\n",
      "2025-12-16 13:23:51.027280: Current learning rate: 0.00751\n",
      "2025-12-16 13:26:09.132943: train_loss -0.8381\n",
      "2025-12-16 13:26:09.132943: val_loss -0.8609\n",
      "2025-12-16 13:26:09.132943: Pseudo dice [0.9235, 0.9512, 0.9315]\n",
      "2025-12-16 13:26:09.132943: Epoch time: 138.12 s\n",
      "2025-12-16 13:26:09.766534: \n",
      "2025-12-16 13:26:09.766534: Epoch 273\n",
      "2025-12-16 13:26:09.766534: Current learning rate: 0.00751\n",
      "2025-12-16 13:28:27.670045: train_loss -0.8346\n",
      "2025-12-16 13:28:27.670045: val_loss -0.8565\n",
      "2025-12-16 13:28:27.670045: Pseudo dice [0.9133, 0.9494, 0.9386]\n",
      "2025-12-16 13:28:27.670045: Epoch time: 137.9 s\n",
      "2025-12-16 13:28:28.462194: \n",
      "2025-12-16 13:28:28.462194: Epoch 274\n",
      "2025-12-16 13:28:28.478274: Current learning rate: 0.0075\n",
      "2025-12-16 13:30:46.607992: train_loss -0.8351\n",
      "2025-12-16 13:30:46.609994: val_loss -0.8476\n",
      "2025-12-16 13:30:46.613736: Pseudo dice [0.9127, 0.947, 0.9189]\n",
      "2025-12-16 13:30:46.615737: Epoch time: 138.15 s\n",
      "2025-12-16 13:30:47.238204: \n",
      "2025-12-16 13:30:47.238204: Epoch 275\n",
      "2025-12-16 13:30:47.238204: Current learning rate: 0.00749\n",
      "2025-12-16 13:33:05.271841: train_loss -0.8346\n",
      "2025-12-16 13:33:05.271841: val_loss -0.8562\n",
      "2025-12-16 13:33:05.287689: Pseudo dice [0.9184, 0.9552, 0.9238]\n",
      "2025-12-16 13:33:05.287689: Epoch time: 138.03 s\n",
      "2025-12-16 13:33:06.000401: \n",
      "2025-12-16 13:33:06.000401: Epoch 276\n",
      "2025-12-16 13:33:06.000401: Current learning rate: 0.00748\n",
      "2025-12-16 13:35:23.896566: train_loss -0.8403\n",
      "2025-12-16 13:35:23.896566: val_loss -0.8599\n",
      "2025-12-16 13:35:23.898381: Pseudo dice [0.9209, 0.9532, 0.9317]\n",
      "2025-12-16 13:35:23.901805: Epoch time: 137.9 s\n",
      "2025-12-16 13:35:24.530998: \n",
      "2025-12-16 13:35:24.530998: Epoch 277\n",
      "2025-12-16 13:35:24.530998: Current learning rate: 0.00747\n",
      "2025-12-16 13:37:42.490875: train_loss -0.8382\n",
      "2025-12-16 13:37:42.490875: val_loss -0.8518\n",
      "2025-12-16 13:37:42.494617: Pseudo dice [0.9149, 0.9479, 0.926]\n",
      "2025-12-16 13:37:42.496619: Epoch time: 137.96 s\n",
      "2025-12-16 13:37:43.115216: \n",
      "2025-12-16 13:37:43.115216: Epoch 278\n",
      "2025-12-16 13:37:43.131001: Current learning rate: 0.00746\n",
      "2025-12-16 13:40:00.917869: train_loss -0.8258\n",
      "2025-12-16 13:40:00.917869: val_loss -0.8464\n",
      "2025-12-16 13:40:00.933627: Pseudo dice [0.9174, 0.9536, 0.9139]\n",
      "2025-12-16 13:40:00.933627: Epoch time: 137.8 s\n",
      "2025-12-16 13:40:01.678714: \n",
      "2025-12-16 13:40:01.678714: Epoch 279\n",
      "2025-12-16 13:40:01.678714: Current learning rate: 0.00745\n",
      "2025-12-16 13:42:19.526969: train_loss -0.8191\n",
      "2025-12-16 13:42:19.528971: val_loss -0.8424\n",
      "2025-12-16 13:42:19.530973: Pseudo dice [0.9162, 0.9501, 0.9046]\n",
      "2025-12-16 13:42:19.532975: Epoch time: 137.85 s\n",
      "2025-12-16 13:42:20.313725: \n",
      "2025-12-16 13:42:20.313725: Epoch 280\n",
      "2025-12-16 13:42:20.313725: Current learning rate: 0.00744\n",
      "2025-12-16 13:44:38.240517: train_loss -0.821\n",
      "2025-12-16 13:44:38.240517: val_loss -0.8508\n",
      "2025-12-16 13:44:38.240517: Pseudo dice [0.9148, 0.949, 0.9261]\n",
      "2025-12-16 13:44:38.256261: Epoch time: 137.93 s\n",
      "2025-12-16 13:44:38.874468: \n",
      "2025-12-16 13:44:38.874468: Epoch 281\n",
      "2025-12-16 13:44:38.874468: Current learning rate: 0.00743\n",
      "2025-12-16 13:46:56.735304: train_loss -0.8321\n",
      "2025-12-16 13:46:56.735304: val_loss -0.8502\n",
      "2025-12-16 13:46:56.739309: Pseudo dice [0.9166, 0.9523, 0.918]\n",
      "2025-12-16 13:46:56.741311: Epoch time: 137.86 s\n",
      "2025-12-16 13:46:57.560270: \n",
      "2025-12-16 13:46:57.560270: Epoch 282\n",
      "2025-12-16 13:46:57.560270: Current learning rate: 0.00742\n",
      "2025-12-16 13:49:15.360062: train_loss -0.8328\n",
      "2025-12-16 13:49:15.360062: val_loss -0.8517\n",
      "2025-12-16 13:49:15.369801: Pseudo dice [0.9154, 0.948, 0.9245]\n",
      "2025-12-16 13:49:15.369801: Epoch time: 137.8 s\n",
      "2025-12-16 13:49:15.994339: \n",
      "2025-12-16 13:49:15.994339: Epoch 283\n",
      "2025-12-16 13:49:16.010116: Current learning rate: 0.00741\n",
      "2025-12-16 13:51:33.927567: train_loss -0.8395\n",
      "2025-12-16 13:51:33.927567: val_loss -0.8555\n",
      "2025-12-16 13:51:33.931571: Pseudo dice [0.9149, 0.9505, 0.9292]\n",
      "2025-12-16 13:51:33.933311: Epoch time: 137.93 s\n",
      "2025-12-16 13:51:34.588991: \n",
      "2025-12-16 13:51:34.588991: Epoch 284\n",
      "2025-12-16 13:51:34.604935: Current learning rate: 0.0074\n",
      "2025-12-16 13:53:52.656670: train_loss -0.8343\n",
      "2025-12-16 13:53:52.656670: val_loss -0.8551\n",
      "2025-12-16 13:53:52.656670: Pseudo dice [0.9152, 0.9515, 0.9242]\n",
      "2025-12-16 13:53:52.656670: Epoch time: 138.07 s\n",
      "2025-12-16 13:53:53.287755: \n",
      "2025-12-16 13:53:53.287755: Epoch 285\n",
      "2025-12-16 13:53:53.287755: Current learning rate: 0.00739\n",
      "2025-12-16 13:56:11.362836: train_loss -0.841\n",
      "2025-12-16 13:56:11.364839: val_loss -0.8628\n",
      "2025-12-16 13:56:11.368843: Pseudo dice [0.9235, 0.9526, 0.9215]\n",
      "2025-12-16 13:56:11.370845: Epoch time: 138.08 s\n",
      "2025-12-16 13:56:11.993433: \n",
      "2025-12-16 13:56:11.993433: Epoch 286\n",
      "2025-12-16 13:56:12.009234: Current learning rate: 0.00738\n",
      "2025-12-16 13:58:29.988450: train_loss -0.845\n",
      "2025-12-16 13:58:29.988450: val_loss -0.8522\n",
      "2025-12-16 13:58:29.988450: Pseudo dice [0.9141, 0.9492, 0.9208]\n",
      "2025-12-16 13:58:29.988450: Epoch time: 138.0 s\n",
      "2025-12-16 13:58:30.797929: \n",
      "2025-12-16 13:58:30.797929: Epoch 287\n",
      "2025-12-16 13:58:30.797929: Current learning rate: 0.00738\n",
      "2025-12-16 14:00:49.547004: train_loss -0.8404\n",
      "2025-12-16 14:00:49.547004: val_loss -0.8556\n",
      "2025-12-16 14:00:49.547004: Pseudo dice [0.9201, 0.9516, 0.9298]\n",
      "2025-12-16 14:00:49.547004: Epoch time: 138.75 s\n",
      "2025-12-16 14:00:50.179596: \n",
      "2025-12-16 14:00:50.179596: Epoch 288\n",
      "2025-12-16 14:00:50.179596: Current learning rate: 0.00737\n",
      "2025-12-16 14:03:08.517884: train_loss -0.8444\n",
      "2025-12-16 14:03:08.517884: val_loss -0.8696\n",
      "2025-12-16 14:03:08.517884: Pseudo dice [0.922, 0.9529, 0.9454]\n",
      "2025-12-16 14:03:08.517884: Epoch time: 138.34 s\n",
      "2025-12-16 14:03:08.517884: Yayy! New best EMA pseudo Dice: 0.9308\n",
      "2025-12-16 14:03:09.406077: \n",
      "2025-12-16 14:03:09.406077: Epoch 289\n",
      "2025-12-16 14:03:09.406077: Current learning rate: 0.00736\n",
      "2025-12-16 14:05:27.753197: train_loss -0.8379\n",
      "2025-12-16 14:05:27.753197: val_loss -0.8435\n",
      "2025-12-16 14:05:27.769014: Pseudo dice [0.9129, 0.9429, 0.9091]\n",
      "2025-12-16 14:05:27.769014: Epoch time: 138.35 s\n",
      "2025-12-16 14:05:28.404309: \n",
      "2025-12-16 14:05:28.404309: Epoch 290\n",
      "2025-12-16 14:05:28.404309: Current learning rate: 0.00735\n",
      "2025-12-16 14:07:46.755280: train_loss -0.8478\n",
      "2025-12-16 14:07:46.755280: val_loss -0.844\n",
      "2025-12-16 14:07:46.761028: Pseudo dice [0.9088, 0.9379, 0.9321]\n",
      "2025-12-16 14:07:46.764990: Epoch time: 138.37 s\n",
      "2025-12-16 14:07:47.409121: \n",
      "2025-12-16 14:07:47.409121: Epoch 291\n",
      "2025-12-16 14:07:47.409121: Current learning rate: 0.00734\n",
      "2025-12-16 14:10:06.168989: train_loss -0.8432\n",
      "2025-12-16 14:10:06.168989: val_loss -0.8709\n",
      "2025-12-16 14:10:06.173939: Pseudo dice [0.9256, 0.9563, 0.931]\n",
      "2025-12-16 14:10:06.176944: Epoch time: 138.76 s\n",
      "2025-12-16 14:10:06.818124: \n",
      "2025-12-16 14:10:06.818124: Epoch 292\n",
      "2025-12-16 14:10:06.818124: Current learning rate: 0.00733\n",
      "2025-12-16 14:12:25.020449: train_loss -0.8409\n",
      "2025-12-16 14:12:25.020449: val_loss -0.848\n",
      "2025-12-16 14:12:25.024191: Pseudo dice [0.9153, 0.942, 0.9238]\n",
      "2025-12-16 14:12:25.027318: Epoch time: 138.2 s\n",
      "2025-12-16 14:12:25.830955: \n",
      "2025-12-16 14:12:25.830955: Epoch 293\n",
      "2025-12-16 14:12:25.830955: Current learning rate: 0.00732\n",
      "2025-12-16 14:14:43.732345: train_loss -0.838\n",
      "2025-12-16 14:14:43.732345: val_loss -0.8611\n",
      "2025-12-16 14:14:43.732345: Pseudo dice [0.9207, 0.952, 0.93]\n",
      "2025-12-16 14:14:43.732345: Epoch time: 137.9 s\n",
      "2025-12-16 14:14:44.491348: \n",
      "2025-12-16 14:14:44.491348: Epoch 294\n",
      "2025-12-16 14:14:44.491348: Current learning rate: 0.00731\n",
      "2025-12-16 14:17:02.476585: train_loss -0.8417\n",
      "2025-12-16 14:17:02.478587: val_loss -0.8609\n",
      "2025-12-16 14:17:02.480588: Pseudo dice [0.924, 0.9539, 0.9162]\n",
      "2025-12-16 14:17:02.480588: Epoch time: 137.99 s\n",
      "2025-12-16 14:17:03.129117: \n",
      "2025-12-16 14:17:03.129117: Epoch 295\n",
      "2025-12-16 14:17:03.131120: Current learning rate: 0.0073\n",
      "2025-12-16 14:19:21.051135: train_loss -0.842\n",
      "2025-12-16 14:19:21.051135: val_loss -0.8497\n",
      "2025-12-16 14:19:21.051135: Pseudo dice [0.9135, 0.9517, 0.9176]\n",
      "2025-12-16 14:19:21.051135: Epoch time: 137.92 s\n",
      "2025-12-16 14:19:21.702225: \n",
      "2025-12-16 14:19:21.702225: Epoch 296\n",
      "2025-12-16 14:19:21.704295: Current learning rate: 0.00729\n",
      "2025-12-16 14:21:39.543249: train_loss -0.8413\n",
      "2025-12-16 14:21:39.543249: val_loss -0.8555\n",
      "2025-12-16 14:21:39.559107: Pseudo dice [0.9188, 0.9507, 0.9307]\n",
      "2025-12-16 14:21:39.562608: Epoch time: 137.84 s\n",
      "2025-12-16 14:21:40.208998: \n",
      "2025-12-16 14:21:40.208998: Epoch 297\n",
      "2025-12-16 14:21:40.208998: Current learning rate: 0.00728\n",
      "2025-12-16 14:23:58.070627: train_loss -0.8368\n",
      "2025-12-16 14:23:58.070627: val_loss -0.8553\n",
      "2025-12-16 14:23:58.074952: Pseudo dice [0.9134, 0.9522, 0.9307]\n",
      "2025-12-16 14:23:58.076954: Epoch time: 137.88 s\n",
      "2025-12-16 14:23:58.737370: \n",
      "2025-12-16 14:23:58.737370: Epoch 298\n",
      "2025-12-16 14:23:58.753302: Current learning rate: 0.00727\n",
      "2025-12-16 14:26:16.791432: train_loss -0.8382\n",
      "2025-12-16 14:26:16.793434: val_loss -0.8606\n",
      "2025-12-16 14:26:16.797438: Pseudo dice [0.9198, 0.9543, 0.9293]\n",
      "2025-12-16 14:26:16.799440: Epoch time: 138.05 s\n",
      "2025-12-16 14:26:16.803445: Yayy! New best EMA pseudo Dice: 0.9311\n",
      "2025-12-16 14:26:17.861441: \n",
      "2025-12-16 14:26:17.861441: Epoch 299\n",
      "2025-12-16 14:26:17.861441: Current learning rate: 0.00726\n",
      "2025-12-16 14:28:35.893511: train_loss -0.8095\n",
      "2025-12-16 14:28:35.893511: val_loss -0.8096\n",
      "2025-12-16 14:28:35.898499: Pseudo dice [0.8957, 0.9383, 0.9017]\n",
      "2025-12-16 14:28:35.900501: Epoch time: 138.03 s\n",
      "2025-12-16 14:28:36.779014: \n",
      "2025-12-16 14:28:36.779014: Epoch 300\n",
      "2025-12-16 14:28:36.779014: Current learning rate: 0.00725\n",
      "2025-12-16 14:30:54.869938: train_loss -0.7917\n",
      "2025-12-16 14:30:54.869938: val_loss -0.8101\n",
      "2025-12-16 14:30:54.873943: Pseudo dice [0.9003, 0.9285, 0.9156]\n",
      "2025-12-16 14:30:54.875945: Epoch time: 138.09 s\n",
      "2025-12-16 14:30:55.512069: \n",
      "2025-12-16 14:30:55.512069: Epoch 301\n",
      "2025-12-16 14:30:55.512069: Current learning rate: 0.00724\n",
      "2025-12-16 14:33:13.507470: train_loss -0.8023\n",
      "2025-12-16 14:33:13.507470: val_loss -0.8485\n",
      "2025-12-16 14:33:13.511482: Pseudo dice [0.9185, 0.9472, 0.9199]\n",
      "2025-12-16 14:33:13.515487: Epoch time: 138.0 s\n",
      "2025-12-16 14:33:14.186645: \n",
      "2025-12-16 14:33:14.202437: Epoch 302\n",
      "2025-12-16 14:33:14.202437: Current learning rate: 0.00724\n",
      "2025-12-16 14:35:32.183456: train_loss -0.8261\n",
      "2025-12-16 14:35:32.185459: val_loss -0.8536\n",
      "2025-12-16 14:35:32.187462: Pseudo dice [0.9196, 0.9549, 0.9216]\n",
      "2025-12-16 14:35:32.191467: Epoch time: 138.0 s\n",
      "2025-12-16 14:35:32.830057: \n",
      "2025-12-16 14:35:32.830057: Epoch 303\n",
      "2025-12-16 14:35:32.830057: Current learning rate: 0.00723\n",
      "2025-12-16 14:37:50.784102: train_loss -0.8262\n",
      "2025-12-16 14:37:50.784102: val_loss -0.8484\n",
      "2025-12-16 14:37:50.789689: Pseudo dice [0.915, 0.953, 0.9178]\n",
      "2025-12-16 14:37:50.791691: Epoch time: 137.95 s\n",
      "2025-12-16 14:37:51.593725: \n",
      "2025-12-16 14:37:51.593725: Epoch 304\n",
      "2025-12-16 14:37:51.593725: Current learning rate: 0.00722\n",
      "2025-12-16 14:40:09.383492: train_loss -0.8235\n",
      "2025-12-16 14:40:09.387446: val_loss -0.8315\n",
      "2025-12-16 14:40:09.389449: Pseudo dice [0.9111, 0.9424, 0.9083]\n",
      "2025-12-16 14:40:09.393996: Epoch time: 137.79 s\n",
      "2025-12-16 14:40:10.050041: \n",
      "2025-12-16 14:40:10.050041: Epoch 305\n",
      "2025-12-16 14:40:10.050041: Current learning rate: 0.00721\n",
      "2025-12-16 14:42:27.805526: train_loss -0.8014\n",
      "2025-12-16 14:42:27.805526: val_loss -0.8388\n",
      "2025-12-16 14:42:27.821381: Pseudo dice [0.9132, 0.9467, 0.9155]\n",
      "2025-12-16 14:42:27.821381: Epoch time: 137.76 s\n",
      "2025-12-16 14:42:28.450001: \n",
      "2025-12-16 14:42:28.450001: Epoch 306\n",
      "2025-12-16 14:42:28.450001: Current learning rate: 0.0072\n",
      "2025-12-16 14:44:46.521214: train_loss -0.8197\n",
      "2025-12-16 14:44:46.521214: val_loss -0.8344\n",
      "2025-12-16 14:44:46.523216: Pseudo dice [0.9058, 0.9454, 0.9121]\n",
      "2025-12-16 14:44:46.527222: Epoch time: 138.07 s\n",
      "2025-12-16 14:44:47.170465: \n",
      "2025-12-16 14:44:47.170465: Epoch 307\n",
      "2025-12-16 14:44:47.173556: Current learning rate: 0.00719\n",
      "2025-12-16 14:47:05.092702: train_loss -0.8261\n",
      "2025-12-16 14:47:05.092702: val_loss -0.8338\n",
      "2025-12-16 14:47:05.092702: Pseudo dice [0.904, 0.9385, 0.9155]\n",
      "2025-12-16 14:47:05.102821: Epoch time: 137.92 s\n",
      "2025-12-16 14:47:05.868974: \n",
      "2025-12-16 14:47:05.868974: Epoch 308\n",
      "2025-12-16 14:47:05.868974: Current learning rate: 0.00718\n",
      "2025-12-16 14:49:23.829606: train_loss -0.8267\n",
      "2025-12-16 14:49:23.831612: val_loss -0.8591\n",
      "2025-12-16 14:49:23.836623: Pseudo dice [0.9239, 0.9539, 0.9146]\n",
      "2025-12-16 14:49:23.839528: Epoch time: 137.96 s\n",
      "2025-12-16 14:49:24.470594: \n",
      "2025-12-16 14:49:24.470594: Epoch 309\n",
      "2025-12-16 14:49:24.470594: Current learning rate: 0.00717\n",
      "2025-12-16 14:51:42.366717: train_loss -0.8338\n",
      "2025-12-16 14:51:42.368720: val_loss -0.8496\n",
      "2025-12-16 14:51:42.372465: Pseudo dice [0.9191, 0.9522, 0.919]\n",
      "2025-12-16 14:51:42.376470: Epoch time: 137.9 s\n",
      "2025-12-16 14:51:43.004965: \n",
      "2025-12-16 14:51:43.004965: Epoch 310\n",
      "2025-12-16 14:51:43.020902: Current learning rate: 0.00716\n",
      "2025-12-16 14:54:00.947766: train_loss -0.8379\n",
      "2025-12-16 14:54:00.949769: val_loss -0.8584\n",
      "2025-12-16 14:54:00.953774: Pseudo dice [0.9183, 0.952, 0.932]\n",
      "2025-12-16 14:54:00.957782: Epoch time: 137.94 s\n",
      "2025-12-16 14:54:01.863410: \n",
      "2025-12-16 14:54:01.863410: Epoch 311\n",
      "2025-12-16 14:54:01.877366: Current learning rate: 0.00715\n",
      "2025-12-16 14:56:19.816128: train_loss -0.8331\n",
      "2025-12-16 14:56:19.816128: val_loss -0.852\n",
      "2025-12-16 14:56:19.832160: Pseudo dice [0.915, 0.95, 0.9162]\n",
      "2025-12-16 14:56:19.836109: Epoch time: 137.95 s\n",
      "2025-12-16 14:56:20.464346: \n",
      "2025-12-16 14:56:20.464346: Epoch 312\n",
      "2025-12-16 14:56:20.464346: Current learning rate: 0.00714\n",
      "2025-12-16 14:58:38.302219: train_loss -0.8363\n",
      "2025-12-16 14:58:38.302219: val_loss -0.8539\n",
      "2025-12-16 14:58:38.302219: Pseudo dice [0.9162, 0.9483, 0.927]\n",
      "2025-12-16 14:58:38.302219: Epoch time: 137.84 s\n",
      "2025-12-16 14:58:38.951956: \n",
      "2025-12-16 14:58:38.951956: Epoch 313\n",
      "2025-12-16 14:58:38.951956: Current learning rate: 0.00713\n",
      "2025-12-16 15:00:56.891766: train_loss -0.84\n",
      "2025-12-16 15:00:56.891766: val_loss -0.8534\n",
      "2025-12-16 15:00:56.897510: Pseudo dice [0.9207, 0.9516, 0.9114]\n",
      "2025-12-16 15:00:56.899512: Epoch time: 137.94 s\n",
      "2025-12-16 15:00:57.641506: \n",
      "2025-12-16 15:00:57.657204: Epoch 314\n",
      "2025-12-16 15:00:57.659208: Current learning rate: 0.00712\n",
      "2025-12-16 15:03:15.484257: train_loss -0.8357\n",
      "2025-12-16 15:03:15.484257: val_loss -0.8546\n",
      "2025-12-16 15:03:15.485999: Pseudo dice [0.9175, 0.9524, 0.9338]\n",
      "2025-12-16 15:03:15.485999: Epoch time: 137.84 s\n",
      "2025-12-16 15:03:16.182079: \n",
      "2025-12-16 15:03:16.182079: Epoch 315\n",
      "2025-12-16 15:03:16.182079: Current learning rate: 0.00711\n",
      "2025-12-16 15:05:34.196690: train_loss -0.833\n",
      "2025-12-16 15:05:34.196690: val_loss -0.8542\n",
      "2025-12-16 15:05:34.200695: Pseudo dice [0.919, 0.9515, 0.9209]\n",
      "2025-12-16 15:05:34.202697: Epoch time: 138.01 s\n",
      "2025-12-16 15:05:34.842271: \n",
      "2025-12-16 15:05:34.842271: Epoch 316\n",
      "2025-12-16 15:05:34.842271: Current learning rate: 0.0071\n",
      "2025-12-16 15:07:52.788648: train_loss -0.8385\n",
      "2025-12-16 15:07:52.788648: val_loss -0.8502\n",
      "2025-12-16 15:07:52.788648: Pseudo dice [0.9158, 0.9451, 0.9228]\n",
      "2025-12-16 15:07:52.788648: Epoch time: 137.95 s\n",
      "2025-12-16 15:07:53.596341: \n",
      "2025-12-16 15:07:53.596341: Epoch 317\n",
      "2025-12-16 15:07:53.602446: Current learning rate: 0.0071\n",
      "2025-12-16 15:10:12.072904: train_loss -0.8332\n",
      "2025-12-16 15:10:12.072904: val_loss -0.8578\n",
      "2025-12-16 15:10:12.088153: Pseudo dice [0.9195, 0.9531, 0.9241]\n",
      "2025-12-16 15:10:12.090156: Epoch time: 138.48 s\n",
      "2025-12-16 15:10:12.717241: \n",
      "2025-12-16 15:10:12.717241: Epoch 318\n",
      "2025-12-16 15:10:12.717241: Current learning rate: 0.00709\n",
      "2025-12-16 15:12:31.184583: train_loss -0.8404\n",
      "2025-12-16 15:12:31.184583: val_loss -0.8709\n",
      "2025-12-16 15:12:31.184583: Pseudo dice [0.9259, 0.9534, 0.9395]\n",
      "2025-12-16 15:12:31.195888: Epoch time: 138.47 s\n",
      "2025-12-16 15:12:31.851629: \n",
      "2025-12-16 15:12:31.851629: Epoch 319\n",
      "2025-12-16 15:12:31.861345: Current learning rate: 0.00708\n",
      "2025-12-16 15:14:49.837965: train_loss -0.8475\n",
      "2025-12-16 15:14:49.837965: val_loss -0.8512\n",
      "2025-12-16 15:14:49.854087: Pseudo dice [0.913, 0.9485, 0.925]\n",
      "2025-12-16 15:14:49.854087: Epoch time: 137.99 s\n",
      "2025-12-16 15:14:50.488766: \n",
      "2025-12-16 15:14:50.488766: Epoch 320\n",
      "2025-12-16 15:14:50.505813: Current learning rate: 0.00707\n",
      "2025-12-16 15:17:08.444637: train_loss -0.8441\n",
      "2025-12-16 15:17:08.444637: val_loss -0.8679\n",
      "2025-12-16 15:17:08.448643: Pseudo dice [0.9251, 0.958, 0.9327]\n",
      "2025-12-16 15:17:08.452387: Epoch time: 137.96 s\n",
      "2025-12-16 15:17:09.101655: \n",
      "2025-12-16 15:17:09.101655: Epoch 321\n",
      "2025-12-16 15:17:09.101655: Current learning rate: 0.00706\n",
      "2025-12-16 15:19:27.032730: train_loss -0.8438\n",
      "2025-12-16 15:19:27.034732: val_loss -0.8603\n",
      "2025-12-16 15:19:27.038737: Pseudo dice [0.924, 0.9544, 0.9206]\n",
      "2025-12-16 15:19:27.040740: Epoch time: 137.93 s\n",
      "2025-12-16 15:19:27.668708: \n",
      "2025-12-16 15:19:27.684730: Epoch 322\n",
      "2025-12-16 15:19:27.684730: Current learning rate: 0.00705\n",
      "2025-12-16 15:21:45.563630: train_loss -0.839\n",
      "2025-12-16 15:21:45.565632: val_loss -0.8598\n",
      "2025-12-16 15:21:45.567635: Pseudo dice [0.9206, 0.9507, 0.9271]\n",
      "2025-12-16 15:21:45.571640: Epoch time: 137.89 s\n",
      "2025-12-16 15:21:45.573692: Yayy! New best EMA pseudo Dice: 0.9312\n",
      "2025-12-16 15:21:46.640234: \n",
      "2025-12-16 15:21:46.642237: Epoch 323\n",
      "2025-12-16 15:21:46.642237: Current learning rate: 0.00704\n",
      "2025-12-16 15:24:04.753661: train_loss -0.8353\n",
      "2025-12-16 15:24:04.753661: val_loss -0.8597\n",
      "2025-12-16 15:24:04.761276: Pseudo dice [0.9236, 0.9541, 0.9262]\n",
      "2025-12-16 15:24:04.763278: Epoch time: 138.11 s\n",
      "2025-12-16 15:24:04.767282: Yayy! New best EMA pseudo Dice: 0.9315\n",
      "2025-12-16 15:24:05.660927: \n",
      "2025-12-16 15:24:05.660927: Epoch 324\n",
      "2025-12-16 15:24:05.667064: Current learning rate: 0.00703\n",
      "2025-12-16 15:26:23.635105: train_loss -0.8402\n",
      "2025-12-16 15:26:23.635105: val_loss -0.8573\n",
      "2025-12-16 15:26:23.641081: Pseudo dice [0.9234, 0.9518, 0.9202]\n",
      "2025-12-16 15:26:23.641081: Epoch time: 137.97 s\n",
      "2025-12-16 15:26:23.646790: Yayy! New best EMA pseudo Dice: 0.9315\n",
      "2025-12-16 15:26:24.537761: \n",
      "2025-12-16 15:26:24.537761: Epoch 325\n",
      "2025-12-16 15:26:24.537761: Current learning rate: 0.00702\n",
      "2025-12-16 15:28:42.316420: train_loss -0.8446\n",
      "2025-12-16 15:28:42.316420: val_loss -0.8633\n",
      "2025-12-16 15:28:42.325417: Pseudo dice [0.9244, 0.9562, 0.9233]\n",
      "2025-12-16 15:28:42.325417: Epoch time: 137.78 s\n",
      "2025-12-16 15:28:42.325417: Yayy! New best EMA pseudo Dice: 0.9319\n",
      "2025-12-16 15:28:43.230325: \n",
      "2025-12-16 15:28:43.230325: Epoch 326\n",
      "2025-12-16 15:28:43.230325: Current learning rate: 0.00701\n",
      "2025-12-16 15:31:01.237316: train_loss -0.841\n",
      "2025-12-16 15:31:01.237316: val_loss -0.8696\n",
      "2025-12-16 15:31:01.241320: Pseudo dice [0.9262, 0.9552, 0.9321]\n",
      "2025-12-16 15:31:01.245324: Epoch time: 138.01 s\n",
      "2025-12-16 15:31:01.249328: Yayy! New best EMA pseudo Dice: 0.9325\n",
      "2025-12-16 15:31:02.162196: \n",
      "2025-12-16 15:31:02.162196: Epoch 327\n",
      "2025-12-16 15:31:02.162196: Current learning rate: 0.007\n",
      "2025-12-16 15:33:20.393430: train_loss -0.8401\n",
      "2025-12-16 15:33:20.393430: val_loss -0.8661\n",
      "2025-12-16 15:33:20.393430: Pseudo dice [0.9249, 0.9552, 0.9303]\n",
      "2025-12-16 15:33:20.393430: Epoch time: 138.23 s\n",
      "2025-12-16 15:33:20.393430: Yayy! New best EMA pseudo Dice: 0.9329\n",
      "2025-12-16 15:33:21.491601: \n",
      "2025-12-16 15:33:21.491601: Epoch 328\n",
      "2025-12-16 15:33:21.491601: Current learning rate: 0.00699\n",
      "2025-12-16 15:35:39.604518: train_loss -0.8415\n",
      "2025-12-16 15:35:39.604518: val_loss -0.8631\n",
      "2025-12-16 15:35:39.610264: Pseudo dice [0.9215, 0.9523, 0.9327]\n",
      "2025-12-16 15:35:39.612153: Epoch time: 138.11 s\n",
      "2025-12-16 15:35:39.616157: Yayy! New best EMA pseudo Dice: 0.9331\n",
      "2025-12-16 15:35:40.522240: \n",
      "2025-12-16 15:35:40.522240: Epoch 329\n",
      "2025-12-16 15:35:40.522240: Current learning rate: 0.00698\n",
      "2025-12-16 15:37:58.603568: train_loss -0.8411\n",
      "2025-12-16 15:37:58.603568: val_loss -0.8689\n",
      "2025-12-16 15:37:58.607311: Pseudo dice [0.9253, 0.959, 0.9269]\n",
      "2025-12-16 15:37:58.609313: Epoch time: 138.08 s\n",
      "2025-12-16 15:37:58.613317: Yayy! New best EMA pseudo Dice: 0.9335\n",
      "2025-12-16 15:37:59.522043: \n",
      "2025-12-16 15:37:59.522043: Epoch 330\n",
      "2025-12-16 15:37:59.522043: Current learning rate: 0.00697\n",
      "2025-12-16 15:40:17.306255: train_loss -0.8417\n",
      "2025-12-16 15:40:17.306255: val_loss -0.8571\n",
      "2025-12-16 15:40:17.314277: Pseudo dice [0.9185, 0.9521, 0.9269]\n",
      "2025-12-16 15:40:17.318290: Epoch time: 137.79 s\n",
      "2025-12-16 15:40:17.990074: \n",
      "2025-12-16 15:40:17.990074: Epoch 331\n",
      "2025-12-16 15:40:17.990074: Current learning rate: 0.00696\n",
      "2025-12-16 15:42:35.849999: train_loss -0.837\n",
      "2025-12-16 15:42:35.849999: val_loss -0.867\n",
      "2025-12-16 15:42:35.855503: Pseudo dice [0.9219, 0.9577, 0.9311]\n",
      "2025-12-16 15:42:35.857506: Epoch time: 137.86 s\n",
      "2025-12-16 15:42:35.861509: Yayy! New best EMA pseudo Dice: 0.9338\n",
      "2025-12-16 15:42:36.768193: \n",
      "2025-12-16 15:42:36.768193: Epoch 332\n",
      "2025-12-16 15:42:36.784121: Current learning rate: 0.00696\n",
      "2025-12-16 15:44:54.746033: train_loss -0.8402\n",
      "2025-12-16 15:44:54.748036: val_loss -0.851\n",
      "2025-12-16 15:44:54.750039: Pseudo dice [0.9156, 0.9522, 0.9208]\n",
      "2025-12-16 15:44:54.752042: Epoch time: 137.98 s\n",
      "2025-12-16 15:44:55.384390: \n",
      "2025-12-16 15:44:55.384390: Epoch 333\n",
      "2025-12-16 15:44:55.384390: Current learning rate: 0.00695\n",
      "2025-12-16 15:47:13.244583: train_loss -0.8432\n",
      "2025-12-16 15:47:13.244583: val_loss -0.8736\n",
      "2025-12-16 15:47:13.248589: Pseudo dice [0.9281, 0.9575, 0.932]\n",
      "2025-12-16 15:47:13.248589: Epoch time: 137.86 s\n",
      "2025-12-16 15:47:13.248589: Yayy! New best EMA pseudo Dice: 0.9339\n",
      "2025-12-16 15:47:14.359894: \n",
      "2025-12-16 15:47:14.359894: Epoch 334\n",
      "2025-12-16 15:47:14.359894: Current learning rate: 0.00694\n",
      "2025-12-16 15:49:32.307305: train_loss -0.8428\n",
      "2025-12-16 15:49:32.307305: val_loss -0.855\n",
      "2025-12-16 15:49:32.313050: Pseudo dice [0.9154, 0.9514, 0.9304]\n",
      "2025-12-16 15:49:32.315052: Epoch time: 137.96 s\n",
      "2025-12-16 15:49:32.961443: \n",
      "2025-12-16 15:49:32.961443: Epoch 335\n",
      "2025-12-16 15:49:32.977409: Current learning rate: 0.00693\n",
      "2025-12-16 15:51:51.098881: train_loss -0.8402\n",
      "2025-12-16 15:51:51.098881: val_loss -0.8538\n",
      "2025-12-16 15:51:51.100884: Pseudo dice [0.9191, 0.9501, 0.9156]\n",
      "2025-12-16 15:51:51.100884: Epoch time: 138.14 s\n",
      "2025-12-16 15:51:51.740039: \n",
      "2025-12-16 15:51:51.740039: Epoch 336\n",
      "2025-12-16 15:51:51.744152: Current learning rate: 0.00692\n",
      "2025-12-16 15:54:09.938270: train_loss -0.8329\n",
      "2025-12-16 15:54:09.938270: val_loss -0.8583\n",
      "2025-12-16 15:54:09.942289: Pseudo dice [0.9162, 0.9505, 0.9349]\n",
      "2025-12-16 15:54:09.942289: Epoch time: 138.2 s\n",
      "2025-12-16 15:54:10.583430: \n",
      "2025-12-16 15:54:10.583430: Epoch 337\n",
      "2025-12-16 15:54:10.585721: Current learning rate: 0.00691\n",
      "2025-12-16 15:56:28.508367: train_loss -0.8355\n",
      "2025-12-16 15:56:28.508367: val_loss -0.8567\n",
      "2025-12-16 15:56:28.515868: Pseudo dice [0.9195, 0.9501, 0.9249]\n",
      "2025-12-16 15:56:28.517871: Epoch time: 137.93 s\n",
      "2025-12-16 15:56:29.178795: \n",
      "2025-12-16 15:56:29.178795: Epoch 338\n",
      "2025-12-16 15:56:29.182915: Current learning rate: 0.0069\n",
      "2025-12-16 15:58:47.008585: train_loss -0.846\n",
      "2025-12-16 15:58:47.008585: val_loss -0.8701\n",
      "2025-12-16 15:58:47.008585: Pseudo dice [0.9287, 0.9592, 0.9272]\n",
      "2025-12-16 15:58:47.008585: Epoch time: 137.83 s\n",
      "2025-12-16 15:58:47.660354: \n",
      "2025-12-16 15:58:47.660354: Epoch 339\n",
      "2025-12-16 15:58:47.660354: Current learning rate: 0.00689\n",
      "2025-12-16 16:01:05.558041: train_loss -0.8368\n",
      "2025-12-16 16:01:05.560043: val_loss -0.8639\n",
      "2025-12-16 16:01:05.562045: Pseudo dice [0.9221, 0.9522, 0.9295]\n",
      "2025-12-16 16:01:05.565787: Epoch time: 137.9 s\n",
      "2025-12-16 16:01:06.482609: \n",
      "2025-12-16 16:01:06.482609: Epoch 340\n",
      "2025-12-16 16:01:06.482609: Current learning rate: 0.00688\n",
      "2025-12-16 16:03:24.487587: train_loss -0.8479\n",
      "2025-12-16 16:03:24.487587: val_loss -0.8531\n",
      "2025-12-16 16:03:24.493593: Pseudo dice [0.9118, 0.9483, 0.9301]\n",
      "2025-12-16 16:03:24.495595: Epoch time: 138.01 s\n",
      "2025-12-16 16:03:25.130353: \n",
      "2025-12-16 16:03:25.130353: Epoch 341\n",
      "2025-12-16 16:03:25.130353: Current learning rate: 0.00687\n",
      "2025-12-16 16:05:43.166049: train_loss -0.841\n",
      "2025-12-16 16:05:43.166049: val_loss -0.8552\n",
      "2025-12-16 16:05:43.166049: Pseudo dice [0.9178, 0.9474, 0.9248]\n",
      "2025-12-16 16:05:43.166049: Epoch time: 138.04 s\n",
      "2025-12-16 16:05:43.799685: \n",
      "2025-12-16 16:05:43.799685: Epoch 342\n",
      "2025-12-16 16:05:43.799685: Current learning rate: 0.00686\n",
      "2025-12-16 16:08:01.728302: train_loss -0.8437\n",
      "2025-12-16 16:08:01.728302: val_loss -0.875\n",
      "2025-12-16 16:08:01.732309: Pseudo dice [0.931, 0.9606, 0.9328]\n",
      "2025-12-16 16:08:01.734312: Epoch time: 137.93 s\n",
      "2025-12-16 16:08:02.565422: \n",
      "2025-12-16 16:08:02.565422: Epoch 343\n",
      "2025-12-16 16:08:02.565422: Current learning rate: 0.00685\n",
      "2025-12-16 16:10:21.444956: train_loss -0.8429\n",
      "2025-12-16 16:10:21.444956: val_loss -0.8661\n",
      "2025-12-16 16:10:21.460844: Pseudo dice [0.9238, 0.9516, 0.9356]\n",
      "2025-12-16 16:10:21.464850: Epoch time: 138.88 s\n",
      "2025-12-16 16:10:21.466853: Yayy! New best EMA pseudo Dice: 0.9342\n",
      "2025-12-16 16:10:22.353577: \n",
      "2025-12-16 16:10:22.355579: Epoch 344\n",
      "2025-12-16 16:10:22.358858: Current learning rate: 0.00684\n",
      "2025-12-16 16:12:40.082692: train_loss -0.8443\n",
      "2025-12-16 16:12:40.082692: val_loss -0.8635\n",
      "2025-12-16 16:12:40.082692: Pseudo dice [0.9224, 0.9538, 0.9216]\n",
      "2025-12-16 16:12:40.082692: Epoch time: 137.73 s\n",
      "2025-12-16 16:12:40.715704: \n",
      "2025-12-16 16:12:40.715704: Epoch 345\n",
      "2025-12-16 16:12:40.731724: Current learning rate: 0.00683\n",
      "2025-12-16 16:14:58.797804: train_loss -0.8467\n",
      "2025-12-16 16:14:58.797804: val_loss -0.864\n",
      "2025-12-16 16:14:58.797804: Pseudo dice [0.926, 0.9567, 0.9188]\n",
      "2025-12-16 16:14:58.797804: Epoch time: 138.08 s\n",
      "2025-12-16 16:14:59.700279: \n",
      "2025-12-16 16:14:59.700279: Epoch 346\n",
      "2025-12-16 16:14:59.716359: Current learning rate: 0.00682\n",
      "2025-12-16 16:17:17.660641: train_loss -0.8409\n",
      "2025-12-16 16:17:17.660641: val_loss -0.8646\n",
      "2025-12-16 16:17:17.660641: Pseudo dice [0.9284, 0.9573, 0.9271]\n",
      "2025-12-16 16:17:17.660641: Epoch time: 137.96 s\n",
      "2025-12-16 16:17:17.660641: Yayy! New best EMA pseudo Dice: 0.9344\n",
      "2025-12-16 16:17:18.550143: \n",
      "2025-12-16 16:17:18.550143: Epoch 347\n",
      "2025-12-16 16:17:18.550143: Current learning rate: 0.00681\n",
      "2025-12-16 16:19:36.355481: train_loss -0.8423\n",
      "2025-12-16 16:19:36.355481: val_loss -0.8598\n",
      "2025-12-16 16:19:36.363235: Pseudo dice [0.9174, 0.9545, 0.923]\n",
      "2025-12-16 16:19:36.367240: Epoch time: 137.81 s\n",
      "2025-12-16 16:19:37.014271: \n",
      "2025-12-16 16:19:37.014271: Epoch 348\n",
      "2025-12-16 16:19:37.014271: Current learning rate: 0.0068\n",
      "2025-12-16 16:21:55.023301: train_loss -0.8386\n",
      "2025-12-16 16:21:55.025302: val_loss -0.8515\n",
      "2025-12-16 16:21:55.031051: Pseudo dice [0.9136, 0.9505, 0.9201]\n",
      "2025-12-16 16:21:55.035058: Epoch time: 138.01 s\n",
      "2025-12-16 16:21:55.804775: \n",
      "2025-12-16 16:21:55.804775: Epoch 349\n",
      "2025-12-16 16:21:55.804775: Current learning rate: 0.0068\n",
      "2025-12-16 16:24:13.818401: train_loss -0.8382\n",
      "2025-12-16 16:24:13.818401: val_loss -0.8628\n",
      "2025-12-16 16:24:13.818401: Pseudo dice [0.9255, 0.9557, 0.9221]\n",
      "2025-12-16 16:24:13.834202: Epoch time: 138.01 s\n",
      "2025-12-16 16:24:14.708383: \n",
      "2025-12-16 16:24:14.708383: Epoch 350\n",
      "2025-12-16 16:24:14.708383: Current learning rate: 0.00679\n",
      "2025-12-16 16:26:32.570302: train_loss -0.8425\n",
      "2025-12-16 16:26:32.570302: val_loss -0.865\n",
      "2025-12-16 16:26:32.586276: Pseudo dice [0.9241, 0.9556, 0.9323]\n",
      "2025-12-16 16:26:32.588117: Epoch time: 137.86 s\n",
      "2025-12-16 16:26:33.234385: \n",
      "2025-12-16 16:26:33.234385: Epoch 351\n",
      "2025-12-16 16:26:33.234385: Current learning rate: 0.00678\n",
      "2025-12-16 16:28:51.195454: train_loss -0.8471\n",
      "2025-12-16 16:28:51.195454: val_loss -0.8531\n",
      "2025-12-16 16:28:51.199196: Pseudo dice [0.9114, 0.9485, 0.9318]\n",
      "2025-12-16 16:28:51.203200: Epoch time: 137.96 s\n",
      "2025-12-16 16:28:52.178189: \n",
      "2025-12-16 16:28:52.178189: Epoch 352\n",
      "2025-12-16 16:28:52.184231: Current learning rate: 0.00677\n",
      "2025-12-16 16:31:10.271085: train_loss -0.8381\n",
      "2025-12-16 16:31:10.271085: val_loss -0.8601\n",
      "2025-12-16 16:31:10.271085: Pseudo dice [0.9185, 0.9513, 0.9364]\n",
      "2025-12-16 16:31:10.289050: Epoch time: 138.09 s\n",
      "2025-12-16 16:31:10.936507: \n",
      "2025-12-16 16:31:10.936507: Epoch 353\n",
      "2025-12-16 16:31:10.936507: Current learning rate: 0.00676\n",
      "2025-12-16 16:33:28.927839: train_loss -0.8351\n",
      "2025-12-16 16:33:28.927839: val_loss -0.8585\n",
      "2025-12-16 16:33:28.931659: Pseudo dice [0.9174, 0.9542, 0.9371]\n",
      "2025-12-16 16:33:28.933661: Epoch time: 137.99 s\n",
      "2025-12-16 16:33:29.579475: \n",
      "2025-12-16 16:33:29.579475: Epoch 354\n",
      "2025-12-16 16:33:29.579475: Current learning rate: 0.00675\n",
      "2025-12-16 16:35:47.628822: train_loss -0.8337\n",
      "2025-12-16 16:35:47.628822: val_loss -0.8502\n",
      "2025-12-16 16:35:47.636574: Pseudo dice [0.9127, 0.9494, 0.9291]\n",
      "2025-12-16 16:35:47.640579: Epoch time: 138.05 s\n",
      "2025-12-16 16:35:48.420367: \n",
      "2025-12-16 16:35:48.420367: Epoch 355\n",
      "2025-12-16 16:35:48.420367: Current learning rate: 0.00674\n",
      "2025-12-16 16:38:06.266654: train_loss -0.8435\n",
      "2025-12-16 16:38:06.266654: val_loss -0.8478\n",
      "2025-12-16 16:38:06.282327: Pseudo dice [0.9109, 0.9478, 0.9232]\n",
      "2025-12-16 16:38:06.286332: Epoch time: 137.85 s\n",
      "2025-12-16 16:38:06.932170: \n",
      "2025-12-16 16:38:06.932170: Epoch 356\n",
      "2025-12-16 16:38:06.932170: Current learning rate: 0.00673\n",
      "2025-12-16 16:40:24.858468: train_loss -0.8434\n",
      "2025-12-16 16:40:24.858468: val_loss -0.8668\n",
      "2025-12-16 16:40:24.860470: Pseudo dice [0.9252, 0.9594, 0.9317]\n",
      "2025-12-16 16:40:24.860470: Epoch time: 137.93 s\n",
      "2025-12-16 16:40:25.657051: \n",
      "2025-12-16 16:40:25.657051: Epoch 357\n",
      "2025-12-16 16:40:25.671070: Current learning rate: 0.00672\n",
      "2025-12-16 16:42:43.574767: train_loss -0.8273\n",
      "2025-12-16 16:42:43.574767: val_loss -0.8451\n",
      "2025-12-16 16:42:43.574767: Pseudo dice [0.9103, 0.9483, 0.9211]\n",
      "2025-12-16 16:42:43.574767: Epoch time: 137.92 s\n",
      "2025-12-16 16:42:44.242331: \n",
      "2025-12-16 16:42:44.242331: Epoch 358\n",
      "2025-12-16 16:42:44.242331: Current learning rate: 0.00671\n",
      "2025-12-16 16:45:02.248922: train_loss -0.824\n",
      "2025-12-16 16:45:02.248922: val_loss -0.8395\n",
      "2025-12-16 16:45:02.264626: Pseudo dice [0.9141, 0.9515, 0.9118]\n",
      "2025-12-16 16:45:02.264626: Epoch time: 138.01 s\n",
      "2025-12-16 16:45:02.898663: \n",
      "2025-12-16 16:45:02.898663: Epoch 359\n",
      "2025-12-16 16:45:02.898663: Current learning rate: 0.0067\n",
      "2025-12-16 16:47:20.853943: train_loss -0.8339\n",
      "2025-12-16 16:47:20.855944: val_loss -0.8643\n",
      "2025-12-16 16:47:20.859688: Pseudo dice [0.9242, 0.9558, 0.9328]\n",
      "2025-12-16 16:47:20.861690: Epoch time: 137.96 s\n",
      "2025-12-16 16:47:21.508488: \n",
      "2025-12-16 16:47:21.508488: Epoch 360\n",
      "2025-12-16 16:47:21.508488: Current learning rate: 0.00669\n",
      "2025-12-16 16:49:39.285097: train_loss -0.8388\n",
      "2025-12-16 16:49:39.285097: val_loss -0.8613\n",
      "2025-12-16 16:49:39.289101: Pseudo dice [0.9243, 0.951, 0.9218]\n",
      "2025-12-16 16:49:39.293105: Epoch time: 137.78 s\n",
      "2025-12-16 16:49:39.951447: \n",
      "2025-12-16 16:49:39.951447: Epoch 361\n",
      "2025-12-16 16:49:39.951447: Current learning rate: 0.00668\n",
      "2025-12-16 16:51:57.969033: train_loss -0.8443\n",
      "2025-12-16 16:51:57.969033: val_loss -0.8727\n",
      "2025-12-16 16:51:57.975040: Pseudo dice [0.9285, 0.9612, 0.9271]\n",
      "2025-12-16 16:51:57.977042: Epoch time: 138.02 s\n",
      "2025-12-16 16:51:58.635065: \n",
      "2025-12-16 16:51:58.637068: Epoch 362\n",
      "2025-12-16 16:51:58.637068: Current learning rate: 0.00667\n",
      "2025-12-16 16:54:16.632699: train_loss -0.8445\n",
      "2025-12-16 16:54:16.632699: val_loss -0.8623\n",
      "2025-12-16 16:54:16.638204: Pseudo dice [0.9197, 0.9537, 0.9356]\n",
      "2025-12-16 16:54:16.642210: Epoch time: 138.0 s\n",
      "2025-12-16 16:54:17.291172: \n",
      "2025-12-16 16:54:17.291172: Epoch 363\n",
      "2025-12-16 16:54:17.291172: Current learning rate: 0.00666\n",
      "2025-12-16 16:56:35.335723: train_loss -0.8395\n",
      "2025-12-16 16:56:35.337463: val_loss -0.8613\n",
      "2025-12-16 16:56:35.341469: Pseudo dice [0.9175, 0.9514, 0.934]\n",
      "2025-12-16 16:56:35.345473: Epoch time: 138.05 s\n",
      "2025-12-16 16:56:36.170847: \n",
      "2025-12-16 16:56:36.170847: Epoch 364\n",
      "2025-12-16 16:56:36.172849: Current learning rate: 0.00665\n",
      "2025-12-16 16:58:54.140263: train_loss -0.8403\n",
      "2025-12-16 16:58:54.140263: val_loss -0.8567\n",
      "2025-12-16 16:58:54.144268: Pseudo dice [0.9173, 0.951, 0.9305]\n",
      "2025-12-16 16:58:54.148010: Epoch time: 137.97 s\n",
      "2025-12-16 16:58:54.783741: \n",
      "2025-12-16 16:58:54.783741: Epoch 365\n",
      "2025-12-16 16:58:54.783741: Current learning rate: 0.00665\n",
      "2025-12-16 17:01:12.870845: train_loss -0.8428\n",
      "2025-12-16 17:01:12.870845: val_loss -0.8677\n",
      "2025-12-16 17:01:12.874632: Pseudo dice [0.9259, 0.9559, 0.9319]\n",
      "2025-12-16 17:01:12.876635: Epoch time: 138.09 s\n",
      "2025-12-16 17:01:13.596764: \n",
      "2025-12-16 17:01:13.596764: Epoch 366\n",
      "2025-12-16 17:01:13.604032: Current learning rate: 0.00664\n",
      "2025-12-16 17:03:31.613278: train_loss -0.8467\n",
      "2025-12-16 17:03:31.613278: val_loss -0.8676\n",
      "2025-12-16 17:03:31.625650: Pseudo dice [0.9188, 0.953, 0.9432]\n",
      "2025-12-16 17:03:31.625650: Epoch time: 138.02 s\n",
      "2025-12-16 17:03:31.629360: Yayy! New best EMA pseudo Dice: 0.9345\n",
      "2025-12-16 17:03:32.516093: \n",
      "2025-12-16 17:03:32.516093: Epoch 367\n",
      "2025-12-16 17:03:32.529441: Current learning rate: 0.00663\n",
      "2025-12-16 17:05:50.632636: train_loss -0.8413\n",
      "2025-12-16 17:05:50.632636: val_loss -0.8581\n",
      "2025-12-16 17:05:50.638643: Pseudo dice [0.9221, 0.9513, 0.9259]\n",
      "2025-12-16 17:05:50.640645: Epoch time: 138.12 s\n",
      "2025-12-16 17:05:51.296766: \n",
      "2025-12-16 17:05:51.296766: Epoch 368\n",
      "2025-12-16 17:05:51.304564: Current learning rate: 0.00662\n",
      "2025-12-16 17:08:09.092430: train_loss -0.8405\n",
      "2025-12-16 17:08:09.092430: val_loss -0.8595\n",
      "2025-12-16 17:08:09.108133: Pseudo dice [0.9219, 0.9492, 0.9299]\n",
      "2025-12-16 17:08:09.108133: Epoch time: 137.8 s\n",
      "2025-12-16 17:08:09.886272: \n",
      "2025-12-16 17:08:09.886272: Epoch 369\n",
      "2025-12-16 17:08:09.902277: Current learning rate: 0.00661\n",
      "2025-12-16 17:10:28.446867: train_loss -0.8454\n",
      "2025-12-16 17:10:28.446867: val_loss -0.8692\n",
      "2025-12-16 17:10:28.452874: Pseudo dice [0.9244, 0.9568, 0.9303]\n",
      "2025-12-16 17:10:28.455878: Epoch time: 138.56 s\n",
      "2025-12-16 17:10:28.459882: Yayy! New best EMA pseudo Dice: 0.9346\n",
      "2025-12-16 17:10:29.352103: \n",
      "2025-12-16 17:10:29.352103: Epoch 370\n",
      "2025-12-16 17:10:29.352103: Current learning rate: 0.0066\n",
      "2025-12-16 17:12:47.428914: train_loss -0.8404\n",
      "2025-12-16 17:12:47.428914: val_loss -0.8723\n",
      "2025-12-16 17:12:47.428914: Pseudo dice [0.9265, 0.9595, 0.9365]\n",
      "2025-12-16 17:12:47.428914: Epoch time: 138.08 s\n",
      "2025-12-16 17:12:47.428914: Yayy! New best EMA pseudo Dice: 0.9352\n",
      "2025-12-16 17:12:48.356742: \n",
      "2025-12-16 17:12:48.356742: Epoch 371\n",
      "2025-12-16 17:12:48.356742: Current learning rate: 0.00659\n",
      "2025-12-16 17:15:06.382193: train_loss -0.8458\n",
      "2025-12-16 17:15:06.382193: val_loss -0.8568\n",
      "2025-12-16 17:15:06.386199: Pseudo dice [0.9169, 0.9514, 0.9331]\n",
      "2025-12-16 17:15:06.389943: Epoch time: 138.03 s\n",
      "2025-12-16 17:15:07.056344: \n",
      "2025-12-16 17:15:07.056344: Epoch 372\n",
      "2025-12-16 17:15:07.056344: Current learning rate: 0.00658\n",
      "2025-12-16 17:17:25.037724: train_loss -0.8377\n",
      "2025-12-16 17:17:25.037724: val_loss -0.8445\n",
      "2025-12-16 17:17:25.037724: Pseudo dice [0.9147, 0.9469, 0.9138]\n",
      "2025-12-16 17:17:25.037724: Epoch time: 137.98 s\n",
      "2025-12-16 17:17:25.693233: \n",
      "2025-12-16 17:17:25.693233: Epoch 373\n",
      "2025-12-16 17:17:25.696575: Current learning rate: 0.00657\n",
      "2025-12-16 17:19:43.669031: train_loss -0.8322\n",
      "2025-12-16 17:19:43.669031: val_loss -0.866\n",
      "2025-12-16 17:19:43.672773: Pseudo dice [0.9248, 0.9558, 0.9257]\n",
      "2025-12-16 17:19:43.672773: Epoch time: 137.98 s\n",
      "2025-12-16 17:19:44.321665: \n",
      "2025-12-16 17:19:44.321665: Epoch 374\n",
      "2025-12-16 17:19:44.321665: Current learning rate: 0.00656\n",
      "2025-12-16 17:22:02.365810: train_loss -0.8442\n",
      "2025-12-16 17:22:02.365810: val_loss -0.8607\n",
      "2025-12-16 17:22:02.367812: Pseudo dice [0.9216, 0.9541, 0.928]\n",
      "2025-12-16 17:22:02.367812: Epoch time: 138.04 s\n",
      "2025-12-16 17:22:03.372761: \n",
      "2025-12-16 17:22:03.372761: Epoch 375\n",
      "2025-12-16 17:22:03.372761: Current learning rate: 0.00655\n",
      "2025-12-16 17:24:21.375738: train_loss -0.8432\n",
      "2025-12-16 17:24:21.375738: val_loss -0.8647\n",
      "2025-12-16 17:24:21.391525: Pseudo dice [0.9238, 0.954, 0.9271]\n",
      "2025-12-16 17:24:21.391525: Epoch time: 138.02 s\n",
      "2025-12-16 17:24:22.040853: \n",
      "2025-12-16 17:24:22.040853: Epoch 376\n",
      "2025-12-16 17:24:22.056633: Current learning rate: 0.00654\n",
      "2025-12-16 17:26:39.872334: train_loss -0.8432\n",
      "2025-12-16 17:26:39.872334: val_loss -0.8585\n",
      "2025-12-16 17:26:39.885764: Pseudo dice [0.9154, 0.9492, 0.9323]\n",
      "2025-12-16 17:26:39.887994: Epoch time: 137.83 s\n",
      "2025-12-16 17:26:40.536620: \n",
      "2025-12-16 17:26:40.536620: Epoch 377\n",
      "2025-12-16 17:26:40.536620: Current learning rate: 0.00653\n",
      "2025-12-16 17:28:58.424927: train_loss -0.848\n",
      "2025-12-16 17:28:58.424927: val_loss -0.8652\n",
      "2025-12-16 17:28:58.430192: Pseudo dice [0.9222, 0.9529, 0.9329]\n",
      "2025-12-16 17:28:58.430192: Epoch time: 137.89 s\n",
      "2025-12-16 17:28:59.169821: \n",
      "2025-12-16 17:28:59.169821: Epoch 378\n",
      "2025-12-16 17:28:59.185524: Current learning rate: 0.00652\n",
      "2025-12-16 17:31:16.915413: train_loss -0.8489\n",
      "2025-12-16 17:31:16.915413: val_loss -0.8572\n",
      "2025-12-16 17:31:16.931251: Pseudo dice [0.919, 0.9524, 0.9276]\n",
      "2025-12-16 17:31:16.931251: Epoch time: 137.75 s\n",
      "2025-12-16 17:31:17.581385: \n",
      "2025-12-16 17:31:17.581385: Epoch 379\n",
      "2025-12-16 17:31:17.581385: Current learning rate: 0.00651\n",
      "2025-12-16 17:33:35.514497: train_loss -0.839\n",
      "2025-12-16 17:33:35.514497: val_loss -0.8384\n",
      "2025-12-16 17:33:35.518605: Pseudo dice [0.9118, 0.9481, 0.906]\n",
      "2025-12-16 17:33:35.522609: Epoch time: 137.93 s\n",
      "2025-12-16 17:33:36.162198: \n",
      "2025-12-16 17:33:36.162198: Epoch 380\n",
      "2025-12-16 17:33:36.173231: Current learning rate: 0.0065\n",
      "2025-12-16 17:35:54.095599: train_loss -0.827\n",
      "2025-12-16 17:35:54.095599: val_loss -0.8555\n",
      "2025-12-16 17:35:54.104471: Pseudo dice [0.9199, 0.9519, 0.9271]\n",
      "2025-12-16 17:35:54.108076: Epoch time: 137.93 s\n",
      "2025-12-16 17:35:54.953763: \n",
      "2025-12-16 17:35:54.955766: Epoch 381\n",
      "2025-12-16 17:35:54.958081: Current learning rate: 0.00649\n",
      "2025-12-16 17:38:13.068562: train_loss -0.8264\n",
      "2025-12-16 17:38:13.068562: val_loss -0.8456\n",
      "2025-12-16 17:38:13.079970: Pseudo dice [0.9115, 0.9529, 0.9214]\n",
      "2025-12-16 17:38:13.079970: Epoch time: 138.11 s\n",
      "2025-12-16 17:38:13.733804: \n",
      "2025-12-16 17:38:13.733804: Epoch 382\n",
      "2025-12-16 17:38:13.749869: Current learning rate: 0.00648\n",
      "2025-12-16 17:40:31.665272: train_loss -0.8131\n",
      "2025-12-16 17:40:31.667275: val_loss -0.8333\n",
      "2025-12-16 17:40:31.667275: Pseudo dice [0.9074, 0.944, 0.9063]\n",
      "2025-12-16 17:40:31.667275: Epoch time: 137.93 s\n",
      "2025-12-16 17:40:32.324940: \n",
      "2025-12-16 17:40:32.324940: Epoch 383\n",
      "2025-12-16 17:40:32.330174: Current learning rate: 0.00648\n",
      "2025-12-16 17:42:50.352076: train_loss -0.8082\n",
      "2025-12-16 17:42:50.352076: val_loss -0.8354\n",
      "2025-12-16 17:42:50.352076: Pseudo dice [0.9091, 0.9441, 0.9154]\n",
      "2025-12-16 17:42:50.352076: Epoch time: 138.03 s\n",
      "2025-12-16 17:42:51.001332: \n",
      "2025-12-16 17:42:51.001332: Epoch 384\n",
      "2025-12-16 17:42:51.001332: Current learning rate: 0.00647\n",
      "2025-12-16 17:45:08.959367: train_loss -0.8205\n",
      "2025-12-16 17:45:08.959367: val_loss -0.831\n",
      "2025-12-16 17:45:08.963372: Pseudo dice [0.9044, 0.9412, 0.9239]\n",
      "2025-12-16 17:45:08.965375: Epoch time: 137.96 s\n",
      "2025-12-16 17:45:09.619950: \n",
      "2025-12-16 17:45:09.619950: Epoch 385\n",
      "2025-12-16 17:45:09.619950: Current learning rate: 0.00646\n",
      "2025-12-16 17:47:27.790867: train_loss -0.8284\n",
      "2025-12-16 17:47:27.792870: val_loss -0.8402\n",
      "2025-12-16 17:47:27.796875: Pseudo dice [0.9106, 0.9445, 0.9127]\n",
      "2025-12-16 17:47:27.799722: Epoch time: 138.17 s\n",
      "2025-12-16 17:47:28.460668: \n",
      "2025-12-16 17:47:28.460668: Epoch 386\n",
      "2025-12-16 17:47:28.460668: Current learning rate: 0.00645\n",
      "2025-12-16 17:49:46.594386: train_loss -0.8353\n",
      "2025-12-16 17:49:46.594386: val_loss -0.8534\n",
      "2025-12-16 17:49:46.602155: Pseudo dice [0.9201, 0.9531, 0.9202]\n",
      "2025-12-16 17:49:46.602155: Epoch time: 138.13 s\n",
      "2025-12-16 17:49:47.434402: \n",
      "2025-12-16 17:49:47.434402: Epoch 387\n",
      "2025-12-16 17:49:47.442049: Current learning rate: 0.00644\n",
      "2025-12-16 17:52:05.435626: train_loss -0.8363\n",
      "2025-12-16 17:52:05.435626: val_loss -0.8549\n",
      "2025-12-16 17:52:05.451263: Pseudo dice [0.9109, 0.9499, 0.9338]\n",
      "2025-12-16 17:52:05.455270: Epoch time: 138.0 s\n",
      "2025-12-16 17:52:06.116159: \n",
      "2025-12-16 17:52:06.116159: Epoch 388\n",
      "2025-12-16 17:52:06.116159: Current learning rate: 0.00643\n",
      "2025-12-16 17:54:24.087033: train_loss -0.8349\n",
      "2025-12-16 17:54:24.087033: val_loss -0.8574\n",
      "2025-12-16 17:54:24.102815: Pseudo dice [0.9202, 0.9542, 0.9193]\n",
      "2025-12-16 17:54:24.102815: Epoch time: 137.99 s\n",
      "2025-12-16 17:54:24.768145: \n",
      "2025-12-16 17:54:24.768145: Epoch 389\n",
      "2025-12-16 17:54:24.768145: Current learning rate: 0.00642\n",
      "2025-12-16 17:56:42.749063: train_loss -0.845\n",
      "2025-12-16 17:56:42.751065: val_loss -0.869\n",
      "2025-12-16 17:56:42.753067: Pseudo dice [0.9274, 0.9529, 0.9301]\n",
      "2025-12-16 17:56:42.753067: Epoch time: 137.98 s\n",
      "2025-12-16 17:56:43.402102: \n",
      "2025-12-16 17:56:43.402102: Epoch 390\n",
      "2025-12-16 17:56:43.402102: Current learning rate: 0.00641\n",
      "2025-12-16 17:59:01.483034: train_loss -0.8418\n",
      "2025-12-16 17:59:01.485036: val_loss -0.8568\n",
      "2025-12-16 17:59:01.489042: Pseudo dice [0.9159, 0.9506, 0.9279]\n",
      "2025-12-16 17:59:01.493904: Epoch time: 138.08 s\n",
      "2025-12-16 17:59:02.141582: \n",
      "2025-12-16 17:59:02.141582: Epoch 391\n",
      "2025-12-16 17:59:02.157217: Current learning rate: 0.0064\n",
      "2025-12-16 18:01:20.278382: train_loss -0.8431\n",
      "2025-12-16 18:01:20.278382: val_loss -0.8578\n",
      "2025-12-16 18:01:20.296912: Pseudo dice [0.9215, 0.9513, 0.931]\n",
      "2025-12-16 18:01:20.298913: Epoch time: 138.14 s\n",
      "2025-12-16 18:01:20.943784: \n",
      "2025-12-16 18:01:20.943784: Epoch 392\n",
      "2025-12-16 18:01:20.943784: Current learning rate: 0.00639\n",
      "2025-12-16 18:03:39.132776: train_loss -0.8413\n",
      "2025-12-16 18:03:39.132776: val_loss -0.8594\n",
      "2025-12-16 18:03:39.140795: Pseudo dice [0.9184, 0.9503, 0.929]\n",
      "2025-12-16 18:03:39.146550: Epoch time: 138.19 s\n",
      "2025-12-16 18:03:39.970088: \n",
      "2025-12-16 18:03:39.970088: Epoch 393\n",
      "2025-12-16 18:03:39.975757: Current learning rate: 0.00638\n",
      "2025-12-16 18:05:57.874338: train_loss -0.8395\n",
      "2025-12-16 18:05:57.874338: val_loss -0.8645\n",
      "2025-12-16 18:05:57.879690: Pseudo dice [0.9246, 0.954, 0.9338]\n",
      "2025-12-16 18:05:57.879690: Epoch time: 137.9 s\n",
      "2025-12-16 18:05:58.542282: \n",
      "2025-12-16 18:05:58.542282: Epoch 394\n",
      "2025-12-16 18:05:58.548956: Current learning rate: 0.00637\n",
      "2025-12-16 18:08:16.534985: train_loss -0.8424\n",
      "2025-12-16 18:08:16.534985: val_loss -0.8621\n",
      "2025-12-16 18:08:16.540304: Pseudo dice [0.922, 0.9532, 0.9241]\n",
      "2025-12-16 18:08:16.544047: Epoch time: 137.99 s\n",
      "2025-12-16 18:08:17.223520: \n",
      "2025-12-16 18:08:17.223520: Epoch 395\n",
      "2025-12-16 18:08:17.241609: Current learning rate: 0.00636\n",
      "2025-12-16 18:10:35.660508: train_loss -0.8466\n",
      "2025-12-16 18:10:35.660508: val_loss -0.8644\n",
      "2025-12-16 18:10:35.660508: Pseudo dice [0.9195, 0.9489, 0.9309]\n",
      "2025-12-16 18:10:35.668419: Epoch time: 138.44 s\n",
      "2025-12-16 18:10:36.309067: \n",
      "2025-12-16 18:10:36.309067: Epoch 396\n",
      "2025-12-16 18:10:36.309067: Current learning rate: 0.00635\n",
      "2025-12-16 18:12:53.967327: train_loss -0.8407\n",
      "2025-12-16 18:12:53.967327: val_loss -0.8572\n",
      "2025-12-16 18:12:53.967327: Pseudo dice [0.9182, 0.9532, 0.9274]\n",
      "2025-12-16 18:12:53.967327: Epoch time: 137.66 s\n",
      "2025-12-16 18:12:54.617101: \n",
      "2025-12-16 18:12:54.617101: Epoch 397\n",
      "2025-12-16 18:12:54.617101: Current learning rate: 0.00634\n",
      "2025-12-16 18:15:12.592933: train_loss -0.8437\n",
      "2025-12-16 18:15:12.592933: val_loss -0.8503\n",
      "2025-12-16 18:15:12.596938: Pseudo dice [0.9118, 0.949, 0.9224]\n",
      "2025-12-16 18:15:12.598941: Epoch time: 137.98 s\n",
      "2025-12-16 18:15:13.376490: \n",
      "2025-12-16 18:15:13.376490: Epoch 398\n",
      "2025-12-16 18:15:13.376490: Current learning rate: 0.00633\n",
      "2025-12-16 18:17:31.189253: train_loss -0.8419\n",
      "2025-12-16 18:17:31.191255: val_loss -0.8719\n",
      "2025-12-16 18:17:31.195259: Pseudo dice [0.928, 0.9561, 0.9326]\n",
      "2025-12-16 18:17:31.197261: Epoch time: 137.81 s\n",
      "2025-12-16 18:17:32.017787: \n",
      "2025-12-16 18:17:32.017787: Epoch 399\n",
      "2025-12-16 18:17:32.017787: Current learning rate: 0.00632\n",
      "2025-12-16 18:19:49.892351: train_loss -0.8422\n",
      "2025-12-16 18:19:49.892351: val_loss -0.8622\n",
      "2025-12-16 18:19:49.908313: Pseudo dice [0.924, 0.9551, 0.9217]\n",
      "2025-12-16 18:19:49.908313: Epoch time: 137.87 s\n",
      "2025-12-16 18:19:50.806903: \n",
      "2025-12-16 18:19:50.806903: Epoch 400\n",
      "2025-12-16 18:19:50.806903: Current learning rate: 0.00631\n",
      "2025-12-16 18:22:08.719361: train_loss -0.8441\n",
      "2025-12-16 18:22:08.719361: val_loss -0.8694\n",
      "2025-12-16 18:22:08.719361: Pseudo dice [0.9264, 0.9549, 0.9293]\n",
      "2025-12-16 18:22:08.719361: Epoch time: 137.91 s\n",
      "2025-12-16 18:22:09.556558: \n",
      "2025-12-16 18:22:09.556558: Epoch 401\n",
      "2025-12-16 18:22:09.556558: Current learning rate: 0.0063\n",
      "2025-12-16 18:24:27.279963: train_loss -0.8414\n",
      "2025-12-16 18:24:27.279963: val_loss -0.8749\n",
      "2025-12-16 18:24:27.284659: Pseudo dice [0.9266, 0.9585, 0.9356]\n",
      "2025-12-16 18:24:27.288664: Epoch time: 137.72 s\n",
      "2025-12-16 18:24:27.955549: \n",
      "2025-12-16 18:24:27.955549: Epoch 402\n",
      "2025-12-16 18:24:27.957291: Current learning rate: 0.0063\n",
      "2025-12-16 18:26:45.964283: train_loss -0.8389\n",
      "2025-12-16 18:26:45.966284: val_loss -0.8523\n",
      "2025-12-16 18:26:45.970754: Pseudo dice [0.913, 0.9494, 0.9296]\n",
      "2025-12-16 18:26:45.974258: Epoch time: 138.01 s\n",
      "2025-12-16 18:26:46.638224: \n",
      "2025-12-16 18:26:46.638224: Epoch 403\n",
      "2025-12-16 18:26:46.638224: Current learning rate: 0.00629\n",
      "2025-12-16 18:29:04.696649: train_loss -0.8358\n",
      "2025-12-16 18:29:04.696649: val_loss -0.8621\n",
      "2025-12-16 18:29:04.699654: Pseudo dice [0.9257, 0.9555, 0.9288]\n",
      "2025-12-16 18:29:04.703659: Epoch time: 138.06 s\n",
      "2025-12-16 18:29:05.492182: \n",
      "2025-12-16 18:29:05.492182: Epoch 404\n",
      "2025-12-16 18:29:05.492182: Current learning rate: 0.00628\n",
      "2025-12-16 18:31:23.549634: train_loss -0.8357\n",
      "2025-12-16 18:31:23.549634: val_loss -0.8552\n",
      "2025-12-16 18:31:23.565466: Pseudo dice [0.9144, 0.9467, 0.9357]\n",
      "2025-12-16 18:31:23.571070: Epoch time: 138.07 s\n",
      "2025-12-16 18:31:24.229368: \n",
      "2025-12-16 18:31:24.229368: Epoch 405\n",
      "2025-12-16 18:31:24.229368: Current learning rate: 0.00627\n",
      "2025-12-16 18:33:42.154241: train_loss -0.8363\n",
      "2025-12-16 18:33:42.154241: val_loss -0.869\n",
      "2025-12-16 18:33:42.165328: Pseudo dice [0.9309, 0.9591, 0.9218]\n",
      "2025-12-16 18:33:42.167331: Epoch time: 137.92 s\n",
      "2025-12-16 18:33:42.818365: \n",
      "2025-12-16 18:33:42.818365: Epoch 406\n",
      "2025-12-16 18:33:42.818365: Current learning rate: 0.00626\n",
      "2025-12-16 18:36:00.696921: train_loss -0.8386\n",
      "2025-12-16 18:36:00.696921: val_loss -0.8654\n",
      "2025-12-16 18:36:00.703244: Pseudo dice [0.9222, 0.9541, 0.934]\n",
      "2025-12-16 18:36:00.705503: Epoch time: 137.88 s\n",
      "2025-12-16 18:36:01.487254: \n",
      "2025-12-16 18:36:01.487254: Epoch 407\n",
      "2025-12-16 18:36:01.505230: Current learning rate: 0.00625\n",
      "2025-12-16 18:38:19.477969: train_loss -0.8413\n",
      "2025-12-16 18:38:19.477969: val_loss -0.8527\n",
      "2025-12-16 18:38:19.482566: Pseudo dice [0.9144, 0.9459, 0.9301]\n",
      "2025-12-16 18:38:19.482566: Epoch time: 137.99 s\n",
      "2025-12-16 18:38:20.132692: \n",
      "2025-12-16 18:38:20.134695: Epoch 408\n",
      "2025-12-16 18:38:20.134695: Current learning rate: 0.00624\n",
      "2025-12-16 18:40:38.154932: train_loss -0.8413\n",
      "2025-12-16 18:40:38.154932: val_loss -0.8606\n",
      "2025-12-16 18:40:38.159507: Pseudo dice [0.9194, 0.9528, 0.9261]\n",
      "2025-12-16 18:40:38.159507: Epoch time: 138.02 s\n",
      "2025-12-16 18:40:38.813004: \n",
      "2025-12-16 18:40:38.813004: Epoch 409\n",
      "2025-12-16 18:40:38.816982: Current learning rate: 0.00623\n",
      "2025-12-16 18:42:56.796607: train_loss -0.8396\n",
      "2025-12-16 18:42:56.796607: val_loss -0.8597\n",
      "2025-12-16 18:42:56.796607: Pseudo dice [0.9208, 0.949, 0.9224]\n",
      "2025-12-16 18:42:56.807902: Epoch time: 137.99 s\n",
      "2025-12-16 18:42:57.742361: \n",
      "2025-12-16 18:42:57.742361: Epoch 410\n",
      "2025-12-16 18:42:57.742361: Current learning rate: 0.00622\n",
      "2025-12-16 18:45:15.856815: train_loss -0.8395\n",
      "2025-12-16 18:45:15.856815: val_loss -0.8659\n",
      "2025-12-16 18:45:15.856815: Pseudo dice [0.9217, 0.9582, 0.9289]\n",
      "2025-12-16 18:45:15.863183: Epoch time: 138.11 s\n",
      "2025-12-16 18:45:16.487565: \n",
      "2025-12-16 18:45:16.487565: Epoch 411\n",
      "2025-12-16 18:45:16.489568: Current learning rate: 0.00621\n",
      "2025-12-16 18:47:34.446537: train_loss -0.8411\n",
      "2025-12-16 18:47:34.446537: val_loss -0.858\n",
      "2025-12-16 18:47:34.450541: Pseudo dice [0.9154, 0.9522, 0.9321]\n",
      "2025-12-16 18:47:34.452281: Epoch time: 137.96 s\n",
      "2025-12-16 18:47:35.117638: \n",
      "2025-12-16 18:47:35.117638: Epoch 412\n",
      "2025-12-16 18:47:35.117638: Current learning rate: 0.0062\n",
      "2025-12-16 18:49:53.051076: train_loss -0.8454\n",
      "2025-12-16 18:49:53.051076: val_loss -0.8736\n",
      "2025-12-16 18:49:53.054936: Pseudo dice [0.9284, 0.9589, 0.9351]\n",
      "2025-12-16 18:49:53.058940: Epoch time: 137.94 s\n",
      "2025-12-16 18:49:53.687002: \n",
      "2025-12-16 18:49:53.687002: Epoch 413\n",
      "2025-12-16 18:49:53.687002: Current learning rate: 0.00619\n",
      "2025-12-16 18:52:11.454782: train_loss -0.8456\n",
      "2025-12-16 18:52:11.456784: val_loss -0.8605\n",
      "2025-12-16 18:52:11.460788: Pseudo dice [0.9185, 0.9579, 0.9305]\n",
      "2025-12-16 18:52:11.462790: Epoch time: 137.77 s\n",
      "2025-12-16 18:52:12.086541: \n",
      "2025-12-16 18:52:12.086541: Epoch 414\n",
      "2025-12-16 18:52:12.086541: Current learning rate: 0.00618\n",
      "2025-12-16 18:54:29.942279: train_loss -0.8492\n",
      "2025-12-16 18:54:29.942279: val_loss -0.8718\n",
      "2025-12-16 18:54:29.948026: Pseudo dice [0.9283, 0.9584, 0.93]\n",
      "2025-12-16 18:54:29.950028: Epoch time: 137.86 s\n",
      "2025-12-16 18:54:30.622512: \n",
      "2025-12-16 18:54:30.622512: Epoch 415\n",
      "2025-12-16 18:54:30.622512: Current learning rate: 0.00617\n",
      "2025-12-16 18:56:48.595593: train_loss -0.8415\n",
      "2025-12-16 18:56:48.595593: val_loss -0.8634\n",
      "2025-12-16 18:56:48.599597: Pseudo dice [0.9211, 0.9501, 0.9324]\n",
      "2025-12-16 18:56:48.603339: Epoch time: 137.97 s\n",
      "2025-12-16 18:56:49.235927: \n",
      "2025-12-16 18:56:49.235927: Epoch 416\n",
      "2025-12-16 18:56:49.235927: Current learning rate: 0.00616\n",
      "2025-12-16 18:59:07.281447: train_loss -0.8417\n",
      "2025-12-16 18:59:07.281447: val_loss -0.8661\n",
      "2025-12-16 18:59:07.287333: Pseudo dice [0.9225, 0.9532, 0.935]\n",
      "2025-12-16 18:59:07.291337: Epoch time: 138.05 s\n",
      "2025-12-16 18:59:08.105480: \n",
      "2025-12-16 18:59:08.105480: Epoch 417\n",
      "2025-12-16 18:59:08.105480: Current learning rate: 0.00615\n",
      "2025-12-16 19:01:26.117665: train_loss -0.8469\n",
      "2025-12-16 19:01:26.117665: val_loss -0.8748\n",
      "2025-12-16 19:01:26.117665: Pseudo dice [0.9284, 0.9562, 0.9326]\n",
      "2025-12-16 19:01:26.123727: Epoch time: 138.03 s\n",
      "2025-12-16 19:01:26.123727: Yayy! New best EMA pseudo Dice: 0.9355\n",
      "2025-12-16 19:01:27.001098: \n",
      "2025-12-16 19:01:27.001098: Epoch 418\n",
      "2025-12-16 19:01:27.004843: Current learning rate: 0.00614\n",
      "2025-12-16 19:03:44.898361: train_loss -0.8441\n",
      "2025-12-16 19:03:44.898361: val_loss -0.8801\n",
      "2025-12-16 19:03:44.898361: Pseudo dice [0.931, 0.9621, 0.9351]\n",
      "2025-12-16 19:03:44.898361: Epoch time: 137.9 s\n",
      "2025-12-16 19:03:44.898361: Yayy! New best EMA pseudo Dice: 0.9362\n",
      "2025-12-16 19:03:45.812279: \n",
      "2025-12-16 19:03:45.812279: Epoch 419\n",
      "2025-12-16 19:03:45.812279: Current learning rate: 0.00613\n",
      "2025-12-16 19:06:03.604110: train_loss -0.8506\n",
      "2025-12-16 19:06:03.606112: val_loss -0.8609\n",
      "2025-12-16 19:06:03.608113: Pseudo dice [0.9177, 0.9529, 0.9343]\n",
      "2025-12-16 19:06:03.612620: Epoch time: 137.79 s\n",
      "2025-12-16 19:06:04.229003: \n",
      "2025-12-16 19:06:04.229003: Epoch 420\n",
      "2025-12-16 19:06:04.244684: Current learning rate: 0.00612\n",
      "2025-12-16 19:08:22.158577: train_loss -0.8491\n",
      "2025-12-16 19:08:22.158577: val_loss -0.8716\n",
      "2025-12-16 19:08:22.158577: Pseudo dice [0.9241, 0.9594, 0.9376]\n",
      "2025-12-16 19:08:22.158577: Epoch time: 137.93 s\n",
      "2025-12-16 19:08:22.158577: Yayy! New best EMA pseudo Dice: 0.9365\n",
      "2025-12-16 19:08:23.092963: \n",
      "2025-12-16 19:08:23.092963: Epoch 421\n",
      "2025-12-16 19:08:23.092963: Current learning rate: 0.00612\n",
      "2025-12-16 19:10:41.775160: train_loss -0.8418\n",
      "2025-12-16 19:10:41.775160: val_loss -0.8614\n",
      "2025-12-16 19:10:41.783170: Pseudo dice [0.9171, 0.9511, 0.9363]\n",
      "2025-12-16 19:10:41.785171: Epoch time: 138.68 s\n",
      "2025-12-16 19:10:42.422434: \n",
      "2025-12-16 19:10:42.422434: Epoch 422\n",
      "2025-12-16 19:10:42.422434: Current learning rate: 0.00611\n",
      "2025-12-16 19:13:00.752432: train_loss -0.8451\n",
      "2025-12-16 19:13:00.752432: val_loss -0.8664\n",
      "2025-12-16 19:13:00.758439: Pseudo dice [0.9255, 0.9548, 0.9316]\n",
      "2025-12-16 19:13:00.762443: Epoch time: 138.33 s\n",
      "2025-12-16 19:13:01.556349: \n",
      "2025-12-16 19:13:01.556349: Epoch 423\n",
      "2025-12-16 19:13:01.572252: Current learning rate: 0.0061\n",
      "2025-12-16 19:15:21.103728: train_loss -0.8467\n",
      "2025-12-16 19:15:21.105730: val_loss -0.8656\n",
      "2025-12-16 19:15:21.109734: Pseudo dice [0.9228, 0.9551, 0.9307]\n",
      "2025-12-16 19:15:21.113477: Epoch time: 139.55 s\n",
      "2025-12-16 19:15:21.739836: \n",
      "2025-12-16 19:15:21.739836: Epoch 424\n",
      "2025-12-16 19:15:21.739836: Current learning rate: 0.00609\n",
      "2025-12-16 19:17:40.214980: train_loss -0.845\n",
      "2025-12-16 19:17:40.214980: val_loss -0.8616\n",
      "2025-12-16 19:17:40.220986: Pseudo dice [0.9221, 0.9542, 0.9252]\n",
      "2025-12-16 19:17:40.224728: Epoch time: 138.48 s\n",
      "2025-12-16 19:17:40.897086: \n",
      "2025-12-16 19:17:40.897086: Epoch 425\n",
      "2025-12-16 19:17:40.897086: Current learning rate: 0.00608\n",
      "2025-12-16 19:19:59.327404: train_loss -0.8417\n",
      "2025-12-16 19:19:59.327404: val_loss -0.8696\n",
      "2025-12-16 19:19:59.332411: Pseudo dice [0.923, 0.9565, 0.9266]\n",
      "2025-12-16 19:19:59.332411: Epoch time: 138.43 s\n",
      "2025-12-16 19:19:59.977972: \n",
      "2025-12-16 19:19:59.977972: Epoch 426\n",
      "2025-12-16 19:19:59.977972: Current learning rate: 0.00607\n",
      "2025-12-16 19:22:18.497241: train_loss -0.8438\n",
      "2025-12-16 19:22:18.499244: val_loss -0.8712\n",
      "2025-12-16 19:22:18.502957: Pseudo dice [0.9258, 0.9573, 0.9334]\n",
      "2025-12-16 19:22:18.502957: Epoch time: 138.52 s\n",
      "2025-12-16 19:22:19.132976: \n",
      "2025-12-16 19:22:19.132976: Epoch 427\n",
      "2025-12-16 19:22:19.132976: Current learning rate: 0.00606\n",
      "2025-12-16 19:24:37.801542: train_loss -0.8387\n",
      "2025-12-16 19:24:37.801542: val_loss -0.8567\n",
      "2025-12-16 19:24:37.815299: Pseudo dice [0.9165, 0.9495, 0.9317]\n",
      "2025-12-16 19:24:37.820801: Epoch time: 138.67 s\n",
      "2025-12-16 19:24:38.519576: \n",
      "2025-12-16 19:24:38.520579: Epoch 428\n",
      "2025-12-16 19:24:38.524434: Current learning rate: 0.00605\n",
      "2025-12-16 19:26:56.841174: train_loss -0.8444\n",
      "2025-12-16 19:26:56.841174: val_loss -0.8552\n",
      "2025-12-16 19:26:56.843176: Pseudo dice [0.9172, 0.9511, 0.9156]\n",
      "2025-12-16 19:26:56.843176: Epoch time: 138.32 s\n",
      "2025-12-16 19:26:57.646091: \n",
      "2025-12-16 19:26:57.646091: Epoch 429\n",
      "2025-12-16 19:26:57.661981: Current learning rate: 0.00604\n",
      "2025-12-16 19:29:15.729109: train_loss -0.844\n",
      "2025-12-16 19:29:15.729109: val_loss -0.8744\n",
      "2025-12-16 19:29:15.733113: Pseudo dice [0.9295, 0.956, 0.9368]\n",
      "2025-12-16 19:29:15.737118: Epoch time: 138.08 s\n",
      "2025-12-16 19:29:16.366930: \n",
      "2025-12-16 19:29:16.366930: Epoch 430\n",
      "2025-12-16 19:29:16.366930: Current learning rate: 0.00603\n",
      "2025-12-16 19:31:34.264107: train_loss -0.8481\n",
      "2025-12-16 19:31:34.264107: val_loss -0.873\n",
      "2025-12-16 19:31:34.279879: Pseudo dice [0.9258, 0.9596, 0.9321]\n",
      "2025-12-16 19:31:34.279879: Epoch time: 137.9 s\n",
      "2025-12-16 19:31:35.056462: \n",
      "2025-12-16 19:31:35.056462: Epoch 431\n",
      "2025-12-16 19:31:35.058721: Current learning rate: 0.00602\n",
      "2025-12-16 19:33:53.098221: train_loss -0.845\n",
      "2025-12-16 19:33:53.099223: val_loss -0.8726\n",
      "2025-12-16 19:33:53.099223: Pseudo dice [0.9235, 0.9563, 0.9366]\n",
      "2025-12-16 19:33:53.105001: Epoch time: 138.04 s\n",
      "2025-12-16 19:33:53.732644: \n",
      "2025-12-16 19:33:53.732644: Epoch 432\n",
      "2025-12-16 19:33:53.732644: Current learning rate: 0.00601\n",
      "2025-12-16 19:36:11.613399: train_loss -0.8497\n",
      "2025-12-16 19:36:11.613399: val_loss -0.8684\n",
      "2025-12-16 19:36:11.613399: Pseudo dice [0.9272, 0.9566, 0.9249]\n",
      "2025-12-16 19:36:11.627282: Epoch time: 137.88 s\n",
      "2025-12-16 19:36:12.244753: \n",
      "2025-12-16 19:36:12.244753: Epoch 433\n",
      "2025-12-16 19:36:12.260539: Current learning rate: 0.006\n",
      "2025-12-16 19:38:30.128240: train_loss -0.8485\n",
      "2025-12-16 19:38:30.128240: val_loss -0.8558\n",
      "2025-12-16 19:38:30.128240: Pseudo dice [0.9202, 0.9505, 0.9153]\n",
      "2025-12-16 19:38:30.128240: Epoch time: 137.88 s\n",
      "2025-12-16 19:38:30.761548: \n",
      "2025-12-16 19:38:30.761548: Epoch 434\n",
      "2025-12-16 19:38:30.761548: Current learning rate: 0.00599\n",
      "2025-12-16 19:40:48.991729: train_loss -0.8502\n",
      "2025-12-16 19:40:48.991729: val_loss -0.8669\n",
      "2025-12-16 19:40:48.991729: Pseudo dice [0.9268, 0.9531, 0.926]\n",
      "2025-12-16 19:40:48.991729: Epoch time: 138.23 s\n",
      "2025-12-16 19:40:49.624595: \n",
      "2025-12-16 19:40:49.624595: Epoch 435\n",
      "2025-12-16 19:40:49.640365: Current learning rate: 0.00598\n",
      "2025-12-16 19:43:07.669314: train_loss -0.8448\n",
      "2025-12-16 19:43:07.671316: val_loss -0.8585\n",
      "2025-12-16 19:43:07.675320: Pseudo dice [0.9154, 0.9515, 0.9362]\n",
      "2025-12-16 19:43:07.677323: Epoch time: 138.04 s\n",
      "2025-12-16 19:43:08.476622: \n",
      "2025-12-16 19:43:08.476622: Epoch 436\n",
      "2025-12-16 19:43:08.476622: Current learning rate: 0.00597\n",
      "2025-12-16 19:45:26.536670: train_loss -0.8464\n",
      "2025-12-16 19:45:26.536670: val_loss -0.8551\n",
      "2025-12-16 19:45:26.536670: Pseudo dice [0.9178, 0.946, 0.9271]\n",
      "2025-12-16 19:45:26.536670: Epoch time: 138.06 s\n",
      "2025-12-16 19:45:27.212769: \n",
      "2025-12-16 19:45:27.212769: Epoch 437\n",
      "2025-12-16 19:45:27.216280: Current learning rate: 0.00596\n",
      "2025-12-16 19:47:45.160871: train_loss -0.8455\n",
      "2025-12-16 19:47:45.160871: val_loss -0.8551\n",
      "2025-12-16 19:47:45.162873: Pseudo dice [0.9132, 0.9503, 0.9257]\n",
      "2025-12-16 19:47:45.162873: Epoch time: 137.95 s\n",
      "2025-12-16 19:47:45.788126: \n",
      "2025-12-16 19:47:45.788126: Epoch 438\n",
      "2025-12-16 19:47:45.788126: Current learning rate: 0.00595\n",
      "2025-12-16 19:50:03.734903: train_loss -0.8446\n",
      "2025-12-16 19:50:03.734903: val_loss -0.8582\n",
      "2025-12-16 19:50:03.734903: Pseudo dice [0.9182, 0.9522, 0.9266]\n",
      "2025-12-16 19:50:03.734903: Epoch time: 137.95 s\n",
      "2025-12-16 19:50:04.370886: \n",
      "2025-12-16 19:50:04.370886: Epoch 439\n",
      "2025-12-16 19:50:04.370886: Current learning rate: 0.00594\n",
      "2025-12-16 19:52:22.487235: train_loss -0.8441\n",
      "2025-12-16 19:52:22.489237: val_loss -0.8681\n",
      "2025-12-16 19:52:22.493241: Pseudo dice [0.9236, 0.9567, 0.9348]\n",
      "2025-12-16 19:52:22.496984: Epoch time: 138.12 s\n",
      "2025-12-16 19:52:23.132651: \n",
      "2025-12-16 19:52:23.132651: Epoch 440\n",
      "2025-12-16 19:52:23.132651: Current learning rate: 0.00593\n",
      "2025-12-16 19:54:41.218299: train_loss -0.8418\n",
      "2025-12-16 19:54:41.218299: val_loss -0.8654\n",
      "2025-12-16 19:54:41.218299: Pseudo dice [0.9233, 0.9584, 0.9299]\n",
      "2025-12-16 19:54:41.218299: Epoch time: 138.09 s\n",
      "2025-12-16 19:54:41.869401: \n",
      "2025-12-16 19:54:41.869401: Epoch 441\n",
      "2025-12-16 19:54:41.869401: Current learning rate: 0.00592\n",
      "2025-12-16 19:56:59.823715: train_loss -0.845\n",
      "2025-12-16 19:56:59.823715: val_loss -0.8573\n",
      "2025-12-16 19:56:59.823715: Pseudo dice [0.9178, 0.949, 0.9292]\n",
      "2025-12-16 19:56:59.823715: Epoch time: 137.95 s\n",
      "2025-12-16 19:57:00.617046: \n",
      "2025-12-16 19:57:00.617046: Epoch 442\n",
      "2025-12-16 19:57:00.617046: Current learning rate: 0.00592\n",
      "2025-12-16 19:59:18.529584: train_loss -0.8456\n",
      "2025-12-16 19:59:18.529584: val_loss -0.8669\n",
      "2025-12-16 19:59:18.529584: Pseudo dice [0.9245, 0.9524, 0.9293]\n",
      "2025-12-16 19:59:18.529584: Epoch time: 137.91 s\n",
      "2025-12-16 19:59:19.163188: \n",
      "2025-12-16 19:59:19.163188: Epoch 443\n",
      "2025-12-16 19:59:19.179052: Current learning rate: 0.00591\n",
      "2025-12-16 20:01:37.267950: train_loss -0.8489\n",
      "2025-12-16 20:01:37.267950: val_loss -0.8637\n",
      "2025-12-16 20:01:37.271692: Pseudo dice [0.9202, 0.9526, 0.9314]\n",
      "2025-12-16 20:01:37.271692: Epoch time: 138.1 s\n",
      "2025-12-16 20:01:37.888583: \n",
      "2025-12-16 20:01:37.888583: Epoch 444\n",
      "2025-12-16 20:01:37.904396: Current learning rate: 0.0059\n",
      "2025-12-16 20:03:55.995528: train_loss -0.8445\n",
      "2025-12-16 20:03:55.995528: val_loss -0.8561\n",
      "2025-12-16 20:03:55.999532: Pseudo dice [0.9123, 0.9449, 0.9312]\n",
      "2025-12-16 20:03:56.003536: Epoch time: 138.11 s\n",
      "2025-12-16 20:03:56.748378: \n",
      "2025-12-16 20:03:56.748378: Epoch 445\n",
      "2025-12-16 20:03:56.748378: Current learning rate: 0.00589\n",
      "2025-12-16 20:06:14.822788: train_loss -0.8484\n",
      "2025-12-16 20:06:14.822788: val_loss -0.8606\n",
      "2025-12-16 20:06:14.824529: Pseudo dice [0.9221, 0.9508, 0.9289]\n",
      "2025-12-16 20:06:14.824529: Epoch time: 138.09 s\n",
      "2025-12-16 20:06:15.460633: \n",
      "2025-12-16 20:06:15.460633: Epoch 446\n",
      "2025-12-16 20:06:15.462636: Current learning rate: 0.00588\n",
      "2025-12-16 20:08:33.403282: train_loss -0.8502\n",
      "2025-12-16 20:08:33.405285: val_loss -0.867\n",
      "2025-12-16 20:08:33.411014: Pseudo dice [0.9233, 0.9549, 0.9243]\n",
      "2025-12-16 20:08:33.415020: Epoch time: 137.94 s\n",
      "2025-12-16 20:08:34.044461: \n",
      "2025-12-16 20:08:34.044461: Epoch 447\n",
      "2025-12-16 20:08:34.044461: Current learning rate: 0.00587\n",
      "2025-12-16 20:10:52.914047: train_loss -0.8497\n",
      "2025-12-16 20:10:52.914047: val_loss -0.861\n",
      "2025-12-16 20:10:52.929921: Pseudo dice [0.9194, 0.9527, 0.9298]\n",
      "2025-12-16 20:10:52.929921: Epoch time: 138.87 s\n",
      "2025-12-16 20:10:53.611416: \n",
      "2025-12-16 20:10:53.611416: Epoch 448\n",
      "2025-12-16 20:10:53.611416: Current learning rate: 0.00586\n",
      "2025-12-16 20:13:11.640960: train_loss -0.8506\n",
      "2025-12-16 20:13:11.640960: val_loss -0.8604\n",
      "2025-12-16 20:13:11.644966: Pseudo dice [0.9185, 0.9521, 0.9283]\n",
      "2025-12-16 20:13:11.648709: Epoch time: 138.03 s\n",
      "2025-12-16 20:13:12.442876: \n",
      "2025-12-16 20:13:12.442876: Epoch 449\n",
      "2025-12-16 20:13:12.442876: Current learning rate: 0.00585\n",
      "2025-12-16 20:15:30.501364: train_loss -0.8452\n",
      "2025-12-16 20:15:30.501364: val_loss -0.863\n",
      "2025-12-16 20:15:30.507370: Pseudo dice [0.9202, 0.9522, 0.9287]\n",
      "2025-12-16 20:15:30.509372: Epoch time: 138.07 s\n",
      "2025-12-16 20:15:31.389742: \n",
      "2025-12-16 20:15:31.391744: Epoch 450\n",
      "2025-12-16 20:15:31.391744: Current learning rate: 0.00584\n",
      "2025-12-16 20:17:49.374163: train_loss -0.8368\n",
      "2025-12-16 20:17:49.374163: val_loss -0.8617\n",
      "2025-12-16 20:17:49.380172: Pseudo dice [0.9202, 0.9558, 0.9281]\n",
      "2025-12-16 20:17:49.382174: Epoch time: 137.98 s\n",
      "2025-12-16 20:17:50.140373: \n",
      "2025-12-16 20:17:50.140373: Epoch 451\n",
      "2025-12-16 20:17:50.141376: Current learning rate: 0.00583\n",
      "2025-12-16 20:20:08.242680: train_loss -0.8464\n",
      "2025-12-16 20:20:08.242680: val_loss -0.8621\n",
      "2025-12-16 20:20:08.246684: Pseudo dice [0.9155, 0.9528, 0.9317]\n",
      "2025-12-16 20:20:08.250688: Epoch time: 138.1 s\n",
      "2025-12-16 20:20:08.880311: \n",
      "2025-12-16 20:20:08.880311: Epoch 452\n",
      "2025-12-16 20:20:08.880311: Current learning rate: 0.00582\n",
      "2025-12-16 20:22:26.954685: train_loss -0.8451\n",
      "2025-12-16 20:22:26.954685: val_loss -0.8731\n",
      "2025-12-16 20:22:26.963671: Pseudo dice [0.9232, 0.9545, 0.9365]\n",
      "2025-12-16 20:22:26.965673: Epoch time: 138.08 s\n",
      "2025-12-16 20:22:27.590791: \n",
      "2025-12-16 20:22:27.590791: Epoch 453\n",
      "2025-12-16 20:22:27.590791: Current learning rate: 0.00581\n",
      "2025-12-16 20:24:45.699599: train_loss -0.8504\n",
      "2025-12-16 20:24:45.701602: val_loss -0.8543\n",
      "2025-12-16 20:24:45.704504: Pseudo dice [0.9145, 0.9459, 0.9296]\n",
      "2025-12-16 20:24:45.708510: Epoch time: 138.11 s\n",
      "2025-12-16 20:24:46.467828: \n",
      "2025-12-16 20:24:46.467828: Epoch 454\n",
      "2025-12-16 20:24:46.467828: Current learning rate: 0.0058\n",
      "2025-12-16 20:27:05.109702: train_loss -0.8467\n",
      "2025-12-16 20:27:05.109702: val_loss -0.874\n",
      "2025-12-16 20:27:05.115655: Pseudo dice [0.9285, 0.9569, 0.9341]\n",
      "2025-12-16 20:27:05.118663: Epoch time: 138.64 s\n",
      "2025-12-16 20:27:05.954666: \n",
      "2025-12-16 20:27:05.955671: Epoch 455\n",
      "2025-12-16 20:27:05.959650: Current learning rate: 0.00579\n",
      "2025-12-16 20:29:25.265375: train_loss -0.8445\n",
      "2025-12-16 20:29:25.265375: val_loss -0.8734\n",
      "2025-12-16 20:29:25.270876: Pseudo dice [0.9265, 0.9571, 0.9364]\n",
      "2025-12-16 20:29:25.272879: Epoch time: 139.31 s\n",
      "2025-12-16 20:29:25.882171: \n",
      "2025-12-16 20:29:25.882171: Epoch 456\n",
      "2025-12-16 20:29:25.898088: Current learning rate: 0.00578\n",
      "2025-12-16 20:31:44.392802: train_loss -0.8524\n",
      "2025-12-16 20:31:44.394804: val_loss -0.862\n",
      "2025-12-16 20:31:44.398808: Pseudo dice [0.9214, 0.9547, 0.9235]\n",
      "2025-12-16 20:31:44.402813: Epoch time: 138.51 s\n",
      "2025-12-16 20:31:45.023466: \n",
      "2025-12-16 20:31:45.023466: Epoch 457\n",
      "2025-12-16 20:31:45.023466: Current learning rate: 0.00577\n",
      "2025-12-16 20:34:03.411894: train_loss -0.8505\n",
      "2025-12-16 20:34:03.413896: val_loss -0.8604\n",
      "2025-12-16 20:34:03.417900: Pseudo dice [0.9211, 0.9519, 0.9242]\n",
      "2025-12-16 20:34:03.421904: Epoch time: 138.39 s\n",
      "2025-12-16 20:34:04.053124: \n",
      "2025-12-16 20:34:04.053124: Epoch 458\n",
      "2025-12-16 20:34:04.053124: Current learning rate: 0.00576\n",
      "2025-12-16 20:36:22.517487: train_loss -0.8456\n",
      "2025-12-16 20:36:22.517487: val_loss -0.8732\n",
      "2025-12-16 20:36:22.521492: Pseudo dice [0.9225, 0.9566, 0.9398]\n",
      "2025-12-16 20:36:22.525496: Epoch time: 138.47 s\n",
      "2025-12-16 20:36:23.134120: \n",
      "2025-12-16 20:36:23.134120: Epoch 459\n",
      "2025-12-16 20:36:23.134120: Current learning rate: 0.00575\n",
      "2025-12-16 20:38:41.519741: train_loss -0.8451\n",
      "2025-12-16 20:38:41.521744: val_loss -0.8631\n",
      "2025-12-16 20:38:41.530764: Pseudo dice [0.9216, 0.9549, 0.927]\n",
      "2025-12-16 20:38:41.534560: Epoch time: 138.39 s\n",
      "2025-12-16 20:38:42.157044: \n",
      "2025-12-16 20:38:42.157044: Epoch 460\n",
      "2025-12-16 20:38:42.157044: Current learning rate: 0.00574\n",
      "2025-12-16 20:41:00.019465: train_loss -0.8444\n",
      "2025-12-16 20:41:00.021468: val_loss -0.8658\n",
      "2025-12-16 20:41:00.023471: Pseudo dice [0.927, 0.9548, 0.9236]\n",
      "2025-12-16 20:41:00.027475: Epoch time: 137.86 s\n",
      "2025-12-16 20:41:00.654974: \n",
      "2025-12-16 20:41:00.656976: Epoch 461\n",
      "2025-12-16 20:41:00.656976: Current learning rate: 0.00573\n",
      "2025-12-16 20:43:18.688304: train_loss -0.8135\n",
      "2025-12-16 20:43:18.690307: val_loss -0.8282\n",
      "2025-12-16 20:43:18.690307: Pseudo dice [0.9042, 0.9422, 0.9158]\n",
      "2025-12-16 20:43:18.694699: Epoch time: 138.03 s\n",
      "2025-12-16 20:43:19.503662: \n",
      "2025-12-16 20:43:19.503662: Epoch 462\n",
      "2025-12-16 20:43:19.503662: Current learning rate: 0.00572\n",
      "2025-12-16 20:45:37.527626: train_loss -0.8144\n",
      "2025-12-16 20:45:37.527626: val_loss -0.8473\n",
      "2025-12-16 20:45:37.527626: Pseudo dice [0.9107, 0.9513, 0.9192]\n",
      "2025-12-16 20:45:37.527626: Epoch time: 138.03 s\n",
      "2025-12-16 20:45:38.154027: \n",
      "2025-12-16 20:45:38.154027: Epoch 463\n",
      "2025-12-16 20:45:38.156312: Current learning rate: 0.00571\n",
      "2025-12-16 20:47:56.125874: train_loss -0.8045\n",
      "2025-12-16 20:47:56.125874: val_loss -0.839\n",
      "2025-12-16 20:47:56.129878: Pseudo dice [0.9122, 0.9499, 0.9171]\n",
      "2025-12-16 20:47:56.133882: Epoch time: 137.97 s\n",
      "2025-12-16 20:47:56.759041: \n",
      "2025-12-16 20:47:56.759041: Epoch 464\n",
      "2025-12-16 20:47:56.759041: Current learning rate: 0.0057\n",
      "2025-12-16 20:50:14.633269: train_loss -0.8243\n",
      "2025-12-16 20:50:14.633269: val_loss -0.8616\n",
      "2025-12-16 20:50:14.639487: Pseudo dice [0.9269, 0.9525, 0.9268]\n",
      "2025-12-16 20:50:14.643491: Epoch time: 137.88 s\n",
      "2025-12-16 20:50:15.266062: \n",
      "2025-12-16 20:50:15.266062: Epoch 465\n",
      "2025-12-16 20:50:15.266062: Current learning rate: 0.0057\n",
      "2025-12-16 20:52:33.322645: train_loss -0.8274\n",
      "2025-12-16 20:52:33.324648: val_loss -0.8523\n",
      "2025-12-16 20:52:33.328654: Pseudo dice [0.9158, 0.9501, 0.9288]\n",
      "2025-12-16 20:52:33.332659: Epoch time: 138.06 s\n",
      "2025-12-16 20:52:34.020302: \n",
      "2025-12-16 20:52:34.020302: Epoch 466\n",
      "2025-12-16 20:52:34.020302: Current learning rate: 0.00569\n",
      "2025-12-16 20:54:51.934254: train_loss -0.8392\n",
      "2025-12-16 20:54:51.935995: val_loss -0.8549\n",
      "2025-12-16 20:54:51.946011: Pseudo dice [0.916, 0.9498, 0.9274]\n",
      "2025-12-16 20:54:51.951756: Epoch time: 137.91 s\n",
      "2025-12-16 20:54:52.569674: \n",
      "2025-12-16 20:54:52.569674: Epoch 467\n",
      "2025-12-16 20:54:52.569674: Current learning rate: 0.00568\n",
      "2025-12-16 20:57:10.593950: train_loss -0.8394\n",
      "2025-12-16 20:57:10.593950: val_loss -0.8634\n",
      "2025-12-16 20:57:10.601966: Pseudo dice [0.9244, 0.953, 0.9273]\n",
      "2025-12-16 20:57:10.605972: Epoch time: 138.02 s\n",
      "2025-12-16 20:57:11.386421: \n",
      "2025-12-16 20:57:11.386421: Epoch 468\n",
      "2025-12-16 20:57:11.386421: Current learning rate: 0.00567\n",
      "2025-12-16 20:59:29.328824: train_loss -0.8444\n",
      "2025-12-16 20:59:29.328824: val_loss -0.8631\n",
      "2025-12-16 20:59:29.328824: Pseudo dice [0.9223, 0.9555, 0.9269]\n",
      "2025-12-16 20:59:29.328824: Epoch time: 137.94 s\n",
      "2025-12-16 20:59:30.084284: \n",
      "2025-12-16 20:59:30.084284: Epoch 469\n",
      "2025-12-16 20:59:30.084284: Current learning rate: 0.00566\n",
      "2025-12-16 21:01:48.171266: train_loss -0.8445\n",
      "2025-12-16 21:01:48.173269: val_loss -0.8697\n",
      "2025-12-16 21:01:48.175508: Pseudo dice [0.9287, 0.9576, 0.9277]\n",
      "2025-12-16 21:01:48.175508: Epoch time: 138.09 s\n",
      "2025-12-16 21:01:48.792206: \n",
      "2025-12-16 21:01:48.792206: Epoch 470\n",
      "2025-12-16 21:01:48.792206: Current learning rate: 0.00565\n",
      "2025-12-16 21:04:06.889475: train_loss -0.8392\n",
      "2025-12-16 21:04:06.889475: val_loss -0.8662\n",
      "2025-12-16 21:04:06.907164: Pseudo dice [0.9263, 0.9557, 0.918]\n",
      "2025-12-16 21:04:06.909167: Epoch time: 138.1 s\n",
      "2025-12-16 21:04:07.538261: \n",
      "2025-12-16 21:04:07.538261: Epoch 471\n",
      "2025-12-16 21:04:07.538261: Current learning rate: 0.00564\n",
      "2025-12-16 21:06:25.558456: train_loss -0.8412\n",
      "2025-12-16 21:06:25.560458: val_loss -0.872\n",
      "2025-12-16 21:06:25.564462: Pseudo dice [0.9267, 0.9566, 0.9335]\n",
      "2025-12-16 21:06:25.570207: Epoch time: 138.02 s\n",
      "2025-12-16 21:06:26.335181: \n",
      "2025-12-16 21:06:26.335181: Epoch 472\n",
      "2025-12-16 21:06:26.335181: Current learning rate: 0.00563\n",
      "2025-12-16 21:08:44.404472: train_loss -0.8441\n",
      "2025-12-16 21:08:44.404472: val_loss -0.8655\n",
      "2025-12-16 21:08:44.420234: Pseudo dice [0.923, 0.9544, 0.9405]\n",
      "2025-12-16 21:08:44.420234: Epoch time: 138.07 s\n",
      "2025-12-16 21:08:45.048992: \n",
      "2025-12-16 21:08:45.048992: Epoch 473\n",
      "2025-12-16 21:08:45.048992: Current learning rate: 0.00562\n",
      "2025-12-16 21:11:03.337438: train_loss -0.8412\n",
      "2025-12-16 21:11:03.337438: val_loss -0.8566\n",
      "2025-12-16 21:11:03.343182: Pseudo dice [0.9186, 0.95, 0.9206]\n",
      "2025-12-16 21:11:03.347186: Epoch time: 138.29 s\n",
      "2025-12-16 21:11:04.005595: \n",
      "2025-12-16 21:11:04.005595: Epoch 474\n",
      "2025-12-16 21:11:04.021402: Current learning rate: 0.00561\n",
      "2025-12-16 21:13:21.958828: train_loss -0.8459\n",
      "2025-12-16 21:13:21.958828: val_loss -0.8678\n",
      "2025-12-16 21:13:21.964573: Pseudo dice [0.9257, 0.9569, 0.9267]\n",
      "2025-12-16 21:13:21.968578: Epoch time: 137.95 s\n",
      "2025-12-16 21:13:22.977116: \n",
      "2025-12-16 21:13:22.977116: Epoch 475\n",
      "2025-12-16 21:13:22.977116: Current learning rate: 0.0056\n",
      "2025-12-16 21:15:41.120329: train_loss -0.8451\n",
      "2025-12-16 21:15:41.120329: val_loss -0.8604\n",
      "2025-12-16 21:15:41.136193: Pseudo dice [0.9212, 0.9478, 0.9295]\n",
      "2025-12-16 21:15:41.136193: Epoch time: 138.15 s\n",
      "2025-12-16 21:15:41.752750: \n",
      "2025-12-16 21:15:41.752750: Epoch 476\n",
      "2025-12-16 21:15:41.752750: Current learning rate: 0.00559\n",
      "2025-12-16 21:17:59.741783: train_loss -0.8471\n",
      "2025-12-16 21:17:59.741783: val_loss -0.8723\n",
      "2025-12-16 21:17:59.757443: Pseudo dice [0.9307, 0.9589, 0.9271]\n",
      "2025-12-16 21:17:59.757443: Epoch time: 137.99 s\n",
      "2025-12-16 21:18:00.374003: \n",
      "2025-12-16 21:18:00.374003: Epoch 477\n",
      "2025-12-16 21:18:00.389815: Current learning rate: 0.00558\n",
      "2025-12-16 21:20:18.383543: train_loss -0.8472\n",
      "2025-12-16 21:20:18.385283: val_loss -0.8612\n",
      "2025-12-16 21:20:18.385283: Pseudo dice [0.92, 0.9519, 0.9272]\n",
      "2025-12-16 21:20:18.385283: Epoch time: 138.01 s\n",
      "2025-12-16 21:20:19.109971: \n",
      "2025-12-16 21:20:19.109971: Epoch 478\n",
      "2025-12-16 21:20:19.125755: Current learning rate: 0.00557\n",
      "2025-12-16 21:22:36.984832: train_loss -0.8439\n",
      "2025-12-16 21:22:36.984832: val_loss -0.851\n",
      "2025-12-16 21:22:36.984832: Pseudo dice [0.9205, 0.9474, 0.9184]\n",
      "2025-12-16 21:22:36.984832: Epoch time: 137.87 s\n",
      "2025-12-16 21:22:37.660295: \n",
      "2025-12-16 21:22:37.660295: Epoch 479\n",
      "2025-12-16 21:22:37.660295: Current learning rate: 0.00556\n",
      "2025-12-16 21:24:55.681529: train_loss -0.8472\n",
      "2025-12-16 21:24:55.681529: val_loss -0.8621\n",
      "2025-12-16 21:24:55.685533: Pseudo dice [0.9222, 0.953, 0.9307]\n",
      "2025-12-16 21:24:55.691031: Epoch time: 138.02 s\n",
      "2025-12-16 21:24:56.312411: \n",
      "2025-12-16 21:24:56.312411: Epoch 480\n",
      "2025-12-16 21:24:56.328296: Current learning rate: 0.00555\n",
      "2025-12-16 21:27:14.349971: train_loss -0.8322\n",
      "2025-12-16 21:27:14.349971: val_loss -0.8342\n",
      "2025-12-16 21:27:14.353974: Pseudo dice [0.9108, 0.9454, 0.9095]\n",
      "2025-12-16 21:27:14.357978: Epoch time: 138.04 s\n",
      "2025-12-16 21:27:15.137050: \n",
      "2025-12-16 21:27:15.137050: Epoch 481\n",
      "2025-12-16 21:27:15.145203: Current learning rate: 0.00554\n",
      "2025-12-16 21:29:32.983634: train_loss -0.8017\n",
      "2025-12-16 21:29:32.983634: val_loss -0.8253\n",
      "2025-12-16 21:29:32.987638: Pseudo dice [0.9026, 0.9443, 0.9112]\n",
      "2025-12-16 21:29:32.991642: Epoch time: 137.85 s\n",
      "2025-12-16 21:29:33.780982: \n",
      "2025-12-16 21:29:33.780982: Epoch 482\n",
      "2025-12-16 21:29:33.796938: Current learning rate: 0.00553\n",
      "2025-12-16 21:31:51.802152: train_loss -0.8157\n",
      "2025-12-16 21:31:51.802152: val_loss -0.8377\n",
      "2025-12-16 21:31:51.802152: Pseudo dice [0.9082, 0.9447, 0.9239]\n",
      "2025-12-16 21:31:51.817661: Epoch time: 138.02 s\n",
      "2025-12-16 21:31:52.437091: \n",
      "2025-12-16 21:31:52.437091: Epoch 483\n",
      "2025-12-16 21:31:52.453011: Current learning rate: 0.00552\n",
      "2025-12-16 21:34:10.384289: train_loss -0.8261\n",
      "2025-12-16 21:34:10.386030: val_loss -0.8517\n",
      "2025-12-16 21:34:10.387985: Pseudo dice [0.9145, 0.9514, 0.9244]\n",
      "2025-12-16 21:34:10.392651: Epoch time: 137.95 s\n",
      "2025-12-16 21:34:11.050450: \n",
      "2025-12-16 21:34:11.050450: Epoch 484\n",
      "2025-12-16 21:34:11.050450: Current learning rate: 0.00551\n",
      "2025-12-16 21:36:28.907433: train_loss -0.8446\n",
      "2025-12-16 21:36:28.907433: val_loss -0.8566\n",
      "2025-12-16 21:36:28.907433: Pseudo dice [0.9175, 0.9451, 0.937]\n",
      "2025-12-16 21:36:28.907433: Epoch time: 137.86 s\n",
      "2025-12-16 21:36:29.539602: \n",
      "2025-12-16 21:36:29.539602: Epoch 485\n",
      "2025-12-16 21:36:29.539602: Current learning rate: 0.0055\n",
      "2025-12-16 21:38:47.492053: train_loss -0.8282\n",
      "2025-12-16 21:38:47.492053: val_loss -0.8554\n",
      "2025-12-16 21:38:47.510116: Pseudo dice [0.9117, 0.9481, 0.9382]\n",
      "2025-12-16 21:38:47.514120: Epoch time: 137.95 s\n",
      "2025-12-16 21:38:48.141548: \n",
      "2025-12-16 21:38:48.141548: Epoch 486\n",
      "2025-12-16 21:38:48.141548: Current learning rate: 0.00549\n",
      "2025-12-16 21:41:06.194919: train_loss -0.8439\n",
      "2025-12-16 21:41:06.194919: val_loss -0.8698\n",
      "2025-12-16 21:41:06.200924: Pseudo dice [0.9231, 0.9524, 0.9412]\n",
      "2025-12-16 21:41:06.202665: Epoch time: 138.05 s\n",
      "2025-12-16 21:41:06.833265: \n",
      "2025-12-16 21:41:06.835267: Epoch 487\n",
      "2025-12-16 21:41:06.835267: Current learning rate: 0.00548\n",
      "2025-12-16 21:43:24.672453: train_loss -0.8413\n",
      "2025-12-16 21:43:24.672453: val_loss -0.8557\n",
      "2025-12-16 21:43:24.688529: Pseudo dice [0.9184, 0.9487, 0.9279]\n",
      "2025-12-16 21:43:24.691189: Epoch time: 137.84 s\n",
      "2025-12-16 21:43:25.478629: \n",
      "2025-12-16 21:43:25.478629: Epoch 488\n",
      "2025-12-16 21:43:25.488447: Current learning rate: 0.00547\n",
      "2025-12-16 21:45:43.554374: train_loss -0.8437\n",
      "2025-12-16 21:45:43.554374: val_loss -0.8635\n",
      "2025-12-16 21:45:43.560119: Pseudo dice [0.9218, 0.9571, 0.928]\n",
      "2025-12-16 21:45:43.568137: Epoch time: 138.08 s\n",
      "2025-12-16 21:45:44.253845: \n",
      "2025-12-16 21:45:44.253845: Epoch 489\n",
      "2025-12-16 21:45:44.253845: Current learning rate: 0.00546\n",
      "2025-12-16 21:48:02.176204: train_loss -0.8415\n",
      "2025-12-16 21:48:02.178206: val_loss -0.8689\n",
      "2025-12-16 21:48:02.182210: Pseudo dice [0.9245, 0.9553, 0.9326]\n",
      "2025-12-16 21:48:02.189960: Epoch time: 137.94 s\n",
      "2025-12-16 21:48:02.836333: \n",
      "2025-12-16 21:48:02.836333: Epoch 490\n",
      "2025-12-16 21:48:02.842005: Current learning rate: 0.00546\n",
      "2025-12-16 21:50:20.860263: train_loss -0.8433\n",
      "2025-12-16 21:50:20.860263: val_loss -0.8518\n",
      "2025-12-16 21:50:20.871592: Pseudo dice [0.9133, 0.9456, 0.9339]\n",
      "2025-12-16 21:50:20.871592: Epoch time: 138.02 s\n",
      "2025-12-16 21:50:21.534104: \n",
      "2025-12-16 21:50:21.534104: Epoch 491\n",
      "2025-12-16 21:50:21.534104: Current learning rate: 0.00545\n",
      "2025-12-16 21:52:39.462233: train_loss -0.8423\n",
      "2025-12-16 21:52:39.462233: val_loss -0.848\n",
      "2025-12-16 21:52:39.468241: Pseudo dice [0.9111, 0.9499, 0.9289]\n",
      "2025-12-16 21:52:39.472247: Epoch time: 137.92 s\n",
      "2025-12-16 21:52:40.115746: \n",
      "2025-12-16 21:52:40.115746: Epoch 492\n",
      "2025-12-16 21:52:40.131646: Current learning rate: 0.00544\n",
      "2025-12-16 21:54:58.081194: train_loss -0.8415\n",
      "2025-12-16 21:54:58.081194: val_loss -0.8598\n",
      "2025-12-16 21:54:58.087383: Pseudo dice [0.9205, 0.9527, 0.9244]\n",
      "2025-12-16 21:54:58.091609: Epoch time: 137.97 s\n",
      "2025-12-16 21:54:58.715506: \n",
      "2025-12-16 21:54:58.715506: Epoch 493\n",
      "2025-12-16 21:54:58.715506: Current learning rate: 0.00543\n",
      "2025-12-16 21:57:16.593784: train_loss -0.8452\n",
      "2025-12-16 21:57:16.593784: val_loss -0.8707\n",
      "2025-12-16 21:57:16.593784: Pseudo dice [0.9294, 0.9574, 0.9279]\n",
      "2025-12-16 21:57:16.593784: Epoch time: 137.88 s\n",
      "2025-12-16 21:57:17.386518: \n",
      "2025-12-16 21:57:17.386518: Epoch 494\n",
      "2025-12-16 21:57:17.386518: Current learning rate: 0.00542\n",
      "2025-12-16 21:59:35.154833: train_loss -0.8435\n",
      "2025-12-16 21:59:35.154833: val_loss -0.8662\n",
      "2025-12-16 21:59:35.160839: Pseudo dice [0.9247, 0.9549, 0.9373]\n",
      "2025-12-16 21:59:35.166588: Epoch time: 137.77 s\n",
      "2025-12-16 21:59:35.925518: \n",
      "2025-12-16 21:59:35.925518: Epoch 495\n",
      "2025-12-16 21:59:35.927521: Current learning rate: 0.00541\n",
      "2025-12-16 22:01:54.037618: train_loss -0.8343\n",
      "2025-12-16 22:01:54.037618: val_loss -0.8517\n",
      "2025-12-16 22:01:54.048522: Pseudo dice [0.916, 0.9501, 0.9266]\n",
      "2025-12-16 22:01:54.048522: Epoch time: 138.11 s\n",
      "2025-12-16 22:01:54.669915: \n",
      "2025-12-16 22:01:54.669915: Epoch 496\n",
      "2025-12-16 22:01:54.685652: Current learning rate: 0.0054\n",
      "2025-12-16 22:04:12.651016: train_loss -0.8395\n",
      "2025-12-16 22:04:12.651016: val_loss -0.8644\n",
      "2025-12-16 22:04:12.662595: Pseudo dice [0.9227, 0.9547, 0.9342]\n",
      "2025-12-16 22:04:12.668766: Epoch time: 137.98 s\n",
      "2025-12-16 22:04:13.285946: \n",
      "2025-12-16 22:04:13.285946: Epoch 497\n",
      "2025-12-16 22:04:13.285946: Current learning rate: 0.00539\n",
      "2025-12-16 22:06:31.242451: train_loss -0.8442\n",
      "2025-12-16 22:06:31.244193: val_loss -0.8622\n",
      "2025-12-16 22:06:31.248199: Pseudo dice [0.9156, 0.9525, 0.9383]\n",
      "2025-12-16 22:06:31.252205: Epoch time: 137.96 s\n",
      "2025-12-16 22:06:31.981857: \n",
      "2025-12-16 22:06:31.983860: Epoch 498\n",
      "2025-12-16 22:06:31.987462: Current learning rate: 0.00538\n",
      "2025-12-16 22:08:50.133425: train_loss -0.8427\n",
      "2025-12-16 22:08:50.133425: val_loss -0.8663\n",
      "2025-12-16 22:08:50.133425: Pseudo dice [0.9227, 0.9563, 0.9283]\n",
      "2025-12-16 22:08:50.133425: Epoch time: 138.15 s\n",
      "2025-12-16 22:08:50.781282: \n",
      "2025-12-16 22:08:50.781282: Epoch 499\n",
      "2025-12-16 22:08:50.785696: Current learning rate: 0.00537\n",
      "2025-12-16 22:11:09.058425: train_loss -0.8461\n",
      "2025-12-16 22:11:09.060427: val_loss -0.8694\n",
      "2025-12-16 22:11:09.064432: Pseudo dice [0.9271, 0.9559, 0.9338]\n",
      "2025-12-16 22:11:09.068435: Epoch time: 138.29 s\n",
      "2025-12-16 22:11:10.115057: \n",
      "2025-12-16 22:11:10.115057: Epoch 500\n",
      "2025-12-16 22:11:10.119488: Current learning rate: 0.00536\n",
      "2025-12-16 22:13:28.091738: train_loss -0.8433\n",
      "2025-12-16 22:13:28.091738: val_loss -0.8693\n",
      "2025-12-16 22:13:28.099487: Pseudo dice [0.9257, 0.9549, 0.9327]\n",
      "2025-12-16 22:13:28.103491: Epoch time: 137.98 s\n",
      "2025-12-16 22:13:28.848239: \n",
      "2025-12-16 22:13:28.848239: Epoch 501\n",
      "2025-12-16 22:13:28.848239: Current learning rate: 0.00535\n",
      "2025-12-16 22:15:46.835968: train_loss -0.8438\n",
      "2025-12-16 22:15:46.835968: val_loss -0.8671\n",
      "2025-12-16 22:15:46.843716: Pseudo dice [0.9253, 0.9561, 0.9314]\n",
      "2025-12-16 22:15:46.845681: Epoch time: 137.99 s\n",
      "2025-12-16 22:15:47.462445: \n",
      "2025-12-16 22:15:47.462445: Epoch 502\n",
      "2025-12-16 22:15:47.479676: Current learning rate: 0.00534\n",
      "2025-12-16 22:18:05.248397: train_loss -0.8458\n",
      "2025-12-16 22:18:05.248397: val_loss -0.8563\n",
      "2025-12-16 22:18:05.254403: Pseudo dice [0.9213, 0.9471, 0.9321]\n",
      "2025-12-16 22:18:05.258145: Epoch time: 137.79 s\n",
      "2025-12-16 22:18:05.892391: \n",
      "2025-12-16 22:18:05.892391: Epoch 503\n",
      "2025-12-16 22:18:05.892391: Current learning rate: 0.00533\n",
      "2025-12-16 22:20:23.730002: train_loss -0.8439\n",
      "2025-12-16 22:20:23.730002: val_loss -0.8727\n",
      "2025-12-16 22:20:23.736008: Pseudo dice [0.9262, 0.9548, 0.9334]\n",
      "2025-12-16 22:20:23.740013: Epoch time: 137.84 s\n",
      "2025-12-16 22:20:24.435225: \n",
      "2025-12-16 22:20:24.435225: Epoch 504\n",
      "2025-12-16 22:20:24.435225: Current learning rate: 0.00532\n",
      "2025-12-16 22:22:42.490448: train_loss -0.8432\n",
      "2025-12-16 22:22:42.490448: val_loss -0.8658\n",
      "2025-12-16 22:22:42.497950: Pseudo dice [0.9225, 0.9516, 0.9314]\n",
      "2025-12-16 22:22:42.501955: Epoch time: 138.07 s\n",
      "2025-12-16 22:22:43.118252: \n",
      "2025-12-16 22:22:43.134053: Epoch 505\n",
      "2025-12-16 22:22:43.134053: Current learning rate: 0.00531\n",
      "2025-12-16 22:25:01.242064: train_loss -0.8459\n",
      "2025-12-16 22:25:01.242064: val_loss -0.8631\n",
      "2025-12-16 22:25:01.246069: Pseudo dice [0.9182, 0.9513, 0.9304]\n",
      "2025-12-16 22:25:01.250072: Epoch time: 138.12 s\n",
      "2025-12-16 22:25:01.882929: \n",
      "2025-12-16 22:25:01.882929: Epoch 506\n",
      "2025-12-16 22:25:01.883933: Current learning rate: 0.0053\n",
      "2025-12-16 22:27:19.801040: train_loss -0.8507\n",
      "2025-12-16 22:27:19.801040: val_loss -0.8653\n",
      "2025-12-16 22:27:19.807046: Pseudo dice [0.9229, 0.9538, 0.9329]\n",
      "2025-12-16 22:27:19.809048: Epoch time: 137.92 s\n",
      "2025-12-16 22:27:20.628261: \n",
      "2025-12-16 22:27:20.628261: Epoch 507\n",
      "2025-12-16 22:27:20.628261: Current learning rate: 0.00529\n",
      "2025-12-16 22:29:38.588051: train_loss -0.8473\n",
      "2025-12-16 22:29:38.588051: val_loss -0.8709\n",
      "2025-12-16 22:29:38.593560: Pseudo dice [0.9264, 0.9532, 0.9358]\n",
      "2025-12-16 22:29:38.597564: Epoch time: 137.96 s\n",
      "2025-12-16 22:29:39.234393: \n",
      "2025-12-16 22:29:39.236395: Epoch 508\n",
      "2025-12-16 22:29:39.236395: Current learning rate: 0.00528\n",
      "2025-12-16 22:31:57.186723: train_loss -0.8474\n",
      "2025-12-16 22:31:57.188726: val_loss -0.8608\n",
      "2025-12-16 22:31:57.190729: Pseudo dice [0.9241, 0.9512, 0.9251]\n",
      "2025-12-16 22:31:57.198338: Epoch time: 137.95 s\n",
      "2025-12-16 22:31:57.831174: \n",
      "2025-12-16 22:31:57.831174: Epoch 509\n",
      "2025-12-16 22:31:57.831174: Current learning rate: 0.00527\n",
      "2025-12-16 22:34:15.756208: train_loss -0.8489\n",
      "2025-12-16 22:34:15.758210: val_loss -0.8808\n",
      "2025-12-16 22:34:15.761703: Pseudo dice [0.9332, 0.9591, 0.9358]\n",
      "2025-12-16 22:34:15.765707: Epoch time: 137.93 s\n",
      "2025-12-16 22:34:16.404130: \n",
      "2025-12-16 22:34:16.404130: Epoch 510\n",
      "2025-12-16 22:34:16.404130: Current learning rate: 0.00526\n",
      "2025-12-16 22:36:34.335037: train_loss -0.848\n",
      "2025-12-16 22:36:34.337039: val_loss -0.8695\n",
      "2025-12-16 22:36:34.340915: Pseudo dice [0.9262, 0.9561, 0.9328]\n",
      "2025-12-16 22:36:34.344919: Epoch time: 137.93 s\n",
      "2025-12-16 22:36:34.973929: \n",
      "2025-12-16 22:36:34.973929: Epoch 511\n",
      "2025-12-16 22:36:34.973929: Current learning rate: 0.00525\n",
      "2025-12-16 22:38:52.955019: train_loss -0.8508\n",
      "2025-12-16 22:38:52.955019: val_loss -0.8717\n",
      "2025-12-16 22:38:52.971014: Pseudo dice [0.9256, 0.9552, 0.9342]\n",
      "2025-12-16 22:38:52.971014: Epoch time: 137.98 s\n",
      "2025-12-16 22:38:53.637440: \n",
      "2025-12-16 22:38:53.637440: Epoch 512\n",
      "2025-12-16 22:38:53.637440: Current learning rate: 0.00524\n",
      "2025-12-16 22:41:11.618928: train_loss -0.8468\n",
      "2025-12-16 22:41:11.618928: val_loss -0.8581\n",
      "2025-12-16 22:41:11.622932: Pseudo dice [0.9156, 0.9527, 0.9307]\n",
      "2025-12-16 22:41:11.626936: Epoch time: 137.98 s\n",
      "2025-12-16 22:41:12.257987: \n",
      "2025-12-16 22:41:12.257987: Epoch 513\n",
      "2025-12-16 22:41:12.257987: Current learning rate: 0.00523\n",
      "2025-12-16 22:43:30.135499: train_loss -0.853\n",
      "2025-12-16 22:43:30.137502: val_loss -0.8654\n",
      "2025-12-16 22:43:30.137502: Pseudo dice [0.9181, 0.9503, 0.9344]\n",
      "2025-12-16 22:43:30.143986: Epoch time: 137.88 s\n",
      "2025-12-16 22:43:30.951734: \n",
      "2025-12-16 22:43:30.951734: Epoch 514\n",
      "2025-12-16 22:43:30.951734: Current learning rate: 0.00522\n",
      "2025-12-16 22:45:48.913779: train_loss -0.8494\n",
      "2025-12-16 22:45:48.913779: val_loss -0.8747\n",
      "2025-12-16 22:45:48.919786: Pseudo dice [0.9257, 0.959, 0.9338]\n",
      "2025-12-16 22:45:48.923791: Epoch time: 137.96 s\n",
      "2025-12-16 22:45:49.567534: \n",
      "2025-12-16 22:45:49.567534: Epoch 515\n",
      "2025-12-16 22:45:49.567534: Current learning rate: 0.00521\n",
      "2025-12-16 22:48:07.508243: train_loss -0.8491\n",
      "2025-12-16 22:48:07.510246: val_loss -0.8778\n",
      "2025-12-16 22:48:07.510246: Pseudo dice [0.9297, 0.9578, 0.9344]\n",
      "2025-12-16 22:48:07.518000: Epoch time: 137.94 s\n",
      "2025-12-16 22:48:07.518000: Yayy! New best EMA pseudo Dice: 0.9368\n",
      "2025-12-16 22:48:08.382491: \n",
      "2025-12-16 22:48:08.382491: Epoch 516\n",
      "2025-12-16 22:48:08.382491: Current learning rate: 0.0052\n",
      "2025-12-16 22:50:26.291947: train_loss -0.8529\n",
      "2025-12-16 22:50:26.291947: val_loss -0.8767\n",
      "2025-12-16 22:50:26.297692: Pseudo dice [0.9308, 0.9594, 0.9341]\n",
      "2025-12-16 22:50:26.297692: Epoch time: 137.91 s\n",
      "2025-12-16 22:50:26.303282: Yayy! New best EMA pseudo Dice: 0.9372\n",
      "2025-12-16 22:50:27.215556: \n",
      "2025-12-16 22:50:27.215556: Epoch 517\n",
      "2025-12-16 22:50:27.215556: Current learning rate: 0.00519\n",
      "2025-12-16 22:52:45.192095: train_loss -0.8523\n",
      "2025-12-16 22:52:45.192095: val_loss -0.8649\n",
      "2025-12-16 22:52:45.200107: Pseudo dice [0.9203, 0.9533, 0.9274]\n",
      "2025-12-16 22:52:45.206129: Epoch time: 137.98 s\n",
      "2025-12-16 22:52:45.842349: \n",
      "2025-12-16 22:52:45.842349: Epoch 518\n",
      "2025-12-16 22:52:45.858214: Current learning rate: 0.00518\n",
      "2025-12-16 22:55:03.798854: train_loss -0.85\n",
      "2025-12-16 22:55:03.798854: val_loss -0.8673\n",
      "2025-12-16 22:55:03.804456: Pseudo dice [0.9239, 0.9565, 0.9287]\n",
      "2025-12-16 22:55:03.804456: Epoch time: 137.96 s\n",
      "2025-12-16 22:55:04.436721: \n",
      "2025-12-16 22:55:04.436721: Epoch 519\n",
      "2025-12-16 22:55:04.436721: Current learning rate: 0.00518\n",
      "2025-12-16 22:57:22.455354: train_loss -0.8486\n",
      "2025-12-16 22:57:22.455354: val_loss -0.874\n",
      "2025-12-16 22:57:22.462366: Pseudo dice [0.9282, 0.9625, 0.9254]\n",
      "2025-12-16 22:57:22.466370: Epoch time: 138.02 s\n",
      "2025-12-16 22:57:23.268373: \n",
      "2025-12-16 22:57:23.268373: Epoch 520\n",
      "2025-12-16 22:57:23.268373: Current learning rate: 0.00517\n",
      "2025-12-16 22:59:41.492136: train_loss -0.8403\n",
      "2025-12-16 22:59:41.492136: val_loss -0.86\n",
      "2025-12-16 22:59:41.497330: Pseudo dice [0.9194, 0.9504, 0.9282]\n",
      "2025-12-16 22:59:41.501200: Epoch time: 138.22 s\n",
      "2025-12-16 22:59:42.225393: \n",
      "2025-12-16 22:59:42.225393: Epoch 521\n",
      "2025-12-16 22:59:42.227397: Current learning rate: 0.00516\n",
      "2025-12-16 23:02:00.265389: train_loss -0.8471\n",
      "2025-12-16 23:02:00.265389: val_loss -0.8701\n",
      "2025-12-16 23:02:00.265389: Pseudo dice [0.9244, 0.9545, 0.9355]\n",
      "2025-12-16 23:02:00.281038: Epoch time: 138.04 s\n",
      "2025-12-16 23:02:00.939689: \n",
      "2025-12-16 23:02:00.943236: Epoch 522\n",
      "2025-12-16 23:02:00.943236: Current learning rate: 0.00515\n",
      "2025-12-16 23:04:18.959757: train_loss -0.8401\n",
      "2025-12-16 23:04:18.961759: val_loss -0.8743\n",
      "2025-12-16 23:04:18.961759: Pseudo dice [0.9239, 0.9595, 0.9386]\n",
      "2025-12-16 23:04:18.961759: Epoch time: 138.02 s\n",
      "2025-12-16 23:04:19.627439: \n",
      "2025-12-16 23:04:19.627439: Epoch 523\n",
      "2025-12-16 23:04:19.627439: Current learning rate: 0.00514\n",
      "2025-12-16 23:06:37.642678: train_loss -0.8311\n",
      "2025-12-16 23:06:37.642678: val_loss -0.8337\n",
      "2025-12-16 23:06:37.642678: Pseudo dice [0.9018, 0.9386, 0.9172]\n",
      "2025-12-16 23:06:37.658430: Epoch time: 138.02 s\n",
      "2025-12-16 23:06:38.416097: \n",
      "2025-12-16 23:06:38.416097: Epoch 524\n",
      "2025-12-16 23:06:38.416097: Current learning rate: 0.00513\n",
      "2025-12-16 23:08:56.626466: train_loss -0.8326\n",
      "2025-12-16 23:08:56.628468: val_loss -0.872\n",
      "2025-12-16 23:08:56.633712: Pseudo dice [0.928, 0.9603, 0.9277]\n",
      "2025-12-16 23:08:56.633712: Epoch time: 138.21 s\n",
      "2025-12-16 23:08:57.276244: \n",
      "2025-12-16 23:08:57.276244: Epoch 525\n",
      "2025-12-16 23:08:57.279263: Current learning rate: 0.00512\n",
      "2025-12-16 23:11:15.609744: train_loss -0.8387\n",
      "2025-12-16 23:11:15.609744: val_loss -0.8549\n",
      "2025-12-16 23:11:15.611747: Pseudo dice [0.9148, 0.9511, 0.9347]\n",
      "2025-12-16 23:11:15.611747: Epoch time: 138.33 s\n",
      "2025-12-16 23:11:16.242721: \n",
      "2025-12-16 23:11:16.242721: Epoch 526\n",
      "2025-12-16 23:11:16.242721: Current learning rate: 0.00511\n",
      "2025-12-16 23:13:34.100958: train_loss -0.8466\n",
      "2025-12-16 23:13:34.100958: val_loss -0.8595\n",
      "2025-12-16 23:13:34.106966: Pseudo dice [0.92, 0.9516, 0.9287]\n",
      "2025-12-16 23:13:34.111973: Epoch time: 137.86 s\n",
      "2025-12-16 23:13:35.105134: \n",
      "2025-12-16 23:13:35.105134: Epoch 527\n",
      "2025-12-16 23:13:35.105134: Current learning rate: 0.0051\n",
      "2025-12-16 23:15:53.047938: train_loss -0.8448\n",
      "2025-12-16 23:15:53.047938: val_loss -0.8708\n",
      "2025-12-16 23:15:53.047938: Pseudo dice [0.9254, 0.9581, 0.9386]\n",
      "2025-12-16 23:15:53.063674: Epoch time: 137.94 s\n",
      "2025-12-16 23:15:53.681442: \n",
      "2025-12-16 23:15:53.681442: Epoch 528\n",
      "2025-12-16 23:15:53.681442: Current learning rate: 0.00509\n",
      "2025-12-16 23:18:11.623014: train_loss -0.8499\n",
      "2025-12-16 23:18:11.623014: val_loss -0.8573\n",
      "2025-12-16 23:18:11.638747: Pseudo dice [0.9162, 0.9505, 0.9242]\n",
      "2025-12-16 23:18:11.638747: Epoch time: 137.94 s\n",
      "2025-12-16 23:18:12.272194: \n",
      "2025-12-16 23:18:12.272194: Epoch 529\n",
      "2025-12-16 23:18:12.272194: Current learning rate: 0.00508\n",
      "2025-12-16 23:20:30.224138: train_loss -0.8446\n",
      "2025-12-16 23:20:30.224138: val_loss -0.8686\n",
      "2025-12-16 23:20:30.224138: Pseudo dice [0.9232, 0.9574, 0.9388]\n",
      "2025-12-16 23:20:30.224138: Epoch time: 137.95 s\n",
      "2025-12-16 23:20:30.967708: \n",
      "2025-12-16 23:20:30.967708: Epoch 530\n",
      "2025-12-16 23:20:30.967708: Current learning rate: 0.00507\n",
      "2025-12-16 23:22:49.083268: train_loss -0.846\n",
      "2025-12-16 23:22:49.083268: val_loss -0.8568\n",
      "2025-12-16 23:22:49.099187: Pseudo dice [0.9156, 0.9463, 0.9312]\n",
      "2025-12-16 23:22:49.104690: Epoch time: 138.12 s\n",
      "2025-12-16 23:22:49.741072: \n",
      "2025-12-16 23:22:49.741072: Epoch 531\n",
      "2025-12-16 23:22:49.741072: Current learning rate: 0.00506\n",
      "2025-12-16 23:25:07.645882: train_loss -0.8506\n",
      "2025-12-16 23:25:07.645882: val_loss -0.8622\n",
      "2025-12-16 23:25:07.650097: Pseudo dice [0.917, 0.9532, 0.9346]\n",
      "2025-12-16 23:25:07.650097: Epoch time: 137.9 s\n",
      "2025-12-16 23:25:08.278608: \n",
      "2025-12-16 23:25:08.278608: Epoch 532\n",
      "2025-12-16 23:25:08.278608: Current learning rate: 0.00505\n",
      "2025-12-16 23:27:26.276951: train_loss -0.8483\n",
      "2025-12-16 23:27:26.276951: val_loss -0.8666\n",
      "2025-12-16 23:27:26.282956: Pseudo dice [0.9267, 0.9567, 0.9244]\n",
      "2025-12-16 23:27:26.286589: Epoch time: 138.01 s\n",
      "2025-12-16 23:27:27.196351: \n",
      "2025-12-16 23:27:27.196351: Epoch 533\n",
      "2025-12-16 23:27:27.196351: Current learning rate: 0.00504\n",
      "2025-12-16 23:29:44.911436: train_loss -0.8477\n",
      "2025-12-16 23:29:44.911436: val_loss -0.8806\n",
      "2025-12-16 23:29:44.911436: Pseudo dice [0.934, 0.9605, 0.9319]\n",
      "2025-12-16 23:29:44.911436: Epoch time: 137.72 s\n",
      "2025-12-16 23:29:45.542774: \n",
      "2025-12-16 23:29:45.542774: Epoch 534\n",
      "2025-12-16 23:29:45.542774: Current learning rate: 0.00503\n",
      "2025-12-16 23:32:03.626071: train_loss -0.8532\n",
      "2025-12-16 23:32:03.628073: val_loss -0.8764\n",
      "2025-12-16 23:32:03.632205: Pseudo dice [0.927, 0.9557, 0.9462]\n",
      "2025-12-16 23:32:03.632205: Epoch time: 138.08 s\n",
      "2025-12-16 23:32:04.260896: \n",
      "2025-12-16 23:32:04.260896: Epoch 535\n",
      "2025-12-16 23:32:04.263562: Current learning rate: 0.00502\n",
      "2025-12-16 23:34:22.018298: train_loss -0.8478\n",
      "2025-12-16 23:34:22.018298: val_loss -0.8704\n",
      "2025-12-16 23:34:22.025306: Pseudo dice [0.9234, 0.9554, 0.9336]\n",
      "2025-12-16 23:34:22.029310: Epoch time: 137.76 s\n",
      "2025-12-16 23:34:22.661637: \n",
      "2025-12-16 23:34:22.661637: Epoch 536\n",
      "2025-12-16 23:34:22.661637: Current learning rate: 0.00501\n",
      "2025-12-16 23:36:40.695772: train_loss -0.8486\n",
      "2025-12-16 23:36:40.695772: val_loss -0.8712\n",
      "2025-12-16 23:36:40.701637: Pseudo dice [0.9256, 0.9589, 0.9341]\n",
      "2025-12-16 23:36:40.707382: Epoch time: 138.04 s\n",
      "2025-12-16 23:36:41.337382: \n",
      "2025-12-16 23:36:41.337382: Epoch 537\n",
      "2025-12-16 23:36:41.337382: Current learning rate: 0.005\n",
      "2025-12-16 23:38:59.205022: train_loss -0.846\n",
      "2025-12-16 23:38:59.205022: val_loss -0.8793\n",
      "2025-12-16 23:38:59.206763: Pseudo dice [0.9327, 0.9613, 0.9337]\n",
      "2025-12-16 23:38:59.214092: Epoch time: 137.87 s\n",
      "2025-12-16 23:38:59.218096: Yayy! New best EMA pseudo Dice: 0.9376\n",
      "2025-12-16 23:39:00.101050: \n",
      "2025-12-16 23:39:00.101050: Epoch 538\n",
      "2025-12-16 23:39:00.101050: Current learning rate: 0.00499\n",
      "2025-12-16 23:41:18.141097: train_loss -0.8446\n",
      "2025-12-16 23:41:18.141097: val_loss -0.8734\n",
      "2025-12-16 23:41:18.147416: Pseudo dice [0.926, 0.9568, 0.9343]\n",
      "2025-12-16 23:41:18.150420: Epoch time: 138.04 s\n",
      "2025-12-16 23:41:18.154263: Yayy! New best EMA pseudo Dice: 0.9377\n",
      "2025-12-16 23:41:19.253420: \n",
      "2025-12-16 23:41:19.253420: Epoch 539\n",
      "2025-12-16 23:41:19.259427: Current learning rate: 0.00498\n",
      "2025-12-16 23:43:37.111270: train_loss -0.8424\n",
      "2025-12-16 23:43:37.111270: val_loss -0.8534\n",
      "2025-12-16 23:43:37.117276: Pseudo dice [0.9147, 0.9469, 0.9214]\n",
      "2025-12-16 23:43:37.119278: Epoch time: 137.86 s\n",
      "2025-12-16 23:43:37.736912: \n",
      "2025-12-16 23:43:37.736912: Epoch 540\n",
      "2025-12-16 23:43:37.736912: Current learning rate: 0.00497\n",
      "2025-12-16 23:45:55.621509: train_loss -0.8458\n",
      "2025-12-16 23:45:55.621509: val_loss -0.8626\n",
      "2025-12-16 23:45:55.628577: Pseudo dice [0.9213, 0.9519, 0.93]\n",
      "2025-12-16 23:45:55.632581: Epoch time: 137.88 s\n",
      "2025-12-16 23:45:56.256287: \n",
      "2025-12-16 23:45:56.256287: Epoch 541\n",
      "2025-12-16 23:45:56.272010: Current learning rate: 0.00496\n",
      "2025-12-16 23:48:14.074400: train_loss -0.8515\n",
      "2025-12-16 23:48:14.074400: val_loss -0.8617\n",
      "2025-12-16 23:48:14.074400: Pseudo dice [0.9194, 0.9508, 0.9196]\n",
      "2025-12-16 23:48:14.090405: Epoch time: 137.82 s\n",
      "2025-12-16 23:48:14.709805: \n",
      "2025-12-16 23:48:14.709805: Epoch 542\n",
      "2025-12-16 23:48:14.709805: Current learning rate: 0.00495\n",
      "2025-12-16 23:50:32.586123: train_loss -0.8508\n",
      "2025-12-16 23:50:32.586123: val_loss -0.8694\n",
      "2025-12-16 23:50:32.591867: Pseudo dice [0.9264, 0.9555, 0.9267]\n",
      "2025-12-16 23:50:32.595871: Epoch time: 137.88 s\n",
      "2025-12-16 23:50:33.228677: \n",
      "2025-12-16 23:50:33.228677: Epoch 543\n",
      "2025-12-16 23:50:33.228677: Current learning rate: 0.00494\n",
      "2025-12-16 23:52:51.128171: train_loss -0.8497\n",
      "2025-12-16 23:52:51.130173: val_loss -0.8668\n",
      "2025-12-16 23:52:51.134177: Pseudo dice [0.9265, 0.9553, 0.9184]\n",
      "2025-12-16 23:52:51.138181: Epoch time: 137.9 s\n",
      "2025-12-16 23:52:51.805887: \n",
      "2025-12-16 23:52:51.805887: Epoch 544\n",
      "2025-12-16 23:52:51.805887: Current learning rate: 0.00493\n",
      "2025-12-16 23:55:09.813072: train_loss -0.8494\n",
      "2025-12-16 23:55:09.813072: val_loss -0.865\n",
      "2025-12-16 23:55:09.820129: Pseudo dice [0.9221, 0.9553, 0.9307]\n",
      "2025-12-16 23:55:09.824897: Epoch time: 138.01 s\n",
      "2025-12-16 23:55:10.627525: \n",
      "2025-12-16 23:55:10.627525: Epoch 545\n",
      "2025-12-16 23:55:10.627525: Current learning rate: 0.00492\n",
      "2025-12-16 23:57:28.482924: train_loss -0.8491\n",
      "2025-12-16 23:57:28.482924: val_loss -0.8702\n",
      "2025-12-16 23:57:28.489932: Pseudo dice [0.9258, 0.9579, 0.9254]\n",
      "2025-12-16 23:57:28.493936: Epoch time: 137.86 s\n",
      "2025-12-16 23:57:29.120295: \n",
      "2025-12-16 23:57:29.120295: Epoch 546\n",
      "2025-12-16 23:57:29.120295: Current learning rate: 0.00491\n",
      "2025-12-16 23:59:46.961279: train_loss -0.8523\n",
      "2025-12-16 23:59:46.963281: val_loss -0.8564\n",
      "2025-12-16 23:59:46.969289: Pseudo dice [0.9128, 0.95, 0.9301]\n",
      "2025-12-16 23:59:46.975035: Epoch time: 137.84 s\n",
      "2025-12-16 23:59:47.764292: \n",
      "2025-12-16 23:59:47.764292: Epoch 547\n",
      "2025-12-16 23:59:47.774510: Current learning rate: 0.0049\n",
      "2025-12-17 00:02:05.551368: train_loss -0.8551\n",
      "2025-12-17 00:02:05.551368: val_loss -0.877\n",
      "2025-12-17 00:02:05.561382: Pseudo dice [0.9292, 0.9604, 0.9303]\n",
      "2025-12-17 00:02:05.567129: Epoch time: 137.79 s\n",
      "2025-12-17 00:02:06.196535: \n",
      "2025-12-17 00:02:06.196535: Epoch 548\n",
      "2025-12-17 00:02:06.196535: Current learning rate: 0.00489\n",
      "2025-12-17 00:04:24.191232: train_loss -0.8489\n",
      "2025-12-17 00:04:24.191232: val_loss -0.8701\n",
      "2025-12-17 00:04:24.197078: Pseudo dice [0.928, 0.9552, 0.933]\n",
      "2025-12-17 00:04:24.201082: Epoch time: 137.99 s\n",
      "2025-12-17 00:04:24.829745: \n",
      "2025-12-17 00:04:24.829745: Epoch 549\n",
      "2025-12-17 00:04:24.829745: Current learning rate: 0.00488\n",
      "2025-12-17 00:06:42.741640: train_loss -0.8462\n",
      "2025-12-17 00:06:42.743642: val_loss -0.8781\n",
      "2025-12-17 00:06:42.749388: Pseudo dice [0.9297, 0.9567, 0.9411]\n",
      "2025-12-17 00:06:42.753393: Epoch time: 137.91 s\n",
      "2025-12-17 00:06:43.749096: \n",
      "2025-12-17 00:06:43.749096: Epoch 550\n",
      "2025-12-17 00:06:43.749096: Current learning rate: 0.00487\n",
      "2025-12-17 00:09:01.806283: train_loss -0.8483\n",
      "2025-12-17 00:09:01.808285: val_loss -0.86\n",
      "2025-12-17 00:09:01.810287: Pseudo dice [0.9197, 0.9481, 0.9312]\n",
      "2025-12-17 00:09:01.816379: Epoch time: 138.06 s\n",
      "2025-12-17 00:09:02.619255: \n",
      "2025-12-17 00:09:02.619255: Epoch 551\n",
      "2025-12-17 00:09:02.619255: Current learning rate: 0.00486\n",
      "2025-12-17 00:11:21.025829: train_loss -0.8465\n",
      "2025-12-17 00:11:21.027832: val_loss -0.8677\n",
      "2025-12-17 00:11:21.031837: Pseudo dice [0.9208, 0.9565, 0.9337]\n",
      "2025-12-17 00:11:21.035841: Epoch time: 138.41 s\n",
      "2025-12-17 00:11:21.655698: \n",
      "2025-12-17 00:11:21.671614: Epoch 552\n",
      "2025-12-17 00:11:21.675549: Current learning rate: 0.00485\n",
      "2025-12-17 00:13:39.767436: train_loss -0.849\n",
      "2025-12-17 00:13:39.767436: val_loss -0.8663\n",
      "2025-12-17 00:13:39.775202: Pseudo dice [0.9256, 0.9526, 0.9325]\n",
      "2025-12-17 00:13:39.777204: Epoch time: 138.11 s\n",
      "2025-12-17 00:13:40.573645: \n",
      "2025-12-17 00:13:40.573645: Epoch 553\n",
      "2025-12-17 00:13:40.573645: Current learning rate: 0.00484\n",
      "2025-12-17 00:15:58.535255: train_loss -0.847\n",
      "2025-12-17 00:15:58.537257: val_loss -0.8715\n",
      "2025-12-17 00:15:58.537257: Pseudo dice [0.9243, 0.9583, 0.9418]\n",
      "2025-12-17 00:15:58.537257: Epoch time: 137.97 s\n",
      "2025-12-17 00:15:59.171130: \n",
      "2025-12-17 00:15:59.171130: Epoch 554\n",
      "2025-12-17 00:15:59.175551: Current learning rate: 0.00484\n",
      "2025-12-17 00:18:17.124830: train_loss -0.8493\n",
      "2025-12-17 00:18:17.124830: val_loss -0.8716\n",
      "2025-12-17 00:18:17.129837: Pseudo dice [0.9311, 0.9548, 0.9334]\n",
      "2025-12-17 00:18:17.133666: Epoch time: 137.95 s\n",
      "2025-12-17 00:18:17.810056: \n",
      "2025-12-17 00:18:17.810056: Epoch 555\n",
      "2025-12-17 00:18:17.816064: Current learning rate: 0.00483\n",
      "2025-12-17 00:20:35.773886: train_loss -0.8504\n",
      "2025-12-17 00:20:35.775888: val_loss -0.8795\n",
      "2025-12-17 00:20:35.779894: Pseudo dice [0.9274, 0.9587, 0.9419]\n",
      "2025-12-17 00:20:35.781896: Epoch time: 137.97 s\n",
      "2025-12-17 00:20:35.781896: Yayy! New best EMA pseudo Dice: 0.9378\n",
      "2025-12-17 00:20:36.782190: \n",
      "2025-12-17 00:20:36.782190: Epoch 556\n",
      "2025-12-17 00:20:36.792039: Current learning rate: 0.00482\n",
      "2025-12-17 00:22:54.666601: train_loss -0.8492\n",
      "2025-12-17 00:22:54.670139: val_loss -0.8688\n",
      "2025-12-17 00:22:54.674994: Pseudo dice [0.9226, 0.9572, 0.9292]\n",
      "2025-12-17 00:22:54.676996: Epoch time: 137.88 s\n",
      "2025-12-17 00:22:55.317425: \n",
      "2025-12-17 00:22:55.317425: Epoch 557\n",
      "2025-12-17 00:22:55.317425: Current learning rate: 0.00481\n",
      "2025-12-17 00:25:13.298236: train_loss -0.8488\n",
      "2025-12-17 00:25:13.298236: val_loss -0.8666\n",
      "2025-12-17 00:25:13.314021: Pseudo dice [0.9184, 0.951, 0.9406]\n",
      "2025-12-17 00:25:13.314021: Epoch time: 137.98 s\n",
      "2025-12-17 00:25:14.107169: \n",
      "2025-12-17 00:25:14.107169: Epoch 558\n",
      "2025-12-17 00:25:14.107169: Current learning rate: 0.0048\n",
      "2025-12-17 00:27:32.054046: train_loss -0.8497\n",
      "2025-12-17 00:27:32.056049: val_loss -0.8675\n",
      "2025-12-17 00:27:32.060055: Pseudo dice [0.9217, 0.9514, 0.9376]\n",
      "2025-12-17 00:27:32.061796: Epoch time: 137.95 s\n",
      "2025-12-17 00:27:32.806911: \n",
      "2025-12-17 00:27:32.806911: Epoch 559\n",
      "2025-12-17 00:27:32.813166: Current learning rate: 0.00479\n",
      "2025-12-17 00:29:50.729719: train_loss -0.8539\n",
      "2025-12-17 00:29:50.729719: val_loss -0.8624\n",
      "2025-12-17 00:29:50.735465: Pseudo dice [0.9216, 0.952, 0.9315]\n",
      "2025-12-17 00:29:50.741471: Epoch time: 137.92 s\n",
      "2025-12-17 00:29:51.402236: \n",
      "2025-12-17 00:29:51.402236: Epoch 560\n",
      "2025-12-17 00:29:51.402236: Current learning rate: 0.00478\n",
      "2025-12-17 00:32:09.380013: train_loss -0.85\n",
      "2025-12-17 00:32:09.395811: val_loss -0.8786\n",
      "2025-12-17 00:32:09.399817: Pseudo dice [0.9318, 0.9574, 0.9349]\n",
      "2025-12-17 00:32:09.402122: Epoch time: 137.98 s\n",
      "2025-12-17 00:32:10.029626: \n",
      "2025-12-17 00:32:10.029626: Epoch 561\n",
      "2025-12-17 00:32:10.029626: Current learning rate: 0.00477\n",
      "2025-12-17 00:34:27.945403: train_loss -0.8491\n",
      "2025-12-17 00:34:27.945403: val_loss -0.8595\n",
      "2025-12-17 00:34:27.961118: Pseudo dice [0.9164, 0.9497, 0.937]\n",
      "2025-12-17 00:34:27.966423: Epoch time: 137.92 s\n",
      "2025-12-17 00:34:28.594382: \n",
      "2025-12-17 00:34:28.594382: Epoch 562\n",
      "2025-12-17 00:34:28.609781: Current learning rate: 0.00476\n",
      "2025-12-17 00:36:46.603366: train_loss -0.8528\n",
      "2025-12-17 00:36:46.603366: val_loss -0.8775\n",
      "2025-12-17 00:36:46.603366: Pseudo dice [0.9275, 0.9591, 0.9394]\n",
      "2025-12-17 00:36:46.619173: Epoch time: 138.01 s\n",
      "2025-12-17 00:36:46.622838: Yayy! New best EMA pseudo Dice: 0.9378\n",
      "2025-12-17 00:36:47.476973: \n",
      "2025-12-17 00:36:47.476973: Epoch 563\n",
      "2025-12-17 00:36:47.496258: Current learning rate: 0.00475\n",
      "2025-12-17 00:39:05.644304: train_loss -0.8532\n",
      "2025-12-17 00:39:05.644304: val_loss -0.8732\n",
      "2025-12-17 00:39:05.658163: Pseudo dice [0.9243, 0.9573, 0.9325]\n",
      "2025-12-17 00:39:05.660166: Epoch time: 138.17 s\n",
      "2025-12-17 00:39:05.660166: Yayy! New best EMA pseudo Dice: 0.9378\n",
      "2025-12-17 00:39:06.721900: \n",
      "2025-12-17 00:39:06.721900: Epoch 564\n",
      "2025-12-17 00:39:06.730046: Current learning rate: 0.00474\n",
      "2025-12-17 00:41:24.626337: train_loss -0.848\n",
      "2025-12-17 00:41:24.626337: val_loss -0.877\n",
      "2025-12-17 00:41:24.634086: Pseudo dice [0.9322, 0.963, 0.9244]\n",
      "2025-12-17 00:41:24.638093: Epoch time: 137.9 s\n",
      "2025-12-17 00:41:24.640096: Yayy! New best EMA pseudo Dice: 0.938\n",
      "2025-12-17 00:41:25.553245: \n",
      "2025-12-17 00:41:25.553245: Epoch 565\n",
      "2025-12-17 00:41:25.553245: Current learning rate: 0.00473\n",
      "2025-12-17 00:43:43.559346: train_loss -0.8518\n",
      "2025-12-17 00:43:43.561348: val_loss -0.8561\n",
      "2025-12-17 00:43:43.565888: Pseudo dice [0.9117, 0.9501, 0.9321]\n",
      "2025-12-17 00:43:43.571300: Epoch time: 138.01 s\n",
      "2025-12-17 00:43:44.211757: \n",
      "2025-12-17 00:43:44.211757: Epoch 566\n",
      "2025-12-17 00:43:44.211757: Current learning rate: 0.00472\n",
      "2025-12-17 00:46:02.195060: train_loss -0.8464\n",
      "2025-12-17 00:46:02.197062: val_loss -0.8855\n",
      "2025-12-17 00:46:02.203068: Pseudo dice [0.9316, 0.9583, 0.9458]\n",
      "2025-12-17 00:46:02.208818: Epoch time: 137.98 s\n",
      "2025-12-17 00:46:02.214835: Yayy! New best EMA pseudo Dice: 0.9381\n",
      "2025-12-17 00:46:03.167927: \n",
      "2025-12-17 00:46:03.167927: Epoch 567\n",
      "2025-12-17 00:46:03.171669: Current learning rate: 0.00471\n",
      "2025-12-17 00:48:21.043832: train_loss -0.8486\n",
      "2025-12-17 00:48:21.043832: val_loss -0.8638\n",
      "2025-12-17 00:48:21.050206: Pseudo dice [0.922, 0.9548, 0.9235]\n",
      "2025-12-17 00:48:21.050206: Epoch time: 137.88 s\n",
      "2025-12-17 00:48:21.667648: \n",
      "2025-12-17 00:48:21.667648: Epoch 568\n",
      "2025-12-17 00:48:21.683552: Current learning rate: 0.0047\n",
      "2025-12-17 00:50:39.798221: train_loss -0.8481\n",
      "2025-12-17 00:50:39.798221: val_loss -0.8704\n",
      "2025-12-17 00:50:39.806647: Pseudo dice [0.9271, 0.956, 0.9305]\n",
      "2025-12-17 00:50:39.810935: Epoch time: 138.13 s\n",
      "2025-12-17 00:50:40.432894: \n",
      "2025-12-17 00:50:40.432894: Epoch 569\n",
      "2025-12-17 00:50:40.448699: Current learning rate: 0.00469\n",
      "2025-12-17 00:52:58.401642: train_loss -0.8505\n",
      "2025-12-17 00:52:58.401642: val_loss -0.8711\n",
      "2025-12-17 00:52:58.401642: Pseudo dice [0.9268, 0.9584, 0.9293]\n",
      "2025-12-17 00:52:58.401642: Epoch time: 137.97 s\n",
      "2025-12-17 00:52:59.241240: \n",
      "2025-12-17 00:52:59.241240: Epoch 570\n",
      "2025-12-17 00:52:59.241240: Current learning rate: 0.00468\n",
      "2025-12-17 00:55:17.226207: train_loss -0.8485\n",
      "2025-12-17 00:55:17.226207: val_loss -0.8724\n",
      "2025-12-17 00:55:17.242176: Pseudo dice [0.9302, 0.9594, 0.9308]\n",
      "2025-12-17 00:55:17.248994: Epoch time: 138.0 s\n",
      "2025-12-17 00:55:17.876584: \n",
      "2025-12-17 00:55:17.876584: Epoch 571\n",
      "2025-12-17 00:55:17.876584: Current learning rate: 0.00467\n",
      "2025-12-17 00:57:35.924664: train_loss -0.8502\n",
      "2025-12-17 00:57:35.926667: val_loss -0.8674\n",
      "2025-12-17 00:57:35.931734: Pseudo dice [0.9209, 0.9548, 0.9279]\n",
      "2025-12-17 00:57:35.935738: Epoch time: 138.05 s\n",
      "2025-12-17 00:57:36.562647: \n",
      "2025-12-17 00:57:36.562647: Epoch 572\n",
      "2025-12-17 00:57:36.562647: Current learning rate: 0.00466\n",
      "2025-12-17 00:59:54.464628: train_loss -0.8531\n",
      "2025-12-17 00:59:54.466630: val_loss -0.8616\n",
      "2025-12-17 00:59:54.472375: Pseudo dice [0.92, 0.954, 0.9319]\n",
      "2025-12-17 00:59:54.476379: Epoch time: 137.9 s\n",
      "2025-12-17 00:59:55.104356: \n",
      "2025-12-17 00:59:55.104356: Epoch 573\n",
      "2025-12-17 00:59:55.120436: Current learning rate: 0.00465\n",
      "2025-12-17 01:02:13.100853: train_loss -0.8509\n",
      "2025-12-17 01:02:13.100853: val_loss -0.8667\n",
      "2025-12-17 01:02:13.116902: Pseudo dice [0.9262, 0.9553, 0.9271]\n",
      "2025-12-17 01:02:13.116902: Epoch time: 138.0 s\n",
      "2025-12-17 01:02:13.797668: \n",
      "2025-12-17 01:02:13.797668: Epoch 574\n",
      "2025-12-17 01:02:13.797668: Current learning rate: 0.00464\n",
      "2025-12-17 01:04:31.809324: train_loss -0.8526\n",
      "2025-12-17 01:04:31.809324: val_loss -0.8781\n",
      "2025-12-17 01:04:31.821999: Pseudo dice [0.9298, 0.9601, 0.9363]\n",
      "2025-12-17 01:04:31.825191: Epoch time: 138.01 s\n",
      "2025-12-17 01:04:32.459983: \n",
      "2025-12-17 01:04:32.459983: Epoch 575\n",
      "2025-12-17 01:04:32.478126: Current learning rate: 0.00463\n",
      "2025-12-17 01:06:50.475384: train_loss -0.8544\n",
      "2025-12-17 01:06:50.475384: val_loss -0.8659\n",
      "2025-12-17 01:06:50.481390: Pseudo dice [0.9202, 0.9495, 0.9382]\n",
      "2025-12-17 01:06:50.485395: Epoch time: 138.02 s\n",
      "2025-12-17 01:06:51.378602: \n",
      "2025-12-17 01:06:51.378602: Epoch 576\n",
      "2025-12-17 01:06:51.378602: Current learning rate: 0.00462\n",
      "2025-12-17 01:09:09.729863: train_loss -0.8517\n",
      "2025-12-17 01:09:09.729863: val_loss -0.8781\n",
      "2025-12-17 01:09:09.737422: Pseudo dice [0.9313, 0.9563, 0.932]\n",
      "2025-12-17 01:09:09.737422: Epoch time: 138.35 s\n",
      "2025-12-17 01:09:10.396080: \n",
      "2025-12-17 01:09:10.396080: Epoch 577\n",
      "2025-12-17 01:09:10.396080: Current learning rate: 0.00461\n",
      "2025-12-17 01:11:28.769387: train_loss -0.8472\n",
      "2025-12-17 01:11:28.769387: val_loss -0.8693\n",
      "2025-12-17 01:11:28.774375: Pseudo dice [0.9263, 0.9558, 0.9345]\n",
      "2025-12-17 01:11:28.777063: Epoch time: 138.37 s\n",
      "2025-12-17 01:11:29.404816: \n",
      "2025-12-17 01:11:29.404816: Epoch 578\n",
      "2025-12-17 01:11:29.415082: Current learning rate: 0.0046\n",
      "2025-12-17 01:13:47.522790: train_loss -0.8482\n",
      "2025-12-17 01:13:47.522790: val_loss -0.8695\n",
      "2025-12-17 01:13:47.530816: Pseudo dice [0.9228, 0.952, 0.9323]\n",
      "2025-12-17 01:13:47.534820: Epoch time: 138.12 s\n",
      "2025-12-17 01:13:48.303066: \n",
      "2025-12-17 01:13:48.303066: Epoch 579\n",
      "2025-12-17 01:13:48.303066: Current learning rate: 0.00459\n",
      "2025-12-17 01:16:06.265033: train_loss -0.8529\n",
      "2025-12-17 01:16:06.265033: val_loss -0.8601\n",
      "2025-12-17 01:16:06.266973: Pseudo dice [0.9249, 0.9517, 0.9251]\n",
      "2025-12-17 01:16:06.266973: Epoch time: 137.96 s\n",
      "2025-12-17 01:16:06.898795: \n",
      "2025-12-17 01:16:06.898795: Epoch 580\n",
      "2025-12-17 01:16:06.914565: Current learning rate: 0.00458\n",
      "2025-12-17 01:18:24.823357: train_loss -0.8498\n",
      "2025-12-17 01:18:24.825360: val_loss -0.8648\n",
      "2025-12-17 01:18:24.831371: Pseudo dice [0.9236, 0.9541, 0.9279]\n",
      "2025-12-17 01:18:24.837379: Epoch time: 137.92 s\n",
      "2025-12-17 01:18:25.472030: \n",
      "2025-12-17 01:18:25.472030: Epoch 581\n",
      "2025-12-17 01:18:25.480968: Current learning rate: 0.00457\n",
      "2025-12-17 01:20:43.455305: train_loss -0.8495\n",
      "2025-12-17 01:20:43.455305: val_loss -0.8582\n",
      "2025-12-17 01:20:43.471413: Pseudo dice [0.9179, 0.9515, 0.9315]\n",
      "2025-12-17 01:20:43.471413: Epoch time: 137.98 s\n",
      "2025-12-17 01:20:44.452224: \n",
      "2025-12-17 01:20:44.452224: Epoch 582\n",
      "2025-12-17 01:20:44.456896: Current learning rate: 0.00456\n",
      "2025-12-17 01:23:02.401942: train_loss -0.8512\n",
      "2025-12-17 01:23:02.401942: val_loss -0.8802\n",
      "2025-12-17 01:23:02.404927: Pseudo dice [0.9299, 0.9609, 0.942]\n",
      "2025-12-17 01:23:02.411781: Epoch time: 137.97 s\n",
      "2025-12-17 01:23:03.102813: \n",
      "2025-12-17 01:23:03.102813: Epoch 583\n",
      "2025-12-17 01:23:03.115786: Current learning rate: 0.00455\n",
      "2025-12-17 01:25:21.094128: train_loss -0.848\n",
      "2025-12-17 01:25:21.094128: val_loss -0.8716\n",
      "2025-12-17 01:25:21.110088: Pseudo dice [0.926, 0.9572, 0.9324]\n",
      "2025-12-17 01:25:21.110088: Epoch time: 137.99 s\n",
      "2025-12-17 01:25:21.744437: \n",
      "2025-12-17 01:25:21.744437: Epoch 584\n",
      "2025-12-17 01:25:21.744437: Current learning rate: 0.00454\n",
      "2025-12-17 01:27:39.720405: train_loss -0.8519\n",
      "2025-12-17 01:27:39.722406: val_loss -0.8619\n",
      "2025-12-17 01:27:39.726151: Pseudo dice [0.9218, 0.956, 0.9309]\n",
      "2025-12-17 01:27:39.730870: Epoch time: 137.98 s\n",
      "2025-12-17 01:27:40.497090: \n",
      "2025-12-17 01:27:40.499094: Epoch 585\n",
      "2025-12-17 01:27:40.502840: Current learning rate: 0.00453\n",
      "2025-12-17 01:29:58.543181: train_loss -0.8504\n",
      "2025-12-17 01:29:58.543181: val_loss -0.8743\n",
      "2025-12-17 01:29:58.545183: Pseudo dice [0.9228, 0.9558, 0.9416]\n",
      "2025-12-17 01:29:58.545183: Epoch time: 138.05 s\n",
      "2025-12-17 01:29:59.189007: \n",
      "2025-12-17 01:29:59.189007: Epoch 586\n",
      "2025-12-17 01:29:59.189007: Current learning rate: 0.00452\n",
      "2025-12-17 01:32:17.399182: train_loss -0.8515\n",
      "2025-12-17 01:32:17.399182: val_loss -0.8719\n",
      "2025-12-17 01:32:17.404929: Pseudo dice [0.9245, 0.9553, 0.933]\n",
      "2025-12-17 01:32:17.408933: Epoch time: 138.21 s\n",
      "2025-12-17 01:32:18.052118: \n",
      "2025-12-17 01:32:18.052118: Epoch 587\n",
      "2025-12-17 01:32:18.052118: Current learning rate: 0.00451\n",
      "2025-12-17 01:34:35.935807: train_loss -0.8494\n",
      "2025-12-17 01:34:35.935807: val_loss -0.8579\n",
      "2025-12-17 01:34:35.939552: Pseudo dice [0.9123, 0.9505, 0.9342]\n",
      "2025-12-17 01:34:35.939552: Epoch time: 137.88 s\n",
      "2025-12-17 01:34:36.890167: \n",
      "2025-12-17 01:34:36.890167: Epoch 588\n",
      "2025-12-17 01:34:36.890167: Current learning rate: 0.0045\n",
      "2025-12-17 01:36:54.730054: train_loss -0.8565\n",
      "2025-12-17 01:36:54.730054: val_loss -0.8757\n",
      "2025-12-17 01:36:54.745776: Pseudo dice [0.928, 0.9561, 0.9347]\n",
      "2025-12-17 01:36:54.745776: Epoch time: 137.84 s\n",
      "2025-12-17 01:36:55.383013: \n",
      "2025-12-17 01:36:55.383013: Epoch 589\n",
      "2025-12-17 01:36:55.398796: Current learning rate: 0.00449\n",
      "2025-12-17 01:39:13.364539: train_loss -0.8494\n",
      "2025-12-17 01:39:13.364539: val_loss -0.8839\n",
      "2025-12-17 01:39:13.364539: Pseudo dice [0.9327, 0.9571, 0.937]\n",
      "2025-12-17 01:39:13.380343: Epoch time: 137.98 s\n",
      "2025-12-17 01:39:14.015530: \n",
      "2025-12-17 01:39:14.015530: Epoch 590\n",
      "2025-12-17 01:39:14.015530: Current learning rate: 0.00448\n",
      "2025-12-17 01:41:32.137440: train_loss -0.8485\n",
      "2025-12-17 01:41:32.137440: val_loss -0.8752\n",
      "2025-12-17 01:41:32.141445: Pseudo dice [0.9238, 0.9582, 0.9365]\n",
      "2025-12-17 01:41:32.147190: Epoch time: 138.12 s\n",
      "2025-12-17 01:41:32.938691: \n",
      "2025-12-17 01:41:32.938691: Epoch 591\n",
      "2025-12-17 01:41:32.954578: Current learning rate: 0.00447\n",
      "2025-12-17 01:43:51.193350: train_loss -0.8437\n",
      "2025-12-17 01:43:51.195352: val_loss -0.8739\n",
      "2025-12-17 01:43:51.198466: Pseudo dice [0.9301, 0.958, 0.9337]\n",
      "2025-12-17 01:43:51.198466: Epoch time: 138.25 s\n",
      "2025-12-17 01:43:51.198466: Yayy! New best EMA pseudo Dice: 0.9383\n",
      "2025-12-17 01:43:52.090947: \n",
      "2025-12-17 01:43:52.090947: Epoch 592\n",
      "2025-12-17 01:43:52.090947: Current learning rate: 0.00446\n",
      "2025-12-17 01:46:10.076378: train_loss -0.8488\n",
      "2025-12-17 01:46:10.076378: val_loss -0.856\n",
      "2025-12-17 01:46:10.076378: Pseudo dice [0.9155, 0.9489, 0.9361]\n",
      "2025-12-17 01:46:10.076378: Epoch time: 137.99 s\n",
      "2025-12-17 01:46:10.710753: \n",
      "2025-12-17 01:46:10.710753: Epoch 593\n",
      "2025-12-17 01:46:10.726586: Current learning rate: 0.00445\n",
      "2025-12-17 01:48:28.593069: train_loss -0.8515\n",
      "2025-12-17 01:48:28.593069: val_loss -0.8643\n",
      "2025-12-17 01:48:28.599077: Pseudo dice [0.9256, 0.9507, 0.9247]\n",
      "2025-12-17 01:48:28.606912: Epoch time: 137.88 s\n",
      "2025-12-17 01:48:29.418196: \n",
      "2025-12-17 01:48:29.418196: Epoch 594\n",
      "2025-12-17 01:48:29.425089: Current learning rate: 0.00444\n",
      "2025-12-17 01:50:47.167241: train_loss -0.8438\n",
      "2025-12-17 01:50:47.169243: val_loss -0.8713\n",
      "2025-12-17 01:50:47.173247: Pseudo dice [0.9246, 0.9566, 0.9359]\n",
      "2025-12-17 01:50:47.177253: Epoch time: 137.75 s\n",
      "2025-12-17 01:50:47.820850: \n",
      "2025-12-17 01:50:47.820850: Epoch 595\n",
      "2025-12-17 01:50:47.820850: Current learning rate: 0.00443\n",
      "2025-12-17 01:53:05.820215: train_loss -0.8495\n",
      "2025-12-17 01:53:05.820215: val_loss -0.876\n",
      "2025-12-17 01:53:05.820215: Pseudo dice [0.9291, 0.9568, 0.937]\n",
      "2025-12-17 01:53:05.820215: Epoch time: 138.02 s\n",
      "2025-12-17 01:53:06.470138: \n",
      "2025-12-17 01:53:06.470138: Epoch 596\n",
      "2025-12-17 01:53:06.470138: Current learning rate: 0.00442\n",
      "2025-12-17 01:55:24.541516: train_loss -0.8507\n",
      "2025-12-17 01:55:24.541516: val_loss -0.8703\n",
      "2025-12-17 01:55:24.546350: Pseudo dice [0.9219, 0.9557, 0.9379]\n",
      "2025-12-17 01:55:24.549066: Epoch time: 138.07 s\n",
      "2025-12-17 01:55:25.175804: \n",
      "2025-12-17 01:55:25.175804: Epoch 597\n",
      "2025-12-17 01:55:25.189121: Current learning rate: 0.00441\n",
      "2025-12-17 01:57:43.203102: train_loss -0.8459\n",
      "2025-12-17 01:57:43.203102: val_loss -0.8848\n",
      "2025-12-17 01:57:43.217697: Pseudo dice [0.9331, 0.9649, 0.9385]\n",
      "2025-12-17 01:57:43.221202: Epoch time: 138.03 s\n",
      "2025-12-17 01:57:43.226177: Yayy! New best EMA pseudo Dice: 0.9387\n",
      "2025-12-17 01:57:44.105969: \n",
      "2025-12-17 01:57:44.105969: Epoch 598\n",
      "2025-12-17 01:57:44.121727: Current learning rate: 0.0044\n",
      "2025-12-17 02:00:02.023286: train_loss -0.8519\n",
      "2025-12-17 02:00:02.023286: val_loss -0.877\n",
      "2025-12-17 02:00:02.038980: Pseudo dice [0.9292, 0.9608, 0.9337]\n",
      "2025-12-17 02:00:02.044482: Epoch time: 137.92 s\n",
      "2025-12-17 02:00:02.048487: Yayy! New best EMA pseudo Dice: 0.939\n",
      "2025-12-17 02:00:03.017873: \n",
      "2025-12-17 02:00:03.017873: Epoch 599\n",
      "2025-12-17 02:00:03.017873: Current learning rate: 0.00439\n",
      "2025-12-17 02:02:20.985895: train_loss -0.8531\n",
      "2025-12-17 02:02:20.987897: val_loss -0.8568\n",
      "2025-12-17 02:02:20.989900: Pseudo dice [0.9174, 0.952, 0.9264]\n",
      "2025-12-17 02:02:20.995397: Epoch time: 137.97 s\n",
      "2025-12-17 02:02:22.050371: \n",
      "2025-12-17 02:02:22.050371: Epoch 600\n",
      "2025-12-17 02:02:22.050371: Current learning rate: 0.00438\n",
      "2025-12-17 02:04:40.097097: train_loss -0.8537\n",
      "2025-12-17 02:04:40.099099: val_loss -0.8524\n",
      "2025-12-17 02:04:40.103103: Pseudo dice [0.9165, 0.947, 0.9325]\n",
      "2025-12-17 02:04:40.108525: Epoch time: 138.05 s\n",
      "2025-12-17 02:04:40.745076: \n",
      "2025-12-17 02:04:40.745076: Epoch 601\n",
      "2025-12-17 02:04:40.761199: Current learning rate: 0.00437\n",
      "2025-12-17 02:06:58.910087: train_loss -0.851\n",
      "2025-12-17 02:06:58.910087: val_loss -0.8625\n",
      "2025-12-17 02:06:58.918096: Pseudo dice [0.9173, 0.9535, 0.9391]\n",
      "2025-12-17 02:06:58.922100: Epoch time: 138.17 s\n",
      "2025-12-17 02:06:59.563745: \n",
      "2025-12-17 02:06:59.563745: Epoch 602\n",
      "2025-12-17 02:06:59.563745: Current learning rate: 0.00436\n",
      "2025-12-17 02:09:17.706974: train_loss -0.8537\n",
      "2025-12-17 02:09:17.706974: val_loss -0.8662\n",
      "2025-12-17 02:09:17.710978: Pseudo dice [0.9177, 0.952, 0.9399]\n",
      "2025-12-17 02:09:17.714983: Epoch time: 138.15 s\n",
      "2025-12-17 02:09:18.366422: \n",
      "2025-12-17 02:09:18.366422: Epoch 603\n",
      "2025-12-17 02:09:18.366422: Current learning rate: 0.00435\n",
      "2025-12-17 02:11:36.531259: train_loss -0.8536\n",
      "2025-12-17 02:11:36.533261: val_loss -0.8709\n",
      "2025-12-17 02:11:36.539005: Pseudo dice [0.9241, 0.9572, 0.9283]\n",
      "2025-12-17 02:11:36.547015: Epoch time: 138.17 s\n",
      "2025-12-17 02:11:37.188267: \n",
      "2025-12-17 02:11:37.188267: Epoch 604\n",
      "2025-12-17 02:11:37.188267: Current learning rate: 0.00434\n",
      "2025-12-17 02:13:55.190045: train_loss -0.8553\n",
      "2025-12-17 02:13:55.190045: val_loss -0.8695\n",
      "2025-12-17 02:13:55.190045: Pseudo dice [0.9231, 0.9587, 0.9353]\n",
      "2025-12-17 02:13:55.190045: Epoch time: 138.0 s\n",
      "2025-12-17 02:13:55.862578: \n",
      "2025-12-17 02:13:55.862578: Epoch 605\n",
      "2025-12-17 02:13:55.862578: Current learning rate: 0.00433\n",
      "2025-12-17 02:16:14.118293: train_loss -0.8542\n",
      "2025-12-17 02:16:14.118293: val_loss -0.8723\n",
      "2025-12-17 02:16:14.122298: Pseudo dice [0.9229, 0.9534, 0.9363]\n",
      "2025-12-17 02:16:14.124038: Epoch time: 138.26 s\n",
      "2025-12-17 02:16:14.759933: \n",
      "2025-12-17 02:16:14.759933: Epoch 606\n",
      "2025-12-17 02:16:14.775939: Current learning rate: 0.00432\n",
      "2025-12-17 02:18:32.672380: train_loss -0.8509\n",
      "2025-12-17 02:18:32.674381: val_loss -0.8717\n",
      "2025-12-17 02:18:32.678385: Pseudo dice [0.9244, 0.9554, 0.9363]\n",
      "2025-12-17 02:18:32.682128: Epoch time: 137.91 s\n",
      "2025-12-17 02:18:33.316503: \n",
      "2025-12-17 02:18:33.316503: Epoch 607\n",
      "2025-12-17 02:18:33.316503: Current learning rate: 0.00431\n",
      "2025-12-17 02:20:51.299778: train_loss -0.8529\n",
      "2025-12-17 02:20:51.299778: val_loss -0.8687\n",
      "2025-12-17 02:20:51.302392: Pseudo dice [0.9221, 0.9562, 0.9313]\n",
      "2025-12-17 02:20:51.302392: Epoch time: 137.98 s\n",
      "2025-12-17 02:20:51.933538: \n",
      "2025-12-17 02:20:51.933538: Epoch 608\n",
      "2025-12-17 02:20:51.933538: Current learning rate: 0.0043\n",
      "2025-12-17 02:23:09.996652: train_loss -0.8547\n",
      "2025-12-17 02:23:09.998654: val_loss -0.8766\n",
      "2025-12-17 02:23:10.004161: Pseudo dice [0.9248, 0.9557, 0.9413]\n",
      "2025-12-17 02:23:10.010166: Epoch time: 138.06 s\n",
      "2025-12-17 02:23:10.645068: \n",
      "2025-12-17 02:23:10.645068: Epoch 609\n",
      "2025-12-17 02:23:10.660824: Current learning rate: 0.00429\n",
      "2025-12-17 02:25:28.606348: train_loss -0.851\n",
      "2025-12-17 02:25:28.609604: val_loss -0.8633\n",
      "2025-12-17 02:25:28.613488: Pseudo dice [0.918, 0.95, 0.9301]\n",
      "2025-12-17 02:25:28.619493: Epoch time: 137.96 s\n",
      "2025-12-17 02:25:29.264815: \n",
      "2025-12-17 02:25:29.264815: Epoch 610\n",
      "2025-12-17 02:25:29.264815: Current learning rate: 0.00429\n",
      "2025-12-17 02:27:47.267899: train_loss -0.8506\n",
      "2025-12-17 02:27:47.267899: val_loss -0.8513\n",
      "2025-12-17 02:27:47.267899: Pseudo dice [0.9108, 0.9532, 0.9339]\n",
      "2025-12-17 02:27:47.283850: Epoch time: 138.01 s\n",
      "2025-12-17 02:27:47.924708: \n",
      "2025-12-17 02:27:47.924708: Epoch 611\n",
      "2025-12-17 02:27:47.928517: Current learning rate: 0.00428\n",
      "2025-12-17 02:30:05.734025: train_loss -0.8362\n",
      "2025-12-17 02:30:05.734025: val_loss -0.8547\n",
      "2025-12-17 02:30:05.750007: Pseudo dice [0.9194, 0.9506, 0.9192]\n",
      "2025-12-17 02:30:05.755512: Epoch time: 137.81 s\n",
      "2025-12-17 02:30:06.565152: \n",
      "2025-12-17 02:30:06.565152: Epoch 612\n",
      "2025-12-17 02:30:06.571137: Current learning rate: 0.00427\n",
      "2025-12-17 02:32:24.536493: train_loss -0.8329\n",
      "2025-12-17 02:32:24.538495: val_loss -0.8588\n",
      "2025-12-17 02:32:24.544501: Pseudo dice [0.9232, 0.9531, 0.9302]\n",
      "2025-12-17 02:32:24.550508: Epoch time: 137.97 s\n",
      "2025-12-17 02:32:25.197386: \n",
      "2025-12-17 02:32:25.197386: Epoch 613\n",
      "2025-12-17 02:32:25.197386: Current learning rate: 0.00426\n",
      "2025-12-17 02:34:43.079106: train_loss -0.8472\n",
      "2025-12-17 02:34:43.079106: val_loss -0.8751\n",
      "2025-12-17 02:34:43.083111: Pseudo dice [0.9283, 0.9598, 0.9294]\n",
      "2025-12-17 02:34:43.087115: Epoch time: 137.9 s\n",
      "2025-12-17 02:34:43.839330: \n",
      "2025-12-17 02:34:43.839330: Epoch 614\n",
      "2025-12-17 02:34:43.855251: Current learning rate: 0.00425\n",
      "2025-12-17 02:37:01.801078: train_loss -0.8421\n",
      "2025-12-17 02:37:01.801078: val_loss -0.8588\n",
      "2025-12-17 02:37:01.801078: Pseudo dice [0.9169, 0.954, 0.9267]\n",
      "2025-12-17 02:37:01.801078: Epoch time: 137.96 s\n",
      "2025-12-17 02:37:02.437081: \n",
      "2025-12-17 02:37:02.452997: Epoch 615\n",
      "2025-12-17 02:37:02.452997: Current learning rate: 0.00424\n",
      "2025-12-17 02:39:20.410561: train_loss -0.8516\n",
      "2025-12-17 02:39:20.410561: val_loss -0.8697\n",
      "2025-12-17 02:39:20.418309: Pseudo dice [0.9231, 0.9533, 0.9315]\n",
      "2025-12-17 02:39:20.424317: Epoch time: 137.97 s\n",
      "2025-12-17 02:39:21.080973: \n",
      "2025-12-17 02:39:21.080973: Epoch 616\n",
      "2025-12-17 02:39:21.085032: Current learning rate: 0.00423\n",
      "2025-12-17 02:41:39.008240: train_loss -0.8516\n",
      "2025-12-17 02:41:39.008240: val_loss -0.8737\n",
      "2025-12-17 02:41:39.013744: Pseudo dice [0.9272, 0.9556, 0.9301]\n",
      "2025-12-17 02:41:39.017748: Epoch time: 137.93 s\n",
      "2025-12-17 02:41:39.779552: \n",
      "2025-12-17 02:41:39.795216: Epoch 617\n",
      "2025-12-17 02:41:39.795216: Current learning rate: 0.00422\n",
      "2025-12-17 02:43:57.815466: train_loss -0.853\n",
      "2025-12-17 02:43:57.815466: val_loss -0.8696\n",
      "2025-12-17 02:43:57.815466: Pseudo dice [0.9207, 0.9522, 0.9413]\n",
      "2025-12-17 02:43:57.823918: Epoch time: 138.04 s\n",
      "2025-12-17 02:43:58.657731: \n",
      "2025-12-17 02:43:58.657731: Epoch 618\n",
      "2025-12-17 02:43:58.673654: Current learning rate: 0.00421\n",
      "2025-12-17 02:46:16.546140: train_loss -0.85\n",
      "2025-12-17 02:46:16.546140: val_loss -0.8784\n",
      "2025-12-17 02:46:16.562124: Pseudo dice [0.9285, 0.9588, 0.9347]\n",
      "2025-12-17 02:46:16.562124: Epoch time: 137.89 s\n",
      "2025-12-17 02:46:17.214615: \n",
      "2025-12-17 02:46:17.214615: Epoch 619\n",
      "2025-12-17 02:46:17.214615: Current learning rate: 0.0042\n",
      "2025-12-17 02:48:35.072469: train_loss -0.8521\n",
      "2025-12-17 02:48:35.074470: val_loss -0.8774\n",
      "2025-12-17 02:48:35.078474: Pseudo dice [0.9305, 0.9581, 0.9353]\n",
      "2025-12-17 02:48:35.082405: Epoch time: 137.86 s\n",
      "2025-12-17 02:48:35.848099: \n",
      "2025-12-17 02:48:35.848099: Epoch 620\n",
      "2025-12-17 02:48:35.863852: Current learning rate: 0.00419\n",
      "2025-12-17 02:50:53.855551: train_loss -0.8547\n",
      "2025-12-17 02:50:53.857553: val_loss -0.873\n",
      "2025-12-17 02:50:53.863560: Pseudo dice [0.9222, 0.9527, 0.9411]\n",
      "2025-12-17 02:50:53.867564: Epoch time: 138.01 s\n",
      "2025-12-17 02:50:54.527857: \n",
      "2025-12-17 02:50:54.527857: Epoch 621\n",
      "2025-12-17 02:50:54.527857: Current learning rate: 0.00418\n",
      "2025-12-17 02:53:12.615321: train_loss -0.8524\n",
      "2025-12-17 02:53:12.615321: val_loss -0.8803\n",
      "2025-12-17 02:53:12.620670: Pseudo dice [0.9334, 0.9624, 0.933]\n",
      "2025-12-17 02:53:12.624674: Epoch time: 138.09 s\n",
      "2025-12-17 02:53:13.327668: \n",
      "2025-12-17 02:53:13.327668: Epoch 622\n",
      "2025-12-17 02:53:13.327668: Current learning rate: 0.00417\n",
      "2025-12-17 02:55:31.358751: train_loss -0.8537\n",
      "2025-12-17 02:55:31.360754: val_loss -0.8608\n",
      "2025-12-17 02:55:31.364366: Pseudo dice [0.9186, 0.949, 0.922]\n",
      "2025-12-17 02:55:31.368370: Epoch time: 138.03 s\n",
      "2025-12-17 02:55:32.134923: \n",
      "2025-12-17 02:55:32.134923: Epoch 623\n",
      "2025-12-17 02:55:32.134923: Current learning rate: 0.00416\n",
      "2025-12-17 02:57:50.079661: train_loss -0.8509\n",
      "2025-12-17 02:57:50.079661: val_loss -0.8663\n",
      "2025-12-17 02:57:50.079661: Pseudo dice [0.9178, 0.9546, 0.9315]\n",
      "2025-12-17 02:57:50.079661: Epoch time: 137.94 s\n",
      "2025-12-17 02:57:50.935086: \n",
      "2025-12-17 02:57:50.935086: Epoch 624\n",
      "2025-12-17 02:57:50.935086: Current learning rate: 0.00415\n",
      "2025-12-17 03:00:08.835399: train_loss -0.8514\n",
      "2025-12-17 03:00:08.837402: val_loss -0.871\n",
      "2025-12-17 03:00:08.841405: Pseudo dice [0.9185, 0.9574, 0.9454]\n",
      "2025-12-17 03:00:08.845409: Epoch time: 137.9 s\n",
      "2025-12-17 03:00:09.484222: \n",
      "2025-12-17 03:00:09.484222: Epoch 625\n",
      "2025-12-17 03:00:09.484222: Current learning rate: 0.00414\n",
      "2025-12-17 03:02:27.579864: train_loss -0.853\n",
      "2025-12-17 03:02:27.579864: val_loss -0.8709\n",
      "2025-12-17 03:02:27.583868: Pseudo dice [0.9255, 0.9551, 0.9378]\n",
      "2025-12-17 03:02:27.587677: Epoch time: 138.1 s\n",
      "2025-12-17 03:02:28.227738: \n",
      "2025-12-17 03:02:28.227738: Epoch 626\n",
      "2025-12-17 03:02:28.243536: Current learning rate: 0.00413\n",
      "2025-12-17 03:04:46.120181: train_loss -0.8523\n",
      "2025-12-17 03:04:46.120181: val_loss -0.8705\n",
      "2025-12-17 03:04:46.120181: Pseudo dice [0.9212, 0.955, 0.9435]\n",
      "2025-12-17 03:04:46.120181: Epoch time: 137.89 s\n",
      "2025-12-17 03:04:46.757991: \n",
      "2025-12-17 03:04:46.759732: Epoch 627\n",
      "2025-12-17 03:04:46.763662: Current learning rate: 0.00412\n",
      "2025-12-17 03:07:04.899273: train_loss -0.8487\n",
      "2025-12-17 03:07:04.899273: val_loss -0.8665\n",
      "2025-12-17 03:07:04.905017: Pseudo dice [0.9244, 0.9514, 0.929]\n",
      "2025-12-17 03:07:04.909022: Epoch time: 138.14 s\n",
      "2025-12-17 03:07:05.558505: \n",
      "2025-12-17 03:07:05.558505: Epoch 628\n",
      "2025-12-17 03:07:05.560509: Current learning rate: 0.00411\n",
      "2025-12-17 03:09:23.811800: train_loss -0.8486\n",
      "2025-12-17 03:09:23.811800: val_loss -0.8783\n",
      "2025-12-17 03:09:23.811800: Pseudo dice [0.9289, 0.9554, 0.9401]\n",
      "2025-12-17 03:09:23.811800: Epoch time: 138.25 s\n",
      "2025-12-17 03:09:24.473881: \n",
      "2025-12-17 03:09:24.473881: Epoch 629\n",
      "2025-12-17 03:09:24.473881: Current learning rate: 0.0041\n",
      "2025-12-17 03:11:42.520255: train_loss -0.8531\n",
      "2025-12-17 03:11:42.520255: val_loss -0.8683\n",
      "2025-12-17 03:11:42.520255: Pseudo dice [0.9233, 0.9565, 0.9332]\n",
      "2025-12-17 03:11:42.520255: Epoch time: 138.05 s\n",
      "2025-12-17 03:11:43.169255: \n",
      "2025-12-17 03:11:43.169255: Epoch 630\n",
      "2025-12-17 03:11:43.171257: Current learning rate: 0.00409\n",
      "2025-12-17 03:14:01.052418: train_loss -0.852\n",
      "2025-12-17 03:14:01.052418: val_loss -0.8652\n",
      "2025-12-17 03:14:01.056160: Pseudo dice [0.9207, 0.9517, 0.9343]\n",
      "2025-12-17 03:14:01.062721: Epoch time: 137.88 s\n",
      "2025-12-17 03:14:01.881317: \n",
      "2025-12-17 03:14:01.881317: Epoch 631\n",
      "2025-12-17 03:14:01.881317: Current learning rate: 0.00408\n",
      "2025-12-17 03:16:19.824701: train_loss -0.8511\n",
      "2025-12-17 03:16:19.824701: val_loss -0.869\n",
      "2025-12-17 03:16:19.830709: Pseudo dice [0.9285, 0.9549, 0.9291]\n",
      "2025-12-17 03:16:19.836291: Epoch time: 137.94 s\n",
      "2025-12-17 03:16:20.463186: \n",
      "2025-12-17 03:16:20.463186: Epoch 632\n",
      "2025-12-17 03:16:20.478975: Current learning rate: 0.00407\n",
      "2025-12-17 03:18:38.313350: train_loss -0.8571\n",
      "2025-12-17 03:18:38.313350: val_loss -0.8728\n",
      "2025-12-17 03:18:38.319356: Pseudo dice [0.9258, 0.9558, 0.935]\n",
      "2025-12-17 03:18:38.323359: Epoch time: 137.85 s\n",
      "2025-12-17 03:18:38.953073: \n",
      "2025-12-17 03:18:38.953073: Epoch 633\n",
      "2025-12-17 03:18:38.968830: Current learning rate: 0.00406\n",
      "2025-12-17 03:20:57.095095: train_loss -0.8497\n",
      "2025-12-17 03:20:57.095095: val_loss -0.8729\n",
      "2025-12-17 03:20:57.111147: Pseudo dice [0.9265, 0.9581, 0.9391]\n",
      "2025-12-17 03:20:57.111147: Epoch time: 138.14 s\n",
      "2025-12-17 03:20:57.808913: \n",
      "2025-12-17 03:20:57.808913: Epoch 634\n",
      "2025-12-17 03:20:57.808913: Current learning rate: 0.00405\n",
      "2025-12-17 03:23:15.658015: train_loss -0.8579\n",
      "2025-12-17 03:23:15.658015: val_loss -0.8743\n",
      "2025-12-17 03:23:15.664994: Pseudo dice [0.9254, 0.9577, 0.9367]\n",
      "2025-12-17 03:23:15.671000: Epoch time: 137.85 s\n",
      "2025-12-17 03:23:16.317063: \n",
      "2025-12-17 03:23:16.319066: Epoch 635\n",
      "2025-12-17 03:23:16.319066: Current learning rate: 0.00404\n",
      "2025-12-17 03:25:34.331852: train_loss -0.8518\n",
      "2025-12-17 03:25:34.331852: val_loss -0.8746\n",
      "2025-12-17 03:25:34.338446: Pseudo dice [0.9281, 0.9594, 0.9314]\n",
      "2025-12-17 03:25:34.338446: Epoch time: 138.01 s\n",
      "2025-12-17 03:25:34.979499: \n",
      "2025-12-17 03:25:34.979499: Epoch 636\n",
      "2025-12-17 03:25:34.983188: Current learning rate: 0.00403\n",
      "2025-12-17 03:27:52.914060: train_loss -0.8505\n",
      "2025-12-17 03:27:52.914060: val_loss -0.87\n",
      "2025-12-17 03:27:52.920070: Pseudo dice [0.925, 0.9546, 0.9401]\n",
      "2025-12-17 03:27:52.924074: Epoch time: 137.94 s\n",
      "2025-12-17 03:27:53.772425: \n",
      "2025-12-17 03:27:53.772425: Epoch 637\n",
      "2025-12-17 03:27:53.772425: Current learning rate: 0.00402\n",
      "2025-12-17 03:30:11.652548: train_loss -0.8507\n",
      "2025-12-17 03:30:11.652548: val_loss -0.8643\n",
      "2025-12-17 03:30:11.658554: Pseudo dice [0.9214, 0.9528, 0.9256]\n",
      "2025-12-17 03:30:11.658554: Epoch time: 137.88 s\n",
      "2025-12-17 03:30:12.308270: \n",
      "2025-12-17 03:30:12.308270: Epoch 638\n",
      "2025-12-17 03:30:12.308270: Current learning rate: 0.00401\n",
      "2025-12-17 03:32:30.422300: train_loss -0.853\n",
      "2025-12-17 03:32:30.424304: val_loss -0.8778\n",
      "2025-12-17 03:32:30.430311: Pseudo dice [0.9326, 0.9617, 0.9287]\n",
      "2025-12-17 03:32:30.432312: Epoch time: 138.12 s\n",
      "2025-12-17 03:32:31.079193: \n",
      "2025-12-17 03:32:31.079193: Epoch 639\n",
      "2025-12-17 03:32:31.079193: Current learning rate: 0.004\n",
      "2025-12-17 03:34:48.964612: train_loss -0.8583\n",
      "2025-12-17 03:34:48.964612: val_loss -0.8757\n",
      "2025-12-17 03:34:48.972277: Pseudo dice [0.9254, 0.9582, 0.9396]\n",
      "2025-12-17 03:34:48.978283: Epoch time: 137.89 s\n",
      "2025-12-17 03:34:49.795814: \n",
      "2025-12-17 03:34:49.795814: Epoch 640\n",
      "2025-12-17 03:34:49.795814: Current learning rate: 0.00399\n",
      "2025-12-17 03:37:07.583748: train_loss -0.8583\n",
      "2025-12-17 03:37:07.583748: val_loss -0.87\n",
      "2025-12-17 03:37:07.589136: Pseudo dice [0.924, 0.9555, 0.935]\n",
      "2025-12-17 03:37:07.594757: Epoch time: 137.79 s\n",
      "2025-12-17 03:37:08.239693: \n",
      "2025-12-17 03:37:08.239693: Epoch 641\n",
      "2025-12-17 03:37:08.239693: Current learning rate: 0.00398\n",
      "2025-12-17 03:39:26.101383: train_loss -0.8523\n",
      "2025-12-17 03:39:26.101383: val_loss -0.8733\n",
      "2025-12-17 03:39:26.107128: Pseudo dice [0.9274, 0.957, 0.9339]\n",
      "2025-12-17 03:39:26.111132: Epoch time: 137.86 s\n",
      "2025-12-17 03:39:26.750716: \n",
      "2025-12-17 03:39:26.750716: Epoch 642\n",
      "2025-12-17 03:39:26.766809: Current learning rate: 0.00397\n",
      "2025-12-17 03:41:44.871837: train_loss -0.8586\n",
      "2025-12-17 03:41:44.871837: val_loss -0.8697\n",
      "2025-12-17 03:41:44.885685: Pseudo dice [0.9273, 0.9547, 0.9313]\n",
      "2025-12-17 03:41:44.885685: Epoch time: 138.12 s\n",
      "2025-12-17 03:41:45.865872: \n",
      "2025-12-17 03:41:45.865872: Epoch 643\n",
      "2025-12-17 03:41:45.883504: Current learning rate: 0.00396\n",
      "2025-12-17 03:44:03.873284: train_loss -0.8576\n",
      "2025-12-17 03:44:03.873284: val_loss -0.8669\n",
      "2025-12-17 03:44:03.880345: Pseudo dice [0.9274, 0.9533, 0.9229]\n",
      "2025-12-17 03:44:03.886351: Epoch time: 138.01 s\n",
      "2025-12-17 03:44:04.522755: \n",
      "2025-12-17 03:44:04.522755: Epoch 644\n",
      "2025-12-17 03:44:04.522755: Current learning rate: 0.00395\n",
      "2025-12-17 03:46:22.576723: train_loss -0.8516\n",
      "2025-12-17 03:46:22.579197: val_loss -0.8729\n",
      "2025-12-17 03:46:22.584226: Pseudo dice [0.9264, 0.9553, 0.933]\n",
      "2025-12-17 03:46:22.588231: Epoch time: 138.05 s\n",
      "2025-12-17 03:46:23.242818: \n",
      "2025-12-17 03:46:23.242818: Epoch 645\n",
      "2025-12-17 03:46:23.242818: Current learning rate: 0.00394\n",
      "2025-12-17 03:48:41.294638: train_loss -0.8532\n",
      "2025-12-17 03:48:41.294638: val_loss -0.8755\n",
      "2025-12-17 03:48:41.302386: Pseudo dice [0.9257, 0.9594, 0.9371]\n",
      "2025-12-17 03:48:41.308393: Epoch time: 138.05 s\n",
      "2025-12-17 03:48:42.066164: \n",
      "2025-12-17 03:48:42.066164: Epoch 646\n",
      "2025-12-17 03:48:42.072140: Current learning rate: 0.00393\n",
      "2025-12-17 03:51:00.008419: train_loss -0.8599\n",
      "2025-12-17 03:51:00.010220: val_loss -0.8676\n",
      "2025-12-17 03:51:00.014225: Pseudo dice [0.9214, 0.9549, 0.9309]\n",
      "2025-12-17 03:51:00.020232: Epoch time: 137.94 s\n",
      "2025-12-17 03:51:00.666509: \n",
      "2025-12-17 03:51:00.666509: Epoch 647\n",
      "2025-12-17 03:51:00.666509: Current learning rate: 0.00392\n",
      "2025-12-17 03:53:18.594612: train_loss -0.8459\n",
      "2025-12-17 03:53:18.594612: val_loss -0.8706\n",
      "2025-12-17 03:53:18.613749: Pseudo dice [0.9238, 0.9565, 0.9377]\n",
      "2025-12-17 03:53:18.617752: Epoch time: 137.93 s\n",
      "2025-12-17 03:53:19.260684: \n",
      "2025-12-17 03:53:19.260684: Epoch 648\n",
      "2025-12-17 03:53:19.260684: Current learning rate: 0.00391\n",
      "2025-12-17 03:55:37.302595: train_loss -0.8529\n",
      "2025-12-17 03:55:37.304597: val_loss -0.8629\n",
      "2025-12-17 03:55:37.308601: Pseudo dice [0.9194, 0.9526, 0.9301]\n",
      "2025-12-17 03:55:37.314607: Epoch time: 138.04 s\n",
      "2025-12-17 03:55:38.060156: \n",
      "2025-12-17 03:55:38.060156: Epoch 649\n",
      "2025-12-17 03:55:38.075992: Current learning rate: 0.0039\n",
      "2025-12-17 03:57:56.055606: train_loss -0.851\n",
      "2025-12-17 03:57:56.071524: val_loss -0.8673\n",
      "2025-12-17 03:57:56.071524: Pseudo dice [0.9243, 0.9559, 0.9315]\n",
      "2025-12-17 03:57:56.071524: Epoch time: 138.0 s\n",
      "2025-12-17 03:57:57.197014: \n",
      "2025-12-17 03:57:57.197014: Epoch 650\n",
      "2025-12-17 03:57:57.197014: Current learning rate: 0.00389\n",
      "2025-12-17 04:00:15.206191: train_loss -0.8562\n",
      "2025-12-17 04:00:15.206191: val_loss -0.8768\n",
      "2025-12-17 04:00:15.210980: Pseudo dice [0.9304, 0.9591, 0.9333]\n",
      "2025-12-17 04:00:15.216987: Epoch time: 138.01 s\n",
      "2025-12-17 04:00:15.871228: \n",
      "2025-12-17 04:00:15.871228: Epoch 651\n",
      "2025-12-17 04:00:15.871228: Current learning rate: 0.00388\n",
      "2025-12-17 04:02:33.878583: train_loss -0.8481\n",
      "2025-12-17 04:02:33.880585: val_loss -0.8645\n",
      "2025-12-17 04:02:33.884588: Pseudo dice [0.9184, 0.9474, 0.9346]\n",
      "2025-12-17 04:02:33.890085: Epoch time: 138.01 s\n",
      "2025-12-17 04:02:34.722716: \n",
      "2025-12-17 04:02:34.722716: Epoch 652\n",
      "2025-12-17 04:02:34.728725: Current learning rate: 0.00387\n",
      "2025-12-17 04:04:52.815162: train_loss -0.8502\n",
      "2025-12-17 04:04:52.817165: val_loss -0.8698\n",
      "2025-12-17 04:04:52.817165: Pseudo dice [0.9231, 0.953, 0.9401]\n",
      "2025-12-17 04:04:52.817165: Epoch time: 138.09 s\n",
      "2025-12-17 04:04:53.459472: \n",
      "2025-12-17 04:04:53.459472: Epoch 653\n",
      "2025-12-17 04:04:53.463734: Current learning rate: 0.00386\n",
      "2025-12-17 04:07:11.614798: train_loss -0.8552\n",
      "2025-12-17 04:07:11.614798: val_loss -0.8742\n",
      "2025-12-17 04:07:11.630788: Pseudo dice [0.925, 0.9535, 0.9424]\n",
      "2025-12-17 04:07:11.632791: Epoch time: 138.16 s\n",
      "2025-12-17 04:07:12.283854: \n",
      "2025-12-17 04:07:12.283854: Epoch 654\n",
      "2025-12-17 04:07:12.283854: Current learning rate: 0.00385\n",
      "2025-12-17 04:09:30.682436: train_loss -0.8559\n",
      "2025-12-17 04:09:30.684438: val_loss -0.8739\n",
      "2025-12-17 04:09:30.690444: Pseudo dice [0.9253, 0.9582, 0.9291]\n",
      "2025-12-17 04:09:30.696451: Epoch time: 138.4 s\n",
      "2025-12-17 04:09:31.460246: \n",
      "2025-12-17 04:09:31.462248: Epoch 655\n",
      "2025-12-17 04:09:31.462248: Current learning rate: 0.00384\n",
      "2025-12-17 04:11:49.392292: train_loss -0.8577\n",
      "2025-12-17 04:11:49.394294: val_loss -0.8744\n",
      "2025-12-17 04:11:49.398298: Pseudo dice [0.9212, 0.9553, 0.9381]\n",
      "2025-12-17 04:11:49.404304: Epoch time: 137.93 s\n",
      "2025-12-17 04:11:50.213223: \n",
      "2025-12-17 04:11:50.213223: Epoch 656\n",
      "2025-12-17 04:11:50.213223: Current learning rate: 0.00383\n",
      "2025-12-17 04:14:08.270751: train_loss -0.8507\n",
      "2025-12-17 04:14:08.272710: val_loss -0.8748\n",
      "2025-12-17 04:14:08.278716: Pseudo dice [0.9259, 0.9574, 0.9362]\n",
      "2025-12-17 04:14:08.284696: Epoch time: 138.06 s\n",
      "2025-12-17 04:14:08.932776: \n",
      "2025-12-17 04:14:08.932776: Epoch 657\n",
      "2025-12-17 04:14:08.934779: Current learning rate: 0.00382\n",
      "2025-12-17 04:16:27.044936: train_loss -0.8482\n",
      "2025-12-17 04:16:27.044936: val_loss -0.8737\n",
      "2025-12-17 04:16:27.052945: Pseudo dice [0.9272, 0.9537, 0.9317]\n",
      "2025-12-17 04:16:27.058292: Epoch time: 138.11 s\n",
      "2025-12-17 04:16:27.851237: \n",
      "2025-12-17 04:16:27.851237: Epoch 658\n",
      "2025-12-17 04:16:27.867079: Current learning rate: 0.00381\n",
      "2025-12-17 04:18:45.720022: train_loss -0.8494\n",
      "2025-12-17 04:18:45.720022: val_loss -0.8778\n",
      "2025-12-17 04:18:45.726029: Pseudo dice [0.9291, 0.9606, 0.9303]\n",
      "2025-12-17 04:18:45.729034: Epoch time: 137.87 s\n",
      "2025-12-17 04:18:46.377110: \n",
      "2025-12-17 04:18:46.377110: Epoch 659\n",
      "2025-12-17 04:18:46.377110: Current learning rate: 0.0038\n",
      "2025-12-17 04:21:04.349470: train_loss -0.8529\n",
      "2025-12-17 04:21:04.349470: val_loss -0.8784\n",
      "2025-12-17 04:21:04.353966: Pseudo dice [0.9332, 0.9596, 0.9375]\n",
      "2025-12-17 04:21:04.359974: Epoch time: 137.99 s\n",
      "2025-12-17 04:21:05.001913: \n",
      "2025-12-17 04:21:05.001913: Epoch 660\n",
      "2025-12-17 04:21:05.001913: Current learning rate: 0.00379\n",
      "2025-12-17 04:23:22.881802: train_loss -0.8598\n",
      "2025-12-17 04:23:22.881802: val_loss -0.8723\n",
      "2025-12-17 04:23:22.891047: Pseudo dice [0.9256, 0.9589, 0.9367]\n",
      "2025-12-17 04:23:22.895051: Epoch time: 137.88 s\n",
      "2025-12-17 04:23:23.528756: \n",
      "2025-12-17 04:23:23.528756: Epoch 661\n",
      "2025-12-17 04:23:23.528756: Current learning rate: 0.00378\n",
      "2025-12-17 04:25:41.615592: train_loss -0.8541\n",
      "2025-12-17 04:25:41.615592: val_loss -0.8712\n",
      "2025-12-17 04:25:41.631411: Pseudo dice [0.9238, 0.9541, 0.9393]\n",
      "2025-12-17 04:25:41.633698: Epoch time: 138.09 s\n",
      "2025-12-17 04:25:42.451535: \n",
      "2025-12-17 04:25:42.451535: Epoch 662\n",
      "2025-12-17 04:25:42.451535: Current learning rate: 0.00377\n",
      "2025-12-17 04:28:00.464739: train_loss -0.8495\n",
      "2025-12-17 04:28:00.464739: val_loss -0.8688\n",
      "2025-12-17 04:28:00.471600: Pseudo dice [0.9217, 0.9536, 0.9281]\n",
      "2025-12-17 04:28:00.475605: Epoch time: 138.03 s\n",
      "2025-12-17 04:28:01.154447: \n",
      "2025-12-17 04:28:01.154447: Epoch 663\n",
      "2025-12-17 04:28:01.154447: Current learning rate: 0.00376\n",
      "2025-12-17 04:30:19.153574: train_loss -0.8517\n",
      "2025-12-17 04:30:19.153574: val_loss -0.8704\n",
      "2025-12-17 04:30:19.161240: Pseudo dice [0.9202, 0.953, 0.9412]\n",
      "2025-12-17 04:30:19.167246: Epoch time: 138.01 s\n",
      "2025-12-17 04:30:19.817813: \n",
      "2025-12-17 04:30:19.817813: Epoch 664\n",
      "2025-12-17 04:30:19.817813: Current learning rate: 0.00375\n",
      "2025-12-17 04:32:37.799442: train_loss -0.8534\n",
      "2025-12-17 04:32:37.799442: val_loss -0.8773\n",
      "2025-12-17 04:32:37.806264: Pseudo dice [0.9277, 0.96, 0.9333]\n",
      "2025-12-17 04:32:37.808266: Epoch time: 137.98 s\n",
      "2025-12-17 04:32:38.455046: \n",
      "2025-12-17 04:32:38.455046: Epoch 665\n",
      "2025-12-17 04:32:38.461075: Current learning rate: 0.00374\n",
      "2025-12-17 04:34:56.628524: train_loss -0.8513\n",
      "2025-12-17 04:34:56.630530: val_loss -0.8783\n",
      "2025-12-17 04:34:56.638188: Pseudo dice [0.9319, 0.9586, 0.9364]\n",
      "2025-12-17 04:34:56.644195: Epoch time: 138.17 s\n",
      "2025-12-17 04:34:56.647937: Yayy! New best EMA pseudo Dice: 0.939\n",
      "2025-12-17 04:34:57.581508: \n",
      "2025-12-17 04:34:57.581508: Epoch 666\n",
      "2025-12-17 04:34:57.581508: Current learning rate: 0.00373\n",
      "2025-12-17 04:37:15.670704: train_loss -0.8567\n",
      "2025-12-17 04:37:15.670704: val_loss -0.8723\n",
      "2025-12-17 04:37:15.678051: Pseudo dice [0.9217, 0.9549, 0.9371]\n",
      "2025-12-17 04:37:15.682056: Epoch time: 138.09 s\n",
      "2025-12-17 04:37:16.326495: \n",
      "2025-12-17 04:37:16.326495: Epoch 667\n",
      "2025-12-17 04:37:16.326495: Current learning rate: 0.00372\n",
      "2025-12-17 04:39:34.240799: train_loss -0.855\n",
      "2025-12-17 04:39:34.256736: val_loss -0.8806\n",
      "2025-12-17 04:39:34.260236: Pseudo dice [0.9282, 0.9594, 0.9388]\n",
      "2025-12-17 04:39:34.266243: Epoch time: 137.92 s\n",
      "2025-12-17 04:39:34.270247: Yayy! New best EMA pseudo Dice: 0.9392\n",
      "2025-12-17 04:39:35.372937: \n",
      "2025-12-17 04:39:35.372937: Epoch 668\n",
      "2025-12-17 04:39:35.372937: Current learning rate: 0.00371\n",
      "2025-12-17 04:41:53.503549: train_loss -0.8542\n",
      "2025-12-17 04:41:53.505552: val_loss -0.8766\n",
      "2025-12-17 04:41:53.509557: Pseudo dice [0.9269, 0.957, 0.9399]\n",
      "2025-12-17 04:41:53.515055: Epoch time: 138.13 s\n",
      "2025-12-17 04:41:53.519059: Yayy! New best EMA pseudo Dice: 0.9394\n",
      "2025-12-17 04:41:54.468998: \n",
      "2025-12-17 04:41:54.468998: Epoch 669\n",
      "2025-12-17 04:41:54.484892: Current learning rate: 0.0037\n",
      "2025-12-17 04:44:12.326876: train_loss -0.8583\n",
      "2025-12-17 04:44:12.326876: val_loss -0.8735\n",
      "2025-12-17 04:44:12.332621: Pseudo dice [0.9268, 0.9552, 0.9271]\n",
      "2025-12-17 04:44:12.339075: Epoch time: 137.86 s\n",
      "2025-12-17 04:44:12.991088: \n",
      "2025-12-17 04:44:12.991088: Epoch 670\n",
      "2025-12-17 04:44:12.993709: Current learning rate: 0.00369\n",
      "2025-12-17 04:46:30.944799: train_loss -0.8558\n",
      "2025-12-17 04:46:30.944799: val_loss -0.8771\n",
      "2025-12-17 04:46:30.944799: Pseudo dice [0.9285, 0.9598, 0.9309]\n",
      "2025-12-17 04:46:30.951228: Epoch time: 137.96 s\n",
      "2025-12-17 04:46:31.594728: \n",
      "2025-12-17 04:46:31.594728: Epoch 671\n",
      "2025-12-17 04:46:31.594728: Current learning rate: 0.00368\n",
      "2025-12-17 04:48:49.588668: train_loss -0.8497\n",
      "2025-12-17 04:48:49.588668: val_loss -0.8712\n",
      "2025-12-17 04:48:49.608714: Pseudo dice [0.9243, 0.9507, 0.9363]\n",
      "2025-12-17 04:48:49.612718: Epoch time: 137.99 s\n",
      "2025-12-17 04:48:50.316697: \n",
      "2025-12-17 04:48:50.316697: Epoch 672\n",
      "2025-12-17 04:48:50.322539: Current learning rate: 0.00367\n",
      "2025-12-17 04:51:08.435888: train_loss -0.8514\n",
      "2025-12-17 04:51:08.435888: val_loss -0.8738\n",
      "2025-12-17 04:51:08.443194: Pseudo dice [0.9304, 0.9571, 0.9254]\n",
      "2025-12-17 04:51:08.447198: Epoch time: 138.13 s\n",
      "2025-12-17 04:51:09.102788: \n",
      "2025-12-17 04:51:09.102788: Epoch 673\n",
      "2025-12-17 04:51:09.102788: Current learning rate: 0.00366\n",
      "2025-12-17 04:53:27.033101: train_loss -0.8526\n",
      "2025-12-17 04:53:27.033101: val_loss -0.8588\n",
      "2025-12-17 04:53:27.041108: Pseudo dice [0.9178, 0.9513, 0.926]\n",
      "2025-12-17 04:53:27.046853: Epoch time: 137.93 s\n",
      "2025-12-17 04:53:27.901905: \n",
      "2025-12-17 04:53:27.901905: Epoch 674\n",
      "2025-12-17 04:53:27.901905: Current learning rate: 0.00365\n",
      "2025-12-17 04:55:46.028977: train_loss -0.8513\n",
      "2025-12-17 04:55:46.028977: val_loss -0.8696\n",
      "2025-12-17 04:55:46.034983: Pseudo dice [0.9213, 0.958, 0.9369]\n",
      "2025-12-17 04:55:46.041296: Epoch time: 138.13 s\n",
      "2025-12-17 04:55:46.678134: \n",
      "2025-12-17 04:55:46.678134: Epoch 675\n",
      "2025-12-17 04:55:46.694057: Current learning rate: 0.00364\n",
      "2025-12-17 04:58:04.751680: train_loss -0.8504\n",
      "2025-12-17 04:58:04.751680: val_loss -0.8794\n",
      "2025-12-17 04:58:04.757613: Pseudo dice [0.9298, 0.9584, 0.9391]\n",
      "2025-12-17 04:58:04.761617: Epoch time: 138.07 s\n",
      "2025-12-17 04:58:05.412941: \n",
      "2025-12-17 04:58:05.412941: Epoch 676\n",
      "2025-12-17 04:58:05.428847: Current learning rate: 0.00363\n",
      "2025-12-17 05:00:23.350408: train_loss -0.8494\n",
      "2025-12-17 05:00:23.350408: val_loss -0.8726\n",
      "2025-12-17 05:00:23.369913: Pseudo dice [0.9261, 0.9585, 0.9389]\n",
      "2025-12-17 05:00:23.373917: Epoch time: 137.94 s\n",
      "2025-12-17 05:00:24.023338: \n",
      "2025-12-17 05:00:24.023338: Epoch 677\n",
      "2025-12-17 05:00:24.029054: Current learning rate: 0.00362\n",
      "2025-12-17 05:02:42.081515: train_loss -0.8535\n",
      "2025-12-17 05:02:42.081515: val_loss -0.8765\n",
      "2025-12-17 05:02:42.088219: Pseudo dice [0.9306, 0.9582, 0.9352]\n",
      "2025-12-17 05:02:42.091827: Epoch time: 138.06 s\n",
      "2025-12-17 05:02:42.831032: \n",
      "2025-12-17 05:02:42.833035: Epoch 678\n",
      "2025-12-17 05:02:42.833035: Current learning rate: 0.00361\n",
      "2025-12-17 05:05:00.777138: train_loss -0.8584\n",
      "2025-12-17 05:05:00.777138: val_loss -0.8683\n",
      "2025-12-17 05:05:00.777138: Pseudo dice [0.9241, 0.9558, 0.9306]\n",
      "2025-12-17 05:05:00.786640: Epoch time: 137.95 s\n",
      "2025-12-17 05:05:01.442411: \n",
      "2025-12-17 05:05:01.444413: Epoch 679\n",
      "2025-12-17 05:05:01.444413: Current learning rate: 0.0036\n",
      "2025-12-17 05:07:19.314924: train_loss -0.859\n",
      "2025-12-17 05:07:19.316427: val_loss -0.8833\n",
      "2025-12-17 05:07:19.321935: Pseudo dice [0.9297, 0.9588, 0.9462]\n",
      "2025-12-17 05:07:19.325939: Epoch time: 137.87 s\n",
      "2025-12-17 05:07:19.329944: Yayy! New best EMA pseudo Dice: 0.9395\n",
      "2025-12-17 05:07:20.446551: \n",
      "2025-12-17 05:07:20.446551: Epoch 680\n",
      "2025-12-17 05:07:20.446551: Current learning rate: 0.00359\n",
      "2025-12-17 05:09:38.755850: train_loss -0.859\n",
      "2025-12-17 05:09:38.755850: val_loss -0.8691\n",
      "2025-12-17 05:09:38.759768: Pseudo dice [0.9256, 0.9535, 0.9329]\n",
      "2025-12-17 05:09:38.767371: Epoch time: 138.31 s\n",
      "2025-12-17 05:09:39.419207: \n",
      "2025-12-17 05:09:39.419207: Epoch 681\n",
      "2025-12-17 05:09:39.419207: Current learning rate: 0.00358\n",
      "2025-12-17 05:11:57.747702: train_loss -0.8526\n",
      "2025-12-17 05:11:57.749704: val_loss -0.8863\n",
      "2025-12-17 05:11:57.754370: Pseudo dice [0.9279, 0.9594, 0.9457]\n",
      "2025-12-17 05:11:57.758374: Epoch time: 138.33 s\n",
      "2025-12-17 05:11:57.764379: Yayy! New best EMA pseudo Dice: 0.9398\n",
      "2025-12-17 05:11:58.688704: \n",
      "2025-12-17 05:11:58.688704: Epoch 682\n",
      "2025-12-17 05:11:58.688704: Current learning rate: 0.00357\n",
      "2025-12-17 05:14:16.787184: train_loss -0.8512\n",
      "2025-12-17 05:14:16.803022: val_loss -0.8693\n",
      "2025-12-17 05:14:16.803022: Pseudo dice [0.9209, 0.9542, 0.9293]\n",
      "2025-12-17 05:14:16.803022: Epoch time: 138.1 s\n",
      "2025-12-17 05:14:17.459116: \n",
      "2025-12-17 05:14:17.459116: Epoch 683\n",
      "2025-12-17 05:14:17.459116: Current learning rate: 0.00356\n",
      "2025-12-17 05:16:35.440996: train_loss -0.8489\n",
      "2025-12-17 05:16:35.441997: val_loss -0.8712\n",
      "2025-12-17 05:16:35.447511: Pseudo dice [0.9248, 0.9583, 0.9307]\n",
      "2025-12-17 05:16:35.451516: Epoch time: 137.98 s\n",
      "2025-12-17 05:16:36.107935: \n",
      "2025-12-17 05:16:36.107935: Epoch 684\n",
      "2025-12-17 05:16:36.107935: Current learning rate: 0.00355\n",
      "2025-12-17 05:18:54.115041: train_loss -0.851\n",
      "2025-12-17 05:18:54.115041: val_loss -0.8808\n",
      "2025-12-17 05:18:54.115041: Pseudo dice [0.9263, 0.9586, 0.942]\n",
      "2025-12-17 05:18:54.130728: Epoch time: 138.01 s\n",
      "2025-12-17 05:18:54.767511: \n",
      "2025-12-17 05:18:54.767511: Epoch 685\n",
      "2025-12-17 05:18:54.767511: Current learning rate: 0.00354\n",
      "2025-12-17 05:21:12.860991: train_loss -0.8605\n",
      "2025-12-17 05:21:12.860991: val_loss -0.8732\n",
      "2025-12-17 05:21:12.863457: Pseudo dice [0.9248, 0.9569, 0.9372]\n",
      "2025-12-17 05:21:12.863457: Epoch time: 138.09 s\n",
      "2025-12-17 05:21:13.671869: \n",
      "2025-12-17 05:21:13.671869: Epoch 686\n",
      "2025-12-17 05:21:13.687633: Current learning rate: 0.00353\n",
      "2025-12-17 05:23:31.576338: train_loss -0.8557\n",
      "2025-12-17 05:23:31.576338: val_loss -0.8775\n",
      "2025-12-17 05:23:31.586239: Pseudo dice [0.9284, 0.9596, 0.9324]\n",
      "2025-12-17 05:23:31.591793: Epoch time: 137.9 s\n",
      "2025-12-17 05:23:32.353684: \n",
      "2025-12-17 05:23:32.353684: Epoch 687\n",
      "2025-12-17 05:23:32.353684: Current learning rate: 0.00352\n",
      "2025-12-17 05:25:50.124187: train_loss -0.8561\n",
      "2025-12-17 05:25:50.124187: val_loss -0.8764\n",
      "2025-12-17 05:25:50.128190: Pseudo dice [0.9336, 0.9618, 0.9272]\n",
      "2025-12-17 05:25:50.133492: Epoch time: 137.77 s\n",
      "2025-12-17 05:25:50.780940: \n",
      "2025-12-17 05:25:50.780940: Epoch 688\n",
      "2025-12-17 05:25:50.780940: Current learning rate: 0.00351\n",
      "2025-12-17 05:28:08.716059: train_loss -0.8561\n",
      "2025-12-17 05:28:08.716059: val_loss -0.8743\n",
      "2025-12-17 05:28:08.727305: Pseudo dice [0.9259, 0.9563, 0.9377]\n",
      "2025-12-17 05:28:08.733669: Epoch time: 137.94 s\n",
      "2025-12-17 05:28:09.381375: \n",
      "2025-12-17 05:28:09.381375: Epoch 689\n",
      "2025-12-17 05:28:09.381375: Current learning rate: 0.0035\n",
      "2025-12-17 05:30:27.341323: train_loss -0.8537\n",
      "2025-12-17 05:30:27.343327: val_loss -0.8632\n",
      "2025-12-17 05:30:27.349335: Pseudo dice [0.9193, 0.9519, 0.933]\n",
      "2025-12-17 05:30:27.357083: Epoch time: 137.96 s\n",
      "2025-12-17 05:30:28.144154: \n",
      "2025-12-17 05:30:28.144154: Epoch 690\n",
      "2025-12-17 05:30:28.150155: Current learning rate: 0.00349\n",
      "2025-12-17 05:32:46.054209: train_loss -0.8518\n",
      "2025-12-17 05:32:46.054209: val_loss -0.8801\n",
      "2025-12-17 05:32:46.061828: Pseudo dice [0.929, 0.9591, 0.9377]\n",
      "2025-12-17 05:32:46.061828: Epoch time: 137.91 s\n",
      "2025-12-17 05:32:46.720624: \n",
      "2025-12-17 05:32:46.720624: Epoch 691\n",
      "2025-12-17 05:32:46.720624: Current learning rate: 0.00348\n",
      "2025-12-17 05:35:04.677664: train_loss -0.86\n",
      "2025-12-17 05:35:04.677664: val_loss -0.8668\n",
      "2025-12-17 05:35:04.685412: Pseudo dice [0.9216, 0.947, 0.9392]\n",
      "2025-12-17 05:35:04.693219: Epoch time: 137.96 s\n",
      "2025-12-17 05:35:05.525225: \n",
      "2025-12-17 05:35:05.527050: Epoch 692\n",
      "2025-12-17 05:35:05.527050: Current learning rate: 0.00346\n",
      "2025-12-17 05:37:23.445695: train_loss -0.856\n",
      "2025-12-17 05:37:23.445695: val_loss -0.8726\n",
      "2025-12-17 05:37:23.445695: Pseudo dice [0.9271, 0.9561, 0.9361]\n",
      "2025-12-17 05:37:23.461688: Epoch time: 137.92 s\n",
      "2025-12-17 05:37:24.184757: \n",
      "2025-12-17 05:37:24.184757: Epoch 693\n",
      "2025-12-17 05:37:24.190731: Current learning rate: 0.00345\n",
      "2025-12-17 05:39:42.112182: train_loss -0.8541\n",
      "2025-12-17 05:39:42.114184: val_loss -0.877\n",
      "2025-12-17 05:39:42.120190: Pseudo dice [0.9258, 0.9602, 0.9372]\n",
      "2025-12-17 05:39:42.124194: Epoch time: 137.93 s\n",
      "2025-12-17 05:39:42.805880: \n",
      "2025-12-17 05:39:42.805880: Epoch 694\n",
      "2025-12-17 05:39:42.823402: Current learning rate: 0.00344\n",
      "2025-12-17 05:42:00.650708: train_loss -0.8559\n",
      "2025-12-17 05:42:00.650708: val_loss -0.8664\n",
      "2025-12-17 05:42:00.656085: Pseudo dice [0.9222, 0.9544, 0.9317]\n",
      "2025-12-17 05:42:00.662386: Epoch time: 137.84 s\n",
      "2025-12-17 05:42:01.308015: \n",
      "2025-12-17 05:42:01.308015: Epoch 695\n",
      "2025-12-17 05:42:01.324085: Current learning rate: 0.00343\n",
      "2025-12-17 05:44:19.315192: train_loss -0.857\n",
      "2025-12-17 05:44:19.315192: val_loss -0.8751\n",
      "2025-12-17 05:44:19.328669: Pseudo dice [0.9279, 0.9556, 0.9369]\n",
      "2025-12-17 05:44:19.332791: Epoch time: 138.01 s\n",
      "2025-12-17 05:44:20.010732: \n",
      "2025-12-17 05:44:20.010732: Epoch 696\n",
      "2025-12-17 05:44:20.020543: Current learning rate: 0.00342\n",
      "2025-12-17 05:46:38.069507: train_loss -0.8533\n",
      "2025-12-17 05:46:38.069507: val_loss -0.8734\n",
      "2025-12-17 05:46:38.085220: Pseudo dice [0.9246, 0.9561, 0.9339]\n",
      "2025-12-17 05:46:38.090166: Epoch time: 138.06 s\n",
      "2025-12-17 05:46:38.737815: \n",
      "2025-12-17 05:46:38.737815: Epoch 697\n",
      "2025-12-17 05:46:38.747971: Current learning rate: 0.00341\n",
      "2025-12-17 05:48:56.583882: train_loss -0.8513\n",
      "2025-12-17 05:48:56.583882: val_loss -0.882\n",
      "2025-12-17 05:48:56.583882: Pseudo dice [0.9346, 0.9608, 0.9306]\n",
      "2025-12-17 05:48:56.583882: Epoch time: 137.85 s\n",
      "2025-12-17 05:48:57.280315: \n",
      "2025-12-17 05:48:57.280315: Epoch 698\n",
      "2025-12-17 05:48:57.296031: Current learning rate: 0.0034\n",
      "2025-12-17 05:51:15.294213: train_loss -0.8569\n",
      "2025-12-17 05:51:15.294213: val_loss -0.8755\n",
      "2025-12-17 05:51:15.310019: Pseudo dice [0.9288, 0.9554, 0.9339]\n",
      "2025-12-17 05:51:15.317975: Epoch time: 138.01 s\n",
      "2025-12-17 05:51:16.150251: \n",
      "2025-12-17 05:51:16.150251: Epoch 699\n",
      "2025-12-17 05:51:16.155522: Current learning rate: 0.00339\n",
      "2025-12-17 05:53:34.221311: train_loss -0.8553\n",
      "2025-12-17 05:53:34.221311: val_loss -0.8845\n",
      "2025-12-17 05:53:34.228385: Pseudo dice [0.9327, 0.9643, 0.9372]\n",
      "2025-12-17 05:53:34.235207: Epoch time: 138.07 s\n",
      "2025-12-17 05:53:34.503101: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-17 05:53:35.431818: \n",
      "2025-12-17 05:53:35.431818: Epoch 700\n",
      "2025-12-17 05:53:35.435822: Current learning rate: 0.00338\n",
      "2025-12-17 05:55:53.562802: train_loss -0.8529\n",
      "2025-12-17 05:55:53.564806: val_loss -0.8753\n",
      "2025-12-17 05:55:53.568811: Pseudo dice [0.927, 0.9588, 0.9349]\n",
      "2025-12-17 05:55:53.572815: Epoch time: 138.13 s\n",
      "2025-12-17 05:55:53.578358: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-17 05:55:54.518657: \n",
      "2025-12-17 05:55:54.518657: Epoch 701\n",
      "2025-12-17 05:55:54.518657: Current learning rate: 0.00337\n",
      "2025-12-17 05:58:12.672543: train_loss -0.8554\n",
      "2025-12-17 05:58:12.672543: val_loss -0.874\n",
      "2025-12-17 05:58:12.688534: Pseudo dice [0.9255, 0.9558, 0.9317]\n",
      "2025-12-17 05:58:12.694089: Epoch time: 138.15 s\n",
      "2025-12-17 05:58:13.384972: \n",
      "2025-12-17 05:58:13.386974: Epoch 702\n",
      "2025-12-17 05:58:13.386974: Current learning rate: 0.00336\n",
      "2025-12-17 06:00:31.380780: train_loss -0.853\n",
      "2025-12-17 06:00:31.380780: val_loss -0.8813\n",
      "2025-12-17 06:00:31.391770: Pseudo dice [0.931, 0.9609, 0.9355]\n",
      "2025-12-17 06:00:31.395774: Epoch time: 138.0 s\n",
      "2025-12-17 06:00:31.396776: Yayy! New best EMA pseudo Dice: 0.94\n",
      "2025-12-17 06:00:32.344404: \n",
      "2025-12-17 06:00:32.344404: Epoch 703\n",
      "2025-12-17 06:00:32.344404: Current learning rate: 0.00335\n",
      "2025-12-17 06:02:50.382591: train_loss -0.8513\n",
      "2025-12-17 06:02:50.384593: val_loss -0.8676\n",
      "2025-12-17 06:02:50.396618: Pseudo dice [0.9237, 0.954, 0.9301]\n",
      "2025-12-17 06:02:50.401256: Epoch time: 138.04 s\n",
      "2025-12-17 06:02:51.262614: \n",
      "2025-12-17 06:02:51.262614: Epoch 704\n",
      "2025-12-17 06:02:51.262614: Current learning rate: 0.00334\n",
      "2025-12-17 06:05:09.299294: train_loss -0.8546\n",
      "2025-12-17 06:05:09.301297: val_loss -0.8745\n",
      "2025-12-17 06:05:09.306894: Pseudo dice [0.9235, 0.9557, 0.9404]\n",
      "2025-12-17 06:05:09.312900: Epoch time: 138.04 s\n",
      "2025-12-17 06:05:09.960340: \n",
      "2025-12-17 06:05:09.963873: Epoch 705\n",
      "2025-12-17 06:05:09.963873: Current learning rate: 0.00333\n",
      "2025-12-17 06:07:28.076318: train_loss -0.852\n",
      "2025-12-17 06:07:28.078320: val_loss -0.8813\n",
      "2025-12-17 06:07:28.086067: Pseudo dice [0.9342, 0.9574, 0.9413]\n",
      "2025-12-17 06:07:28.094082: Epoch time: 138.12 s\n",
      "2025-12-17 06:07:28.099772: Yayy! New best EMA pseudo Dice: 0.9401\n",
      "2025-12-17 06:07:29.012239: \n",
      "2025-12-17 06:07:29.012239: Epoch 706\n",
      "2025-12-17 06:07:29.012239: Current learning rate: 0.00332\n",
      "2025-12-17 06:09:47.287621: train_loss -0.8565\n",
      "2025-12-17 06:09:47.287621: val_loss -0.8684\n",
      "2025-12-17 06:09:47.293626: Pseudo dice [0.9213, 0.9534, 0.9361]\n",
      "2025-12-17 06:09:47.297631: Epoch time: 138.28 s\n",
      "2025-12-17 06:09:47.982255: \n",
      "2025-12-17 06:09:47.982255: Epoch 707\n",
      "2025-12-17 06:09:47.982255: Current learning rate: 0.00331\n",
      "2025-12-17 06:12:06.206474: train_loss -0.852\n",
      "2025-12-17 06:12:06.206474: val_loss -0.8692\n",
      "2025-12-17 06:12:06.216225: Pseudo dice [0.9251, 0.9554, 0.9231]\n",
      "2025-12-17 06:12:06.220231: Epoch time: 138.22 s\n",
      "2025-12-17 06:12:06.881407: \n",
      "2025-12-17 06:12:06.881407: Epoch 708\n",
      "2025-12-17 06:12:06.881407: Current learning rate: 0.0033\n",
      "2025-12-17 06:14:24.832857: train_loss -0.8577\n",
      "2025-12-17 06:14:24.832857: val_loss -0.8646\n",
      "2025-12-17 06:14:24.846098: Pseudo dice [0.9173, 0.9526, 0.9351]\n",
      "2025-12-17 06:14:24.850102: Epoch time: 137.95 s\n",
      "2025-12-17 06:14:25.497864: \n",
      "2025-12-17 06:14:25.497864: Epoch 709\n",
      "2025-12-17 06:14:25.497864: Current learning rate: 0.00329\n",
      "2025-12-17 06:16:43.423959: train_loss -0.8545\n",
      "2025-12-17 06:16:43.423959: val_loss -0.8698\n",
      "2025-12-17 06:16:43.431705: Pseudo dice [0.9203, 0.9544, 0.9398]\n",
      "2025-12-17 06:16:43.438547: Epoch time: 137.93 s\n",
      "2025-12-17 06:16:44.296370: \n",
      "2025-12-17 06:16:44.296370: Epoch 710\n",
      "2025-12-17 06:16:44.312165: Current learning rate: 0.00328\n",
      "2025-12-17 06:19:02.273308: train_loss -0.8507\n",
      "2025-12-17 06:19:02.275311: val_loss -0.8484\n",
      "2025-12-17 06:19:02.275311: Pseudo dice [0.9128, 0.9478, 0.9252]\n",
      "2025-12-17 06:19:02.275311: Epoch time: 137.98 s\n",
      "2025-12-17 06:19:02.922105: \n",
      "2025-12-17 06:19:02.922105: Epoch 711\n",
      "2025-12-17 06:19:02.922105: Current learning rate: 0.00327\n",
      "2025-12-17 06:21:20.936497: train_loss -0.851\n",
      "2025-12-17 06:21:20.936497: val_loss -0.8786\n",
      "2025-12-17 06:21:20.942098: Pseudo dice [0.93, 0.959, 0.9326]\n",
      "2025-12-17 06:21:20.942098: Epoch time: 138.01 s\n",
      "2025-12-17 06:21:21.595405: \n",
      "2025-12-17 06:21:21.595405: Epoch 712\n",
      "2025-12-17 06:21:21.595405: Current learning rate: 0.00326\n",
      "2025-12-17 06:23:39.617251: train_loss -0.8542\n",
      "2025-12-17 06:23:39.617251: val_loss -0.877\n",
      "2025-12-17 06:23:39.625000: Pseudo dice [0.9293, 0.9574, 0.9335]\n",
      "2025-12-17 06:23:39.631007: Epoch time: 138.02 s\n",
      "2025-12-17 06:23:40.326941: \n",
      "2025-12-17 06:23:40.326941: Epoch 713\n",
      "2025-12-17 06:23:40.328943: Current learning rate: 0.00325\n",
      "2025-12-17 06:25:58.270658: train_loss -0.8541\n",
      "2025-12-17 06:25:58.270658: val_loss -0.8723\n",
      "2025-12-17 06:25:58.270658: Pseudo dice [0.9317, 0.9564, 0.9296]\n",
      "2025-12-17 06:25:58.278792: Epoch time: 137.94 s\n",
      "2025-12-17 06:25:58.925942: \n",
      "2025-12-17 06:25:58.925942: Epoch 714\n",
      "2025-12-17 06:25:58.925942: Current learning rate: 0.00324\n",
      "2025-12-17 06:28:16.798317: train_loss -0.855\n",
      "2025-12-17 06:28:16.798317: val_loss -0.8654\n",
      "2025-12-17 06:28:16.798317: Pseudo dice [0.9194, 0.9515, 0.9323]\n",
      "2025-12-17 06:28:16.814234: Epoch time: 137.87 s\n",
      "2025-12-17 06:28:17.465424: \n",
      "2025-12-17 06:28:17.465424: Epoch 715\n",
      "2025-12-17 06:28:17.465424: Current learning rate: 0.00323\n",
      "2025-12-17 06:30:35.328348: train_loss -0.8533\n",
      "2025-12-17 06:30:35.328348: val_loss -0.8648\n",
      "2025-12-17 06:30:35.334358: Pseudo dice [0.9189, 0.953, 0.9314]\n",
      "2025-12-17 06:30:35.336360: Epoch time: 137.87 s\n",
      "2025-12-17 06:30:36.209278: \n",
      "2025-12-17 06:30:36.209278: Epoch 716\n",
      "2025-12-17 06:30:36.209278: Current learning rate: 0.00322\n",
      "2025-12-17 06:32:54.235897: train_loss -0.855\n",
      "2025-12-17 06:32:54.235897: val_loss -0.8663\n",
      "2025-12-17 06:32:54.251884: Pseudo dice [0.9236, 0.9501, 0.9287]\n",
      "2025-12-17 06:32:54.257505: Epoch time: 138.03 s\n",
      "2025-12-17 06:32:54.917436: \n",
      "2025-12-17 06:32:54.917436: Epoch 717\n",
      "2025-12-17 06:32:54.917436: Current learning rate: 0.00321\n",
      "2025-12-17 06:35:13.081665: train_loss -0.8554\n",
      "2025-12-17 06:35:13.083668: val_loss -0.8816\n",
      "2025-12-17 06:35:13.089673: Pseudo dice [0.9318, 0.9607, 0.9332]\n",
      "2025-12-17 06:35:13.091675: Epoch time: 138.16 s\n",
      "2025-12-17 06:35:13.741895: \n",
      "2025-12-17 06:35:13.741895: Epoch 718\n",
      "2025-12-17 06:35:13.741895: Current learning rate: 0.0032\n",
      "2025-12-17 06:37:31.636667: train_loss -0.8552\n",
      "2025-12-17 06:37:31.636667: val_loss -0.879\n",
      "2025-12-17 06:37:31.642411: Pseudo dice [0.9305, 0.9594, 0.9377]\n",
      "2025-12-17 06:37:31.642411: Epoch time: 137.89 s\n",
      "2025-12-17 06:37:32.414300: \n",
      "2025-12-17 06:37:32.414300: Epoch 719\n",
      "2025-12-17 06:37:32.414300: Current learning rate: 0.00319\n",
      "2025-12-17 06:39:50.356631: train_loss -0.8565\n",
      "2025-12-17 06:39:50.356631: val_loss -0.8685\n",
      "2025-12-17 06:39:50.376195: Pseudo dice [0.924, 0.9517, 0.9348]\n",
      "2025-12-17 06:39:50.382201: Epoch time: 137.96 s\n",
      "2025-12-17 06:39:51.043509: \n",
      "2025-12-17 06:39:51.043509: Epoch 720\n",
      "2025-12-17 06:39:51.045745: Current learning rate: 0.00318\n",
      "2025-12-17 06:42:09.121271: train_loss -0.8546\n",
      "2025-12-17 06:42:09.121271: val_loss -0.8662\n",
      "2025-12-17 06:42:09.128919: Pseudo dice [0.9191, 0.9546, 0.933]\n",
      "2025-12-17 06:42:09.133420: Epoch time: 138.08 s\n",
      "2025-12-17 06:42:09.782929: \n",
      "2025-12-17 06:42:09.782929: Epoch 721\n",
      "2025-12-17 06:42:09.782929: Current learning rate: 0.00317\n",
      "2025-12-17 06:44:27.795993: train_loss -0.8548\n",
      "2025-12-17 06:44:27.795993: val_loss -0.8794\n",
      "2025-12-17 06:44:27.807750: Pseudo dice [0.9298, 0.9578, 0.9419]\n",
      "2025-12-17 06:44:27.813709: Epoch time: 138.01 s\n",
      "2025-12-17 06:44:28.799843: \n",
      "2025-12-17 06:44:28.799843: Epoch 722\n",
      "2025-12-17 06:44:28.803052: Current learning rate: 0.00316\n",
      "2025-12-17 06:46:46.822498: train_loss -0.8542\n",
      "2025-12-17 06:46:46.824501: val_loss -0.8604\n",
      "2025-12-17 06:46:46.824501: Pseudo dice [0.9174, 0.9493, 0.9311]\n",
      "2025-12-17 06:46:46.832438: Epoch time: 138.04 s\n",
      "2025-12-17 06:46:47.520252: \n",
      "2025-12-17 06:46:47.520252: Epoch 723\n",
      "2025-12-17 06:46:47.523752: Current learning rate: 0.00315\n",
      "2025-12-17 06:49:05.405571: train_loss -0.8506\n",
      "2025-12-17 06:49:05.407573: val_loss -0.8698\n",
      "2025-12-17 06:49:05.413158: Pseudo dice [0.9239, 0.9534, 0.9342]\n",
      "2025-12-17 06:49:05.417162: Epoch time: 137.89 s\n",
      "2025-12-17 06:49:06.073475: \n",
      "2025-12-17 06:49:06.073475: Epoch 724\n",
      "2025-12-17 06:49:06.075477: Current learning rate: 0.00314\n",
      "2025-12-17 06:51:23.967473: train_loss -0.8538\n",
      "2025-12-17 06:51:23.967473: val_loss -0.8724\n",
      "2025-12-17 06:51:23.975483: Pseudo dice [0.9276, 0.9534, 0.9345]\n",
      "2025-12-17 06:51:23.980490: Epoch time: 137.89 s\n",
      "2025-12-17 06:51:24.766684: \n",
      "2025-12-17 06:51:24.766684: Epoch 725\n",
      "2025-12-17 06:51:24.782555: Current learning rate: 0.00313\n",
      "2025-12-17 06:53:42.695747: train_loss -0.8592\n",
      "2025-12-17 06:53:42.695747: val_loss -0.8733\n",
      "2025-12-17 06:53:42.711426: Pseudo dice [0.9278, 0.955, 0.9301]\n",
      "2025-12-17 06:53:42.711426: Epoch time: 137.93 s\n",
      "2025-12-17 06:53:43.360950: \n",
      "2025-12-17 06:53:43.360950: Epoch 726\n",
      "2025-12-17 06:53:43.360950: Current learning rate: 0.00312\n",
      "2025-12-17 06:56:01.394023: train_loss -0.8523\n",
      "2025-12-17 06:56:01.394023: val_loss -0.8779\n",
      "2025-12-17 06:56:01.404653: Pseudo dice [0.9295, 0.9584, 0.9335]\n",
      "2025-12-17 06:56:01.408656: Epoch time: 138.03 s\n",
      "2025-12-17 06:56:02.091831: \n",
      "2025-12-17 06:56:02.091831: Epoch 727\n",
      "2025-12-17 06:56:02.107609: Current learning rate: 0.00311\n",
      "2025-12-17 06:58:20.071718: train_loss -0.8499\n",
      "2025-12-17 06:58:20.071718: val_loss -0.8713\n",
      "2025-12-17 06:58:20.079727: Pseudo dice [0.9237, 0.9568, 0.9341]\n",
      "2025-12-17 06:58:20.085472: Epoch time: 137.98 s\n",
      "2025-12-17 06:58:20.873423: \n",
      "2025-12-17 06:58:20.873423: Epoch 728\n",
      "2025-12-17 06:58:20.889311: Current learning rate: 0.0031\n",
      "2025-12-17 07:00:38.850282: train_loss -0.8471\n",
      "2025-12-17 07:00:38.850282: val_loss -0.8755\n",
      "2025-12-17 07:00:38.850282: Pseudo dice [0.9302, 0.9565, 0.9384]\n",
      "2025-12-17 07:00:38.850282: Epoch time: 137.98 s\n",
      "2025-12-17 07:00:39.562065: \n",
      "2025-12-17 07:00:39.562065: Epoch 729\n",
      "2025-12-17 07:00:39.562065: Current learning rate: 0.00309\n",
      "2025-12-17 07:02:57.627130: train_loss -0.842\n",
      "2025-12-17 07:02:57.627130: val_loss -0.8627\n",
      "2025-12-17 07:02:57.627130: Pseudo dice [0.9182, 0.9503, 0.9383]\n",
      "2025-12-17 07:02:57.637363: Epoch time: 138.07 s\n",
      "2025-12-17 07:02:58.281868: \n",
      "2025-12-17 07:02:58.281868: Epoch 730\n",
      "2025-12-17 07:02:58.283610: Current learning rate: 0.00308\n",
      "2025-12-17 07:05:16.327221: train_loss -0.8531\n",
      "2025-12-17 07:05:16.327221: val_loss -0.8671\n",
      "2025-12-17 07:05:16.335231: Pseudo dice [0.9247, 0.9524, 0.9341]\n",
      "2025-12-17 07:05:16.342978: Epoch time: 138.05 s\n",
      "2025-12-17 07:05:17.105987: \n",
      "2025-12-17 07:05:17.105987: Epoch 731\n",
      "2025-12-17 07:05:17.105987: Current learning rate: 0.00307\n",
      "2025-12-17 07:07:35.104692: train_loss -0.849\n",
      "2025-12-17 07:07:35.104692: val_loss -0.8701\n",
      "2025-12-17 07:07:35.115707: Pseudo dice [0.9237, 0.9542, 0.9335]\n",
      "2025-12-17 07:07:35.120714: Epoch time: 138.0 s\n",
      "2025-12-17 07:07:35.776943: \n",
      "2025-12-17 07:07:35.776943: Epoch 732\n",
      "2025-12-17 07:07:35.781507: Current learning rate: 0.00306\n",
      "2025-12-17 07:09:54.290355: train_loss -0.856\n",
      "2025-12-17 07:09:54.292096: val_loss -0.8682\n",
      "2025-12-17 07:09:54.296502: Pseudo dice [0.9218, 0.9519, 0.9318]\n",
      "2025-12-17 07:09:54.300506: Epoch time: 138.51 s\n",
      "2025-12-17 07:09:54.955899: \n",
      "2025-12-17 07:09:54.955899: Epoch 733\n",
      "2025-12-17 07:09:54.957901: Current learning rate: 0.00305\n",
      "2025-12-17 07:12:12.981962: train_loss -0.8561\n",
      "2025-12-17 07:12:12.983964: val_loss -0.8671\n",
      "2025-12-17 07:12:12.989971: Pseudo dice [0.9225, 0.9523, 0.9372]\n",
      "2025-12-17 07:12:12.993976: Epoch time: 138.03 s\n",
      "2025-12-17 07:12:13.933045: \n",
      "2025-12-17 07:12:13.933045: Epoch 734\n",
      "2025-12-17 07:12:13.938875: Current learning rate: 0.00304\n",
      "2025-12-17 07:14:31.935041: train_loss -0.8558\n",
      "2025-12-17 07:14:31.935041: val_loss -0.8773\n",
      "2025-12-17 07:14:31.941047: Pseudo dice [0.9247, 0.9598, 0.9377]\n",
      "2025-12-17 07:14:31.941047: Epoch time: 138.0 s\n",
      "2025-12-17 07:14:32.605740: \n",
      "2025-12-17 07:14:32.605740: Epoch 735\n",
      "2025-12-17 07:14:32.605740: Current learning rate: 0.00303\n",
      "2025-12-17 07:16:50.657426: train_loss -0.8501\n",
      "2025-12-17 07:16:50.657426: val_loss -0.8595\n",
      "2025-12-17 07:16:50.663280: Pseudo dice [0.9166, 0.9481, 0.936]\n",
      "2025-12-17 07:16:50.667284: Epoch time: 138.05 s\n",
      "2025-12-17 07:16:51.312850: \n",
      "2025-12-17 07:16:51.312850: Epoch 736\n",
      "2025-12-17 07:16:51.328608: Current learning rate: 0.00302\n",
      "2025-12-17 07:19:09.243991: train_loss -0.8507\n",
      "2025-12-17 07:19:09.245731: val_loss -0.8762\n",
      "2025-12-17 07:19:09.253745: Pseudo dice [0.9271, 0.9557, 0.9401]\n",
      "2025-12-17 07:19:09.261495: Epoch time: 137.93 s\n",
      "2025-12-17 07:19:09.992947: \n",
      "2025-12-17 07:19:09.992947: Epoch 737\n",
      "2025-12-17 07:19:09.998956: Current learning rate: 0.00301\n",
      "2025-12-17 07:21:28.050951: train_loss -0.8509\n",
      "2025-12-17 07:21:28.050951: val_loss -0.8827\n",
      "2025-12-17 07:21:28.066647: Pseudo dice [0.9289, 0.9643, 0.9339]\n",
      "2025-12-17 07:21:28.068490: Epoch time: 138.06 s\n",
      "2025-12-17 07:21:28.714057: \n",
      "2025-12-17 07:21:28.714057: Epoch 738\n",
      "2025-12-17 07:21:28.731775: Current learning rate: 0.003\n",
      "2025-12-17 07:23:46.871770: train_loss -0.8549\n",
      "2025-12-17 07:23:46.871770: val_loss -0.8752\n",
      "2025-12-17 07:23:46.880935: Pseudo dice [0.9283, 0.9579, 0.9366]\n",
      "2025-12-17 07:23:46.886943: Epoch time: 138.16 s\n",
      "2025-12-17 07:23:47.535396: \n",
      "2025-12-17 07:23:47.535396: Epoch 739\n",
      "2025-12-17 07:23:47.551230: Current learning rate: 0.00299\n",
      "2025-12-17 07:26:05.510138: train_loss -0.8529\n",
      "2025-12-17 07:26:05.510138: val_loss -0.8712\n",
      "2025-12-17 07:26:05.525825: Pseudo dice [0.9223, 0.9569, 0.9309]\n",
      "2025-12-17 07:26:05.534628: Epoch time: 137.97 s\n",
      "2025-12-17 07:26:06.475115: \n",
      "2025-12-17 07:26:06.475115: Epoch 740\n",
      "2025-12-17 07:26:06.481226: Current learning rate: 0.00297\n",
      "2025-12-17 07:28:24.542363: train_loss -0.8524\n",
      "2025-12-17 07:28:24.542363: val_loss -0.8789\n",
      "2025-12-17 07:28:24.542363: Pseudo dice [0.9292, 0.959, 0.9439]\n",
      "2025-12-17 07:28:24.555936: Epoch time: 138.07 s\n",
      "2025-12-17 07:28:25.192135: \n",
      "2025-12-17 07:28:25.192135: Epoch 741\n",
      "2025-12-17 07:28:25.204360: Current learning rate: 0.00296\n",
      "2025-12-17 07:30:43.154201: train_loss -0.8569\n",
      "2025-12-17 07:30:43.154201: val_loss -0.876\n",
      "2025-12-17 07:30:43.170094: Pseudo dice [0.927, 0.9578, 0.9367]\n",
      "2025-12-17 07:30:43.170094: Epoch time: 137.96 s\n",
      "2025-12-17 07:30:43.834760: \n",
      "2025-12-17 07:30:43.834760: Epoch 742\n",
      "2025-12-17 07:30:43.834760: Current learning rate: 0.00295\n",
      "2025-12-17 07:33:02.680707: train_loss -0.8604\n",
      "2025-12-17 07:33:02.680707: val_loss -0.8853\n",
      "2025-12-17 07:33:02.696528: Pseudo dice [0.9326, 0.9606, 0.9431]\n",
      "2025-12-17 07:33:02.696528: Epoch time: 138.85 s\n",
      "2025-12-17 07:33:03.510008: \n",
      "2025-12-17 07:33:03.510913: Epoch 743\n",
      "2025-12-17 07:33:03.515815: Current learning rate: 0.00294\n",
      "2025-12-17 07:35:22.073826: train_loss -0.8497\n",
      "2025-12-17 07:35:22.073826: val_loss -0.8696\n",
      "2025-12-17 07:35:22.089764: Pseudo dice [0.9211, 0.9537, 0.931]\n",
      "2025-12-17 07:35:22.094630: Epoch time: 138.58 s\n",
      "2025-12-17 07:35:22.734924: \n",
      "2025-12-17 07:35:22.734924: Epoch 744\n",
      "2025-12-17 07:35:22.750735: Current learning rate: 0.00293\n",
      "2025-12-17 07:37:41.051374: train_loss -0.85\n",
      "2025-12-17 07:37:41.051374: val_loss -0.8583\n",
      "2025-12-17 07:37:41.065438: Pseudo dice [0.9156, 0.9496, 0.9352]\n",
      "2025-12-17 07:37:41.065438: Epoch time: 138.32 s\n",
      "2025-12-17 07:37:41.754327: \n",
      "2025-12-17 07:37:41.754327: Epoch 745\n",
      "2025-12-17 07:37:41.760222: Current learning rate: 0.00292\n",
      "2025-12-17 07:40:00.182260: train_loss -0.8561\n",
      "2025-12-17 07:40:00.182260: val_loss -0.8765\n",
      "2025-12-17 07:40:00.182260: Pseudo dice [0.9318, 0.9577, 0.9333]\n",
      "2025-12-17 07:40:00.198361: Epoch time: 138.43 s\n",
      "2025-12-17 07:40:00.972332: \n",
      "2025-12-17 07:40:00.972332: Epoch 746\n",
      "2025-12-17 07:40:00.976738: Current learning rate: 0.00291\n",
      "2025-12-17 07:42:19.588353: train_loss -0.8577\n",
      "2025-12-17 07:42:19.588353: val_loss -0.8768\n",
      "2025-12-17 07:42:19.600652: Pseudo dice [0.9289, 0.9588, 0.9412]\n",
      "2025-12-17 07:42:19.604211: Epoch time: 138.63 s\n",
      "2025-12-17 07:42:20.238810: \n",
      "2025-12-17 07:42:20.238810: Epoch 747\n",
      "2025-12-17 07:42:20.238810: Current learning rate: 0.0029\n",
      "2025-12-17 07:44:38.370906: train_loss -0.8552\n",
      "2025-12-17 07:44:38.370906: val_loss -0.8722\n",
      "2025-12-17 07:44:38.386618: Pseudo dice [0.927, 0.9545, 0.9321]\n",
      "2025-12-17 07:44:38.388484: Epoch time: 138.13 s\n",
      "2025-12-17 07:44:39.036004: \n",
      "2025-12-17 07:44:39.036004: Epoch 748\n",
      "2025-12-17 07:44:39.036004: Current learning rate: 0.00289\n",
      "2025-12-17 07:46:57.031047: train_loss -0.8577\n",
      "2025-12-17 07:46:57.031047: val_loss -0.8766\n",
      "2025-12-17 07:46:57.031047: Pseudo dice [0.9257, 0.9559, 0.945]\n",
      "2025-12-17 07:46:57.031047: Epoch time: 138.0 s\n",
      "2025-12-17 07:46:57.855448: \n",
      "2025-12-17 07:46:57.855448: Epoch 749\n",
      "2025-12-17 07:46:57.855448: Current learning rate: 0.00288\n",
      "2025-12-17 07:49:15.790802: train_loss -0.8567\n",
      "2025-12-17 07:49:15.790802: val_loss -0.8806\n",
      "2025-12-17 07:49:15.806901: Pseudo dice [0.9303, 0.9565, 0.9428]\n",
      "2025-12-17 07:49:15.815144: Epoch time: 137.95 s\n",
      "2025-12-17 07:49:16.726214: \n",
      "2025-12-17 07:49:16.726214: Epoch 750\n",
      "2025-12-17 07:49:16.735339: Current learning rate: 0.00287\n",
      "2025-12-17 07:51:34.907009: train_loss -0.8541\n",
      "2025-12-17 07:51:34.907009: val_loss -0.8736\n",
      "2025-12-17 07:51:34.907009: Pseudo dice [0.931, 0.9609, 0.9332]\n",
      "2025-12-17 07:51:34.922957: Epoch time: 138.18 s\n",
      "2025-12-17 07:51:35.573685: \n",
      "2025-12-17 07:51:35.573685: Epoch 751\n",
      "2025-12-17 07:51:35.573685: Current learning rate: 0.00286\n",
      "2025-12-17 07:53:53.751546: train_loss -0.8578\n",
      "2025-12-17 07:53:53.751546: val_loss -0.8761\n",
      "2025-12-17 07:53:53.751546: Pseudo dice [0.9275, 0.9569, 0.9374]\n",
      "2025-12-17 07:53:53.765362: Epoch time: 138.19 s\n",
      "2025-12-17 07:53:53.772278: Yayy! New best EMA pseudo Dice: 0.9401\n",
      "2025-12-17 07:53:55.033051: \n",
      "2025-12-17 07:53:55.033051: Epoch 752\n",
      "2025-12-17 07:53:55.048968: Current learning rate: 0.00285\n",
      "2025-12-17 07:56:12.950633: train_loss -0.8574\n",
      "2025-12-17 07:56:12.950633: val_loss -0.8723\n",
      "2025-12-17 07:56:12.955095: Pseudo dice [0.9258, 0.9525, 0.9376]\n",
      "2025-12-17 07:56:12.955095: Epoch time: 137.92 s\n",
      "2025-12-17 07:56:13.604259: \n",
      "2025-12-17 07:56:13.604259: Epoch 753\n",
      "2025-12-17 07:56:13.620326: Current learning rate: 0.00284\n",
      "2025-12-17 07:58:31.685736: train_loss -0.8566\n",
      "2025-12-17 07:58:31.685736: val_loss -0.8728\n",
      "2025-12-17 07:58:31.689571: Pseudo dice [0.9217, 0.9569, 0.9354]\n",
      "2025-12-17 07:58:31.689571: Epoch time: 138.08 s\n",
      "2025-12-17 07:58:32.354322: \n",
      "2025-12-17 07:58:32.354322: Epoch 754\n",
      "2025-12-17 07:58:32.354322: Current learning rate: 0.00283\n",
      "2025-12-17 08:00:50.488382: train_loss -0.857\n",
      "2025-12-17 08:00:50.488382: val_loss -0.8706\n",
      "2025-12-17 08:00:50.488382: Pseudo dice [0.9228, 0.9546, 0.9381]\n",
      "2025-12-17 08:00:50.488382: Epoch time: 138.13 s\n",
      "2025-12-17 08:00:51.279912: \n",
      "2025-12-17 08:00:51.279912: Epoch 755\n",
      "2025-12-17 08:00:51.279912: Current learning rate: 0.00282\n",
      "2025-12-17 08:03:09.192065: train_loss -0.8525\n",
      "2025-12-17 08:03:09.192065: val_loss -0.8775\n",
      "2025-12-17 08:03:09.198071: Pseudo dice [0.9303, 0.9606, 0.9313]\n",
      "2025-12-17 08:03:09.198071: Epoch time: 137.91 s\n",
      "2025-12-17 08:03:09.847175: \n",
      "2025-12-17 08:03:09.847175: Epoch 756\n",
      "2025-12-17 08:03:09.863087: Current learning rate: 0.00281\n",
      "2025-12-17 08:05:28.022110: train_loss -0.8514\n",
      "2025-12-17 08:05:28.022110: val_loss -0.8702\n",
      "2025-12-17 08:05:28.037900: Pseudo dice [0.9269, 0.953, 0.9328]\n",
      "2025-12-17 08:05:28.037900: Epoch time: 138.17 s\n",
      "2025-12-17 08:05:28.686127: \n",
      "2025-12-17 08:05:28.686127: Epoch 757\n",
      "2025-12-17 08:05:28.702188: Current learning rate: 0.0028\n",
      "2025-12-17 08:07:46.793185: train_loss -0.8532\n",
      "2025-12-17 08:07:46.793185: val_loss -0.875\n",
      "2025-12-17 08:07:46.804323: Pseudo dice [0.9268, 0.955, 0.936]\n",
      "2025-12-17 08:07:46.808133: Epoch time: 138.11 s\n",
      "2025-12-17 08:07:47.792971: \n",
      "2025-12-17 08:07:47.792971: Epoch 758\n",
      "2025-12-17 08:07:47.799161: Current learning rate: 0.00279\n",
      "2025-12-17 08:10:06.051565: train_loss -0.8582\n",
      "2025-12-17 08:10:06.051565: val_loss -0.87\n",
      "2025-12-17 08:10:06.067603: Pseudo dice [0.9188, 0.9543, 0.9379]\n",
      "2025-12-17 08:10:06.067603: Epoch time: 138.26 s\n",
      "2025-12-17 08:10:06.718068: \n",
      "2025-12-17 08:10:06.718068: Epoch 759\n",
      "2025-12-17 08:10:06.718068: Current learning rate: 0.00278\n",
      "2025-12-17 08:12:24.669481: train_loss -0.8585\n",
      "2025-12-17 08:12:24.669481: val_loss -0.8691\n",
      "2025-12-17 08:12:24.685600: Pseudo dice [0.9241, 0.9523, 0.9409]\n",
      "2025-12-17 08:12:24.694561: Epoch time: 137.95 s\n",
      "2025-12-17 08:12:25.349388: \n",
      "2025-12-17 08:12:25.349388: Epoch 760\n",
      "2025-12-17 08:12:25.349388: Current learning rate: 0.00277\n",
      "2025-12-17 08:14:43.399189: train_loss -0.8613\n",
      "2025-12-17 08:14:43.400930: val_loss -0.8788\n",
      "2025-12-17 08:14:43.412955: Pseudo dice [0.9277, 0.9564, 0.9406]\n",
      "2025-12-17 08:14:43.418700: Epoch time: 138.07 s\n",
      "2025-12-17 08:14:44.208572: \n",
      "2025-12-17 08:14:44.208572: Epoch 761\n",
      "2025-12-17 08:14:44.208572: Current learning rate: 0.00276\n",
      "2025-12-17 08:17:02.198362: train_loss -0.8601\n",
      "2025-12-17 08:17:02.198362: val_loss -0.8699\n",
      "2025-12-17 08:17:02.198362: Pseudo dice [0.9243, 0.9502, 0.9351]\n",
      "2025-12-17 08:17:02.218374: Epoch time: 137.99 s\n",
      "2025-12-17 08:17:02.863680: \n",
      "2025-12-17 08:17:02.863680: Epoch 762\n",
      "2025-12-17 08:17:02.883015: Current learning rate: 0.00275\n",
      "2025-12-17 08:19:20.854955: train_loss -0.8526\n",
      "2025-12-17 08:19:20.854955: val_loss -0.879\n",
      "2025-12-17 08:19:20.862972: Pseudo dice [0.9293, 0.9597, 0.9367]\n",
      "2025-12-17 08:19:20.870720: Epoch time: 137.99 s\n",
      "2025-12-17 08:19:21.565834: \n",
      "2025-12-17 08:19:21.565834: Epoch 763\n",
      "2025-12-17 08:19:21.577203: Current learning rate: 0.00274\n",
      "2025-12-17 08:21:39.275389: train_loss -0.8494\n",
      "2025-12-17 08:21:39.277391: val_loss -0.8634\n",
      "2025-12-17 08:21:39.279393: Pseudo dice [0.9242, 0.9502, 0.9283]\n",
      "2025-12-17 08:21:39.289404: Epoch time: 137.71 s\n",
      "2025-12-17 08:21:40.157630: \n",
      "2025-12-17 08:21:40.157630: Epoch 764\n",
      "2025-12-17 08:21:40.157630: Current learning rate: 0.00273\n",
      "2025-12-17 08:23:58.090352: train_loss -0.8534\n",
      "2025-12-17 08:23:58.092357: val_loss -0.8787\n",
      "2025-12-17 08:23:58.100368: Pseudo dice [0.9288, 0.9587, 0.9385]\n",
      "2025-12-17 08:23:58.106114: Epoch time: 137.93 s\n",
      "2025-12-17 08:23:58.768577: \n",
      "2025-12-17 08:23:58.768577: Epoch 765\n",
      "2025-12-17 08:23:58.768577: Current learning rate: 0.00272\n",
      "2025-12-17 08:26:16.542268: train_loss -0.8579\n",
      "2025-12-17 08:26:16.544271: val_loss -0.8679\n",
      "2025-12-17 08:26:16.544271: Pseudo dice [0.9228, 0.9511, 0.9289]\n",
      "2025-12-17 08:26:16.552784: Epoch time: 137.77 s\n",
      "2025-12-17 08:26:17.279422: \n",
      "2025-12-17 08:26:17.279422: Epoch 766\n",
      "2025-12-17 08:26:17.291475: Current learning rate: 0.00271\n",
      "2025-12-17 08:28:35.149895: train_loss -0.8608\n",
      "2025-12-17 08:28:35.149895: val_loss -0.8845\n",
      "2025-12-17 08:28:35.149895: Pseudo dice [0.9322, 0.9609, 0.94]\n",
      "2025-12-17 08:28:35.165080: Epoch time: 137.87 s\n",
      "2025-12-17 08:28:35.816694: \n",
      "2025-12-17 08:28:35.816694: Epoch 767\n",
      "2025-12-17 08:28:35.822672: Current learning rate: 0.0027\n",
      "2025-12-17 08:30:53.882058: train_loss -0.857\n",
      "2025-12-17 08:30:53.884060: val_loss -0.8785\n",
      "2025-12-17 08:30:53.885800: Pseudo dice [0.9265, 0.9585, 0.9451]\n",
      "2025-12-17 08:30:53.885800: Epoch time: 138.08 s\n",
      "2025-12-17 08:30:54.553230: \n",
      "2025-12-17 08:30:54.553230: Epoch 768\n",
      "2025-12-17 08:30:54.553230: Current learning rate: 0.00268\n",
      "2025-12-17 08:33:12.384436: train_loss -0.8593\n",
      "2025-12-17 08:33:12.386176: val_loss -0.8862\n",
      "2025-12-17 08:33:12.394186: Pseudo dice [0.9362, 0.963, 0.9388]\n",
      "2025-12-17 08:33:12.400194: Epoch time: 137.83 s\n",
      "2025-12-17 08:33:12.403937: Yayy! New best EMA pseudo Dice: 0.9403\n",
      "2025-12-17 08:33:13.398932: \n",
      "2025-12-17 08:33:13.398932: Epoch 769\n",
      "2025-12-17 08:33:13.398932: Current learning rate: 0.00267\n",
      "2025-12-17 08:35:31.524671: train_loss -0.8544\n",
      "2025-12-17 08:35:31.524671: val_loss -0.8786\n",
      "2025-12-17 08:35:31.530236: Pseudo dice [0.9297, 0.9617, 0.934]\n",
      "2025-12-17 08:35:31.535980: Epoch time: 138.13 s\n",
      "2025-12-17 08:35:31.539984: Yayy! New best EMA pseudo Dice: 0.9405\n",
      "2025-12-17 08:35:32.706048: \n",
      "2025-12-17 08:35:32.706048: Epoch 770\n",
      "2025-12-17 08:35:32.715571: Current learning rate: 0.00266\n",
      "2025-12-17 08:37:50.710016: train_loss -0.8554\n",
      "2025-12-17 08:37:50.710016: val_loss -0.8742\n",
      "2025-12-17 08:37:50.715307: Pseudo dice [0.9246, 0.955, 0.9372]\n",
      "2025-12-17 08:37:50.721313: Epoch time: 138.0 s\n",
      "2025-12-17 08:37:51.374452: \n",
      "2025-12-17 08:37:51.374452: Epoch 771\n",
      "2025-12-17 08:37:51.390361: Current learning rate: 0.00265\n",
      "2025-12-17 08:40:09.421940: train_loss -0.8552\n",
      "2025-12-17 08:40:09.421940: val_loss -0.8777\n",
      "2025-12-17 08:40:09.421940: Pseudo dice [0.9278, 0.9566, 0.9415]\n",
      "2025-12-17 08:40:09.421940: Epoch time: 138.05 s\n",
      "2025-12-17 08:40:09.437976: Yayy! New best EMA pseudo Dice: 0.9405\n",
      "2025-12-17 08:40:10.419434: \n",
      "2025-12-17 08:40:10.419434: Epoch 772\n",
      "2025-12-17 08:40:10.419434: Current learning rate: 0.00264\n",
      "2025-12-17 08:42:28.389662: train_loss -0.8553\n",
      "2025-12-17 08:42:28.389662: val_loss -0.8767\n",
      "2025-12-17 08:42:28.404176: Pseudo dice [0.9298, 0.9615, 0.9361]\n",
      "2025-12-17 08:42:28.409178: Epoch time: 137.97 s\n",
      "2025-12-17 08:42:28.413183: Yayy! New best EMA pseudo Dice: 0.9407\n",
      "2025-12-17 08:42:29.348253: \n",
      "2025-12-17 08:42:29.348253: Epoch 773\n",
      "2025-12-17 08:42:29.348253: Current learning rate: 0.00263\n",
      "2025-12-17 08:44:47.339545: train_loss -0.8468\n",
      "2025-12-17 08:44:47.344310: val_loss -0.8583\n",
      "2025-12-17 08:44:47.350316: Pseudo dice [0.9203, 0.9504, 0.934]\n",
      "2025-12-17 08:44:47.354320: Epoch time: 137.99 s\n",
      "2025-12-17 08:44:48.003407: \n",
      "2025-12-17 08:44:48.003407: Epoch 774\n",
      "2025-12-17 08:44:48.003407: Current learning rate: 0.00262\n",
      "2025-12-17 08:47:05.954117: train_loss -0.8475\n",
      "2025-12-17 08:47:05.954117: val_loss -0.8693\n",
      "2025-12-17 08:47:05.969890: Pseudo dice [0.9233, 0.9528, 0.9336]\n",
      "2025-12-17 08:47:05.969890: Epoch time: 137.95 s\n",
      "2025-12-17 08:47:06.840656: \n",
      "2025-12-17 08:47:06.842659: Epoch 775\n",
      "2025-12-17 08:47:06.842659: Current learning rate: 0.00261\n",
      "2025-12-17 08:49:24.826193: train_loss -0.8563\n",
      "2025-12-17 08:49:24.826193: val_loss -0.8755\n",
      "2025-12-17 08:49:24.835171: Pseudo dice [0.9277, 0.9573, 0.9317]\n",
      "2025-12-17 08:49:24.841178: Epoch time: 137.99 s\n",
      "2025-12-17 08:49:25.504042: \n",
      "2025-12-17 08:49:25.504042: Epoch 776\n",
      "2025-12-17 08:49:25.519368: Current learning rate: 0.0026\n",
      "2025-12-17 08:51:43.529173: train_loss -0.8606\n",
      "2025-12-17 08:51:43.529173: val_loss -0.8763\n",
      "2025-12-17 08:51:43.536067: Pseudo dice [0.9259, 0.9584, 0.9427]\n",
      "2025-12-17 08:51:43.540071: Epoch time: 138.03 s\n",
      "2025-12-17 08:51:44.250414: \n",
      "2025-12-17 08:51:44.250414: Epoch 777\n",
      "2025-12-17 08:51:44.254900: Current learning rate: 0.00259\n",
      "2025-12-17 08:54:02.194286: train_loss -0.858\n",
      "2025-12-17 08:54:02.196289: val_loss -0.8776\n",
      "2025-12-17 08:54:02.201862: Pseudo dice [0.9288, 0.9603, 0.9347]\n",
      "2025-12-17 08:54:02.205868: Epoch time: 137.95 s\n",
      "2025-12-17 08:54:02.917298: \n",
      "2025-12-17 08:54:02.917298: Epoch 778\n",
      "2025-12-17 08:54:02.917298: Current learning rate: 0.00258\n",
      "2025-12-17 08:56:21.004160: train_loss -0.8543\n",
      "2025-12-17 08:56:21.006162: val_loss -0.8761\n",
      "2025-12-17 08:56:21.012148: Pseudo dice [0.9262, 0.9558, 0.9364]\n",
      "2025-12-17 08:56:21.016152: Epoch time: 138.09 s\n",
      "2025-12-17 08:56:21.681341: \n",
      "2025-12-17 08:56:21.681341: Epoch 779\n",
      "2025-12-17 08:56:21.684152: Current learning rate: 0.00257\n",
      "2025-12-17 08:58:39.816616: train_loss -0.8582\n",
      "2025-12-17 08:58:39.816616: val_loss -0.8733\n",
      "2025-12-17 08:58:39.824624: Pseudo dice [0.9232, 0.9558, 0.9401]\n",
      "2025-12-17 08:58:39.828366: Epoch time: 138.14 s\n",
      "2025-12-17 08:58:40.507285: \n",
      "2025-12-17 08:58:40.509287: Epoch 780\n",
      "2025-12-17 08:58:40.515294: Current learning rate: 0.00256\n",
      "2025-12-17 09:00:58.380043: train_loss -0.8609\n",
      "2025-12-17 09:00:58.382045: val_loss -0.8821\n",
      "2025-12-17 09:00:58.388051: Pseudo dice [0.9293, 0.9629, 0.9396]\n",
      "2025-12-17 09:00:58.393636: Epoch time: 137.87 s\n",
      "2025-12-17 09:00:59.366683: \n",
      "2025-12-17 09:00:59.366683: Epoch 781\n",
      "2025-12-17 09:00:59.382595: Current learning rate: 0.00255\n",
      "2025-12-17 09:03:17.253428: train_loss -0.8608\n",
      "2025-12-17 09:03:17.253428: val_loss -0.8638\n",
      "2025-12-17 09:03:17.272937: Pseudo dice [0.9192, 0.9521, 0.936]\n",
      "2025-12-17 09:03:17.276942: Epoch time: 137.89 s\n",
      "2025-12-17 09:03:17.994596: \n",
      "2025-12-17 09:03:17.994596: Epoch 782\n",
      "2025-12-17 09:03:17.994596: Current learning rate: 0.00254\n",
      "2025-12-17 09:05:36.008457: train_loss -0.8586\n",
      "2025-12-17 09:05:36.010460: val_loss -0.8665\n",
      "2025-12-17 09:05:36.017226: Pseudo dice [0.9233, 0.9563, 0.929]\n",
      "2025-12-17 09:05:36.024234: Epoch time: 138.01 s\n",
      "2025-12-17 09:05:36.671821: \n",
      "2025-12-17 09:05:36.671821: Epoch 783\n",
      "2025-12-17 09:05:36.689645: Current learning rate: 0.00253\n",
      "2025-12-17 09:07:54.690593: train_loss -0.8607\n",
      "2025-12-17 09:07:54.690593: val_loss -0.88\n",
      "2025-12-17 09:07:54.696602: Pseudo dice [0.9275, 0.9615, 0.94]\n",
      "2025-12-17 09:07:54.702611: Epoch time: 138.02 s\n",
      "2025-12-17 09:07:55.497833: \n",
      "2025-12-17 09:07:55.497833: Epoch 784\n",
      "2025-12-17 09:07:55.509798: Current learning rate: 0.00252\n",
      "2025-12-17 09:10:13.733655: train_loss -0.8579\n",
      "2025-12-17 09:10:13.733655: val_loss -0.8687\n",
      "2025-12-17 09:10:13.745028: Pseudo dice [0.9223, 0.9543, 0.9381]\n",
      "2025-12-17 09:10:13.749032: Epoch time: 138.24 s\n",
      "2025-12-17 09:10:14.411351: \n",
      "2025-12-17 09:10:14.411351: Epoch 785\n",
      "2025-12-17 09:10:14.411351: Current learning rate: 0.00251\n",
      "2025-12-17 09:12:32.352041: train_loss -0.8585\n",
      "2025-12-17 09:12:32.352041: val_loss -0.8772\n",
      "2025-12-17 09:12:32.373406: Pseudo dice [0.928, 0.9578, 0.9396]\n",
      "2025-12-17 09:12:32.377411: Epoch time: 137.94 s\n",
      "2025-12-17 09:12:33.041726: \n",
      "2025-12-17 09:12:33.041726: Epoch 786\n",
      "2025-12-17 09:12:33.041726: Current learning rate: 0.0025\n",
      "2025-12-17 09:14:51.096663: train_loss -0.8581\n",
      "2025-12-17 09:14:51.096663: val_loss -0.8777\n",
      "2025-12-17 09:14:51.102612: Pseudo dice [0.9262, 0.9608, 0.9388]\n",
      "2025-12-17 09:14:51.106617: Epoch time: 138.06 s\n",
      "2025-12-17 09:14:52.076708: \n",
      "2025-12-17 09:14:52.076708: Epoch 787\n",
      "2025-12-17 09:14:52.076708: Current learning rate: 0.00249\n",
      "2025-12-17 09:17:10.186805: train_loss -0.855\n",
      "2025-12-17 09:17:10.188807: val_loss -0.8697\n",
      "2025-12-17 09:17:10.188807: Pseudo dice [0.9229, 0.9529, 0.9375]\n",
      "2025-12-17 09:17:10.188807: Epoch time: 138.11 s\n",
      "2025-12-17 09:17:10.854235: \n",
      "2025-12-17 09:17:10.854235: Epoch 788\n",
      "2025-12-17 09:17:10.858954: Current learning rate: 0.00248\n",
      "2025-12-17 09:19:28.831073: train_loss -0.8593\n",
      "2025-12-17 09:19:28.833075: val_loss -0.878\n",
      "2025-12-17 09:19:28.840039: Pseudo dice [0.9281, 0.9589, 0.94]\n",
      "2025-12-17 09:19:28.843781: Epoch time: 137.98 s\n",
      "2025-12-17 09:19:29.508080: \n",
      "2025-12-17 09:19:29.508080: Epoch 789\n",
      "2025-12-17 09:19:29.508080: Current learning rate: 0.00247\n",
      "2025-12-17 09:21:47.573198: train_loss -0.8527\n",
      "2025-12-17 09:21:47.573198: val_loss -0.8791\n",
      "2025-12-17 09:21:47.582704: Pseudo dice [0.9305, 0.9552, 0.9396]\n",
      "2025-12-17 09:21:47.586708: Epoch time: 138.07 s\n",
      "2025-12-17 09:21:48.347221: \n",
      "2025-12-17 09:21:48.347221: Epoch 790\n",
      "2025-12-17 09:21:48.366520: Current learning rate: 0.00245\n",
      "2025-12-17 09:24:06.397331: train_loss -0.8586\n",
      "2025-12-17 09:24:06.399333: val_loss -0.8775\n",
      "2025-12-17 09:24:06.405079: Pseudo dice [0.9312, 0.9593, 0.9343]\n",
      "2025-12-17 09:24:06.409878: Epoch time: 138.05 s\n",
      "2025-12-17 09:24:07.072896: \n",
      "2025-12-17 09:24:07.072896: Epoch 791\n",
      "2025-12-17 09:24:07.072896: Current learning rate: 0.00244\n",
      "2025-12-17 09:26:25.162376: train_loss -0.8574\n",
      "2025-12-17 09:26:25.162376: val_loss -0.8642\n",
      "2025-12-17 09:26:25.170384: Pseudo dice [0.9203, 0.9509, 0.9355]\n",
      "2025-12-17 09:26:25.176390: Epoch time: 138.09 s\n",
      "2025-12-17 09:26:25.835412: \n",
      "2025-12-17 09:26:25.835412: Epoch 792\n",
      "2025-12-17 09:26:25.835412: Current learning rate: 0.00243\n",
      "2025-12-17 09:28:43.875692: train_loss -0.8543\n",
      "2025-12-17 09:28:43.875692: val_loss -0.8827\n",
      "2025-12-17 09:28:43.882980: Pseudo dice [0.9319, 0.96, 0.9389]\n",
      "2025-12-17 09:28:43.888987: Epoch time: 138.04 s\n",
      "2025-12-17 09:28:44.903265: \n",
      "2025-12-17 09:28:44.903265: Epoch 793\n",
      "2025-12-17 09:28:44.921285: Current learning rate: 0.00242\n",
      "2025-12-17 09:31:02.959103: train_loss -0.8562\n",
      "2025-12-17 09:31:02.959103: val_loss -0.8738\n",
      "2025-12-17 09:31:02.966850: Pseudo dice [0.927, 0.9579, 0.9369]\n",
      "2025-12-17 09:31:02.974858: Epoch time: 138.06 s\n",
      "2025-12-17 09:31:03.624922: \n",
      "2025-12-17 09:31:03.624922: Epoch 794\n",
      "2025-12-17 09:31:03.624922: Current learning rate: 0.00241\n",
      "2025-12-17 09:33:21.646036: train_loss -0.8637\n",
      "2025-12-17 09:33:21.646036: val_loss -0.8787\n",
      "2025-12-17 09:33:21.653781: Pseudo dice [0.9302, 0.9575, 0.9356]\n",
      "2025-12-17 09:33:21.657786: Epoch time: 138.02 s\n",
      "2025-12-17 09:33:22.321352: \n",
      "2025-12-17 09:33:22.321352: Epoch 795\n",
      "2025-12-17 09:33:22.321352: Current learning rate: 0.0024\n",
      "2025-12-17 09:35:40.381273: train_loss -0.858\n",
      "2025-12-17 09:35:40.381273: val_loss -0.8843\n",
      "2025-12-17 09:35:40.389720: Pseudo dice [0.929, 0.9613, 0.944]\n",
      "2025-12-17 09:35:40.393724: Epoch time: 138.06 s\n",
      "2025-12-17 09:35:40.397228: Yayy! New best EMA pseudo Dice: 0.9408\n",
      "2025-12-17 09:35:41.454340: \n",
      "2025-12-17 09:35:41.454340: Epoch 796\n",
      "2025-12-17 09:35:41.465598: Current learning rate: 0.00239\n",
      "2025-12-17 09:37:59.486953: train_loss -0.8558\n",
      "2025-12-17 09:37:59.486953: val_loss -0.877\n",
      "2025-12-17 09:37:59.500250: Pseudo dice [0.9286, 0.9587, 0.9416]\n",
      "2025-12-17 09:37:59.506897: Epoch time: 138.03 s\n",
      "2025-12-17 09:37:59.510902: Yayy! New best EMA pseudo Dice: 0.9411\n",
      "2025-12-17 09:38:00.463704: \n",
      "2025-12-17 09:38:00.463704: Epoch 797\n",
      "2025-12-17 09:38:00.479491: Current learning rate: 0.00238\n",
      "2025-12-17 09:40:18.380634: train_loss -0.8601\n",
      "2025-12-17 09:40:18.382637: val_loss -0.874\n",
      "2025-12-17 09:40:18.387644: Pseudo dice [0.9243, 0.9543, 0.9338]\n",
      "2025-12-17 09:40:18.393524: Epoch time: 137.92 s\n",
      "2025-12-17 09:40:19.113399: \n",
      "2025-12-17 09:40:19.113399: Epoch 798\n",
      "2025-12-17 09:40:19.129200: Current learning rate: 0.00237\n",
      "2025-12-17 09:42:37.031454: train_loss -0.8598\n",
      "2025-12-17 09:42:37.031454: val_loss -0.8825\n",
      "2025-12-17 09:42:37.038878: Pseudo dice [0.9267, 0.9592, 0.9461]\n",
      "2025-12-17 09:42:37.044884: Epoch time: 137.92 s\n",
      "2025-12-17 09:42:38.040444: \n",
      "2025-12-17 09:42:38.040444: Epoch 799\n",
      "2025-12-17 09:42:38.046219: Current learning rate: 0.00236\n",
      "2025-12-17 09:44:56.024779: train_loss -0.8572\n",
      "2025-12-17 09:44:56.026782: val_loss -0.8679\n",
      "2025-12-17 09:44:56.033462: Pseudo dice [0.9206, 0.9548, 0.9378]\n",
      "2025-12-17 09:44:56.037465: Epoch time: 137.99 s\n",
      "2025-12-17 09:44:56.963174: \n",
      "2025-12-17 09:44:56.963174: Epoch 800\n",
      "2025-12-17 09:44:56.963174: Current learning rate: 0.00235\n",
      "2025-12-17 09:47:14.880645: train_loss -0.8563\n",
      "2025-12-17 09:47:14.880645: val_loss -0.8814\n",
      "2025-12-17 09:47:14.896456: Pseudo dice [0.93, 0.9582, 0.9391]\n",
      "2025-12-17 09:47:14.896456: Epoch time: 137.92 s\n",
      "2025-12-17 09:47:15.605057: \n",
      "2025-12-17 09:47:15.608555: Epoch 801\n",
      "2025-12-17 09:47:15.608555: Current learning rate: 0.00234\n",
      "2025-12-17 09:49:33.537828: train_loss -0.8577\n",
      "2025-12-17 09:49:33.537828: val_loss -0.874\n",
      "2025-12-17 09:49:33.543833: Pseudo dice [0.9229, 0.9589, 0.9404]\n",
      "2025-12-17 09:49:33.543833: Epoch time: 137.93 s\n",
      "2025-12-17 09:49:34.331638: \n",
      "2025-12-17 09:49:34.331638: Epoch 802\n",
      "2025-12-17 09:49:34.337141: Current learning rate: 0.00233\n",
      "2025-12-17 09:51:52.406989: train_loss -0.8571\n",
      "2025-12-17 09:51:52.406989: val_loss -0.8678\n",
      "2025-12-17 09:51:52.419855: Pseudo dice [0.9177, 0.9515, 0.9466]\n",
      "2025-12-17 09:51:52.424860: Epoch time: 138.09 s\n",
      "2025-12-17 09:51:53.090289: \n",
      "2025-12-17 09:51:53.090289: Epoch 803\n",
      "2025-12-17 09:51:53.090289: Current learning rate: 0.00232\n",
      "2025-12-17 09:54:11.130482: train_loss -0.8569\n",
      "2025-12-17 09:54:11.130482: val_loss -0.8785\n",
      "2025-12-17 09:54:11.136378: Pseudo dice [0.9295, 0.9601, 0.9323]\n",
      "2025-12-17 09:54:11.142384: Epoch time: 138.06 s\n",
      "2025-12-17 09:54:11.798340: \n",
      "2025-12-17 09:54:11.798340: Epoch 804\n",
      "2025-12-17 09:54:11.814253: Current learning rate: 0.00231\n",
      "2025-12-17 09:56:29.736296: train_loss -0.8619\n",
      "2025-12-17 09:56:29.736296: val_loss -0.8786\n",
      "2025-12-17 09:56:29.743907: Pseudo dice [0.929, 0.9596, 0.9365]\n",
      "2025-12-17 09:56:29.749913: Epoch time: 137.94 s\n",
      "2025-12-17 09:56:30.688726: \n",
      "2025-12-17 09:56:30.688726: Epoch 805\n",
      "2025-12-17 09:56:30.699244: Current learning rate: 0.0023\n",
      "2025-12-17 09:58:48.666000: train_loss -0.8589\n",
      "2025-12-17 09:58:48.666000: val_loss -0.8857\n",
      "2025-12-17 09:58:48.670222: Pseudo dice [0.9319, 0.96, 0.9481]\n",
      "2025-12-17 09:58:48.677761: Epoch time: 137.98 s\n",
      "2025-12-17 09:58:48.681764: Yayy! New best EMA pseudo Dice: 0.9413\n",
      "2025-12-17 09:58:49.633919: \n",
      "2025-12-17 09:58:49.633919: Epoch 806\n",
      "2025-12-17 09:58:49.633919: Current learning rate: 0.00229\n",
      "2025-12-17 10:01:07.617384: train_loss -0.8601\n",
      "2025-12-17 10:01:07.617384: val_loss -0.8701\n",
      "2025-12-17 10:01:07.621651: Pseudo dice [0.9246, 0.952, 0.9378]\n",
      "2025-12-17 10:01:07.621651: Epoch time: 137.98 s\n",
      "2025-12-17 10:01:08.282780: \n",
      "2025-12-17 10:01:08.282780: Epoch 807\n",
      "2025-12-17 10:01:08.282780: Current learning rate: 0.00228\n",
      "2025-12-17 10:03:26.408457: train_loss -0.8561\n",
      "2025-12-17 10:03:26.408457: val_loss -0.8953\n",
      "2025-12-17 10:03:26.422224: Pseudo dice [0.9395, 0.9653, 0.9412]\n",
      "2025-12-17 10:03:26.430234: Epoch time: 138.13 s\n",
      "2025-12-17 10:03:26.435978: Yayy! New best EMA pseudo Dice: 0.9418\n",
      "2025-12-17 10:03:27.511124: \n",
      "2025-12-17 10:03:27.511124: Epoch 808\n",
      "2025-12-17 10:03:27.516173: Current learning rate: 0.00226\n",
      "2025-12-17 10:05:45.639707: train_loss -0.8599\n",
      "2025-12-17 10:05:45.639707: val_loss -0.8825\n",
      "2025-12-17 10:05:45.648324: Pseudo dice [0.9301, 0.96, 0.9404]\n",
      "2025-12-17 10:05:45.653830: Epoch time: 138.13 s\n",
      "2025-12-17 10:05:45.661075: Yayy! New best EMA pseudo Dice: 0.942\n",
      "2025-12-17 10:05:46.625424: \n",
      "2025-12-17 10:05:46.625424: Epoch 809\n",
      "2025-12-17 10:05:46.631170: Current learning rate: 0.00225\n",
      "2025-12-17 10:08:04.648284: train_loss -0.8627\n",
      "2025-12-17 10:08:04.650024: val_loss -0.884\n",
      "2025-12-17 10:08:04.654028: Pseudo dice [0.9345, 0.9607, 0.9389]\n",
      "2025-12-17 10:08:04.662039: Epoch time: 138.02 s\n",
      "2025-12-17 10:08:04.667848: Yayy! New best EMA pseudo Dice: 0.9422\n",
      "2025-12-17 10:08:05.809166: \n",
      "2025-12-17 10:08:05.809166: Epoch 810\n",
      "2025-12-17 10:08:05.816000: Current learning rate: 0.00224\n",
      "2025-12-17 10:10:24.182188: train_loss -0.856\n",
      "2025-12-17 10:10:24.182188: val_loss -0.8701\n",
      "2025-12-17 10:10:24.198285: Pseudo dice [0.9251, 0.9532, 0.9358]\n",
      "2025-12-17 10:10:24.198285: Epoch time: 138.37 s\n",
      "2025-12-17 10:10:24.991306: \n",
      "2025-12-17 10:10:24.991306: Epoch 811\n",
      "2025-12-17 10:10:24.991306: Current learning rate: 0.00223\n",
      "2025-12-17 10:12:43.088718: train_loss -0.861\n",
      "2025-12-17 10:12:43.088718: val_loss -0.8898\n",
      "2025-12-17 10:12:43.096725: Pseudo dice [0.932, 0.9638, 0.9434]\n",
      "2025-12-17 10:12:43.102470: Epoch time: 138.11 s\n",
      "2025-12-17 10:12:43.106396: Yayy! New best EMA pseudo Dice: 0.9423\n",
      "2025-12-17 10:12:44.037644: \n",
      "2025-12-17 10:12:44.037644: Epoch 812\n",
      "2025-12-17 10:12:44.053483: Current learning rate: 0.00222\n",
      "2025-12-17 10:15:01.958627: train_loss -0.8598\n",
      "2025-12-17 10:15:01.958627: val_loss -0.8787\n",
      "2025-12-17 10:15:01.958627: Pseudo dice [0.9266, 0.9565, 0.9455]\n",
      "2025-12-17 10:15:01.958627: Epoch time: 137.92 s\n",
      "2025-12-17 10:15:01.970711: Yayy! New best EMA pseudo Dice: 0.9423\n",
      "2025-12-17 10:15:02.913891: \n",
      "2025-12-17 10:15:02.913891: Epoch 813\n",
      "2025-12-17 10:15:02.913891: Current learning rate: 0.00221\n",
      "2025-12-17 10:17:20.859974: train_loss -0.8589\n",
      "2025-12-17 10:17:20.859974: val_loss -0.8786\n",
      "2025-12-17 10:17:20.866401: Pseudo dice [0.931, 0.9572, 0.9372]\n",
      "2025-12-17 10:17:20.872410: Epoch time: 137.95 s\n",
      "2025-12-17 10:17:21.715485: \n",
      "2025-12-17 10:17:21.715485: Epoch 814\n",
      "2025-12-17 10:17:21.715485: Current learning rate: 0.0022\n",
      "2025-12-17 10:19:39.731581: train_loss -0.8631\n",
      "2025-12-17 10:19:39.731581: val_loss -0.8907\n",
      "2025-12-17 10:19:39.738174: Pseudo dice [0.9373, 0.9633, 0.9404]\n",
      "2025-12-17 10:19:39.738174: Epoch time: 138.02 s\n",
      "2025-12-17 10:19:39.738174: Yayy! New best EMA pseudo Dice: 0.9427\n",
      "2025-12-17 10:19:40.864912: \n",
      "2025-12-17 10:19:40.864912: Epoch 815\n",
      "2025-12-17 10:19:40.869403: Current learning rate: 0.00219\n",
      "2025-12-17 10:21:58.863794: train_loss -0.8629\n",
      "2025-12-17 10:21:58.863794: val_loss -0.8792\n",
      "2025-12-17 10:21:58.882683: Pseudo dice [0.9286, 0.9557, 0.9437]\n",
      "2025-12-17 10:21:58.888691: Epoch time: 138.0 s\n",
      "2025-12-17 10:21:59.617532: \n",
      "2025-12-17 10:21:59.617532: Epoch 816\n",
      "2025-12-17 10:21:59.617532: Current learning rate: 0.00218\n",
      "2025-12-17 10:24:17.690269: train_loss -0.8481\n",
      "2025-12-17 10:24:17.690269: val_loss -0.8809\n",
      "2025-12-17 10:24:17.691270: Pseudo dice [0.9295, 0.9591, 0.9341]\n",
      "2025-12-17 10:24:17.691270: Epoch time: 138.07 s\n",
      "2025-12-17 10:24:18.525679: \n",
      "2025-12-17 10:24:18.525679: Epoch 817\n",
      "2025-12-17 10:24:18.525679: Current learning rate: 0.00217\n",
      "2025-12-17 10:26:36.763293: train_loss -0.8508\n",
      "2025-12-17 10:26:36.763293: val_loss -0.8726\n",
      "2025-12-17 10:26:36.773314: Pseudo dice [0.9232, 0.9549, 0.9391]\n",
      "2025-12-17 10:26:36.779059: Epoch time: 138.24 s\n",
      "2025-12-17 10:26:37.450606: \n",
      "2025-12-17 10:26:37.450606: Epoch 818\n",
      "2025-12-17 10:26:37.458677: Current learning rate: 0.00216\n",
      "2025-12-17 10:28:55.480337: train_loss -0.8561\n",
      "2025-12-17 10:28:55.480337: val_loss -0.8676\n",
      "2025-12-17 10:28:55.488044: Pseudo dice [0.9241, 0.9536, 0.9258]\n",
      "2025-12-17 10:28:55.492048: Epoch time: 138.03 s\n",
      "2025-12-17 10:28:56.228395: \n",
      "2025-12-17 10:28:56.228395: Epoch 819\n",
      "2025-12-17 10:28:56.230398: Current learning rate: 0.00215\n",
      "2025-12-17 10:31:14.346609: train_loss -0.8549\n",
      "2025-12-17 10:31:14.346609: val_loss -0.8771\n",
      "2025-12-17 10:31:14.352115: Pseudo dice [0.9268, 0.9555, 0.9388]\n",
      "2025-12-17 10:31:14.358121: Epoch time: 138.12 s\n",
      "2025-12-17 10:31:15.149465: \n",
      "2025-12-17 10:31:15.149465: Epoch 820\n",
      "2025-12-17 10:31:15.149465: Current learning rate: 0.00214\n",
      "2025-12-17 10:33:33.114954: train_loss -0.8583\n",
      "2025-12-17 10:33:33.114954: val_loss -0.8809\n",
      "2025-12-17 10:33:33.122457: Pseudo dice [0.9319, 0.9634, 0.937]\n",
      "2025-12-17 10:33:33.126461: Epoch time: 137.97 s\n",
      "2025-12-17 10:33:33.763072: \n",
      "2025-12-17 10:33:33.763072: Epoch 821\n",
      "2025-12-17 10:33:33.763072: Current learning rate: 0.00213\n",
      "2025-12-17 10:35:52.078055: train_loss -0.8577\n",
      "2025-12-17 10:35:52.078055: val_loss -0.8735\n",
      "2025-12-17 10:35:52.084060: Pseudo dice [0.9291, 0.9585, 0.9309]\n",
      "2025-12-17 10:35:52.089567: Epoch time: 138.31 s\n",
      "2025-12-17 10:35:52.723299: \n",
      "2025-12-17 10:35:52.723299: Epoch 822\n",
      "2025-12-17 10:35:52.723299: Current learning rate: 0.00212\n",
      "2025-12-17 10:38:10.620257: train_loss -0.8583\n",
      "2025-12-17 10:38:10.620257: val_loss -0.874\n",
      "2025-12-17 10:38:10.622704: Pseudo dice [0.9275, 0.9546, 0.9316]\n",
      "2025-12-17 10:38:10.622704: Epoch time: 137.9 s\n",
      "2025-12-17 10:38:11.397070: \n",
      "2025-12-17 10:38:11.397070: Epoch 823\n",
      "2025-12-17 10:38:11.412970: Current learning rate: 0.0021\n",
      "2025-12-17 10:40:29.259158: train_loss -0.859\n",
      "2025-12-17 10:40:29.259158: val_loss -0.866\n",
      "2025-12-17 10:40:29.259158: Pseudo dice [0.9196, 0.9569, 0.9306]\n",
      "2025-12-17 10:40:29.274934: Epoch time: 137.86 s\n",
      "2025-12-17 10:40:29.902236: \n",
      "2025-12-17 10:40:29.902236: Epoch 824\n",
      "2025-12-17 10:40:29.902236: Current learning rate: 0.00209\n",
      "2025-12-17 10:42:47.707947: train_loss -0.8634\n",
      "2025-12-17 10:42:47.707947: val_loss -0.8874\n",
      "2025-12-17 10:42:47.709950: Pseudo dice [0.9372, 0.964, 0.9353]\n",
      "2025-12-17 10:42:47.720232: Epoch time: 137.81 s\n",
      "2025-12-17 10:42:48.348080: \n",
      "2025-12-17 10:42:48.348080: Epoch 825\n",
      "2025-12-17 10:42:48.350581: Current learning rate: 0.00208\n",
      "2025-12-17 10:45:06.483437: train_loss -0.8576\n",
      "2025-12-17 10:45:06.483437: val_loss -0.8745\n",
      "2025-12-17 10:45:06.489034: Pseudo dice [0.9213, 0.9591, 0.9353]\n",
      "2025-12-17 10:45:06.495040: Epoch time: 138.14 s\n",
      "2025-12-17 10:45:07.227362: \n",
      "2025-12-17 10:45:07.227362: Epoch 826\n",
      "2025-12-17 10:45:07.230635: Current learning rate: 0.00207\n",
      "2025-12-17 10:47:25.105278: train_loss -0.8596\n",
      "2025-12-17 10:47:25.105278: val_loss -0.8622\n",
      "2025-12-17 10:47:25.112286: Pseudo dice [0.9139, 0.9513, 0.9356]\n",
      "2025-12-17 10:47:25.119610: Epoch time: 137.88 s\n",
      "2025-12-17 10:47:25.757931: \n",
      "2025-12-17 10:47:25.757931: Epoch 827\n",
      "2025-12-17 10:47:25.759935: Current learning rate: 0.00206\n",
      "2025-12-17 10:49:43.584652: train_loss -0.861\n",
      "2025-12-17 10:49:43.584652: val_loss -0.8775\n",
      "2025-12-17 10:49:43.594401: Pseudo dice [0.9267, 0.9556, 0.936]\n",
      "2025-12-17 10:49:43.600407: Epoch time: 137.83 s\n",
      "2025-12-17 10:49:44.392454: \n",
      "2025-12-17 10:49:44.392454: Epoch 828\n",
      "2025-12-17 10:49:44.392454: Current learning rate: 0.00205\n",
      "2025-12-17 10:52:02.541373: train_loss -0.8609\n",
      "2025-12-17 10:52:02.541373: val_loss -0.8876\n",
      "2025-12-17 10:52:02.549119: Pseudo dice [0.9289, 0.9629, 0.9433]\n",
      "2025-12-17 10:52:02.553840: Epoch time: 138.15 s\n",
      "2025-12-17 10:52:03.267669: \n",
      "2025-12-17 10:52:03.267669: Epoch 829\n",
      "2025-12-17 10:52:03.267669: Current learning rate: 0.00204\n",
      "2025-12-17 10:54:21.252535: train_loss -0.8604\n",
      "2025-12-17 10:54:21.254536: val_loss -0.8781\n",
      "2025-12-17 10:54:21.261282: Pseudo dice [0.923, 0.9553, 0.9469]\n",
      "2025-12-17 10:54:21.267001: Epoch time: 137.99 s\n",
      "2025-12-17 10:54:21.895874: \n",
      "2025-12-17 10:54:21.911663: Epoch 830\n",
      "2025-12-17 10:54:21.911663: Current learning rate: 0.00203\n",
      "2025-12-17 10:56:39.801846: train_loss -0.8565\n",
      "2025-12-17 10:56:39.801846: val_loss -0.8773\n",
      "2025-12-17 10:56:39.805592: Pseudo dice [0.9293, 0.961, 0.9372]\n",
      "2025-12-17 10:56:39.805592: Epoch time: 137.91 s\n",
      "2025-12-17 10:56:40.444730: \n",
      "2025-12-17 10:56:40.444730: Epoch 831\n",
      "2025-12-17 10:56:40.450331: Current learning rate: 0.00202\n",
      "2025-12-17 10:58:58.469626: train_loss -0.8592\n",
      "2025-12-17 10:58:58.469626: val_loss -0.8764\n",
      "2025-12-17 10:58:58.469626: Pseudo dice [0.9277, 0.9574, 0.9324]\n",
      "2025-12-17 10:58:58.480201: Epoch time: 138.03 s\n",
      "2025-12-17 10:58:59.103186: \n",
      "2025-12-17 10:58:59.103186: Epoch 832\n",
      "2025-12-17 10:58:59.118911: Current learning rate: 0.00201\n",
      "2025-12-17 11:01:17.158932: train_loss -0.8599\n",
      "2025-12-17 11:01:17.158932: val_loss -0.8774\n",
      "2025-12-17 11:01:17.179132: Pseudo dice [0.9298, 0.9586, 0.9315]\n",
      "2025-12-17 11:01:17.183645: Epoch time: 138.06 s\n",
      "2025-12-17 11:01:17.808272: \n",
      "2025-12-17 11:01:17.808272: Epoch 833\n",
      "2025-12-17 11:01:17.817786: Current learning rate: 0.002\n",
      "2025-12-17 11:03:35.876930: train_loss -0.856\n",
      "2025-12-17 11:03:35.876930: val_loss -0.8698\n",
      "2025-12-17 11:03:35.894224: Pseudo dice [0.9229, 0.9549, 0.9342]\n",
      "2025-12-17 11:03:35.901259: Epoch time: 138.07 s\n",
      "2025-12-17 11:03:36.732294: \n",
      "2025-12-17 11:03:36.748083: Epoch 834\n",
      "2025-12-17 11:03:36.748083: Current learning rate: 0.00199\n",
      "2025-12-17 11:05:54.638999: train_loss -0.8626\n",
      "2025-12-17 11:05:54.638999: val_loss -0.8814\n",
      "2025-12-17 11:05:54.651939: Pseudo dice [0.9284, 0.9622, 0.9392]\n",
      "2025-12-17 11:05:54.656898: Epoch time: 137.91 s\n",
      "2025-12-17 11:05:55.274065: \n",
      "2025-12-17 11:05:55.274065: Epoch 835\n",
      "2025-12-17 11:05:55.287400: Current learning rate: 0.00198\n",
      "2025-12-17 11:08:13.219464: train_loss -0.8618\n",
      "2025-12-17 11:08:13.219464: val_loss -0.8762\n",
      "2025-12-17 11:08:13.231218: Pseudo dice [0.9282, 0.9546, 0.9396]\n",
      "2025-12-17 11:08:13.237225: Epoch time: 137.95 s\n",
      "2025-12-17 11:08:13.878906: \n",
      "2025-12-17 11:08:13.878906: Epoch 836\n",
      "2025-12-17 11:08:13.878906: Current learning rate: 0.00196\n",
      "2025-12-17 11:10:32.121395: train_loss -0.8614\n",
      "2025-12-17 11:10:32.121395: val_loss -0.8837\n",
      "2025-12-17 11:10:32.131144: Pseudo dice [0.9327, 0.9615, 0.941]\n",
      "2025-12-17 11:10:32.137150: Epoch time: 138.24 s\n",
      "2025-12-17 11:10:32.764196: \n",
      "2025-12-17 11:10:32.764196: Epoch 837\n",
      "2025-12-17 11:10:32.779920: Current learning rate: 0.00195\n",
      "2025-12-17 11:12:50.798951: train_loss -0.8591\n",
      "2025-12-17 11:12:50.798951: val_loss -0.8729\n",
      "2025-12-17 11:12:50.811396: Pseudo dice [0.9239, 0.9528, 0.9392]\n",
      "2025-12-17 11:12:50.811396: Epoch time: 138.03 s\n",
      "2025-12-17 11:12:51.508293: \n",
      "2025-12-17 11:12:51.508293: Epoch 838\n",
      "2025-12-17 11:12:51.524017: Current learning rate: 0.00194\n",
      "2025-12-17 11:15:09.456689: train_loss -0.8628\n",
      "2025-12-17 11:15:09.458692: val_loss -0.873\n",
      "2025-12-17 11:15:09.466441: Pseudo dice [0.9257, 0.9551, 0.9356]\n",
      "2025-12-17 11:15:09.473690: Epoch time: 137.95 s\n",
      "2025-12-17 11:15:10.094206: \n",
      "2025-12-17 11:15:10.094206: Epoch 839\n",
      "2025-12-17 11:15:10.094206: Current learning rate: 0.00193\n",
      "2025-12-17 11:17:28.115740: train_loss -0.8615\n",
      "2025-12-17 11:17:28.115740: val_loss -0.8789\n",
      "2025-12-17 11:17:28.122961: Pseudo dice [0.9275, 0.9573, 0.9406]\n",
      "2025-12-17 11:17:28.122961: Epoch time: 138.02 s\n",
      "2025-12-17 11:17:28.980262: \n",
      "2025-12-17 11:17:28.980262: Epoch 840\n",
      "2025-12-17 11:17:28.994033: Current learning rate: 0.00192\n",
      "2025-12-17 11:19:46.771813: train_loss -0.8622\n",
      "2025-12-17 11:19:46.771813: val_loss -0.8754\n",
      "2025-12-17 11:19:46.787881: Pseudo dice [0.9252, 0.9588, 0.9315]\n",
      "2025-12-17 11:19:46.794775: Epoch time: 137.79 s\n",
      "2025-12-17 11:19:47.468258: \n",
      "2025-12-17 11:19:47.468258: Epoch 841\n",
      "2025-12-17 11:19:47.483902: Current learning rate: 0.00191\n",
      "2025-12-17 11:22:05.543073: train_loss -0.8572\n",
      "2025-12-17 11:22:05.543073: val_loss -0.8739\n",
      "2025-12-17 11:22:05.543073: Pseudo dice [0.9265, 0.9568, 0.9365]\n",
      "2025-12-17 11:22:05.553724: Epoch time: 138.07 s\n",
      "2025-12-17 11:22:06.176910: \n",
      "2025-12-17 11:22:06.176910: Epoch 842\n",
      "2025-12-17 11:22:06.193046: Current learning rate: 0.0019\n",
      "2025-12-17 11:24:24.310418: train_loss -0.8564\n",
      "2025-12-17 11:24:24.310418: val_loss -0.8736\n",
      "2025-12-17 11:24:24.319665: Pseudo dice [0.9222, 0.9533, 0.9445]\n",
      "2025-12-17 11:24:24.324245: Epoch time: 138.13 s\n",
      "2025-12-17 11:24:25.053850: \n",
      "2025-12-17 11:24:25.053850: Epoch 843\n",
      "2025-12-17 11:24:25.064137: Current learning rate: 0.00189\n",
      "2025-12-17 11:26:43.196473: train_loss -0.8542\n",
      "2025-12-17 11:26:43.196473: val_loss -0.8779\n",
      "2025-12-17 11:26:43.202217: Pseudo dice [0.9318, 0.9588, 0.9322]\n",
      "2025-12-17 11:26:43.202217: Epoch time: 138.14 s\n",
      "2025-12-17 11:26:43.836172: \n",
      "2025-12-17 11:26:43.836172: Epoch 844\n",
      "2025-12-17 11:26:43.852162: Current learning rate: 0.00188\n",
      "2025-12-17 11:29:01.754480: train_loss -0.8646\n",
      "2025-12-17 11:29:01.754480: val_loss -0.8661\n",
      "2025-12-17 11:29:01.765816: Pseudo dice [0.9174, 0.9524, 0.9423]\n",
      "2025-12-17 11:29:01.769778: Epoch time: 137.92 s\n",
      "2025-12-17 11:29:02.402209: \n",
      "2025-12-17 11:29:02.402209: Epoch 845\n",
      "2025-12-17 11:29:02.402209: Current learning rate: 0.00187\n",
      "2025-12-17 11:31:20.111189: train_loss -0.8605\n",
      "2025-12-17 11:31:20.111189: val_loss -0.885\n",
      "2025-12-17 11:31:20.120939: Pseudo dice [0.933, 0.9608, 0.9455]\n",
      "2025-12-17 11:31:20.127505: Epoch time: 137.72 s\n",
      "2025-12-17 11:31:20.890734: \n",
      "2025-12-17 11:31:20.890734: Epoch 846\n",
      "2025-12-17 11:31:20.890734: Current learning rate: 0.00186\n",
      "2025-12-17 11:33:38.953015: train_loss -0.8628\n",
      "2025-12-17 11:33:38.953015: val_loss -0.8737\n",
      "2025-12-17 11:33:38.958993: Pseudo dice [0.9266, 0.955, 0.9421]\n",
      "2025-12-17 11:33:38.963630: Epoch time: 138.06 s\n",
      "2025-12-17 11:33:39.592954: \n",
      "2025-12-17 11:33:39.592954: Epoch 847\n",
      "2025-12-17 11:33:39.605540: Current learning rate: 0.00185\n",
      "2025-12-17 11:35:57.625973: train_loss -0.8585\n",
      "2025-12-17 11:35:57.625973: val_loss -0.8863\n",
      "2025-12-17 11:35:57.646712: Pseudo dice [0.9345, 0.9622, 0.9365]\n",
      "2025-12-17 11:35:57.651226: Epoch time: 138.03 s\n",
      "2025-12-17 11:35:58.276109: \n",
      "2025-12-17 11:35:58.276109: Epoch 848\n",
      "2025-12-17 11:35:58.293488: Current learning rate: 0.00184\n",
      "2025-12-17 11:38:16.246539: train_loss -0.8609\n",
      "2025-12-17 11:38:16.246539: val_loss -0.8902\n",
      "2025-12-17 11:38:16.252545: Pseudo dice [0.933, 0.9616, 0.9451]\n",
      "2025-12-17 11:38:16.258289: Epoch time: 137.97 s\n",
      "2025-12-17 11:38:17.001069: \n",
      "2025-12-17 11:38:17.001069: Epoch 849\n",
      "2025-12-17 11:38:17.017315: Current learning rate: 0.00182\n",
      "2025-12-17 11:40:35.164880: train_loss -0.8624\n",
      "2025-12-17 11:40:35.164880: val_loss -0.8846\n",
      "2025-12-17 11:40:35.174630: Pseudo dice [0.9276, 0.9587, 0.9474]\n",
      "2025-12-17 11:40:35.174630: Epoch time: 138.16 s\n",
      "2025-12-17 11:40:36.079139: \n",
      "2025-12-17 11:40:36.079139: Epoch 850\n",
      "2025-12-17 11:40:36.079139: Current learning rate: 0.00181\n",
      "2025-12-17 11:42:54.094324: train_loss -0.8579\n",
      "2025-12-17 11:42:54.094324: val_loss -0.8757\n",
      "2025-12-17 11:42:54.100080: Pseudo dice [0.9234, 0.956, 0.9439]\n",
      "2025-12-17 11:42:54.106086: Epoch time: 138.02 s\n",
      "2025-12-17 11:42:54.719382: \n",
      "2025-12-17 11:42:54.719382: Epoch 851\n",
      "2025-12-17 11:42:54.719382: Current learning rate: 0.0018\n",
      "2025-12-17 11:45:12.690586: train_loss -0.8623\n",
      "2025-12-17 11:45:12.690586: val_loss -0.8778\n",
      "2025-12-17 11:45:12.690586: Pseudo dice [0.9288, 0.9568, 0.9391]\n",
      "2025-12-17 11:45:12.690586: Epoch time: 137.97 s\n",
      "2025-12-17 11:45:13.324539: \n",
      "2025-12-17 11:45:13.324539: Epoch 852\n",
      "2025-12-17 11:45:13.324539: Current learning rate: 0.00179\n",
      "2025-12-17 11:47:31.449921: train_loss -0.861\n",
      "2025-12-17 11:47:31.449921: val_loss -0.8749\n",
      "2025-12-17 11:47:31.457932: Pseudo dice [0.928, 0.9571, 0.9294]\n",
      "2025-12-17 11:47:31.461936: Epoch time: 138.13 s\n",
      "2025-12-17 11:47:32.088702: \n",
      "2025-12-17 11:47:32.088702: Epoch 853\n",
      "2025-12-17 11:47:32.088702: Current learning rate: 0.00178\n",
      "2025-12-17 11:49:49.959007: train_loss -0.8611\n",
      "2025-12-17 11:49:49.959007: val_loss -0.8777\n",
      "2025-12-17 11:49:49.978891: Pseudo dice [0.93, 0.9595, 0.937]\n",
      "2025-12-17 11:49:49.978891: Epoch time: 137.89 s\n",
      "2025-12-17 11:49:50.593300: \n",
      "2025-12-17 11:49:50.593300: Epoch 854\n",
      "2025-12-17 11:49:50.609023: Current learning rate: 0.00177\n",
      "2025-12-17 11:52:08.543971: train_loss -0.8666\n",
      "2025-12-17 11:52:08.543971: val_loss -0.8771\n",
      "2025-12-17 11:52:08.549977: Pseudo dice [0.9276, 0.9548, 0.9342]\n",
      "2025-12-17 11:52:08.555983: Epoch time: 137.95 s\n",
      "2025-12-17 11:52:09.175453: \n",
      "2025-12-17 11:52:09.175453: Epoch 855\n",
      "2025-12-17 11:52:09.191508: Current learning rate: 0.00176\n",
      "2025-12-17 11:54:27.157235: train_loss -0.8596\n",
      "2025-12-17 11:54:27.157235: val_loss -0.8777\n",
      "2025-12-17 11:54:27.157235: Pseudo dice [0.9293, 0.9571, 0.9371]\n",
      "2025-12-17 11:54:27.173220: Epoch time: 137.98 s\n",
      "2025-12-17 11:54:27.791969: \n",
      "2025-12-17 11:54:27.791969: Epoch 856\n",
      "2025-12-17 11:54:27.797755: Current learning rate: 0.00175\n",
      "2025-12-17 11:56:45.792574: train_loss -0.8619\n",
      "2025-12-17 11:56:45.792574: val_loss -0.8749\n",
      "2025-12-17 11:56:45.803259: Pseudo dice [0.9259, 0.9572, 0.9363]\n",
      "2025-12-17 11:56:45.809884: Epoch time: 138.0 s\n",
      "2025-12-17 11:56:46.488778: \n",
      "2025-12-17 11:56:46.488778: Epoch 857\n",
      "2025-12-17 11:56:46.493647: Current learning rate: 0.00174\n",
      "2025-12-17 11:59:04.394329: train_loss -0.8641\n",
      "2025-12-17 11:59:04.394329: val_loss -0.8862\n",
      "2025-12-17 11:59:04.394329: Pseudo dice [0.9334, 0.9604, 0.9391]\n",
      "2025-12-17 11:59:04.410275: Epoch time: 137.92 s\n",
      "2025-12-17 11:59:05.058448: \n",
      "2025-12-17 11:59:05.058448: Epoch 858\n",
      "2025-12-17 11:59:05.068391: Current learning rate: 0.00173\n",
      "2025-12-17 12:01:22.830825: train_loss -0.8608\n",
      "2025-12-17 12:01:22.830825: val_loss -0.8907\n",
      "2025-12-17 12:01:22.848307: Pseudo dice [0.9365, 0.9642, 0.944]\n",
      "2025-12-17 12:01:22.855034: Epoch time: 137.77 s\n",
      "2025-12-17 12:01:23.699395: \n",
      "2025-12-17 12:01:23.715205: Epoch 859\n",
      "2025-12-17 12:01:23.715205: Current learning rate: 0.00172\n",
      "2025-12-17 12:03:41.754437: train_loss -0.8614\n",
      "2025-12-17 12:03:41.756440: val_loss -0.8787\n",
      "2025-12-17 12:03:41.768194: Pseudo dice [0.9273, 0.9609, 0.9418]\n",
      "2025-12-17 12:03:41.774200: Epoch time: 138.06 s\n",
      "2025-12-17 12:03:42.475217: \n",
      "2025-12-17 12:03:42.475217: Epoch 860\n",
      "2025-12-17 12:03:42.475217: Current learning rate: 0.0017\n",
      "2025-12-17 12:06:00.748478: train_loss -0.8622\n",
      "2025-12-17 12:06:00.750480: val_loss -0.8707\n",
      "2025-12-17 12:06:00.756645: Pseudo dice [0.9209, 0.9521, 0.9455]\n",
      "2025-12-17 12:06:00.760649: Epoch time: 138.27 s\n",
      "2025-12-17 12:06:01.420526: \n",
      "2025-12-17 12:06:01.420526: Epoch 861\n",
      "2025-12-17 12:06:01.436516: Current learning rate: 0.00169\n",
      "2025-12-17 12:08:19.384102: train_loss -0.8608\n",
      "2025-12-17 12:08:19.384102: val_loss -0.8826\n",
      "2025-12-17 12:08:19.399788: Pseudo dice [0.9313, 0.9576, 0.9451]\n",
      "2025-12-17 12:08:19.399788: Epoch time: 137.96 s\n",
      "2025-12-17 12:08:20.020044: \n",
      "2025-12-17 12:08:20.020044: Epoch 862\n",
      "2025-12-17 12:08:20.020044: Current learning rate: 0.00168\n",
      "2025-12-17 12:10:38.394692: train_loss -0.8586\n",
      "2025-12-17 12:10:38.394692: val_loss -0.8761\n",
      "2025-12-17 12:10:38.396695: Pseudo dice [0.9284, 0.9584, 0.9406]\n",
      "2025-12-17 12:10:38.396695: Epoch time: 138.39 s\n",
      "2025-12-17 12:10:39.058111: \n",
      "2025-12-17 12:10:39.058111: Epoch 863\n",
      "2025-12-17 12:10:39.073769: Current learning rate: 0.00167\n",
      "2025-12-17 12:12:56.832496: train_loss -0.864\n",
      "2025-12-17 12:12:56.832496: val_loss -0.8779\n",
      "2025-12-17 12:12:56.832496: Pseudo dice [0.9242, 0.9547, 0.9553]\n",
      "2025-12-17 12:12:56.832496: Epoch time: 137.77 s\n",
      "2025-12-17 12:12:57.448710: \n",
      "2025-12-17 12:12:57.464434: Epoch 864\n",
      "2025-12-17 12:12:57.469483: Current learning rate: 0.00166\n",
      "2025-12-17 12:15:15.328161: train_loss -0.8613\n",
      "2025-12-17 12:15:15.328161: val_loss -0.874\n",
      "2025-12-17 12:15:15.328161: Pseudo dice [0.926, 0.9541, 0.9403]\n",
      "2025-12-17 12:15:15.344275: Epoch time: 137.88 s\n",
      "2025-12-17 12:15:15.966537: \n",
      "2025-12-17 12:15:15.966537: Epoch 865\n",
      "2025-12-17 12:15:15.966537: Current learning rate: 0.00165\n",
      "2025-12-17 12:17:33.778480: train_loss -0.8599\n",
      "2025-12-17 12:17:33.780483: val_loss -0.8747\n",
      "2025-12-17 12:17:33.788493: Pseudo dice [0.9248, 0.9574, 0.9362]\n",
      "2025-12-17 12:17:33.800256: Epoch time: 137.81 s\n",
      "2025-12-17 12:17:34.722801: \n",
      "2025-12-17 12:17:34.722801: Epoch 866\n",
      "2025-12-17 12:17:34.728937: Current learning rate: 0.00164\n",
      "2025-12-17 12:19:52.677573: train_loss -0.8648\n",
      "2025-12-17 12:19:52.679575: val_loss -0.8727\n",
      "2025-12-17 12:19:52.687584: Pseudo dice [0.9238, 0.957, 0.934]\n",
      "2025-12-17 12:19:52.695334: Epoch time: 137.96 s\n",
      "2025-12-17 12:19:53.308756: \n",
      "2025-12-17 12:19:53.308756: Epoch 867\n",
      "2025-12-17 12:19:53.322610: Current learning rate: 0.00163\n",
      "2025-12-17 12:22:11.203897: train_loss -0.8635\n",
      "2025-12-17 12:22:11.203897: val_loss -0.8698\n",
      "2025-12-17 12:22:11.211398: Pseudo dice [0.918, 0.955, 0.9417]\n",
      "2025-12-17 12:22:11.217405: Epoch time: 137.9 s\n",
      "2025-12-17 12:22:11.836265: \n",
      "2025-12-17 12:22:11.836265: Epoch 868\n",
      "2025-12-17 12:22:11.852167: Current learning rate: 0.00162\n",
      "2025-12-17 12:24:29.859754: train_loss -0.8625\n",
      "2025-12-17 12:24:29.859754: val_loss -0.8802\n",
      "2025-12-17 12:24:29.863497: Pseudo dice [0.9316, 0.9602, 0.9308]\n",
      "2025-12-17 12:24:29.872994: Epoch time: 138.02 s\n",
      "2025-12-17 12:24:30.573968: \n",
      "2025-12-17 12:24:30.573968: Epoch 869\n",
      "2025-12-17 12:24:30.593923: Current learning rate: 0.00161\n",
      "2025-12-17 12:26:48.559289: train_loss -0.8628\n",
      "2025-12-17 12:26:48.561291: val_loss -0.8928\n",
      "2025-12-17 12:26:48.567296: Pseudo dice [0.9364, 0.9631, 0.9445]\n",
      "2025-12-17 12:26:48.571300: Epoch time: 137.99 s\n",
      "2025-12-17 12:26:49.188753: \n",
      "2025-12-17 12:26:49.188753: Epoch 870\n",
      "2025-12-17 12:26:49.204500: Current learning rate: 0.00159\n",
      "2025-12-17 12:29:07.110759: train_loss -0.8601\n",
      "2025-12-17 12:29:07.110759: val_loss -0.8814\n",
      "2025-12-17 12:29:07.116765: Pseudo dice [0.9314, 0.9601, 0.9322]\n",
      "2025-12-17 12:29:07.122771: Epoch time: 137.92 s\n",
      "2025-12-17 12:29:07.743175: \n",
      "2025-12-17 12:29:07.743175: Epoch 871\n",
      "2025-12-17 12:29:07.743175: Current learning rate: 0.00158\n",
      "2025-12-17 12:31:25.628720: train_loss -0.8613\n",
      "2025-12-17 12:31:25.628720: val_loss -0.8891\n",
      "2025-12-17 12:31:25.636467: Pseudo dice [0.9333, 0.9613, 0.9458]\n",
      "2025-12-17 12:31:25.642501: Epoch time: 137.89 s\n",
      "2025-12-17 12:31:26.429856: \n",
      "2025-12-17 12:31:26.429856: Epoch 872\n",
      "2025-12-17 12:31:26.429856: Current learning rate: 0.00157\n",
      "2025-12-17 12:33:44.468836: train_loss -0.8635\n",
      "2025-12-17 12:33:44.470838: val_loss -0.8889\n",
      "2025-12-17 12:33:44.478584: Pseudo dice [0.9312, 0.9638, 0.9427]\n",
      "2025-12-17 12:33:44.486333: Epoch time: 138.04 s\n",
      "2025-12-17 12:33:45.098499: \n",
      "2025-12-17 12:33:45.098499: Epoch 873\n",
      "2025-12-17 12:33:45.114364: Current learning rate: 0.00156\n",
      "2025-12-17 12:36:03.172366: train_loss -0.8612\n",
      "2025-12-17 12:36:03.174368: val_loss -0.8902\n",
      "2025-12-17 12:36:03.181706: Pseudo dice [0.9342, 0.9624, 0.9433]\n",
      "2025-12-17 12:36:03.189552: Epoch time: 138.07 s\n",
      "2025-12-17 12:36:03.197563: Yayy! New best EMA pseudo Dice: 0.9431\n",
      "2025-12-17 12:36:04.123005: \n",
      "2025-12-17 12:36:04.123005: Epoch 874\n",
      "2025-12-17 12:36:04.138737: Current learning rate: 0.00155\n",
      "2025-12-17 12:38:22.127728: train_loss -0.8638\n",
      "2025-12-17 12:38:22.127728: val_loss -0.8743\n",
      "2025-12-17 12:38:22.134266: Pseudo dice [0.9244, 0.9579, 0.9305]\n",
      "2025-12-17 12:38:22.134266: Epoch time: 138.0 s\n",
      "2025-12-17 12:38:22.823136: \n",
      "2025-12-17 12:38:22.823136: Epoch 875\n",
      "2025-12-17 12:38:22.827378: Current learning rate: 0.00154\n",
      "2025-12-17 12:40:40.841867: train_loss -0.8603\n",
      "2025-12-17 12:40:40.841867: val_loss -0.8752\n",
      "2025-12-17 12:40:40.848409: Pseudo dice [0.9248, 0.9574, 0.9448]\n",
      "2025-12-17 12:40:40.848409: Epoch time: 138.02 s\n",
      "2025-12-17 12:40:41.557398: \n",
      "2025-12-17 12:40:41.557398: Epoch 876\n",
      "2025-12-17 12:40:41.557398: Current learning rate: 0.00153\n",
      "2025-12-17 12:42:59.757409: train_loss -0.861\n",
      "2025-12-17 12:42:59.759414: val_loss -0.881\n",
      "2025-12-17 12:42:59.767154: Pseudo dice [0.9294, 0.9571, 0.9438]\n",
      "2025-12-17 12:42:59.774633: Epoch time: 138.2 s\n",
      "2025-12-17 12:43:00.396988: \n",
      "2025-12-17 12:43:00.398990: Epoch 877\n",
      "2025-12-17 12:43:00.398990: Current learning rate: 0.00152\n",
      "2025-12-17 12:45:18.302139: train_loss -0.8633\n",
      "2025-12-17 12:45:18.302139: val_loss -0.877\n",
      "2025-12-17 12:45:18.318022: Pseudo dice [0.928, 0.9573, 0.9343]\n",
      "2025-12-17 12:45:18.318022: Epoch time: 137.91 s\n",
      "2025-12-17 12:45:18.936598: \n",
      "2025-12-17 12:45:18.936598: Epoch 878\n",
      "2025-12-17 12:45:18.952400: Current learning rate: 0.00151\n",
      "2025-12-17 12:47:37.037474: train_loss -0.8588\n",
      "2025-12-17 12:47:37.037474: val_loss -0.8809\n",
      "2025-12-17 12:47:37.043480: Pseudo dice [0.929, 0.9601, 0.9386]\n",
      "2025-12-17 12:47:37.049487: Epoch time: 138.1 s\n",
      "2025-12-17 12:47:37.826998: \n",
      "2025-12-17 12:47:37.826998: Epoch 879\n",
      "2025-12-17 12:47:37.842825: Current learning rate: 0.00149\n",
      "2025-12-17 12:49:55.704878: train_loss -0.866\n",
      "2025-12-17 12:49:55.706880: val_loss -0.8876\n",
      "2025-12-17 12:49:55.714629: Pseudo dice [0.9359, 0.963, 0.9392]\n",
      "2025-12-17 12:49:55.718635: Epoch time: 137.88 s\n",
      "2025-12-17 12:49:56.341954: \n",
      "2025-12-17 12:49:56.341954: Epoch 880\n",
      "2025-12-17 12:49:56.341954: Current learning rate: 0.00148\n",
      "2025-12-17 12:52:14.418298: train_loss -0.8636\n",
      "2025-12-17 12:52:14.420300: val_loss -0.8694\n",
      "2025-12-17 12:52:14.425306: Pseudo dice [0.9256, 0.9551, 0.9316]\n",
      "2025-12-17 12:52:14.432863: Epoch time: 138.09 s\n",
      "2025-12-17 12:52:15.104384: \n",
      "2025-12-17 12:52:15.104384: Epoch 881\n",
      "2025-12-17 12:52:15.104384: Current learning rate: 0.00147\n",
      "2025-12-17 12:54:33.105657: train_loss -0.86\n",
      "2025-12-17 12:54:33.105657: val_loss -0.8744\n",
      "2025-12-17 12:54:33.105657: Pseudo dice [0.9265, 0.956, 0.9363]\n",
      "2025-12-17 12:54:33.121605: Epoch time: 138.0 s\n",
      "2025-12-17 12:54:33.735401: \n",
      "2025-12-17 12:54:33.735401: Epoch 882\n",
      "2025-12-17 12:54:33.735401: Current learning rate: 0.00146\n",
      "2025-12-17 12:56:51.667472: train_loss -0.864\n",
      "2025-12-17 12:56:51.672029: val_loss -0.8824\n",
      "2025-12-17 12:56:51.672029: Pseudo dice [0.9325, 0.9614, 0.9302]\n",
      "2025-12-17 12:56:51.681112: Epoch time: 137.93 s\n",
      "2025-12-17 12:56:52.361199: \n",
      "2025-12-17 12:56:52.361199: Epoch 883\n",
      "2025-12-17 12:56:52.361199: Current learning rate: 0.00145\n",
      "2025-12-17 12:59:10.229940: train_loss -0.8616\n",
      "2025-12-17 12:59:10.231942: val_loss -0.8794\n",
      "2025-12-17 12:59:10.237949: Pseudo dice [0.9288, 0.9573, 0.9319]\n",
      "2025-12-17 12:59:10.243693: Epoch time: 137.87 s\n",
      "2025-12-17 12:59:10.877724: \n",
      "2025-12-17 12:59:10.877724: Epoch 884\n",
      "2025-12-17 12:59:10.877724: Current learning rate: 0.00144\n",
      "2025-12-17 13:01:28.962428: train_loss -0.8597\n",
      "2025-12-17 13:01:28.962428: val_loss -0.8848\n",
      "2025-12-17 13:01:28.966170: Pseudo dice [0.9292, 0.9598, 0.943]\n",
      "2025-12-17 13:01:28.975413: Epoch time: 138.08 s\n",
      "2025-12-17 13:01:29.826519: \n",
      "2025-12-17 13:01:29.828259: Epoch 885\n",
      "2025-12-17 13:01:29.832003: Current learning rate: 0.00143\n",
      "2025-12-17 13:03:47.803732: train_loss -0.8635\n",
      "2025-12-17 13:03:47.803732: val_loss -0.8747\n",
      "2025-12-17 13:03:47.805473: Pseudo dice [0.9263, 0.957, 0.9327]\n",
      "2025-12-17 13:03:47.817409: Epoch time: 137.98 s\n",
      "2025-12-17 13:03:48.537871: \n",
      "2025-12-17 13:03:48.537871: Epoch 886\n",
      "2025-12-17 13:03:48.537871: Current learning rate: 0.00142\n",
      "2025-12-17 13:06:06.402094: train_loss -0.8609\n",
      "2025-12-17 13:06:06.402094: val_loss -0.8775\n",
      "2025-12-17 13:06:06.418179: Pseudo dice [0.9264, 0.9556, 0.938]\n",
      "2025-12-17 13:06:06.424431: Epoch time: 137.86 s\n",
      "2025-12-17 13:06:07.051738: \n",
      "2025-12-17 13:06:07.051738: Epoch 887\n",
      "2025-12-17 13:06:07.051738: Current learning rate: 0.00141\n",
      "2025-12-17 13:08:24.998394: train_loss -0.8633\n",
      "2025-12-17 13:08:24.998394: val_loss -0.8674\n",
      "2025-12-17 13:08:25.004400: Pseudo dice [0.9228, 0.9536, 0.9342]\n",
      "2025-12-17 13:08:25.010406: Epoch time: 137.95 s\n",
      "2025-12-17 13:08:25.624683: \n",
      "2025-12-17 13:08:25.624683: Epoch 888\n",
      "2025-12-17 13:08:25.624683: Current learning rate: 0.00139\n",
      "2025-12-17 13:10:43.954849: train_loss -0.8589\n",
      "2025-12-17 13:10:43.956854: val_loss -0.8751\n",
      "2025-12-17 13:10:43.966610: Pseudo dice [0.9234, 0.9563, 0.9384]\n",
      "2025-12-17 13:10:43.974617: Epoch time: 138.33 s\n",
      "2025-12-17 13:10:44.737146: \n",
      "2025-12-17 13:10:44.737146: Epoch 889\n",
      "2025-12-17 13:10:44.752959: Current learning rate: 0.00138\n",
      "2025-12-17 13:13:02.663548: train_loss -0.86\n",
      "2025-12-17 13:13:02.679457: val_loss -0.8815\n",
      "2025-12-17 13:13:02.686028: Pseudo dice [0.931, 0.9597, 0.9362]\n",
      "2025-12-17 13:13:02.690033: Epoch time: 137.93 s\n",
      "2025-12-17 13:13:03.308700: \n",
      "2025-12-17 13:13:03.310703: Epoch 890\n",
      "2025-12-17 13:13:03.314481: Current learning rate: 0.00137\n",
      "2025-12-17 13:15:21.466364: train_loss -0.8611\n",
      "2025-12-17 13:15:21.466364: val_loss -0.8767\n",
      "2025-12-17 13:15:21.476374: Pseudo dice [0.9292, 0.957, 0.934]\n",
      "2025-12-17 13:15:21.484123: Epoch time: 138.16 s\n",
      "2025-12-17 13:15:22.114233: \n",
      "2025-12-17 13:15:22.114233: Epoch 891\n",
      "2025-12-17 13:15:22.114233: Current learning rate: 0.00136\n",
      "2025-12-17 13:17:40.291554: train_loss -0.8618\n",
      "2025-12-17 13:17:40.293556: val_loss -0.8858\n",
      "2025-12-17 13:17:40.303306: Pseudo dice [0.9331, 0.9619, 0.9331]\n",
      "2025-12-17 13:17:40.311314: Epoch time: 138.18 s\n",
      "2025-12-17 13:17:41.264542: \n",
      "2025-12-17 13:17:41.264542: Epoch 892\n",
      "2025-12-17 13:17:41.264542: Current learning rate: 0.00135\n",
      "2025-12-17 13:19:59.233136: train_loss -0.863\n",
      "2025-12-17 13:19:59.233136: val_loss -0.8747\n",
      "2025-12-17 13:19:59.235960: Pseudo dice [0.9253, 0.9543, 0.935]\n",
      "2025-12-17 13:19:59.235960: Epoch time: 137.97 s\n",
      "2025-12-17 13:19:59.865459: \n",
      "2025-12-17 13:19:59.865459: Epoch 893\n",
      "2025-12-17 13:19:59.865459: Current learning rate: 0.00134\n",
      "2025-12-17 13:22:17.904038: train_loss -0.8624\n",
      "2025-12-17 13:22:17.904038: val_loss -0.8834\n",
      "2025-12-17 13:22:17.914836: Pseudo dice [0.9298, 0.9591, 0.9456]\n",
      "2025-12-17 13:22:17.918840: Epoch time: 138.05 s\n",
      "2025-12-17 13:22:18.544708: \n",
      "2025-12-17 13:22:18.544708: Epoch 894\n",
      "2025-12-17 13:22:18.544708: Current learning rate: 0.00133\n",
      "2025-12-17 13:24:36.493575: train_loss -0.8577\n",
      "2025-12-17 13:24:36.493575: val_loss -0.8847\n",
      "2025-12-17 13:24:36.499582: Pseudo dice [0.9329, 0.9588, 0.9398]\n",
      "2025-12-17 13:24:36.505326: Epoch time: 137.95 s\n",
      "2025-12-17 13:24:37.221030: \n",
      "2025-12-17 13:24:37.221030: Epoch 895\n",
      "2025-12-17 13:24:37.228286: Current learning rate: 0.00132\n",
      "2025-12-17 13:26:55.163868: train_loss -0.862\n",
      "2025-12-17 13:26:55.163868: val_loss -0.8756\n",
      "2025-12-17 13:26:55.179693: Pseudo dice [0.9247, 0.958, 0.9366]\n",
      "2025-12-17 13:26:55.179693: Epoch time: 137.94 s\n",
      "2025-12-17 13:26:55.796584: \n",
      "2025-12-17 13:26:55.796584: Epoch 896\n",
      "2025-12-17 13:26:55.812469: Current learning rate: 0.0013\n",
      "2025-12-17 13:29:13.828960: train_loss -0.8582\n",
      "2025-12-17 13:29:13.828960: val_loss -0.8699\n",
      "2025-12-17 13:29:13.842260: Pseudo dice [0.9194, 0.9532, 0.9414]\n",
      "2025-12-17 13:29:13.849847: Epoch time: 138.03 s\n",
      "2025-12-17 13:29:14.477807: \n",
      "2025-12-17 13:29:14.477807: Epoch 897\n",
      "2025-12-17 13:29:14.477807: Current learning rate: 0.00129\n",
      "2025-12-17 13:31:32.385821: train_loss -0.8627\n",
      "2025-12-17 13:31:32.385821: val_loss -0.8864\n",
      "2025-12-17 13:31:32.395412: Pseudo dice [0.9279, 0.9636, 0.9452]\n",
      "2025-12-17 13:31:32.399416: Epoch time: 137.91 s\n",
      "2025-12-17 13:31:33.304029: \n",
      "2025-12-17 13:31:33.304029: Epoch 898\n",
      "2025-12-17 13:31:33.324154: Current learning rate: 0.00128\n",
      "2025-12-17 13:33:51.314496: train_loss -0.8655\n",
      "2025-12-17 13:33:51.314496: val_loss -0.8807\n",
      "2025-12-17 13:33:51.330195: Pseudo dice [0.9262, 0.9585, 0.9431]\n",
      "2025-12-17 13:33:51.337702: Epoch time: 138.01 s\n",
      "2025-12-17 13:33:51.978890: \n",
      "2025-12-17 13:33:51.978890: Epoch 899\n",
      "2025-12-17 13:33:51.984402: Current learning rate: 0.00127\n",
      "2025-12-17 13:36:09.858214: train_loss -0.859\n",
      "2025-12-17 13:36:09.860219: val_loss -0.881\n",
      "2025-12-17 13:36:09.869968: Pseudo dice [0.9356, 0.9586, 0.9298]\n",
      "2025-12-17 13:36:09.875975: Epoch time: 137.9 s\n",
      "2025-12-17 13:36:10.829796: \n",
      "2025-12-17 13:36:10.829796: Epoch 900\n",
      "2025-12-17 13:36:10.829796: Current learning rate: 0.00126\n",
      "2025-12-17 13:38:28.729774: train_loss -0.8679\n",
      "2025-12-17 13:38:28.729774: val_loss -0.8875\n",
      "2025-12-17 13:38:28.729774: Pseudo dice [0.9299, 0.9627, 0.9466]\n",
      "2025-12-17 13:38:28.741574: Epoch time: 137.9 s\n",
      "2025-12-17 13:38:29.388655: \n",
      "2025-12-17 13:38:29.388655: Epoch 901\n",
      "2025-12-17 13:38:29.394161: Current learning rate: 0.00125\n",
      "2025-12-17 13:40:47.479645: train_loss -0.8607\n",
      "2025-12-17 13:40:47.479645: val_loss -0.8847\n",
      "2025-12-17 13:40:47.487899: Pseudo dice [0.9354, 0.9637, 0.9336]\n",
      "2025-12-17 13:40:47.493401: Epoch time: 138.09 s\n",
      "2025-12-17 13:40:48.119163: \n",
      "2025-12-17 13:40:48.119163: Epoch 902\n",
      "2025-12-17 13:40:48.119163: Current learning rate: 0.00124\n",
      "2025-12-17 13:43:05.937666: train_loss -0.862\n",
      "2025-12-17 13:43:05.941431: val_loss -0.8758\n",
      "2025-12-17 13:43:05.947438: Pseudo dice [0.9258, 0.9564, 0.9393]\n",
      "2025-12-17 13:43:05.953443: Epoch time: 137.82 s\n",
      "2025-12-17 13:43:06.629529: \n",
      "2025-12-17 13:43:06.645382: Epoch 903\n",
      "2025-12-17 13:43:06.645382: Current learning rate: 0.00122\n",
      "2025-12-17 13:45:24.483920: train_loss -0.8626\n",
      "2025-12-17 13:45:24.483920: val_loss -0.8858\n",
      "2025-12-17 13:45:24.499597: Pseudo dice [0.9336, 0.9602, 0.939]\n",
      "2025-12-17 13:45:24.499597: Epoch time: 137.85 s\n",
      "2025-12-17 13:45:25.118131: \n",
      "2025-12-17 13:45:25.118131: Epoch 904\n",
      "2025-12-17 13:45:25.134048: Current learning rate: 0.00121\n",
      "2025-12-17 13:47:43.141916: train_loss -0.8606\n",
      "2025-12-17 13:47:43.141916: val_loss -0.874\n",
      "2025-12-17 13:47:43.147922: Pseudo dice [0.9227, 0.9548, 0.9389]\n",
      "2025-12-17 13:47:43.153928: Epoch time: 138.02 s\n",
      "2025-12-17 13:47:43.947003: \n",
      "2025-12-17 13:47:43.962770: Epoch 905\n",
      "2025-12-17 13:47:43.962770: Current learning rate: 0.0012\n",
      "2025-12-17 13:50:01.891556: train_loss -0.8633\n",
      "2025-12-17 13:50:01.891556: val_loss -0.8736\n",
      "2025-12-17 13:50:01.898845: Pseudo dice [0.9227, 0.9571, 0.9425]\n",
      "2025-12-17 13:50:01.904851: Epoch time: 137.94 s\n",
      "2025-12-17 13:50:02.573373: \n",
      "2025-12-17 13:50:02.573373: Epoch 906\n",
      "2025-12-17 13:50:02.573373: Current learning rate: 0.00119\n",
      "2025-12-17 13:52:20.462202: train_loss -0.859\n",
      "2025-12-17 13:52:20.462202: val_loss -0.8794\n",
      "2025-12-17 13:52:20.462202: Pseudo dice [0.9281, 0.9584, 0.9383]\n",
      "2025-12-17 13:52:20.462202: Epoch time: 137.89 s\n",
      "2025-12-17 13:52:21.143584: \n",
      "2025-12-17 13:52:21.143584: Epoch 907\n",
      "2025-12-17 13:52:21.143584: Current learning rate: 0.00118\n",
      "2025-12-17 13:54:39.110371: train_loss -0.8611\n",
      "2025-12-17 13:54:39.110371: val_loss -0.8829\n",
      "2025-12-17 13:54:39.116377: Pseudo dice [0.9318, 0.9575, 0.9425]\n",
      "2025-12-17 13:54:39.124211: Epoch time: 137.97 s\n",
      "2025-12-17 13:54:39.819136: \n",
      "2025-12-17 13:54:39.819136: Epoch 908\n",
      "2025-12-17 13:54:39.819136: Current learning rate: 0.00117\n",
      "2025-12-17 13:56:57.550310: train_loss -0.8631\n",
      "2025-12-17 13:56:57.550310: val_loss -0.8742\n",
      "2025-12-17 13:56:57.556315: Pseudo dice [0.924, 0.9556, 0.9435]\n",
      "2025-12-17 13:56:57.562322: Epoch time: 137.73 s\n",
      "2025-12-17 13:56:58.218927: \n",
      "2025-12-17 13:56:58.226451: Epoch 909\n",
      "2025-12-17 13:56:58.226451: Current learning rate: 0.00116\n",
      "2025-12-17 13:59:16.217178: train_loss -0.8629\n",
      "2025-12-17 13:59:16.217178: val_loss -0.8834\n",
      "2025-12-17 13:59:16.219735: Pseudo dice [0.93, 0.9602, 0.939]\n",
      "2025-12-17 13:59:16.219735: Epoch time: 138.0 s\n",
      "2025-12-17 13:59:16.940730: \n",
      "2025-12-17 13:59:16.940730: Epoch 910\n",
      "2025-12-17 13:59:16.945258: Current learning rate: 0.00115\n",
      "2025-12-17 14:01:34.867596: train_loss -0.8617\n",
      "2025-12-17 14:01:34.867596: val_loss -0.8854\n",
      "2025-12-17 14:01:34.874907: Pseudo dice [0.9329, 0.96, 0.9445]\n",
      "2025-12-17 14:01:34.880913: Epoch time: 137.93 s\n",
      "2025-12-17 14:01:35.746050: \n",
      "2025-12-17 14:01:35.746050: Epoch 911\n",
      "2025-12-17 14:01:35.750421: Current learning rate: 0.00113\n",
      "2025-12-17 14:03:53.772098: train_loss -0.8653\n",
      "2025-12-17 14:03:53.772098: val_loss -0.8794\n",
      "2025-12-17 14:03:53.781003: Pseudo dice [0.9262, 0.9569, 0.9429]\n",
      "2025-12-17 14:03:53.786511: Epoch time: 138.03 s\n",
      "2025-12-17 14:03:54.458810: \n",
      "2025-12-17 14:03:54.458810: Epoch 912\n",
      "2025-12-17 14:03:54.474592: Current learning rate: 0.00112\n",
      "2025-12-17 14:06:12.393570: train_loss -0.8612\n",
      "2025-12-17 14:06:12.393570: val_loss -0.8861\n",
      "2025-12-17 14:06:12.401578: Pseudo dice [0.9336, 0.9616, 0.9478]\n",
      "2025-12-17 14:06:12.407584: Epoch time: 137.93 s\n",
      "2025-12-17 14:06:13.027915: \n",
      "2025-12-17 14:06:13.027915: Epoch 913\n",
      "2025-12-17 14:06:13.043782: Current learning rate: 0.00111\n",
      "2025-12-17 14:08:30.935559: train_loss -0.8653\n",
      "2025-12-17 14:08:30.937561: val_loss -0.8777\n",
      "2025-12-17 14:08:30.943567: Pseudo dice [0.9231, 0.9606, 0.9455]\n",
      "2025-12-17 14:08:30.947571: Epoch time: 137.91 s\n",
      "2025-12-17 14:08:31.569907: \n",
      "2025-12-17 14:08:31.569907: Epoch 914\n",
      "2025-12-17 14:08:31.585646: Current learning rate: 0.0011\n",
      "2025-12-17 14:10:49.819625: train_loss -0.8683\n",
      "2025-12-17 14:10:49.821367: val_loss -0.8909\n",
      "2025-12-17 14:10:49.829379: Pseudo dice [0.9366, 0.9606, 0.9466]\n",
      "2025-12-17 14:10:49.833385: Epoch time: 138.25 s\n",
      "2025-12-17 14:10:49.837130: Yayy! New best EMA pseudo Dice: 0.9434\n",
      "2025-12-17 14:10:50.845874: \n",
      "2025-12-17 14:10:50.845874: Epoch 915\n",
      "2025-12-17 14:10:50.845874: Current learning rate: 0.00109\n",
      "2025-12-17 14:13:08.885824: train_loss -0.8613\n",
      "2025-12-17 14:13:08.887826: val_loss -0.8808\n",
      "2025-12-17 14:13:08.891569: Pseudo dice [0.928, 0.9571, 0.9397]\n",
      "2025-12-17 14:13:08.899618: Epoch time: 138.04 s\n",
      "2025-12-17 14:13:09.509328: \n",
      "2025-12-17 14:13:09.509328: Epoch 916\n",
      "2025-12-17 14:13:09.525246: Current learning rate: 0.00108\n",
      "2025-12-17 14:15:27.648361: train_loss -0.8611\n",
      "2025-12-17 14:15:27.648361: val_loss -0.8836\n",
      "2025-12-17 14:15:27.654367: Pseudo dice [0.931, 0.9597, 0.9397]\n",
      "2025-12-17 14:15:27.660111: Epoch time: 138.14 s\n",
      "2025-12-17 14:15:28.292233: \n",
      "2025-12-17 14:15:28.292233: Epoch 917\n",
      "2025-12-17 14:15:28.292233: Current learning rate: 0.00106\n",
      "2025-12-17 14:17:46.333465: train_loss -0.8617\n",
      "2025-12-17 14:17:46.333465: val_loss -0.8861\n",
      "2025-12-17 14:17:46.352971: Pseudo dice [0.9334, 0.9616, 0.9412]\n",
      "2025-12-17 14:17:46.358980: Epoch time: 138.04 s\n",
      "2025-12-17 14:17:46.364988: Yayy! New best EMA pseudo Dice: 0.9435\n",
      "2025-12-17 14:17:47.352198: \n",
      "2025-12-17 14:17:47.352198: Epoch 918\n",
      "2025-12-17 14:17:47.356535: Current learning rate: 0.00105\n",
      "2025-12-17 14:20:05.302997: train_loss -0.8628\n",
      "2025-12-17 14:20:05.302997: val_loss -0.8877\n",
      "2025-12-17 14:20:05.315980: Pseudo dice [0.9351, 0.9631, 0.9413]\n",
      "2025-12-17 14:20:05.322965: Epoch time: 137.95 s\n",
      "2025-12-17 14:20:05.328971: Yayy! New best EMA pseudo Dice: 0.9438\n",
      "2025-12-17 14:20:06.254094: \n",
      "2025-12-17 14:20:06.254094: Epoch 919\n",
      "2025-12-17 14:20:06.254094: Current learning rate: 0.00104\n",
      "2025-12-17 14:22:24.379740: train_loss -0.8622\n",
      "2025-12-17 14:22:24.381742: val_loss -0.8825\n",
      "2025-12-17 14:22:24.387748: Pseudo dice [0.9312, 0.9595, 0.9376]\n",
      "2025-12-17 14:22:24.393255: Epoch time: 138.13 s\n",
      "2025-12-17 14:22:25.015678: \n",
      "2025-12-17 14:22:25.015678: Epoch 920\n",
      "2025-12-17 14:22:25.031322: Current learning rate: 0.00103\n",
      "2025-12-17 14:24:43.087297: train_loss -0.8663\n",
      "2025-12-17 14:24:43.087297: val_loss -0.8831\n",
      "2025-12-17 14:24:43.103073: Pseudo dice [0.9275, 0.9603, 0.9451]\n",
      "2025-12-17 14:24:43.111083: Epoch time: 138.07 s\n",
      "2025-12-17 14:24:43.840449: \n",
      "2025-12-17 14:24:43.841886: Epoch 921\n",
      "2025-12-17 14:24:43.847900: Current learning rate: 0.00102\n",
      "2025-12-17 14:27:01.796833: train_loss -0.8611\n",
      "2025-12-17 14:27:01.798710: val_loss -0.8794\n",
      "2025-12-17 14:27:01.807384: Pseudo dice [0.9266, 0.9569, 0.9389]\n",
      "2025-12-17 14:27:01.814105: Epoch time: 137.96 s\n",
      "2025-12-17 14:27:02.430110: \n",
      "2025-12-17 14:27:02.430110: Epoch 922\n",
      "2025-12-17 14:27:02.445968: Current learning rate: 0.00101\n",
      "2025-12-17 14:29:20.438274: train_loss -0.8609\n",
      "2025-12-17 14:29:20.438274: val_loss -0.8733\n",
      "2025-12-17 14:29:20.451528: Pseudo dice [0.9216, 0.9542, 0.9408]\n",
      "2025-12-17 14:29:20.454046: Epoch time: 138.01 s\n",
      "2025-12-17 14:29:21.215178: \n",
      "2025-12-17 14:29:21.215178: Epoch 923\n",
      "2025-12-17 14:29:21.231284: Current learning rate: 0.001\n",
      "2025-12-17 14:31:39.212682: train_loss -0.8619\n",
      "2025-12-17 14:31:39.212682: val_loss -0.8782\n",
      "2025-12-17 14:31:39.220348: Pseudo dice [0.9219, 0.9603, 0.9401]\n",
      "2025-12-17 14:31:39.224752: Epoch time: 138.0 s\n",
      "2025-12-17 14:31:39.964779: \n",
      "2025-12-17 14:31:39.964779: Epoch 924\n",
      "2025-12-17 14:31:39.970573: Current learning rate: 0.00098\n",
      "2025-12-17 14:33:57.768265: train_loss -0.866\n",
      "2025-12-17 14:33:57.768265: val_loss -0.8783\n",
      "2025-12-17 14:33:57.776165: Pseudo dice [0.924, 0.956, 0.9507]\n",
      "2025-12-17 14:33:57.781305: Epoch time: 137.8 s\n",
      "2025-12-17 14:33:58.401502: \n",
      "2025-12-17 14:33:58.401502: Epoch 925\n",
      "2025-12-17 14:33:58.401502: Current learning rate: 0.00097\n",
      "2025-12-17 14:36:16.269308: train_loss -0.8636\n",
      "2025-12-17 14:36:16.285100: val_loss -0.8784\n",
      "2025-12-17 14:36:16.285100: Pseudo dice [0.9265, 0.9581, 0.9339]\n",
      "2025-12-17 14:36:16.285100: Epoch time: 137.87 s\n",
      "2025-12-17 14:36:16.932904: \n",
      "2025-12-17 14:36:16.932904: Epoch 926\n",
      "2025-12-17 14:36:16.938578: Current learning rate: 0.00096\n",
      "2025-12-17 14:38:34.824274: train_loss -0.8615\n",
      "2025-12-17 14:38:34.824274: val_loss -0.8786\n",
      "2025-12-17 14:38:34.845556: Pseudo dice [0.928, 0.956, 0.9371]\n",
      "2025-12-17 14:38:34.850636: Epoch time: 137.91 s\n",
      "2025-12-17 14:38:35.620563: \n",
      "2025-12-17 14:38:35.620563: Epoch 927\n",
      "2025-12-17 14:38:35.633417: Current learning rate: 0.00095\n",
      "2025-12-17 14:40:53.739277: train_loss -0.8619\n",
      "2025-12-17 14:40:53.739277: val_loss -0.8858\n",
      "2025-12-17 14:40:53.757196: Pseudo dice [0.9303, 0.9578, 0.9481]\n",
      "2025-12-17 14:40:53.759198: Epoch time: 138.12 s\n",
      "2025-12-17 14:40:54.372622: \n",
      "2025-12-17 14:40:54.372622: Epoch 928\n",
      "2025-12-17 14:40:54.388622: Current learning rate: 0.00094\n",
      "2025-12-17 14:43:12.286021: train_loss -0.8667\n",
      "2025-12-17 14:43:12.286021: val_loss -0.8832\n",
      "2025-12-17 14:43:12.300185: Pseudo dice [0.9334, 0.9583, 0.9448]\n",
      "2025-12-17 14:43:12.306319: Epoch time: 137.91 s\n",
      "2025-12-17 14:43:12.948908: \n",
      "2025-12-17 14:43:12.948908: Epoch 929\n",
      "2025-12-17 14:43:12.948908: Current learning rate: 0.00092\n",
      "2025-12-17 14:45:30.932972: train_loss -0.8611\n",
      "2025-12-17 14:45:30.932972: val_loss -0.8807\n",
      "2025-12-17 14:45:30.953700: Pseudo dice [0.9269, 0.961, 0.9403]\n",
      "2025-12-17 14:45:30.957720: Epoch time: 137.98 s\n",
      "2025-12-17 14:45:31.819566: \n",
      "2025-12-17 14:45:31.819566: Epoch 930\n",
      "2025-12-17 14:45:31.839952: Current learning rate: 0.00091\n",
      "2025-12-17 14:47:49.634500: train_loss -0.8647\n",
      "2025-12-17 14:47:49.634500: val_loss -0.876\n",
      "2025-12-17 14:47:49.644256: Pseudo dice [0.929, 0.9551, 0.9384]\n",
      "2025-12-17 14:47:49.655937: Epoch time: 137.81 s\n",
      "2025-12-17 14:47:50.268805: \n",
      "2025-12-17 14:47:50.268805: Epoch 931\n",
      "2025-12-17 14:47:50.284723: Current learning rate: 0.0009\n",
      "2025-12-17 14:50:08.233417: train_loss -0.8628\n",
      "2025-12-17 14:50:08.233417: val_loss -0.8782\n",
      "2025-12-17 14:50:08.233417: Pseudo dice [0.9297, 0.9572, 0.9393]\n",
      "2025-12-17 14:50:08.251086: Epoch time: 137.96 s\n",
      "2025-12-17 14:50:08.864486: \n",
      "2025-12-17 14:50:08.864486: Epoch 932\n",
      "2025-12-17 14:50:08.884036: Current learning rate: 0.00089\n",
      "2025-12-17 14:52:26.799233: train_loss -0.8645\n",
      "2025-12-17 14:52:26.801236: val_loss -0.8789\n",
      "2025-12-17 14:52:26.805241: Pseudo dice [0.9263, 0.9572, 0.9395]\n",
      "2025-12-17 14:52:26.805241: Epoch time: 137.93 s\n",
      "2025-12-17 14:52:27.438505: \n",
      "2025-12-17 14:52:27.438505: Epoch 933\n",
      "2025-12-17 14:52:27.438505: Current learning rate: 0.00088\n",
      "2025-12-17 14:54:45.384278: train_loss -0.8621\n",
      "2025-12-17 14:54:45.386285: val_loss -0.8789\n",
      "2025-12-17 14:54:45.400052: Pseudo dice [0.9283, 0.9576, 0.9389]\n",
      "2025-12-17 14:54:45.405671: Epoch time: 137.95 s\n",
      "2025-12-17 14:54:46.020205: \n",
      "2025-12-17 14:54:46.020205: Epoch 934\n",
      "2025-12-17 14:54:46.036222: Current learning rate: 0.00087\n",
      "2025-12-17 14:57:03.907411: train_loss -0.8676\n",
      "2025-12-17 14:57:03.907411: val_loss -0.8778\n",
      "2025-12-17 14:57:03.917140: Pseudo dice [0.9284, 0.9545, 0.9316]\n",
      "2025-12-17 14:57:03.923215: Epoch time: 137.89 s\n",
      "2025-12-17 14:57:04.540076: \n",
      "2025-12-17 14:57:04.540076: Epoch 935\n",
      "2025-12-17 14:57:04.555935: Current learning rate: 0.00085\n",
      "2025-12-17 14:59:22.495824: train_loss -0.8657\n",
      "2025-12-17 14:59:22.495824: val_loss -0.8718\n",
      "2025-12-17 14:59:22.495824: Pseudo dice [0.9239, 0.9546, 0.9406]\n",
      "2025-12-17 14:59:22.511479: Epoch time: 137.96 s\n",
      "2025-12-17 14:59:23.129826: \n",
      "2025-12-17 14:59:23.129826: Epoch 936\n",
      "2025-12-17 14:59:23.143333: Current learning rate: 0.00084\n",
      "2025-12-17 15:01:40.912666: train_loss -0.8655\n",
      "2025-12-17 15:01:40.912666: val_loss -0.8882\n",
      "2025-12-17 15:01:40.928779: Pseudo dice [0.9334, 0.9622, 0.9442]\n",
      "2025-12-17 15:01:40.928779: Epoch time: 137.78 s\n",
      "2025-12-17 15:01:41.722227: \n",
      "2025-12-17 15:01:41.722227: Epoch 937\n",
      "2025-12-17 15:01:41.722227: Current learning rate: 0.00083\n",
      "2025-12-17 15:03:59.609034: train_loss -0.8667\n",
      "2025-12-17 15:03:59.609034: val_loss -0.8763\n",
      "2025-12-17 15:03:59.609034: Pseudo dice [0.9271, 0.9577, 0.9313]\n",
      "2025-12-17 15:03:59.627035: Epoch time: 137.89 s\n",
      "2025-12-17 15:04:00.383380: \n",
      "2025-12-17 15:04:00.383380: Epoch 938\n",
      "2025-12-17 15:04:00.403039: Current learning rate: 0.00082\n",
      "2025-12-17 15:06:18.266864: train_loss -0.8671\n",
      "2025-12-17 15:06:18.268867: val_loss -0.8797\n",
      "2025-12-17 15:06:18.278708: Pseudo dice [0.9304, 0.9571, 0.9416]\n",
      "2025-12-17 15:06:18.280710: Epoch time: 137.88 s\n",
      "2025-12-17 15:06:18.910720: \n",
      "2025-12-17 15:06:18.910720: Epoch 939\n",
      "2025-12-17 15:06:18.926623: Current learning rate: 0.00081\n",
      "2025-12-17 15:08:36.826262: train_loss -0.8592\n",
      "2025-12-17 15:08:36.828264: val_loss -0.8888\n",
      "2025-12-17 15:08:36.836257: Pseudo dice [0.9355, 0.9624, 0.9367]\n",
      "2025-12-17 15:08:36.844263: Epoch time: 137.92 s\n",
      "2025-12-17 15:08:37.461468: \n",
      "2025-12-17 15:08:37.461468: Epoch 940\n",
      "2025-12-17 15:08:37.477387: Current learning rate: 0.00079\n",
      "2025-12-17 15:10:55.839242: train_loss -0.866\n",
      "2025-12-17 15:10:55.839242: val_loss -0.8931\n",
      "2025-12-17 15:10:55.839242: Pseudo dice [0.9411, 0.9608, 0.9382]\n",
      "2025-12-17 15:10:55.852321: Epoch time: 138.38 s\n",
      "2025-12-17 15:10:56.609401: \n",
      "2025-12-17 15:10:56.609401: Epoch 941\n",
      "2025-12-17 15:10:56.615116: Current learning rate: 0.00078\n",
      "2025-12-17 15:13:14.554959: train_loss -0.8658\n",
      "2025-12-17 15:13:14.554959: val_loss -0.89\n",
      "2025-12-17 15:13:14.570163: Pseudo dice [0.9331, 0.9618, 0.9504]\n",
      "2025-12-17 15:13:14.576093: Epoch time: 137.95 s\n",
      "2025-12-17 15:13:15.188585: \n",
      "2025-12-17 15:13:15.188585: Epoch 942\n",
      "2025-12-17 15:13:15.206230: Current learning rate: 0.00077\n",
      "2025-12-17 15:15:33.320138: train_loss -0.862\n",
      "2025-12-17 15:15:33.320138: val_loss -0.8772\n",
      "2025-12-17 15:15:33.326874: Pseudo dice [0.9265, 0.9539, 0.9405]\n",
      "2025-12-17 15:15:33.332470: Epoch time: 138.13 s\n",
      "2025-12-17 15:15:33.954148: \n",
      "2025-12-17 15:15:33.954148: Epoch 943\n",
      "2025-12-17 15:15:33.969897: Current learning rate: 0.00076\n",
      "2025-12-17 15:17:52.037656: train_loss -0.8624\n",
      "2025-12-17 15:17:52.037656: val_loss -0.8872\n",
      "2025-12-17 15:17:52.045800: Pseudo dice [0.9319, 0.9617, 0.943]\n",
      "2025-12-17 15:17:52.045800: Epoch time: 138.08 s\n",
      "2025-12-17 15:17:53.009124: \n",
      "2025-12-17 15:17:53.009124: Epoch 944\n",
      "2025-12-17 15:17:53.022986: Current learning rate: 0.00075\n",
      "2025-12-17 15:20:10.842083: train_loss -0.8636\n",
      "2025-12-17 15:20:10.842083: val_loss -0.8925\n",
      "2025-12-17 15:20:10.858171: Pseudo dice [0.9377, 0.9661, 0.9394]\n",
      "2025-12-17 15:20:10.866618: Epoch time: 137.83 s\n",
      "2025-12-17 15:20:11.494438: \n",
      "2025-12-17 15:20:11.494438: Epoch 945\n",
      "2025-12-17 15:20:11.494438: Current learning rate: 0.00074\n",
      "2025-12-17 15:22:29.585618: train_loss -0.8684\n",
      "2025-12-17 15:22:29.585618: val_loss -0.883\n",
      "2025-12-17 15:22:29.601547: Pseudo dice [0.9351, 0.9593, 0.9338]\n",
      "2025-12-17 15:22:29.611419: Epoch time: 138.09 s\n",
      "2025-12-17 15:22:30.235808: \n",
      "2025-12-17 15:22:30.235808: Epoch 946\n",
      "2025-12-17 15:22:30.244812: Current learning rate: 0.00072\n",
      "2025-12-17 15:24:48.341200: train_loss -0.8661\n",
      "2025-12-17 15:24:48.341200: val_loss -0.8777\n",
      "2025-12-17 15:24:48.362496: Pseudo dice [0.9247, 0.9532, 0.9423]\n",
      "2025-12-17 15:24:48.368333: Epoch time: 138.11 s\n",
      "2025-12-17 15:24:49.099885: \n",
      "2025-12-17 15:24:49.099885: Epoch 947\n",
      "2025-12-17 15:24:49.109869: Current learning rate: 0.00071\n",
      "2025-12-17 15:27:07.078784: train_loss -0.8628\n",
      "2025-12-17 15:27:07.090486: val_loss -0.881\n",
      "2025-12-17 15:27:07.096791: Pseudo dice [0.9283, 0.9567, 0.9417]\n",
      "2025-12-17 15:27:07.101161: Epoch time: 137.98 s\n",
      "2025-12-17 15:27:07.724307: \n",
      "2025-12-17 15:27:07.724307: Epoch 948\n",
      "2025-12-17 15:27:07.729613: Current learning rate: 0.0007\n",
      "2025-12-17 15:29:25.673737: train_loss -0.862\n",
      "2025-12-17 15:29:25.675739: val_loss -0.8878\n",
      "2025-12-17 15:29:25.681745: Pseudo dice [0.9347, 0.9643, 0.9421]\n",
      "2025-12-17 15:29:25.687490: Epoch time: 137.97 s\n",
      "2025-12-17 15:29:26.328802: \n",
      "2025-12-17 15:29:26.328802: Epoch 949\n",
      "2025-12-17 15:29:26.333542: Current learning rate: 0.00069\n",
      "2025-12-17 15:31:44.168937: train_loss -0.8659\n",
      "2025-12-17 15:31:44.171636: val_loss -0.8726\n",
      "2025-12-17 15:31:44.177644: Pseudo dice [0.9245, 0.955, 0.9408]\n",
      "2025-12-17 15:31:44.183650: Epoch time: 137.84 s\n",
      "2025-12-17 15:31:45.372494: \n",
      "2025-12-17 15:31:45.372494: Epoch 950\n",
      "2025-12-17 15:31:45.379155: Current learning rate: 0.00067\n",
      "2025-12-17 15:34:03.486749: train_loss -0.8646\n",
      "2025-12-17 15:34:03.486749: val_loss -0.8806\n",
      "2025-12-17 15:34:03.493997: Pseudo dice [0.9273, 0.9608, 0.9369]\n",
      "2025-12-17 15:34:03.500822: Epoch time: 138.11 s\n",
      "2025-12-17 15:34:04.153569: \n",
      "2025-12-17 15:34:04.153569: Epoch 951\n",
      "2025-12-17 15:34:04.153569: Current learning rate: 0.00066\n",
      "2025-12-17 15:36:22.068988: train_loss -0.8641\n",
      "2025-12-17 15:36:22.068988: val_loss -0.8768\n",
      "2025-12-17 15:36:22.075787: Pseudo dice [0.9288, 0.9562, 0.9382]\n",
      "2025-12-17 15:36:22.079793: Epoch time: 137.92 s\n",
      "2025-12-17 15:36:22.731088: \n",
      "2025-12-17 15:36:22.731088: Epoch 952\n",
      "2025-12-17 15:36:22.731088: Current learning rate: 0.00065\n",
      "2025-12-17 15:38:40.653054: train_loss -0.862\n",
      "2025-12-17 15:38:40.668944: val_loss -0.8748\n",
      "2025-12-17 15:38:40.668944: Pseudo dice [0.922, 0.9528, 0.9482]\n",
      "2025-12-17 15:38:40.682108: Epoch time: 137.92 s\n",
      "2025-12-17 15:38:41.302078: \n",
      "2025-12-17 15:38:41.302078: Epoch 953\n",
      "2025-12-17 15:38:41.302078: Current learning rate: 0.00064\n",
      "2025-12-17 15:40:59.219960: train_loss -0.8662\n",
      "2025-12-17 15:40:59.221962: val_loss -0.8771\n",
      "2025-12-17 15:40:59.225726: Pseudo dice [0.9242, 0.9589, 0.9424]\n",
      "2025-12-17 15:40:59.233738: Epoch time: 137.92 s\n",
      "2025-12-17 15:40:59.863024: \n",
      "2025-12-17 15:40:59.863024: Epoch 954\n",
      "2025-12-17 15:40:59.882799: Current learning rate: 0.00063\n",
      "2025-12-17 15:43:17.808800: train_loss -0.8636\n",
      "2025-12-17 15:43:17.808800: val_loss -0.8853\n",
      "2025-12-17 15:43:17.816116: Pseudo dice [0.9301, 0.961, 0.9479]\n",
      "2025-12-17 15:43:17.822757: Epoch time: 137.95 s\n",
      "2025-12-17 15:43:18.509740: \n",
      "2025-12-17 15:43:18.509740: Epoch 955\n",
      "2025-12-17 15:43:18.522825: Current learning rate: 0.00061\n",
      "2025-12-17 15:45:36.684216: train_loss -0.862\n",
      "2025-12-17 15:45:36.684216: val_loss -0.8911\n",
      "2025-12-17 15:45:36.700334: Pseudo dice [0.9332, 0.962, 0.9466]\n",
      "2025-12-17 15:45:36.710237: Epoch time: 138.17 s\n",
      "2025-12-17 15:45:37.335423: \n",
      "2025-12-17 15:45:37.351212: Epoch 956\n",
      "2025-12-17 15:45:37.357121: Current learning rate: 0.0006\n",
      "2025-12-17 15:47:55.365235: train_loss -0.8618\n",
      "2025-12-17 15:47:55.365235: val_loss -0.8836\n",
      "2025-12-17 15:47:55.376629: Pseudo dice [0.9309, 0.9592, 0.9349]\n",
      "2025-12-17 15:47:55.381333: Epoch time: 138.03 s\n",
      "2025-12-17 15:47:56.173456: \n",
      "2025-12-17 15:47:56.173456: Epoch 957\n",
      "2025-12-17 15:47:56.193164: Current learning rate: 0.00059\n",
      "2025-12-17 15:50:14.117797: train_loss -0.8668\n",
      "2025-12-17 15:50:14.117797: val_loss -0.8757\n",
      "2025-12-17 15:50:14.133823: Pseudo dice [0.9216, 0.9571, 0.943]\n",
      "2025-12-17 15:50:14.133823: Epoch time: 137.94 s\n",
      "2025-12-17 15:50:14.768177: \n",
      "2025-12-17 15:50:14.768177: Epoch 958\n",
      "2025-12-17 15:50:14.768177: Current learning rate: 0.00058\n",
      "2025-12-17 15:52:32.693604: train_loss -0.8649\n",
      "2025-12-17 15:52:32.693604: val_loss -0.8772\n",
      "2025-12-17 15:52:32.693604: Pseudo dice [0.9286, 0.9563, 0.9415]\n",
      "2025-12-17 15:52:32.707379: Epoch time: 137.93 s\n",
      "2025-12-17 15:52:33.343068: \n",
      "2025-12-17 15:52:33.358867: Epoch 959\n",
      "2025-12-17 15:52:33.363590: Current learning rate: 0.00056\n",
      "2025-12-17 15:54:51.406293: train_loss -0.8641\n",
      "2025-12-17 15:54:51.408295: val_loss -0.8776\n",
      "2025-12-17 15:54:51.415779: Pseudo dice [0.9249, 0.9591, 0.9429]\n",
      "2025-12-17 15:54:51.422282: Epoch time: 138.06 s\n",
      "2025-12-17 15:54:52.051067: \n",
      "2025-12-17 15:54:52.051067: Epoch 960\n",
      "2025-12-17 15:54:52.051067: Current learning rate: 0.00055\n",
      "2025-12-17 15:57:10.031542: train_loss -0.867\n",
      "2025-12-17 15:57:10.031542: val_loss -0.884\n",
      "2025-12-17 15:57:10.037549: Pseudo dice [0.9269, 0.9599, 0.9455]\n",
      "2025-12-17 15:57:10.043293: Epoch time: 137.98 s\n",
      "2025-12-17 15:57:10.772174: \n",
      "2025-12-17 15:57:10.772174: Epoch 961\n",
      "2025-12-17 15:57:10.789327: Current learning rate: 0.00054\n",
      "2025-12-17 15:59:28.843513: train_loss -0.8648\n",
      "2025-12-17 15:59:28.843513: val_loss -0.8913\n",
      "2025-12-17 15:59:28.853525: Pseudo dice [0.9351, 0.9624, 0.948]\n",
      "2025-12-17 15:59:28.859273: Epoch time: 138.07 s\n",
      "2025-12-17 15:59:29.489086: \n",
      "2025-12-17 15:59:29.489086: Epoch 962\n",
      "2025-12-17 15:59:29.499043: Current learning rate: 0.00053\n",
      "2025-12-17 16:01:47.465619: train_loss -0.8682\n",
      "2025-12-17 16:01:47.465619: val_loss -0.8739\n",
      "2025-12-17 16:01:47.465619: Pseudo dice [0.9237, 0.956, 0.9288]\n",
      "2025-12-17 16:01:47.481551: Epoch time: 137.98 s\n",
      "2025-12-17 16:01:48.273541: \n",
      "2025-12-17 16:01:48.273541: Epoch 963\n",
      "2025-12-17 16:01:48.292821: Current learning rate: 0.00051\n",
      "2025-12-17 16:04:06.297511: train_loss -0.8693\n",
      "2025-12-17 16:04:06.297511: val_loss -0.8783\n",
      "2025-12-17 16:04:06.309107: Pseudo dice [0.9256, 0.9585, 0.9395]\n",
      "2025-12-17 16:04:06.316140: Epoch time: 138.02 s\n",
      "2025-12-17 16:04:07.042021: \n",
      "2025-12-17 16:04:07.042021: Epoch 964\n",
      "2025-12-17 16:04:07.054787: Current learning rate: 0.0005\n",
      "2025-12-17 16:06:24.852670: train_loss -0.8691\n",
      "2025-12-17 16:06:24.852670: val_loss -0.877\n",
      "2025-12-17 16:06:24.873656: Pseudo dice [0.9226, 0.9574, 0.9389]\n",
      "2025-12-17 16:06:24.878590: Epoch time: 137.81 s\n",
      "2025-12-17 16:06:25.534289: \n",
      "2025-12-17 16:06:25.534289: Epoch 965\n",
      "2025-12-17 16:06:25.545393: Current learning rate: 0.00049\n",
      "2025-12-17 16:08:43.711433: train_loss -0.8675\n",
      "2025-12-17 16:08:43.711433: val_loss -0.8835\n",
      "2025-12-17 16:08:43.721448: Pseudo dice [0.9281, 0.9575, 0.9434]\n",
      "2025-12-17 16:08:43.729075: Epoch time: 138.18 s\n",
      "2025-12-17 16:08:44.360422: \n",
      "2025-12-17 16:08:44.360422: Epoch 966\n",
      "2025-12-17 16:08:44.374388: Current learning rate: 0.00048\n",
      "2025-12-17 16:11:02.668020: train_loss -0.8673\n",
      "2025-12-17 16:11:02.668020: val_loss -0.8803\n",
      "2025-12-17 16:11:02.678033: Pseudo dice [0.931, 0.957, 0.9351]\n",
      "2025-12-17 16:11:02.685809: Epoch time: 138.31 s\n",
      "2025-12-17 16:11:03.442584: \n",
      "2025-12-17 16:11:03.442584: Epoch 967\n",
      "2025-12-17 16:11:03.451155: Current learning rate: 0.00046\n",
      "2025-12-17 16:13:21.411479: train_loss -0.8667\n",
      "2025-12-17 16:13:21.411479: val_loss -0.8756\n",
      "2025-12-17 16:13:21.419227: Pseudo dice [0.9278, 0.9555, 0.9354]\n",
      "2025-12-17 16:13:21.426416: Epoch time: 137.97 s\n",
      "2025-12-17 16:13:22.098134: \n",
      "2025-12-17 16:13:22.098134: Epoch 968\n",
      "2025-12-17 16:13:22.119735: Current learning rate: 0.00045\n",
      "2025-12-17 16:15:40.064158: train_loss -0.8655\n",
      "2025-12-17 16:15:40.064158: val_loss -0.886\n",
      "2025-12-17 16:15:40.073910: Pseudo dice [0.9282, 0.958, 0.9464]\n",
      "2025-12-17 16:15:40.081918: Epoch time: 137.97 s\n",
      "2025-12-17 16:15:40.940110: \n",
      "2025-12-17 16:15:40.940110: Epoch 969\n",
      "2025-12-17 16:15:40.956032: Current learning rate: 0.00044\n",
      "2025-12-17 16:17:58.948931: train_loss -0.8687\n",
      "2025-12-17 16:17:58.950933: val_loss -0.8809\n",
      "2025-12-17 16:17:58.960685: Pseudo dice [0.9311, 0.9554, 0.9415]\n",
      "2025-12-17 16:17:58.968694: Epoch time: 138.01 s\n",
      "2025-12-17 16:17:59.752908: \n",
      "2025-12-17 16:17:59.754911: Epoch 970\n",
      "2025-12-17 16:17:59.760920: Current learning rate: 0.00043\n",
      "2025-12-17 16:20:17.778899: train_loss -0.8651\n",
      "2025-12-17 16:20:17.778899: val_loss -0.8749\n",
      "2025-12-17 16:20:17.784908: Pseudo dice [0.9262, 0.9555, 0.9342]\n",
      "2025-12-17 16:20:17.790364: Epoch time: 138.03 s\n",
      "2025-12-17 16:20:18.462600: \n",
      "2025-12-17 16:20:18.462600: Epoch 971\n",
      "2025-12-17 16:20:18.462600: Current learning rate: 0.00041\n",
      "2025-12-17 16:22:36.322556: train_loss -0.8675\n",
      "2025-12-17 16:22:36.322556: val_loss -0.8773\n",
      "2025-12-17 16:22:36.328564: Pseudo dice [0.9267, 0.9527, 0.9421]\n",
      "2025-12-17 16:22:36.336333: Epoch time: 137.86 s\n",
      "2025-12-17 16:22:36.996685: \n",
      "2025-12-17 16:22:36.996685: Epoch 972\n",
      "2025-12-17 16:22:36.996685: Current learning rate: 0.0004\n",
      "2025-12-17 16:24:55.092173: train_loss -0.87\n",
      "2025-12-17 16:24:55.092173: val_loss -0.8843\n",
      "2025-12-17 16:24:55.108002: Pseudo dice [0.9283, 0.9576, 0.9394]\n",
      "2025-12-17 16:24:55.108002: Epoch time: 138.1 s\n",
      "2025-12-17 16:24:55.741902: \n",
      "2025-12-17 16:24:55.741902: Epoch 973\n",
      "2025-12-17 16:24:55.759705: Current learning rate: 0.00039\n",
      "2025-12-17 16:27:13.747036: train_loss -0.8674\n",
      "2025-12-17 16:27:13.749038: val_loss -0.8867\n",
      "2025-12-17 16:27:13.753044: Pseudo dice [0.9348, 0.9604, 0.9458]\n",
      "2025-12-17 16:27:13.762527: Epoch time: 138.01 s\n",
      "2025-12-17 16:27:14.482914: \n",
      "2025-12-17 16:27:14.482914: Epoch 974\n",
      "2025-12-17 16:27:14.490243: Current learning rate: 0.00037\n",
      "2025-12-17 16:29:32.366747: train_loss -0.8659\n",
      "2025-12-17 16:29:32.366747: val_loss -0.892\n",
      "2025-12-17 16:29:32.377645: Pseudo dice [0.936, 0.9642, 0.9409]\n",
      "2025-12-17 16:29:32.384465: Epoch time: 137.89 s\n",
      "2025-12-17 16:29:33.062927: \n",
      "2025-12-17 16:29:33.062927: Epoch 975\n",
      "2025-12-17 16:29:33.062927: Current learning rate: 0.00036\n",
      "2025-12-17 16:31:51.277789: train_loss -0.861\n",
      "2025-12-17 16:31:51.277789: val_loss -0.8721\n",
      "2025-12-17 16:31:51.285290: Pseudo dice [0.9204, 0.9541, 0.9411]\n",
      "2025-12-17 16:31:51.290772: Epoch time: 138.23 s\n",
      "2025-12-17 16:31:52.101795: \n",
      "2025-12-17 16:31:52.101795: Epoch 976\n",
      "2025-12-17 16:31:52.112889: Current learning rate: 0.00035\n",
      "2025-12-17 16:34:09.991241: train_loss -0.8678\n",
      "2025-12-17 16:34:09.991241: val_loss -0.8796\n",
      "2025-12-17 16:34:10.003088: Pseudo dice [0.9306, 0.9574, 0.935]\n",
      "2025-12-17 16:34:10.007363: Epoch time: 137.89 s\n",
      "2025-12-17 16:34:10.643160: \n",
      "2025-12-17 16:34:10.643160: Epoch 977\n",
      "2025-12-17 16:34:10.643160: Current learning rate: 0.00034\n",
      "2025-12-17 16:36:28.643147: train_loss -0.8644\n",
      "2025-12-17 16:36:28.643147: val_loss -0.8919\n",
      "2025-12-17 16:36:28.659216: Pseudo dice [0.9358, 0.9641, 0.9384]\n",
      "2025-12-17 16:36:28.659216: Epoch time: 138.0 s\n",
      "2025-12-17 16:36:29.372151: \n",
      "2025-12-17 16:36:29.372151: Epoch 978\n",
      "2025-12-17 16:36:29.381915: Current learning rate: 0.00032\n",
      "2025-12-17 16:38:47.139432: train_loss -0.8663\n",
      "2025-12-17 16:38:47.139432: val_loss -0.8912\n",
      "2025-12-17 16:38:47.151754: Pseudo dice [0.9353, 0.9636, 0.944]\n",
      "2025-12-17 16:38:47.158862: Epoch time: 137.77 s\n",
      "2025-12-17 16:38:47.868384: \n",
      "2025-12-17 16:38:47.868384: Epoch 979\n",
      "2025-12-17 16:38:47.868384: Current learning rate: 0.00031\n",
      "2025-12-17 16:41:05.845916: train_loss -0.8601\n",
      "2025-12-17 16:41:05.845916: val_loss -0.8779\n",
      "2025-12-17 16:41:05.853554: Pseudo dice [0.9265, 0.9564, 0.9423]\n",
      "2025-12-17 16:41:05.860955: Epoch time: 137.98 s\n",
      "2025-12-17 16:41:06.496655: \n",
      "2025-12-17 16:41:06.496655: Epoch 980\n",
      "2025-12-17 16:41:06.512626: Current learning rate: 0.0003\n",
      "2025-12-17 16:43:24.355515: train_loss -0.8674\n",
      "2025-12-17 16:43:24.355515: val_loss -0.8768\n",
      "2025-12-17 16:43:24.355515: Pseudo dice [0.9269, 0.9559, 0.9467]\n",
      "2025-12-17 16:43:24.373278: Epoch time: 137.86 s\n",
      "2025-12-17 16:43:25.099276: \n",
      "2025-12-17 16:43:25.099276: Epoch 981\n",
      "2025-12-17 16:43:25.246450: Current learning rate: 0.00028\n",
      "2025-12-17 16:45:43.380381: train_loss -0.8696\n",
      "2025-12-17 16:45:43.382383: val_loss -0.882\n",
      "2025-12-17 16:45:43.387864: Pseudo dice [0.9307, 0.9603, 0.9406]\n",
      "2025-12-17 16:45:43.387864: Epoch time: 138.28 s\n",
      "2025-12-17 16:45:44.023955: \n",
      "2025-12-17 16:45:44.023955: Epoch 982\n",
      "2025-12-17 16:45:44.023955: Current learning rate: 0.00027\n",
      "2025-12-17 16:48:01.966953: train_loss -0.8663\n",
      "2025-12-17 16:48:01.966953: val_loss -0.8909\n",
      "2025-12-17 16:48:01.982772: Pseudo dice [0.9355, 0.963, 0.9409]\n",
      "2025-12-17 16:48:01.990150: Epoch time: 137.94 s\n",
      "2025-12-17 16:48:02.792415: \n",
      "2025-12-17 16:48:02.792415: Epoch 983\n",
      "2025-12-17 16:48:02.803123: Current learning rate: 0.00026\n",
      "2025-12-17 16:50:20.867853: train_loss -0.8635\n",
      "2025-12-17 16:50:20.869856: val_loss -0.8834\n",
      "2025-12-17 16:50:20.879611: Pseudo dice [0.9338, 0.9601, 0.9273]\n",
      "2025-12-17 16:50:20.887623: Epoch time: 138.08 s\n",
      "2025-12-17 16:50:21.648881: \n",
      "2025-12-17 16:50:21.648881: Epoch 984\n",
      "2025-12-17 16:50:21.648881: Current learning rate: 0.00024\n",
      "2025-12-17 16:52:39.644253: train_loss -0.8663\n",
      "2025-12-17 16:52:39.644253: val_loss -0.8857\n",
      "2025-12-17 16:52:39.664057: Pseudo dice [0.9311, 0.9598, 0.9419]\n",
      "2025-12-17 16:52:39.670063: Epoch time: 138.0 s\n",
      "2025-12-17 16:52:40.308290: \n",
      "2025-12-17 16:52:40.308290: Epoch 985\n",
      "2025-12-17 16:52:40.308290: Current learning rate: 0.00023\n",
      "2025-12-17 16:54:58.263665: train_loss -0.8687\n",
      "2025-12-17 16:54:58.263665: val_loss -0.8893\n",
      "2025-12-17 16:54:58.271672: Pseudo dice [0.9342, 0.9614, 0.9455]\n",
      "2025-12-17 16:54:58.277417: Epoch time: 137.96 s\n",
      "2025-12-17 16:54:58.955588: \n",
      "2025-12-17 16:54:58.955588: Epoch 986\n",
      "2025-12-17 16:54:58.955588: Current learning rate: 0.00021\n",
      "2025-12-17 16:57:16.930303: train_loss -0.8681\n",
      "2025-12-17 16:57:16.930303: val_loss -0.8713\n",
      "2025-12-17 16:57:16.940057: Pseudo dice [0.9238, 0.9545, 0.9389]\n",
      "2025-12-17 16:57:16.946067: Epoch time: 137.97 s\n",
      "2025-12-17 16:57:17.706918: \n",
      "2025-12-17 16:57:17.706918: Epoch 987\n",
      "2025-12-17 16:57:17.706918: Current learning rate: 0.0002\n",
      "2025-12-17 16:59:35.677908: train_loss -0.8658\n",
      "2025-12-17 16:59:35.677908: val_loss -0.8797\n",
      "2025-12-17 16:59:35.699969: Pseudo dice [0.9285, 0.9579, 0.9349]\n",
      "2025-12-17 16:59:35.699969: Epoch time: 137.97 s\n",
      "2025-12-17 16:59:36.342444: \n",
      "2025-12-17 16:59:36.342444: Epoch 988\n",
      "2025-12-17 16:59:36.342444: Current learning rate: 0.00019\n",
      "2025-12-17 17:01:54.392209: train_loss -0.8619\n",
      "2025-12-17 17:01:54.392209: val_loss -0.8824\n",
      "2025-12-17 17:01:54.402663: Pseudo dice [0.9295, 0.9609, 0.9321]\n",
      "2025-12-17 17:01:54.410271: Epoch time: 138.05 s\n",
      "2025-12-17 17:01:55.262354: \n",
      "2025-12-17 17:01:55.262354: Epoch 989\n",
      "2025-12-17 17:01:55.262354: Current learning rate: 0.00017\n",
      "2025-12-17 17:04:13.373979: train_loss -0.8652\n",
      "2025-12-17 17:04:13.373979: val_loss -0.8881\n",
      "2025-12-17 17:04:13.383732: Pseudo dice [0.933, 0.9616, 0.9416]\n",
      "2025-12-17 17:04:13.389740: Epoch time: 138.11 s\n",
      "2025-12-17 17:04:14.135818: \n",
      "2025-12-17 17:04:14.135818: Epoch 990\n",
      "2025-12-17 17:04:14.149598: Current learning rate: 0.00016\n",
      "2025-12-17 17:06:32.216658: train_loss -0.8643\n",
      "2025-12-17 17:06:32.216658: val_loss -0.8838\n",
      "2025-12-17 17:06:32.231327: Pseudo dice [0.9291, 0.958, 0.9412]\n",
      "2025-12-17 17:06:32.234164: Epoch time: 138.08 s\n",
      "2025-12-17 17:06:32.897905: \n",
      "2025-12-17 17:06:32.897905: Epoch 991\n",
      "2025-12-17 17:06:32.897905: Current learning rate: 0.00014\n",
      "2025-12-17 17:08:52.063588: train_loss -0.8629\n",
      "2025-12-17 17:08:52.065591: val_loss -0.8798\n",
      "2025-12-17 17:08:52.070598: Pseudo dice [0.9278, 0.9565, 0.9392]\n",
      "2025-12-17 17:08:52.079388: Epoch time: 139.18 s\n",
      "2025-12-17 17:08:52.716103: \n",
      "2025-12-17 17:08:52.716103: Epoch 992\n",
      "2025-12-17 17:08:52.716103: Current learning rate: 0.00013\n",
      "2025-12-17 17:11:11.413685: train_loss -0.8717\n",
      "2025-12-17 17:11:11.413685: val_loss -0.8814\n",
      "2025-12-17 17:11:11.417199: Pseudo dice [0.9241, 0.9602, 0.9504]\n",
      "2025-12-17 17:11:11.429659: Epoch time: 138.7 s\n",
      "2025-12-17 17:11:12.150451: \n",
      "2025-12-17 17:11:12.150451: Epoch 993\n",
      "2025-12-17 17:11:12.150451: Current learning rate: 0.00011\n",
      "2025-12-17 17:13:30.531043: train_loss -0.8655\n",
      "2025-12-17 17:13:30.531043: val_loss -0.8911\n",
      "2025-12-17 17:13:30.531043: Pseudo dice [0.9382, 0.9622, 0.9414]\n",
      "2025-12-17 17:13:30.531043: Epoch time: 138.38 s\n",
      "2025-12-17 17:13:31.164844: \n",
      "2025-12-17 17:13:31.164844: Epoch 994\n",
      "2025-12-17 17:13:31.182654: Current learning rate: 0.0001\n",
      "2025-12-17 17:15:49.610341: train_loss -0.865\n",
      "2025-12-17 17:15:49.610341: val_loss -0.8859\n",
      "2025-12-17 17:15:49.610341: Pseudo dice [0.936, 0.965, 0.9334]\n",
      "2025-12-17 17:15:49.626030: Epoch time: 138.45 s\n",
      "2025-12-17 17:15:50.419568: \n",
      "2025-12-17 17:15:50.419568: Epoch 995\n",
      "2025-12-17 17:15:50.435199: Current learning rate: 8e-05\n",
      "2025-12-17 17:18:08.936252: train_loss -0.8676\n",
      "2025-12-17 17:18:08.936252: val_loss -0.887\n",
      "2025-12-17 17:18:08.946001: Pseudo dice [0.928, 0.9581, 0.9477]\n",
      "2025-12-17 17:18:08.946001: Epoch time: 138.52 s\n",
      "2025-12-17 17:18:09.657409: \n",
      "2025-12-17 17:18:09.657409: Epoch 996\n",
      "2025-12-17 17:18:09.665639: Current learning rate: 7e-05\n",
      "2025-12-17 17:20:27.665613: train_loss -0.8683\n",
      "2025-12-17 17:20:27.681610: val_loss -0.8814\n",
      "2025-12-17 17:20:27.689618: Pseudo dice [0.9283, 0.9589, 0.9403]\n",
      "2025-12-17 17:20:27.695100: Epoch time: 138.01 s\n",
      "2025-12-17 17:20:28.331136: \n",
      "2025-12-17 17:20:28.331136: Epoch 997\n",
      "2025-12-17 17:20:28.331136: Current learning rate: 5e-05\n",
      "2025-12-17 17:22:46.372478: train_loss -0.864\n",
      "2025-12-17 17:22:46.372478: val_loss -0.878\n",
      "2025-12-17 17:22:46.388383: Pseudo dice [0.9273, 0.9553, 0.9421]\n",
      "2025-12-17 17:22:46.388383: Epoch time: 138.04 s\n",
      "2025-12-17 17:22:47.022077: \n",
      "2025-12-17 17:22:47.022077: Epoch 998\n",
      "2025-12-17 17:22:47.022077: Current learning rate: 4e-05\n",
      "2025-12-17 17:25:05.056607: train_loss -0.8683\n",
      "2025-12-17 17:25:05.056607: val_loss -0.8819\n",
      "2025-12-17 17:25:05.067353: Pseudo dice [0.9287, 0.9606, 0.9399]\n",
      "2025-12-17 17:25:05.073362: Epoch time: 138.03 s\n",
      "2025-12-17 17:25:05.823336: \n",
      "2025-12-17 17:25:05.823336: Epoch 999\n",
      "2025-12-17 17:25:05.839298: Current learning rate: 2e-05\n",
      "2025-12-17 17:27:23.828034: train_loss -0.8646\n",
      "2025-12-17 17:27:23.828034: val_loss -0.885\n",
      "2025-12-17 17:27:23.842091: Pseudo dice [0.9274, 0.9555, 0.9482]\n",
      "2025-12-17 17:27:23.842091: Epoch time: 138.0 s\n",
      "2025-12-17 17:27:24.932116: Training done.\n",
      "2025-12-17 17:27:25.016740: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-17 17:27:25.016740: The split file contains 5 splits.\n",
      "2025-12-17 17:27:25.032480: Desired fold for training: 1\n",
      "2025-12-17 17:27:25.041595: This split has 400 training and 100 validation cases.\n",
      "2025-12-17 17:27:25.048491: predicting OAS30014_MR_d0196_3\n",
      "2025-12-17 17:27:25.222425: OAS30014_MR_d0196_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:27:45.379035: predicting OAS30014_MR_d0196_4\n",
      "2025-12-17 17:27:45.394890: OAS30014_MR_d0196_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:28:02.179777: predicting OAS30017_MR_d0054_6\n",
      "2025-12-17 17:28:02.187159: OAS30017_MR_d0054_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:28:18.992092: predicting OAS30017_MR_d0054_7\n",
      "2025-12-17 17:28:19.008009: OAS30017_MR_d0054_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:28:35.822875: predicting OAS30017_MR_d0054_8\n",
      "2025-12-17 17:28:35.832824: OAS30017_MR_d0054_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:28:52.651367: predicting OAS30025_MR_d0210_1\n",
      "2025-12-17 17:28:52.671181: OAS30025_MR_d0210_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:29:09.474107: predicting OAS30025_MR_d0210_2\n",
      "2025-12-17 17:29:09.483663: OAS30025_MR_d0210_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:29:26.310576: predicting OAS30036_MR_d0059_1\n",
      "2025-12-17 17:29:26.328024: OAS30036_MR_d0059_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:29:43.129139: predicting OAS30036_MR_d0059_10\n",
      "2025-12-17 17:29:43.146597: OAS30036_MR_d0059_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:29:59.958431: predicting OAS30036_MR_d0059_5\n",
      "2025-12-17 17:29:59.965180: OAS30036_MR_d0059_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:30:16.818753: predicting OAS30036_MR_d0059_8\n",
      "2025-12-17 17:30:16.838535: OAS30036_MR_d0059_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:30:33.630441: predicting OAS30039_MR_d1203_6\n",
      "2025-12-17 17:30:33.646527: OAS30039_MR_d1203_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:30:50.472993: predicting OAS30052_MR_d0693_1\n",
      "2025-12-17 17:30:50.482827: OAS30052_MR_d0693_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:31:07.293071: predicting OAS30052_MR_d0693_10\n",
      "2025-12-17 17:31:07.314771: OAS30052_MR_d0693_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:31:24.096781: predicting OAS30052_MR_d0693_4\n",
      "2025-12-17 17:31:24.118437: OAS30052_MR_d0693_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:31:40.960650: predicting OAS30078_MR_d0210_1\n",
      "2025-12-17 17:31:40.978176: OAS30078_MR_d0210_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:31:57.762589: predicting OAS30078_MR_d0210_7\n",
      "2025-12-17 17:31:57.778593: OAS30078_MR_d0210_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:32:14.573452: predicting OAS30083_MR_d0465_3\n",
      "2025-12-17 17:32:14.596919: OAS30083_MR_d0465_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:32:31.418804: predicting OAS30083_MR_d0465_5\n",
      "2025-12-17 17:32:31.428323: OAS30083_MR_d0465_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:32:48.224614: predicting OAS30087_MR_d0260_4\n",
      "2025-12-17 17:32:48.234063: OAS30087_MR_d0260_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:33:05.037126: predicting OAS30087_MR_d0260_7\n",
      "2025-12-17 17:33:05.053249: OAS30087_MR_d0260_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:33:21.890230: predicting OAS30087_MR_d0260_8\n",
      "2025-12-17 17:33:21.912544: OAS30087_MR_d0260_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:33:38.694189: predicting OAS30102_MR_d0024_6\n",
      "2025-12-17 17:33:38.710305: OAS30102_MR_d0024_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:33:55.494685: predicting OAS30104_MR_d0328_2\n",
      "2025-12-17 17:33:55.510473: OAS30104_MR_d0328_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:34:12.311588: predicting OAS30104_MR_d0328_3\n",
      "2025-12-17 17:34:12.321607: OAS30104_MR_d0328_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:34:29.116329: predicting OAS30107_MR_d0387_8\n",
      "2025-12-17 17:34:29.129184: OAS30107_MR_d0387_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:34:45.991322: predicting OAS30125_MR_d0201_2\n",
      "2025-12-17 17:34:46.000592: OAS30125_MR_d0201_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:35:02.793149: predicting OAS30125_MR_d0201_7\n",
      "2025-12-17 17:35:02.803167: OAS30125_MR_d0201_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:35:19.595599: predicting OAS30134_MR_d0080_6\n",
      "2025-12-17 17:35:19.607733: OAS30134_MR_d0080_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:35:36.413559: predicting OAS30134_MR_d0080_7\n",
      "2025-12-17 17:35:36.422626: OAS30134_MR_d0080_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:35:53.213173: predicting OAS30134_MR_d0080_8\n",
      "2025-12-17 17:35:53.225691: OAS30134_MR_d0080_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:36:10.036287: predicting OAS30140_MR_d0172_1\n",
      "2025-12-17 17:36:10.043875: OAS30140_MR_d0172_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:36:26.834854: predicting OAS30140_MR_d0172_10\n",
      "2025-12-17 17:36:26.857645: OAS30140_MR_d0172_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:36:43.656661: predicting OAS30140_MR_d0172_2\n",
      "2025-12-17 17:36:43.664675: OAS30140_MR_d0172_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:37:00.454638: predicting OAS30140_MR_d0172_4\n",
      "2025-12-17 17:37:00.466972: OAS30140_MR_d0172_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:37:17.313198: predicting OAS30140_MR_d0172_8\n",
      "2025-12-17 17:37:17.330995: OAS30140_MR_d0172_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:37:34.142612: predicting OAS30140_MR_d0172_9\n",
      "2025-12-17 17:37:34.162432: OAS30140_MR_d0172_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:37:50.941386: predicting OAS30165_MR_d1763_2\n",
      "2025-12-17 17:37:50.965321: OAS30165_MR_d1763_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:38:07.809006: predicting OAS30165_MR_d1763_6\n",
      "2025-12-17 17:38:07.833379: OAS30165_MR_d1763_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:38:24.626865: predicting OAS30167_MR_d0111_1\n",
      "2025-12-17 17:38:24.640758: OAS30167_MR_d0111_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:38:41.474992: predicting OAS30167_MR_d0111_7\n",
      "2025-12-17 17:38:41.489101: OAS30167_MR_d0111_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:38:58.281679: predicting OAS30176_MR_d0000_4\n",
      "2025-12-17 17:38:58.301457: OAS30176_MR_d0000_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:39:15.089763: predicting OAS30176_MR_d0000_6\n",
      "2025-12-17 17:39:15.102910: OAS30176_MR_d0000_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:39:31.875669: predicting OAS30176_MR_d0000_7\n",
      "2025-12-17 17:39:31.897512: OAS30176_MR_d0000_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:39:48.692472: predicting OAS30195_MR_d1596_3\n",
      "2025-12-17 17:39:48.704287: OAS30195_MR_d1596_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:40:05.542220: predicting OAS30195_MR_d1596_5\n",
      "2025-12-17 17:40:05.550173: OAS30195_MR_d1596_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:40:22.348366: predicting OAS30195_MR_d1596_9\n",
      "2025-12-17 17:40:22.357858: OAS30195_MR_d1596_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:40:39.193252: predicting OAS30226_MR_d0183_3\n",
      "2025-12-17 17:40:39.203053: OAS30226_MR_d0183_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:40:56.030679: predicting OAS30234_MR_d2098_1\n",
      "2025-12-17 17:40:56.044293: OAS30234_MR_d2098_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:41:12.842300: predicting OAS30234_MR_d2098_4\n",
      "2025-12-17 17:41:12.850538: OAS30234_MR_d2098_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:41:29.667969: predicting OAS30234_MR_d2098_7\n",
      "2025-12-17 17:41:29.691296: OAS30234_MR_d2098_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:41:46.469872: predicting OAS30238_MR_d0037_6\n",
      "2025-12-17 17:41:46.487938: OAS30238_MR_d0037_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:42:03.276319: predicting OAS30250_MR_d0389_1\n",
      "2025-12-17 17:42:03.292402: OAS30250_MR_d0389_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:42:20.117654: predicting OAS30250_MR_d0389_8\n",
      "2025-12-17 17:42:20.133415: OAS30250_MR_d0389_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:42:36.973876: predicting OAS30262_MR_d0037_2\n",
      "2025-12-17 17:42:36.982370: OAS30262_MR_d0037_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:42:53.764300: predicting OAS30274_MR_d3332_6\n",
      "2025-12-17 17:42:53.781956: OAS30274_MR_d3332_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:43:10.617465: predicting OAS30297_MR_d1712_1\n",
      "2025-12-17 17:43:10.625216: OAS30297_MR_d1712_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:43:27.402890: predicting OAS30297_MR_d1712_10\n",
      "2025-12-17 17:43:27.420839: OAS30297_MR_d1712_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:43:44.228183: predicting OAS30297_MR_d1712_6\n",
      "2025-12-17 17:43:44.237394: OAS30297_MR_d1712_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:44:01.073708: predicting OAS30300_MR_d0100_1\n",
      "2025-12-17 17:44:01.083220: OAS30300_MR_d0100_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:44:17.936025: predicting OAS30300_MR_d0100_5\n",
      "2025-12-17 17:44:17.957546: OAS30300_MR_d0100_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:44:34.771583: predicting OAS30302_MR_d0262_2\n",
      "2025-12-17 17:44:34.784778: OAS30302_MR_d0262_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:44:51.600384: predicting OAS30302_MR_d0262_4\n",
      "2025-12-17 17:44:51.620322: OAS30302_MR_d0262_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:45:08.448874: predicting OAS30306_MR_d0028_3\n",
      "2025-12-17 17:45:08.466544: OAS30306_MR_d0028_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:45:25.300864: predicting OAS30306_MR_d0028_4\n",
      "2025-12-17 17:45:25.316677: OAS30306_MR_d0028_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:45:42.138417: predicting OAS30306_MR_d0028_6\n",
      "2025-12-17 17:45:42.156516: OAS30306_MR_d0028_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:45:58.961079: predicting OAS30306_MR_d0028_7\n",
      "2025-12-17 17:45:58.974945: OAS30306_MR_d0028_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:46:15.826523: predicting OAS30306_MR_d0028_9\n",
      "2025-12-17 17:46:15.835052: OAS30306_MR_d0028_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:46:32.639658: predicting OAS30321_MR_d3003_5\n",
      "2025-12-17 17:46:32.656971: OAS30321_MR_d3003_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:46:49.431403: predicting OAS30325_MR_d0032_10\n",
      "2025-12-17 17:46:49.451076: OAS30325_MR_d0032_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:47:06.287519: predicting OAS30325_MR_d0032_5\n",
      "2025-12-17 17:47:06.303479: OAS30325_MR_d0032_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:47:23.101742: predicting OAS30343_MR_d4178_1\n",
      "2025-12-17 17:47:23.109248: OAS30343_MR_d4178_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:47:39.920804: predicting OAS30343_MR_d4178_10\n",
      "2025-12-17 17:47:39.929818: OAS30343_MR_d4178_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:47:56.711656: predicting OAS30343_MR_d4178_5\n",
      "2025-12-17 17:47:56.735264: OAS30343_MR_d4178_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:48:13.522372: predicting OAS30350_MR_d0018_6\n",
      "2025-12-17 17:48:13.544046: OAS30350_MR_d0018_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:48:30.386886: predicting OAS30350_MR_d0018_8\n",
      "2025-12-17 17:48:30.401292: OAS30350_MR_d0018_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:48:47.193133: predicting OAS30352_MR_d0099_8\n",
      "2025-12-17 17:48:47.200928: OAS30352_MR_d0099_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:49:04.001901: predicting OAS30355_MR_d0048_2\n",
      "2025-12-17 17:49:04.014699: OAS30355_MR_d0048_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:49:20.805187: predicting OAS30355_MR_d0048_4\n",
      "2025-12-17 17:49:20.821036: OAS30355_MR_d0048_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:49:37.607079: predicting OAS30355_MR_d0048_9\n",
      "2025-12-17 17:49:37.624380: OAS30355_MR_d0048_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:49:54.467251: predicting OAS30361_MR_d1457_10\n",
      "2025-12-17 17:49:54.478939: OAS30361_MR_d1457_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:50:11.288388: predicting OAS30361_MR_d1457_6\n",
      "2025-12-17 17:50:11.303643: OAS30361_MR_d1457_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:50:28.101683: predicting OAS30367_MR_d1540_3\n",
      "2025-12-17 17:50:28.111049: OAS30367_MR_d1540_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:50:44.902414: predicting OAS30367_MR_d1540_4\n",
      "2025-12-17 17:50:44.914173: OAS30367_MR_d1540_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:51:01.738238: predicting OAS30367_MR_d1540_9\n",
      "2025-12-17 17:51:01.760045: OAS30367_MR_d1540_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:51:18.602253: predicting OAS30371_MR_d0338_2\n",
      "2025-12-17 17:51:18.622064: OAS30371_MR_d0338_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:51:35.461982: predicting OAS30371_MR_d0338_4\n",
      "2025-12-17 17:51:35.481954: OAS30371_MR_d0338_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:51:52.327212: predicting OAS30373_MR_d1211_4\n",
      "2025-12-17 17:51:52.339093: OAS30373_MR_d1211_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:52:09.163268: predicting OAS30373_MR_d1211_5\n",
      "2025-12-17 17:52:09.185229: OAS30373_MR_d1211_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:52:25.975944: predicting OAS30373_MR_d1211_6\n",
      "2025-12-17 17:52:25.999184: OAS30373_MR_d1211_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:52:42.838923: predicting OAS30380_MR_d3446_5\n",
      "2025-12-17 17:52:42.858055: OAS30380_MR_d3446_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:52:59.653346: predicting OAS30380_MR_d3446_7\n",
      "2025-12-17 17:52:59.662716: OAS30380_MR_d3446_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:53:16.465717: predicting OAS30380_MR_d3446_8\n",
      "2025-12-17 17:53:16.473534: OAS30380_MR_d3446_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:53:33.257604: predicting OAS30383_MR_d0134_3\n",
      "2025-12-17 17:53:33.267717: OAS30383_MR_d0134_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:53:50.067072: predicting OAS30383_MR_d0134_4\n",
      "2025-12-17 17:53:50.082973: OAS30383_MR_d0134_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:54:06.928697: predicting OAS30383_MR_d0134_7\n",
      "2025-12-17 17:54:06.938531: OAS30383_MR_d0134_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:54:23.782951: predicting OAS30388_MR_d0073_3\n",
      "2025-12-17 17:54:23.796507: OAS30388_MR_d0073_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:54:40.584112: predicting OAS30388_MR_d0073_4\n",
      "2025-12-17 17:54:40.593693: OAS30388_MR_d0073_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:54:57.391587: predicting OAS30388_MR_d0073_5\n",
      "2025-12-17 17:54:57.403281: OAS30388_MR_d0073_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:55:14.243497: predicting OAS30388_MR_d0073_9\n",
      "2025-12-17 17:55:14.253139: OAS30388_MR_d0073_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-17 17:55:55.948436: Validation complete\n",
      "2025-12-17 17:55:55.948436: Mean Validation Dice:  0.9334803764188582\n"
     ]
    }
   ],
   "source": [
    "# Train nnU-Net on fold 1\n",
    "!nnUNetv2_train 500 3d_lowres 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfea471d-d996-47b0-ad45-6f7ff9ebd6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-12-17 17:56:16.709739: do_dummy_2d_data_aug: False\n",
      "2025-12-17 17:56:16.709739: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-17 17:56:16.725434: The split file contains 5 splits.\n",
      "2025-12-17 17:56:16.725434: Desired fold for training: 2\n",
      "2025-12-17 17:56:16.725434: This split has 400 training and 100 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2025-12-17 17:56:49.462181: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_lowres\n",
      " {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [202, 202, 202], 'spacing': [1.2667700813876164, 1.2667700813876164, 1.2667700813876164], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset500_MRI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [256, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.422696590423584, 'median': 0.4194243550300598, 'min': 0.0027002037968486547, 'percentile_00_5': 0.05628390982747078, 'percentile_99_5': 0.8565635681152344, 'std': 0.19347868859767914}}} \n",
      "\n",
      "2025-12-17 17:56:49.462181: unpacking dataset...\n",
      "2025-12-17 17:56:50.016427: unpacking done...\n",
      "2025-12-17 17:56:50.030502: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-12-17 17:56:50.062064: \n",
      "2025-12-17 17:56:50.062064: Epoch 0\n",
      "2025-12-17 17:56:50.062064: Current learning rate: 0.01\n",
      "2025-12-17 17:59:17.804329: train_loss 0.2179\n",
      "2025-12-17 17:59:17.804329: val_loss 0.0076\n",
      "2025-12-17 17:59:17.804329: Pseudo dice [0.4868, 0.5665, 0.006]\n",
      "2025-12-17 17:59:17.804329: Epoch time: 147.74 s\n",
      "2025-12-17 17:59:17.804329: Yayy! New best EMA pseudo Dice: 0.3531\n",
      "2025-12-17 17:59:18.691542: \n",
      "2025-12-17 17:59:18.691542: Epoch 1\n",
      "2025-12-17 17:59:18.691542: Current learning rate: 0.00999\n",
      "2025-12-17 18:01:36.741355: train_loss -0.0391\n",
      "2025-12-17 18:01:36.741355: val_loss -0.1364\n",
      "2025-12-17 18:01:36.756998: Pseudo dice [0.5906, 0.5625, 0.3901]\n",
      "2025-12-17 18:01:36.756998: Epoch time: 138.05 s\n",
      "2025-12-17 18:01:36.756998: Yayy! New best EMA pseudo Dice: 0.3692\n",
      "2025-12-17 18:01:37.758195: \n",
      "2025-12-17 18:01:37.758195: Epoch 2\n",
      "2025-12-17 18:01:37.758195: Current learning rate: 0.00998\n",
      "2025-12-17 18:03:55.764667: train_loss -0.1426\n",
      "2025-12-17 18:03:55.764667: val_loss -0.1792\n",
      "2025-12-17 18:03:55.764667: Pseudo dice [0.5713, 0.5924, 0.4537]\n",
      "2025-12-17 18:03:55.764667: Epoch time: 138.01 s\n",
      "2025-12-17 18:03:55.764667: Yayy! New best EMA pseudo Dice: 0.3862\n",
      "2025-12-17 18:03:56.699483: \n",
      "2025-12-17 18:03:56.699483: Epoch 3\n",
      "2025-12-17 18:03:56.699483: Current learning rate: 0.00997\n",
      "2025-12-17 18:06:14.805130: train_loss -0.2112\n",
      "2025-12-17 18:06:14.807133: val_loss -0.2543\n",
      "2025-12-17 18:06:14.807133: Pseudo dice [0.5849, 0.6672, 0.4747]\n",
      "2025-12-17 18:06:14.807133: Epoch time: 138.11 s\n",
      "2025-12-17 18:06:14.809135: Yayy! New best EMA pseudo Dice: 0.4051\n",
      "2025-12-17 18:06:15.700555: \n",
      "2025-12-17 18:06:15.700555: Epoch 4\n",
      "2025-12-17 18:06:15.700555: Current learning rate: 0.00996\n",
      "2025-12-17 18:08:33.641817: train_loss -0.2474\n",
      "2025-12-17 18:08:33.641817: val_loss -0.3016\n",
      "2025-12-17 18:08:33.641817: Pseudo dice [0.625, 0.7103, 0.5672]\n",
      "2025-12-17 18:08:33.643819: Epoch time: 137.94 s\n",
      "2025-12-17 18:08:33.643819: Yayy! New best EMA pseudo Dice: 0.4281\n",
      "2025-12-17 18:08:34.579304: \n",
      "2025-12-17 18:08:34.579304: Epoch 5\n",
      "2025-12-17 18:08:34.579304: Current learning rate: 0.00995\n",
      "2025-12-17 18:10:53.152526: train_loss -0.303\n",
      "2025-12-17 18:10:53.152526: val_loss -0.3369\n",
      "2025-12-17 18:10:53.154528: Pseudo dice [0.665, 0.6982, 0.6012]\n",
      "2025-12-17 18:10:53.154528: Epoch time: 138.58 s\n",
      "2025-12-17 18:10:53.154528: Yayy! New best EMA pseudo Dice: 0.4507\n",
      "2025-12-17 18:10:54.051890: \n",
      "2025-12-17 18:10:54.051890: Epoch 6\n",
      "2025-12-17 18:10:54.051890: Current learning rate: 0.00995\n",
      "2025-12-17 18:13:11.904171: train_loss -0.3514\n",
      "2025-12-17 18:13:11.906173: val_loss -0.3809\n",
      "2025-12-17 18:13:11.906173: Pseudo dice [0.7151, 0.7655, 0.6052]\n",
      "2025-12-17 18:13:11.906173: Epoch time: 137.85 s\n",
      "2025-12-17 18:13:11.906173: Yayy! New best EMA pseudo Dice: 0.4752\n",
      "2025-12-17 18:13:12.961550: \n",
      "2025-12-17 18:13:12.961550: Epoch 7\n",
      "2025-12-17 18:13:12.961550: Current learning rate: 0.00994\n",
      "2025-12-17 18:15:30.984675: train_loss -0.3971\n",
      "2025-12-17 18:15:30.984675: val_loss -0.4377\n",
      "2025-12-17 18:15:30.984675: Pseudo dice [0.7391, 0.8101, 0.6489]\n",
      "2025-12-17 18:15:30.984675: Epoch time: 138.02 s\n",
      "2025-12-17 18:15:30.984675: Yayy! New best EMA pseudo Dice: 0.5009\n",
      "2025-12-17 18:15:31.905145: \n",
      "2025-12-17 18:15:31.905145: Epoch 8\n",
      "2025-12-17 18:15:31.905145: Current learning rate: 0.00993\n",
      "2025-12-17 18:17:49.836045: train_loss -0.4282\n",
      "2025-12-17 18:17:49.836045: val_loss -0.4583\n",
      "2025-12-17 18:17:49.836045: Pseudo dice [0.7583, 0.8055, 0.6718]\n",
      "2025-12-17 18:17:49.836045: Epoch time: 137.93 s\n",
      "2025-12-17 18:17:49.836045: Yayy! New best EMA pseudo Dice: 0.5254\n",
      "2025-12-17 18:17:50.765044: \n",
      "2025-12-17 18:17:50.765044: Epoch 9\n",
      "2025-12-17 18:17:50.765044: Current learning rate: 0.00992\n",
      "2025-12-17 18:20:10.429959: train_loss -0.4666\n",
      "2025-12-17 18:20:10.429959: val_loss -0.5072\n",
      "2025-12-17 18:20:10.431962: Pseudo dice [0.7716, 0.8404, 0.6826]\n",
      "2025-12-17 18:20:10.433965: Epoch time: 139.67 s\n",
      "2025-12-17 18:20:10.433965: Yayy! New best EMA pseudo Dice: 0.5493\n",
      "2025-12-17 18:20:11.307694: \n",
      "2025-12-17 18:20:11.307694: Epoch 10\n",
      "2025-12-17 18:20:11.307694: Current learning rate: 0.00991\n",
      "2025-12-17 18:22:29.921240: train_loss -0.5081\n",
      "2025-12-17 18:22:29.921240: val_loss -0.5405\n",
      "2025-12-17 18:22:29.923244: Pseudo dice [0.8054, 0.8471, 0.6997]\n",
      "2025-12-17 18:22:29.923244: Epoch time: 138.61 s\n",
      "2025-12-17 18:22:29.923244: Yayy! New best EMA pseudo Dice: 0.5728\n",
      "2025-12-17 18:22:30.905688: \n",
      "2025-12-17 18:22:30.905688: Epoch 11\n",
      "2025-12-17 18:22:30.905688: Current learning rate: 0.0099\n",
      "2025-12-17 18:24:49.783363: train_loss -0.5209\n",
      "2025-12-17 18:24:49.783363: val_loss -0.5456\n",
      "2025-12-17 18:24:49.785364: Pseudo dice [0.7906, 0.8655, 0.7217]\n",
      "2025-12-17 18:24:49.785364: Epoch time: 138.88 s\n",
      "2025-12-17 18:24:49.787366: Yayy! New best EMA pseudo Dice: 0.5948\n",
      "2025-12-17 18:24:50.690579: \n",
      "2025-12-17 18:24:50.690579: Epoch 12\n",
      "2025-12-17 18:24:50.690579: Current learning rate: 0.00989\n",
      "2025-12-17 18:27:09.155813: train_loss -0.5525\n",
      "2025-12-17 18:27:09.155813: val_loss -0.5766\n",
      "2025-12-17 18:27:09.155813: Pseudo dice [0.8202, 0.8639, 0.745]\n",
      "2025-12-17 18:27:09.155813: Epoch time: 138.47 s\n",
      "2025-12-17 18:27:09.155813: Yayy! New best EMA pseudo Dice: 0.6163\n",
      "2025-12-17 18:27:10.254063: \n",
      "2025-12-17 18:27:10.254063: Epoch 13\n",
      "2025-12-17 18:27:10.254063: Current learning rate: 0.00988\n",
      "2025-12-17 18:29:28.864235: train_loss -0.5835\n",
      "2025-12-17 18:29:28.864235: val_loss -0.6054\n",
      "2025-12-17 18:29:28.866237: Pseudo dice [0.8253, 0.8791, 0.7683]\n",
      "2025-12-17 18:29:28.866237: Epoch time: 138.61 s\n",
      "2025-12-17 18:29:28.866237: Yayy! New best EMA pseudo Dice: 0.6371\n",
      "2025-12-17 18:29:29.778209: \n",
      "2025-12-17 18:29:29.780268: Epoch 14\n",
      "2025-12-17 18:29:29.780268: Current learning rate: 0.00987\n",
      "2025-12-17 18:31:47.811123: train_loss -0.5976\n",
      "2025-12-17 18:31:47.811123: val_loss -0.5887\n",
      "2025-12-17 18:31:47.813125: Pseudo dice [0.8101, 0.8761, 0.7851]\n",
      "2025-12-17 18:31:47.813125: Epoch time: 138.03 s\n",
      "2025-12-17 18:31:47.815128: Yayy! New best EMA pseudo Dice: 0.6557\n",
      "2025-12-17 18:31:48.710734: \n",
      "2025-12-17 18:31:48.712736: Epoch 15\n",
      "2025-12-17 18:31:48.712736: Current learning rate: 0.00986\n",
      "2025-12-17 18:34:06.871212: train_loss -0.6009\n",
      "2025-12-17 18:34:06.871212: val_loss -0.6223\n",
      "2025-12-17 18:34:06.873215: Pseudo dice [0.8242, 0.8763, 0.799]\n",
      "2025-12-17 18:34:06.873215: Epoch time: 138.16 s\n",
      "2025-12-17 18:34:06.873215: Yayy! New best EMA pseudo Dice: 0.6735\n",
      "2025-12-17 18:34:07.780793: \n",
      "2025-12-17 18:34:07.780793: Epoch 16\n",
      "2025-12-17 18:34:07.780793: Current learning rate: 0.00986\n",
      "2025-12-17 18:36:25.950313: train_loss -0.6143\n",
      "2025-12-17 18:36:25.950313: val_loss -0.6359\n",
      "2025-12-17 18:36:25.952141: Pseudo dice [0.8266, 0.8921, 0.8072]\n",
      "2025-12-17 18:36:25.954144: Epoch time: 138.17 s\n",
      "2025-12-17 18:36:25.954144: Yayy! New best EMA pseudo Dice: 0.6903\n",
      "2025-12-17 18:36:26.863796: \n",
      "2025-12-17 18:36:26.867303: Epoch 17\n",
      "2025-12-17 18:36:26.867303: Current learning rate: 0.00985\n",
      "2025-12-17 18:38:44.963748: train_loss -0.6322\n",
      "2025-12-17 18:38:44.965754: val_loss -0.6523\n",
      "2025-12-17 18:38:44.965754: Pseudo dice [0.845, 0.8901, 0.794]\n",
      "2025-12-17 18:38:44.967756: Epoch time: 138.1 s\n",
      "2025-12-17 18:38:44.967756: Yayy! New best EMA pseudo Dice: 0.7056\n",
      "2025-12-17 18:38:45.873746: \n",
      "2025-12-17 18:38:45.873746: Epoch 18\n",
      "2025-12-17 18:38:45.873746: Current learning rate: 0.00984\n",
      "2025-12-17 18:41:03.983516: train_loss -0.6398\n",
      "2025-12-17 18:41:03.983516: val_loss -0.6537\n",
      "2025-12-17 18:41:03.985518: Pseudo dice [0.8415, 0.9001, 0.8175]\n",
      "2025-12-17 18:41:03.987520: Epoch time: 138.11 s\n",
      "2025-12-17 18:41:03.987520: Yayy! New best EMA pseudo Dice: 0.7203\n",
      "2025-12-17 18:41:05.070933: \n",
      "2025-12-17 18:41:05.070933: Epoch 19\n",
      "2025-12-17 18:41:05.070933: Current learning rate: 0.00983\n",
      "2025-12-17 18:43:23.157877: train_loss -0.642\n",
      "2025-12-17 18:43:23.157877: val_loss -0.6566\n",
      "2025-12-17 18:43:23.173891: Pseudo dice [0.8379, 0.8893, 0.8239]\n",
      "2025-12-17 18:43:23.175744: Epoch time: 138.09 s\n",
      "2025-12-17 18:43:23.175744: Yayy! New best EMA pseudo Dice: 0.7333\n",
      "2025-12-17 18:43:24.100773: \n",
      "2025-12-17 18:43:24.100773: Epoch 20\n",
      "2025-12-17 18:43:24.102390: Current learning rate: 0.00982\n",
      "2025-12-17 18:45:42.279932: train_loss -0.6556\n",
      "2025-12-17 18:45:42.279932: val_loss -0.6624\n",
      "2025-12-17 18:45:42.281935: Pseudo dice [0.8473, 0.893, 0.8031]\n",
      "2025-12-17 18:45:42.281935: Epoch time: 138.18 s\n",
      "2025-12-17 18:45:42.281935: Yayy! New best EMA pseudo Dice: 0.7448\n",
      "2025-12-17 18:45:43.199595: \n",
      "2025-12-17 18:45:43.199595: Epoch 21\n",
      "2025-12-17 18:45:43.199595: Current learning rate: 0.00981\n",
      "2025-12-17 18:48:01.466516: train_loss -0.6609\n",
      "2025-12-17 18:48:01.466516: val_loss -0.6705\n",
      "2025-12-17 18:48:01.468521: Pseudo dice [0.8454, 0.9013, 0.8239]\n",
      "2025-12-17 18:48:01.470524: Epoch time: 138.27 s\n",
      "2025-12-17 18:48:01.470524: Yayy! New best EMA pseudo Dice: 0.756\n",
      "2025-12-17 18:48:02.357714: \n",
      "2025-12-17 18:48:02.357714: Epoch 22\n",
      "2025-12-17 18:48:02.357714: Current learning rate: 0.0098\n",
      "2025-12-17 18:50:20.368460: train_loss -0.6707\n",
      "2025-12-17 18:50:20.368460: val_loss -0.6924\n",
      "2025-12-17 18:50:20.370462: Pseudo dice [0.8476, 0.9015, 0.848]\n",
      "2025-12-17 18:50:20.370462: Epoch time: 138.01 s\n",
      "2025-12-17 18:50:20.370462: Yayy! New best EMA pseudo Dice: 0.767\n",
      "2025-12-17 18:50:21.273492: \n",
      "2025-12-17 18:50:21.273492: Epoch 23\n",
      "2025-12-17 18:50:21.274965: Current learning rate: 0.00979\n",
      "2025-12-17 18:52:39.565403: train_loss -0.6715\n",
      "2025-12-17 18:52:39.565403: val_loss -0.6972\n",
      "2025-12-17 18:52:39.565403: Pseudo dice [0.8602, 0.9059, 0.8406]\n",
      "2025-12-17 18:52:39.565403: Epoch time: 138.29 s\n",
      "2025-12-17 18:52:39.565403: Yayy! New best EMA pseudo Dice: 0.7772\n",
      "2025-12-17 18:52:40.445761: \n",
      "2025-12-17 18:52:40.445761: Epoch 24\n",
      "2025-12-17 18:52:40.445761: Current learning rate: 0.00978\n",
      "2025-12-17 18:54:58.644864: train_loss -0.6862\n",
      "2025-12-17 18:54:58.644864: val_loss -0.6928\n",
      "2025-12-17 18:54:58.646866: Pseudo dice [0.8516, 0.9061, 0.8515]\n",
      "2025-12-17 18:54:58.646866: Epoch time: 138.2 s\n",
      "2025-12-17 18:54:58.646866: Yayy! New best EMA pseudo Dice: 0.7864\n",
      "2025-12-17 18:54:59.712475: \n",
      "2025-12-17 18:54:59.714382: Epoch 25\n",
      "2025-12-17 18:54:59.714382: Current learning rate: 0.00977\n",
      "2025-12-17 18:57:17.971559: train_loss -0.6993\n",
      "2025-12-17 18:57:17.971559: val_loss -0.701\n",
      "2025-12-17 18:57:17.973300: Pseudo dice [0.8658, 0.9104, 0.8369]\n",
      "2025-12-17 18:57:17.973300: Epoch time: 138.26 s\n",
      "2025-12-17 18:57:17.973300: Yayy! New best EMA pseudo Dice: 0.7949\n",
      "2025-12-17 18:57:18.858812: \n",
      "2025-12-17 18:57:18.862506: Epoch 26\n",
      "2025-12-17 18:57:18.862506: Current learning rate: 0.00977\n",
      "2025-12-17 18:59:36.896723: train_loss -0.7029\n",
      "2025-12-17 18:59:36.896723: val_loss -0.7167\n",
      "2025-12-17 18:59:36.896723: Pseudo dice [0.8632, 0.9095, 0.8503]\n",
      "2025-12-17 18:59:36.896723: Epoch time: 138.04 s\n",
      "2025-12-17 18:59:36.896723: Yayy! New best EMA pseudo Dice: 0.8028\n",
      "2025-12-17 18:59:37.778563: \n",
      "2025-12-17 18:59:37.778563: Epoch 27\n",
      "2025-12-17 18:59:37.778563: Current learning rate: 0.00976\n",
      "2025-12-17 19:01:56.030453: train_loss -0.7075\n",
      "2025-12-17 19:01:56.030453: val_loss -0.7147\n",
      "2025-12-17 19:01:56.030453: Pseudo dice [0.8672, 0.9144, 0.838]\n",
      "2025-12-17 19:01:56.030453: Epoch time: 138.25 s\n",
      "2025-12-17 19:01:56.030453: Yayy! New best EMA pseudo Dice: 0.8099\n",
      "2025-12-17 19:01:56.926839: \n",
      "2025-12-17 19:01:56.928651: Epoch 28\n",
      "2025-12-17 19:01:56.928651: Current learning rate: 0.00975\n",
      "2025-12-17 19:04:15.114075: train_loss -0.7124\n",
      "2025-12-17 19:04:15.116078: val_loss -0.7277\n",
      "2025-12-17 19:04:15.116078: Pseudo dice [0.8705, 0.9211, 0.8531]\n",
      "2025-12-17 19:04:15.118081: Epoch time: 138.19 s\n",
      "2025-12-17 19:04:15.118081: Yayy! New best EMA pseudo Dice: 0.817\n",
      "2025-12-17 19:04:16.013627: \n",
      "2025-12-17 19:04:16.013627: Epoch 29\n",
      "2025-12-17 19:04:16.016074: Current learning rate: 0.00974\n",
      "2025-12-17 19:06:34.317065: train_loss -0.7082\n",
      "2025-12-17 19:06:34.317065: val_loss -0.7244\n",
      "2025-12-17 19:06:34.319068: Pseudo dice [0.8756, 0.9148, 0.8513]\n",
      "2025-12-17 19:06:34.319068: Epoch time: 138.31 s\n",
      "2025-12-17 19:06:34.320808: Yayy! New best EMA pseudo Dice: 0.8234\n",
      "2025-12-17 19:06:35.312697: \n",
      "2025-12-17 19:06:35.313701: Epoch 30\n",
      "2025-12-17 19:06:35.313701: Current learning rate: 0.00973\n",
      "2025-12-17 19:08:53.592438: train_loss -0.7227\n",
      "2025-12-17 19:08:53.592438: val_loss -0.7317\n",
      "2025-12-17 19:08:53.592438: Pseudo dice [0.8705, 0.914, 0.8664]\n",
      "2025-12-17 19:08:53.592438: Epoch time: 138.29 s\n",
      "2025-12-17 19:08:53.592438: Yayy! New best EMA pseudo Dice: 0.8294\n",
      "2025-12-17 19:08:54.669373: \n",
      "2025-12-17 19:08:54.669373: Epoch 31\n",
      "2025-12-17 19:08:54.669373: Current learning rate: 0.00972\n",
      "2025-12-17 19:11:13.042254: train_loss -0.7185\n",
      "2025-12-17 19:11:13.042254: val_loss -0.7299\n",
      "2025-12-17 19:11:13.042254: Pseudo dice [0.8761, 0.9224, 0.8562]\n",
      "2025-12-17 19:11:13.055819: Epoch time: 138.37 s\n",
      "2025-12-17 19:11:13.055819: Yayy! New best EMA pseudo Dice: 0.835\n",
      "2025-12-17 19:11:13.942061: \n",
      "2025-12-17 19:11:13.942061: Epoch 32\n",
      "2025-12-17 19:11:13.942061: Current learning rate: 0.00971\n",
      "2025-12-17 19:13:32.068700: train_loss -0.7255\n",
      "2025-12-17 19:13:32.070601: val_loss -0.7407\n",
      "2025-12-17 19:13:32.072603: Pseudo dice [0.8758, 0.9227, 0.8568]\n",
      "2025-12-17 19:13:32.072603: Epoch time: 138.13 s\n",
      "2025-12-17 19:13:32.074605: Yayy! New best EMA pseudo Dice: 0.84\n",
      "2025-12-17 19:13:33.102107: \n",
      "2025-12-17 19:13:33.102107: Epoch 33\n",
      "2025-12-17 19:13:33.102107: Current learning rate: 0.0097\n",
      "2025-12-17 19:15:51.301232: train_loss -0.7307\n",
      "2025-12-17 19:15:51.303235: val_loss -0.7242\n",
      "2025-12-17 19:15:51.303235: Pseudo dice [0.8644, 0.9075, 0.8651]\n",
      "2025-12-17 19:15:51.303235: Epoch time: 138.2 s\n",
      "2025-12-17 19:15:51.303235: Yayy! New best EMA pseudo Dice: 0.8439\n",
      "2025-12-17 19:15:52.200319: \n",
      "2025-12-17 19:15:52.200319: Epoch 34\n",
      "2025-12-17 19:15:52.202322: Current learning rate: 0.00969\n",
      "2025-12-17 19:18:10.289271: train_loss -0.7207\n",
      "2025-12-17 19:18:10.289271: val_loss -0.7362\n",
      "2025-12-17 19:18:10.289271: Pseudo dice [0.8712, 0.919, 0.8699]\n",
      "2025-12-17 19:18:10.289271: Epoch time: 138.09 s\n",
      "2025-12-17 19:18:10.304928: Yayy! New best EMA pseudo Dice: 0.8482\n",
      "2025-12-17 19:18:11.189397: \n",
      "2025-12-17 19:18:11.189397: Epoch 35\n",
      "2025-12-17 19:18:11.189397: Current learning rate: 0.00968\n",
      "2025-12-17 19:20:29.292554: train_loss -0.7307\n",
      "2025-12-17 19:20:29.294556: val_loss -0.7522\n",
      "2025-12-17 19:20:29.294556: Pseudo dice [0.8747, 0.9239, 0.8831]\n",
      "2025-12-17 19:20:29.296558: Epoch time: 138.11 s\n",
      "2025-12-17 19:20:29.298061: Yayy! New best EMA pseudo Dice: 0.8527\n",
      "2025-12-17 19:20:30.373736: \n",
      "2025-12-17 19:20:30.373736: Epoch 36\n",
      "2025-12-17 19:20:30.373736: Current learning rate: 0.00968\n",
      "2025-12-17 19:22:48.345524: train_loss -0.7394\n",
      "2025-12-17 19:22:48.345524: val_loss -0.7624\n",
      "2025-12-17 19:22:48.361604: Pseudo dice [0.8925, 0.9319, 0.8647]\n",
      "2025-12-17 19:22:48.363609: Epoch time: 137.97 s\n",
      "2025-12-17 19:22:48.363609: Yayy! New best EMA pseudo Dice: 0.8571\n",
      "2025-12-17 19:22:49.443717: \n",
      "2025-12-17 19:22:49.443717: Epoch 37\n",
      "2025-12-17 19:22:49.443717: Current learning rate: 0.00967\n",
      "2025-12-17 19:25:07.499485: train_loss -0.7464\n",
      "2025-12-17 19:25:07.499485: val_loss -0.7529\n",
      "2025-12-17 19:25:07.499485: Pseudo dice [0.8826, 0.9277, 0.8661]\n",
      "2025-12-17 19:25:07.499485: Epoch time: 138.06 s\n",
      "2025-12-17 19:25:07.499485: Yayy! New best EMA pseudo Dice: 0.8606\n",
      "2025-12-17 19:25:08.405560: \n",
      "2025-12-17 19:25:08.405560: Epoch 38\n",
      "2025-12-17 19:25:08.405560: Current learning rate: 0.00966\n",
      "2025-12-17 19:27:26.622573: train_loss -0.7236\n",
      "2025-12-17 19:27:26.622573: val_loss -0.7341\n",
      "2025-12-17 19:27:26.622573: Pseudo dice [0.8694, 0.9173, 0.8672]\n",
      "2025-12-17 19:27:26.622573: Epoch time: 138.22 s\n",
      "2025-12-17 19:27:26.622573: Yayy! New best EMA pseudo Dice: 0.863\n",
      "2025-12-17 19:27:27.671032: \n",
      "2025-12-17 19:27:27.671032: Epoch 39\n",
      "2025-12-17 19:27:27.681045: Current learning rate: 0.00965\n",
      "2025-12-17 19:29:45.712051: train_loss -0.7382\n",
      "2025-12-17 19:29:45.712051: val_loss -0.7411\n",
      "2025-12-17 19:29:45.712051: Pseudo dice [0.8693, 0.9168, 0.8783]\n",
      "2025-12-17 19:29:45.712051: Epoch time: 138.04 s\n",
      "2025-12-17 19:29:45.712051: Yayy! New best EMA pseudo Dice: 0.8655\n",
      "2025-12-17 19:29:46.640359: \n",
      "2025-12-17 19:29:46.640359: Epoch 40\n",
      "2025-12-17 19:29:46.640359: Current learning rate: 0.00964\n",
      "2025-12-17 19:32:04.791519: train_loss -0.742\n",
      "2025-12-17 19:32:04.791519: val_loss -0.751\n",
      "2025-12-17 19:32:04.791519: Pseudo dice [0.8756, 0.9226, 0.8774]\n",
      "2025-12-17 19:32:04.791519: Epoch time: 138.15 s\n",
      "2025-12-17 19:32:04.791519: Yayy! New best EMA pseudo Dice: 0.8682\n",
      "2025-12-17 19:32:05.714746: \n",
      "2025-12-17 19:32:05.714746: Epoch 41\n",
      "2025-12-17 19:32:05.714746: Current learning rate: 0.00963\n",
      "2025-12-17 19:34:23.808249: train_loss -0.7446\n",
      "2025-12-17 19:34:23.808249: val_loss -0.7507\n",
      "2025-12-17 19:34:23.808249: Pseudo dice [0.8755, 0.9201, 0.8748]\n",
      "2025-12-17 19:34:23.808249: Epoch time: 138.09 s\n",
      "2025-12-17 19:34:23.808249: Yayy! New best EMA pseudo Dice: 0.8704\n",
      "2025-12-17 19:34:24.766124: \n",
      "2025-12-17 19:34:24.766124: Epoch 42\n",
      "2025-12-17 19:34:24.766124: Current learning rate: 0.00962\n",
      "2025-12-17 19:36:42.835974: train_loss -0.7433\n",
      "2025-12-17 19:36:42.835974: val_loss -0.7481\n",
      "2025-12-17 19:36:42.835974: Pseudo dice [0.8716, 0.9193, 0.8743]\n",
      "2025-12-17 19:36:42.835974: Epoch time: 138.07 s\n",
      "2025-12-17 19:36:42.835974: Yayy! New best EMA pseudo Dice: 0.8722\n",
      "2025-12-17 19:36:43.892061: \n",
      "2025-12-17 19:36:43.892061: Epoch 43\n",
      "2025-12-17 19:36:43.907905: Current learning rate: 0.00961\n",
      "2025-12-17 19:39:01.922046: train_loss -0.7455\n",
      "2025-12-17 19:39:01.922046: val_loss -0.7539\n",
      "2025-12-17 19:39:01.922046: Pseudo dice [0.8788, 0.922, 0.8752]\n",
      "2025-12-17 19:39:01.922046: Epoch time: 138.03 s\n",
      "2025-12-17 19:39:01.922046: Yayy! New best EMA pseudo Dice: 0.8741\n",
      "2025-12-17 19:39:02.806340: \n",
      "2025-12-17 19:39:02.806340: Epoch 44\n",
      "2025-12-17 19:39:02.806340: Current learning rate: 0.0096\n",
      "2025-12-17 19:41:20.831308: train_loss -0.7541\n",
      "2025-12-17 19:41:20.831308: val_loss -0.7681\n",
      "2025-12-17 19:41:20.833311: Pseudo dice [0.8866, 0.9266, 0.8726]\n",
      "2025-12-17 19:41:20.833311: Epoch time: 138.02 s\n",
      "2025-12-17 19:41:20.835314: Yayy! New best EMA pseudo Dice: 0.8763\n",
      "2025-12-17 19:41:21.751232: \n",
      "2025-12-17 19:41:21.751232: Epoch 45\n",
      "2025-12-17 19:41:21.751232: Current learning rate: 0.00959\n",
      "2025-12-17 19:43:39.677795: train_loss -0.7574\n",
      "2025-12-17 19:43:39.677795: val_loss -0.7761\n",
      "2025-12-17 19:43:39.677795: Pseudo dice [0.8947, 0.9322, 0.8672]\n",
      "2025-12-17 19:43:39.677795: Epoch time: 137.93 s\n",
      "2025-12-17 19:43:39.677795: Yayy! New best EMA pseudo Dice: 0.8784\n",
      "2025-12-17 19:43:40.584545: \n",
      "2025-12-17 19:43:40.584545: Epoch 46\n",
      "2025-12-17 19:43:40.584545: Current learning rate: 0.00959\n",
      "2025-12-17 19:45:58.745522: train_loss -0.7636\n",
      "2025-12-17 19:45:58.745522: val_loss -0.782\n",
      "2025-12-17 19:45:58.747526: Pseudo dice [0.8983, 0.9383, 0.8799]\n",
      "2025-12-17 19:45:58.747526: Epoch time: 138.16 s\n",
      "2025-12-17 19:45:58.747526: Yayy! New best EMA pseudo Dice: 0.8811\n",
      "2025-12-17 19:45:59.608194: \n",
      "2025-12-17 19:45:59.608194: Epoch 47\n",
      "2025-12-17 19:45:59.608194: Current learning rate: 0.00958\n",
      "2025-12-17 19:48:17.724097: train_loss -0.7488\n",
      "2025-12-17 19:48:17.724097: val_loss -0.7627\n",
      "2025-12-17 19:48:17.726099: Pseudo dice [0.8874, 0.9305, 0.8698]\n",
      "2025-12-17 19:48:17.728101: Epoch time: 138.12 s\n",
      "2025-12-17 19:48:17.728101: Yayy! New best EMA pseudo Dice: 0.8826\n",
      "2025-12-17 19:48:18.635992: \n",
      "2025-12-17 19:48:18.635992: Epoch 48\n",
      "2025-12-17 19:48:18.635992: Current learning rate: 0.00957\n",
      "2025-12-17 19:50:37.024578: train_loss -0.7236\n",
      "2025-12-17 19:50:37.024578: val_loss -0.7451\n",
      "2025-12-17 19:50:37.026581: Pseudo dice [0.8733, 0.9121, 0.8751]\n",
      "2025-12-17 19:50:37.028584: Epoch time: 138.39 s\n",
      "2025-12-17 19:50:37.028584: Yayy! New best EMA pseudo Dice: 0.883\n",
      "2025-12-17 19:50:38.087318: \n",
      "2025-12-17 19:50:38.087318: Epoch 49\n",
      "2025-12-17 19:50:38.087318: Current learning rate: 0.00956\n",
      "2025-12-17 19:52:56.292449: train_loss -0.7453\n",
      "2025-12-17 19:52:56.292449: val_loss -0.763\n",
      "2025-12-17 19:52:56.292449: Pseudo dice [0.884, 0.9286, 0.8815]\n",
      "2025-12-17 19:52:56.292449: Epoch time: 138.22 s\n",
      "2025-12-17 19:52:56.546999: Yayy! New best EMA pseudo Dice: 0.8845\n",
      "2025-12-17 19:52:57.431479: \n",
      "2025-12-17 19:52:57.431479: Epoch 50\n",
      "2025-12-17 19:52:57.431479: Current learning rate: 0.00955\n",
      "2025-12-17 19:55:15.656975: train_loss -0.7538\n",
      "2025-12-17 19:55:15.656975: val_loss -0.7715\n",
      "2025-12-17 19:55:15.656975: Pseudo dice [0.8854, 0.9321, 0.8831]\n",
      "2025-12-17 19:55:15.662434: Epoch time: 138.23 s\n",
      "2025-12-17 19:55:15.662434: Yayy! New best EMA pseudo Dice: 0.8861\n",
      "2025-12-17 19:55:16.537007: \n",
      "2025-12-17 19:55:16.537007: Epoch 51\n",
      "2025-12-17 19:55:16.537007: Current learning rate: 0.00954\n",
      "2025-12-17 19:57:34.624240: train_loss -0.7517\n",
      "2025-12-17 19:57:34.624240: val_loss -0.776\n",
      "2025-12-17 19:57:34.624240: Pseudo dice [0.8896, 0.9321, 0.8847]\n",
      "2025-12-17 19:57:34.639914: Epoch time: 138.09 s\n",
      "2025-12-17 19:57:34.639914: Yayy! New best EMA pseudo Dice: 0.8877\n",
      "2025-12-17 19:57:35.533985: \n",
      "2025-12-17 19:57:35.533985: Epoch 52\n",
      "2025-12-17 19:57:35.533985: Current learning rate: 0.00953\n",
      "2025-12-17 19:59:53.717834: train_loss -0.7666\n",
      "2025-12-17 19:59:53.717834: val_loss -0.7935\n",
      "2025-12-17 19:59:53.719837: Pseudo dice [0.8941, 0.9313, 0.8983]\n",
      "2025-12-17 19:59:53.719837: Epoch time: 138.19 s\n",
      "2025-12-17 19:59:53.721839: Yayy! New best EMA pseudo Dice: 0.8897\n",
      "2025-12-17 19:59:54.603572: \n",
      "2025-12-17 19:59:54.603572: Epoch 53\n",
      "2025-12-17 19:59:54.603572: Current learning rate: 0.00952\n",
      "2025-12-17 20:02:12.793779: train_loss -0.7658\n",
      "2025-12-17 20:02:12.793779: val_loss -0.7938\n",
      "2025-12-17 20:02:12.793779: Pseudo dice [0.9022, 0.9395, 0.8834]\n",
      "2025-12-17 20:02:12.793779: Epoch time: 138.19 s\n",
      "2025-12-17 20:02:12.793779: Yayy! New best EMA pseudo Dice: 0.8916\n",
      "2025-12-17 20:02:13.681627: \n",
      "2025-12-17 20:02:13.681627: Epoch 54\n",
      "2025-12-17 20:02:13.681627: Current learning rate: 0.00951\n",
      "2025-12-17 20:04:31.904014: train_loss -0.7684\n",
      "2025-12-17 20:04:31.904014: val_loss -0.7926\n",
      "2025-12-17 20:04:31.904014: Pseudo dice [0.8937, 0.9341, 0.894]\n",
      "2025-12-17 20:04:31.904014: Epoch time: 138.22 s\n",
      "2025-12-17 20:04:31.904014: Yayy! New best EMA pseudo Dice: 0.8932\n",
      "2025-12-17 20:04:33.057154: \n",
      "2025-12-17 20:04:33.057154: Epoch 55\n",
      "2025-12-17 20:04:33.059156: Current learning rate: 0.0095\n",
      "2025-12-17 20:06:51.138654: train_loss -0.7712\n",
      "2025-12-17 20:06:51.138654: val_loss -0.7894\n",
      "2025-12-17 20:06:51.138654: Pseudo dice [0.8936, 0.9356, 0.8891]\n",
      "2025-12-17 20:06:51.154438: Epoch time: 138.08 s\n",
      "2025-12-17 20:06:51.154438: Yayy! New best EMA pseudo Dice: 0.8945\n",
      "2025-12-17 20:06:52.036304: \n",
      "2025-12-17 20:06:52.036304: Epoch 56\n",
      "2025-12-17 20:06:52.036304: Current learning rate: 0.00949\n",
      "2025-12-17 20:09:10.423660: train_loss -0.7783\n",
      "2025-12-17 20:09:10.423660: val_loss -0.7926\n",
      "2025-12-17 20:09:10.423660: Pseudo dice [0.8933, 0.9355, 0.8931]\n",
      "2025-12-17 20:09:10.423660: Epoch time: 138.39 s\n",
      "2025-12-17 20:09:10.423660: Yayy! New best EMA pseudo Dice: 0.8957\n",
      "2025-12-17 20:09:11.321750: \n",
      "2025-12-17 20:09:11.321750: Epoch 57\n",
      "2025-12-17 20:09:11.321750: Current learning rate: 0.00949\n",
      "2025-12-17 20:11:29.865303: train_loss -0.781\n",
      "2025-12-17 20:11:29.865303: val_loss -0.7932\n",
      "2025-12-17 20:11:29.867987: Pseudo dice [0.8975, 0.9364, 0.886]\n",
      "2025-12-17 20:11:29.867987: Epoch time: 138.55 s\n",
      "2025-12-17 20:11:29.869989: Yayy! New best EMA pseudo Dice: 0.8968\n",
      "2025-12-17 20:11:30.879837: \n",
      "2025-12-17 20:11:30.879837: Epoch 58\n",
      "2025-12-17 20:11:30.879837: Current learning rate: 0.00948\n",
      "2025-12-17 20:13:49.054941: train_loss -0.7843\n",
      "2025-12-17 20:13:49.054941: val_loss -0.7942\n",
      "2025-12-17 20:13:49.054941: Pseudo dice [0.8993, 0.9355, 0.8802]\n",
      "2025-12-17 20:13:49.054941: Epoch time: 138.18 s\n",
      "2025-12-17 20:13:49.054941: Yayy! New best EMA pseudo Dice: 0.8976\n",
      "2025-12-17 20:13:49.956759: \n",
      "2025-12-17 20:13:49.956759: Epoch 59\n",
      "2025-12-17 20:13:49.958763: Current learning rate: 0.00947\n",
      "2025-12-17 20:16:08.107338: train_loss -0.7816\n",
      "2025-12-17 20:16:08.107338: val_loss -0.7996\n",
      "2025-12-17 20:16:08.107338: Pseudo dice [0.8965, 0.937, 0.8886]\n",
      "2025-12-17 20:16:08.107338: Epoch time: 138.15 s\n",
      "2025-12-17 20:16:08.107338: Yayy! New best EMA pseudo Dice: 0.8986\n",
      "2025-12-17 20:16:09.003709: \n",
      "2025-12-17 20:16:09.003709: Epoch 60\n",
      "2025-12-17 20:16:09.003709: Current learning rate: 0.00946\n",
      "2025-12-17 20:18:27.214546: train_loss -0.7833\n",
      "2025-12-17 20:18:27.214546: val_loss -0.7914\n",
      "2025-12-17 20:18:27.216291: Pseudo dice [0.8978, 0.9359, 0.8934]\n",
      "2025-12-17 20:18:27.218294: Epoch time: 138.21 s\n",
      "2025-12-17 20:18:27.218294: Yayy! New best EMA pseudo Dice: 0.8997\n",
      "2025-12-17 20:18:28.446021: \n",
      "2025-12-17 20:18:28.448024: Epoch 61\n",
      "2025-12-17 20:18:28.448024: Current learning rate: 0.00945\n",
      "2025-12-17 20:20:46.639174: train_loss -0.7824\n",
      "2025-12-17 20:20:46.639174: val_loss -0.8064\n",
      "2025-12-17 20:20:46.639174: Pseudo dice [0.9032, 0.9376, 0.9003]\n",
      "2025-12-17 20:20:46.639174: Epoch time: 138.2 s\n",
      "2025-12-17 20:20:46.639174: Yayy! New best EMA pseudo Dice: 0.9011\n",
      "2025-12-17 20:20:47.542615: \n",
      "2025-12-17 20:20:47.542615: Epoch 62\n",
      "2025-12-17 20:20:47.542615: Current learning rate: 0.00944\n",
      "2025-12-17 20:23:05.653770: train_loss -0.79\n",
      "2025-12-17 20:23:05.653770: val_loss -0.8058\n",
      "2025-12-17 20:23:05.653770: Pseudo dice [0.907, 0.9434, 0.89]\n",
      "2025-12-17 20:23:05.653770: Epoch time: 138.11 s\n",
      "2025-12-17 20:23:05.653770: Yayy! New best EMA pseudo Dice: 0.9023\n",
      "2025-12-17 20:23:06.561703: \n",
      "2025-12-17 20:23:06.561703: Epoch 63\n",
      "2025-12-17 20:23:06.561703: Current learning rate: 0.00943\n",
      "2025-12-17 20:25:24.674072: train_loss -0.7888\n",
      "2025-12-17 20:25:24.674072: val_loss -0.8082\n",
      "2025-12-17 20:25:24.674072: Pseudo dice [0.9041, 0.9426, 0.9036]\n",
      "2025-12-17 20:25:24.674072: Epoch time: 138.11 s\n",
      "2025-12-17 20:25:24.674072: Yayy! New best EMA pseudo Dice: 0.9037\n",
      "2025-12-17 20:25:25.690915: \n",
      "2025-12-17 20:25:25.690915: Epoch 64\n",
      "2025-12-17 20:25:25.698632: Current learning rate: 0.00942\n",
      "2025-12-17 20:27:43.799008: train_loss -0.7882\n",
      "2025-12-17 20:27:43.799008: val_loss -0.8069\n",
      "2025-12-17 20:27:43.801011: Pseudo dice [0.8968, 0.939, 0.9073]\n",
      "2025-12-17 20:27:43.801011: Epoch time: 138.11 s\n",
      "2025-12-17 20:27:43.803014: Yayy! New best EMA pseudo Dice: 0.9048\n",
      "2025-12-17 20:27:44.703354: \n",
      "2025-12-17 20:27:44.703354: Epoch 65\n",
      "2025-12-17 20:27:44.703354: Current learning rate: 0.00941\n",
      "2025-12-17 20:30:03.014868: train_loss -0.7882\n",
      "2025-12-17 20:30:03.016870: val_loss -0.8072\n",
      "2025-12-17 20:30:03.016870: Pseudo dice [0.8957, 0.9332, 0.9137]\n",
      "2025-12-17 20:30:03.020011: Epoch time: 138.31 s\n",
      "2025-12-17 20:30:03.020011: Yayy! New best EMA pseudo Dice: 0.9057\n",
      "2025-12-17 20:30:03.909886: \n",
      "2025-12-17 20:30:03.909886: Epoch 66\n",
      "2025-12-17 20:30:03.909886: Current learning rate: 0.0094\n",
      "2025-12-17 20:32:22.201562: train_loss -0.7865\n",
      "2025-12-17 20:32:22.201562: val_loss -0.7902\n",
      "2025-12-17 20:32:22.203565: Pseudo dice [0.8887, 0.9308, 0.897]\n",
      "2025-12-17 20:32:22.205567: Epoch time: 138.29 s\n",
      "2025-12-17 20:32:23.000741: \n",
      "2025-12-17 20:32:23.000741: Epoch 67\n",
      "2025-12-17 20:32:23.000741: Current learning rate: 0.00939\n",
      "2025-12-17 20:34:41.430257: train_loss -0.7854\n",
      "2025-12-17 20:34:41.430257: val_loss -0.8066\n",
      "2025-12-17 20:34:41.430257: Pseudo dice [0.9013, 0.934, 0.9092]\n",
      "2025-12-17 20:34:41.430257: Epoch time: 138.43 s\n",
      "2025-12-17 20:34:41.430257: Yayy! New best EMA pseudo Dice: 0.9066\n",
      "2025-12-17 20:34:42.339004: \n",
      "2025-12-17 20:34:42.339004: Epoch 68\n",
      "2025-12-17 20:34:42.339004: Current learning rate: 0.00939\n",
      "2025-12-17 20:37:00.446460: train_loss -0.7934\n",
      "2025-12-17 20:37:00.446460: val_loss -0.8103\n",
      "2025-12-17 20:37:00.446460: Pseudo dice [0.9033, 0.9425, 0.8981]\n",
      "2025-12-17 20:37:00.446460: Epoch time: 138.11 s\n",
      "2025-12-17 20:37:00.446460: Yayy! New best EMA pseudo Dice: 0.9074\n",
      "2025-12-17 20:37:01.366020: \n",
      "2025-12-17 20:37:01.366020: Epoch 69\n",
      "2025-12-17 20:37:01.366020: Current learning rate: 0.00938\n",
      "2025-12-17 20:39:19.651810: train_loss -0.7851\n",
      "2025-12-17 20:39:19.651810: val_loss -0.7938\n",
      "2025-12-17 20:39:19.653554: Pseudo dice [0.8915, 0.9328, 0.9099]\n",
      "2025-12-17 20:39:19.653554: Epoch time: 138.29 s\n",
      "2025-12-17 20:39:19.653554: Yayy! New best EMA pseudo Dice: 0.9078\n",
      "2025-12-17 20:39:20.575526: \n",
      "2025-12-17 20:39:20.575526: Epoch 70\n",
      "2025-12-17 20:39:20.575526: Current learning rate: 0.00937\n",
      "2025-12-17 20:41:38.818868: train_loss -0.7865\n",
      "2025-12-17 20:41:38.819871: val_loss -0.8069\n",
      "2025-12-17 20:41:38.819871: Pseudo dice [0.8995, 0.9349, 0.9079]\n",
      "2025-12-17 20:41:38.819871: Epoch time: 138.24 s\n",
      "2025-12-17 20:41:38.819871: Yayy! New best EMA pseudo Dice: 0.9085\n",
      "2025-12-17 20:41:39.722639: \n",
      "2025-12-17 20:41:39.722639: Epoch 71\n",
      "2025-12-17 20:41:39.722639: Current learning rate: 0.00936\n",
      "2025-12-17 20:43:57.949437: train_loss -0.7949\n",
      "2025-12-17 20:43:57.951440: val_loss -0.8121\n",
      "2025-12-17 20:43:57.951440: Pseudo dice [0.9012, 0.9346, 0.9111]\n",
      "2025-12-17 20:43:57.954157: Epoch time: 138.23 s\n",
      "2025-12-17 20:43:57.954157: Yayy! New best EMA pseudo Dice: 0.9092\n",
      "2025-12-17 20:43:58.861627: \n",
      "2025-12-17 20:43:58.861627: Epoch 72\n",
      "2025-12-17 20:43:58.863632: Current learning rate: 0.00935\n",
      "2025-12-17 20:46:16.905800: train_loss -0.7904\n",
      "2025-12-17 20:46:16.905800: val_loss -0.826\n",
      "2025-12-17 20:46:16.907541: Pseudo dice [0.917, 0.9492, 0.9038]\n",
      "2025-12-17 20:46:16.907541: Epoch time: 138.04 s\n",
      "2025-12-17 20:46:16.911160: Yayy! New best EMA pseudo Dice: 0.9106\n",
      "2025-12-17 20:46:17.983149: \n",
      "2025-12-17 20:46:17.983149: Epoch 73\n",
      "2025-12-17 20:46:17.983149: Current learning rate: 0.00934\n",
      "2025-12-17 20:48:36.131295: train_loss -0.7972\n",
      "2025-12-17 20:48:36.131295: val_loss -0.8186\n",
      "2025-12-17 20:48:36.146965: Pseudo dice [0.9085, 0.9423, 0.904]\n",
      "2025-12-17 20:48:36.146965: Epoch time: 138.15 s\n",
      "2025-12-17 20:48:36.146965: Yayy! New best EMA pseudo Dice: 0.9114\n",
      "2025-12-17 20:48:37.048124: \n",
      "2025-12-17 20:48:37.048124: Epoch 74\n",
      "2025-12-17 20:48:37.048124: Current learning rate: 0.00933\n",
      "2025-12-17 20:50:55.324772: train_loss -0.7919\n",
      "2025-12-17 20:50:55.324772: val_loss -0.8079\n",
      "2025-12-17 20:50:55.324772: Pseudo dice [0.9035, 0.9408, 0.902]\n",
      "2025-12-17 20:50:55.324772: Epoch time: 138.29 s\n",
      "2025-12-17 20:50:55.324772: Yayy! New best EMA pseudo Dice: 0.9118\n",
      "2025-12-17 20:50:56.229044: \n",
      "2025-12-17 20:50:56.229044: Epoch 75\n",
      "2025-12-17 20:50:56.229044: Current learning rate: 0.00932\n",
      "2025-12-17 20:53:14.408983: train_loss -0.7933\n",
      "2025-12-17 20:53:14.410986: val_loss -0.8155\n",
      "2025-12-17 20:53:14.410986: Pseudo dice [0.904, 0.9416, 0.9026]\n",
      "2025-12-17 20:53:14.410986: Epoch time: 138.18 s\n",
      "2025-12-17 20:53:14.410986: Yayy! New best EMA pseudo Dice: 0.9122\n",
      "2025-12-17 20:53:15.321475: \n",
      "2025-12-17 20:53:15.321475: Epoch 76\n",
      "2025-12-17 20:53:15.321475: Current learning rate: 0.00931\n",
      "2025-12-17 20:55:33.830093: train_loss -0.7893\n",
      "2025-12-17 20:55:33.830093: val_loss -0.8111\n",
      "2025-12-17 20:55:33.834100: Pseudo dice [0.9058, 0.9375, 0.9024]\n",
      "2025-12-17 20:55:33.836104: Epoch time: 138.51 s\n",
      "2025-12-17 20:55:33.838110: Yayy! New best EMA pseudo Dice: 0.9125\n",
      "2025-12-17 20:55:34.932472: \n",
      "2025-12-17 20:55:34.932472: Epoch 77\n",
      "2025-12-17 20:55:34.932472: Current learning rate: 0.0093\n",
      "2025-12-17 20:57:53.071099: train_loss -0.7897\n",
      "2025-12-17 20:57:53.073102: val_loss -0.8227\n",
      "2025-12-17 20:57:53.073102: Pseudo dice [0.9153, 0.9423, 0.9065]\n",
      "2025-12-17 20:57:53.075105: Epoch time: 138.14 s\n",
      "2025-12-17 20:57:53.075105: Yayy! New best EMA pseudo Dice: 0.9134\n",
      "2025-12-17 20:57:54.145790: \n",
      "2025-12-17 20:57:54.145790: Epoch 78\n",
      "2025-12-17 20:57:54.145790: Current learning rate: 0.0093\n",
      "2025-12-17 21:00:12.134493: train_loss -0.7917\n",
      "2025-12-17 21:00:12.134493: val_loss -0.8149\n",
      "2025-12-17 21:00:12.134493: Pseudo dice [0.9038, 0.9391, 0.9029]\n",
      "2025-12-17 21:00:12.150494: Epoch time: 137.99 s\n",
      "2025-12-17 21:00:12.150494: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2025-12-17 21:00:13.040751: \n",
      "2025-12-17 21:00:13.040751: Epoch 79\n",
      "2025-12-17 21:00:13.040751: Current learning rate: 0.00929\n",
      "2025-12-17 21:02:31.092196: train_loss -0.7877\n",
      "2025-12-17 21:02:31.092196: val_loss -0.7937\n",
      "2025-12-17 21:02:31.092196: Pseudo dice [0.8923, 0.9317, 0.9057]\n",
      "2025-12-17 21:02:31.108219: Epoch time: 138.05 s\n",
      "2025-12-17 21:02:31.870992: \n",
      "2025-12-17 21:02:31.870992: Epoch 80\n",
      "2025-12-17 21:02:31.870992: Current learning rate: 0.00928\n",
      "2025-12-17 21:04:49.960998: train_loss -0.792\n",
      "2025-12-17 21:04:49.960998: val_loss -0.7963\n",
      "2025-12-17 21:04:49.964492: Pseudo dice [0.9004, 0.9319, 0.8842]\n",
      "2025-12-17 21:04:49.966495: Epoch time: 138.09 s\n",
      "2025-12-17 21:04:50.625875: \n",
      "2025-12-17 21:04:50.625875: Epoch 81\n",
      "2025-12-17 21:04:50.625875: Current learning rate: 0.00927\n",
      "2025-12-17 21:07:08.845693: train_loss -0.7801\n",
      "2025-12-17 21:07:08.845693: val_loss -0.8069\n",
      "2025-12-17 21:07:08.845693: Pseudo dice [0.9031, 0.936, 0.8954]\n",
      "2025-12-17 21:07:08.845693: Epoch time: 138.23 s\n",
      "2025-12-17 21:07:09.495078: \n",
      "2025-12-17 21:07:09.495078: Epoch 82\n",
      "2025-12-17 21:07:09.495078: Current learning rate: 0.00926\n",
      "2025-12-17 21:09:27.807156: train_loss -0.7915\n",
      "2025-12-17 21:09:27.809821: val_loss -0.814\n",
      "2025-12-17 21:09:27.809821: Pseudo dice [0.9088, 0.9419, 0.9002]\n",
      "2025-12-17 21:09:27.809821: Epoch time: 138.31 s\n",
      "2025-12-17 21:09:28.505008: \n",
      "2025-12-17 21:09:28.505008: Epoch 83\n",
      "2025-12-17 21:09:28.505008: Current learning rate: 0.00925\n",
      "2025-12-17 21:11:46.879208: train_loss -0.7957\n",
      "2025-12-17 21:11:46.879208: val_loss -0.8268\n",
      "2025-12-17 21:11:46.879208: Pseudo dice [0.9184, 0.9456, 0.8998]\n",
      "2025-12-17 21:11:46.879208: Epoch time: 138.37 s\n",
      "2025-12-17 21:11:46.879208: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2025-12-17 21:11:47.911749: \n",
      "2025-12-17 21:11:47.911749: Epoch 84\n",
      "2025-12-17 21:11:47.911749: Current learning rate: 0.00924\n",
      "2025-12-17 21:14:06.108286: train_loss -0.8031\n",
      "2025-12-17 21:14:06.110289: val_loss -0.8102\n",
      "2025-12-17 21:14:06.112031: Pseudo dice [0.9023, 0.9388, 0.9037]\n",
      "2025-12-17 21:14:06.112031: Epoch time: 138.2 s\n",
      "2025-12-17 21:14:06.112031: Yayy! New best EMA pseudo Dice: 0.9138\n",
      "2025-12-17 21:14:07.004002: \n",
      "2025-12-17 21:14:07.004002: Epoch 85\n",
      "2025-12-17 21:14:07.004002: Current learning rate: 0.00923\n",
      "2025-12-17 21:16:25.276368: train_loss -0.8009\n",
      "2025-12-17 21:16:25.276368: val_loss -0.8259\n",
      "2025-12-17 21:16:25.284227: Pseudo dice [0.9099, 0.9442, 0.9131]\n",
      "2025-12-17 21:16:25.284227: Epoch time: 138.27 s\n",
      "2025-12-17 21:16:25.284227: Yayy! New best EMA pseudo Dice: 0.9146\n",
      "2025-12-17 21:16:26.360108: \n",
      "2025-12-17 21:16:26.360108: Epoch 86\n",
      "2025-12-17 21:16:26.360108: Current learning rate: 0.00922\n",
      "2025-12-17 21:18:44.487730: train_loss -0.7958\n",
      "2025-12-17 21:18:44.487730: val_loss -0.814\n",
      "2025-12-17 21:18:44.492166: Pseudo dice [0.9096, 0.9436, 0.8924]\n",
      "2025-12-17 21:18:44.492166: Epoch time: 138.13 s\n",
      "2025-12-17 21:18:44.494168: Yayy! New best EMA pseudo Dice: 0.9147\n",
      "2025-12-17 21:18:45.382031: \n",
      "2025-12-17 21:18:45.382031: Epoch 87\n",
      "2025-12-17 21:18:45.382031: Current learning rate: 0.00921\n",
      "2025-12-17 21:21:03.484743: train_loss -0.805\n",
      "2025-12-17 21:21:03.484743: val_loss -0.819\n",
      "2025-12-17 21:21:03.484743: Pseudo dice [0.9047, 0.9428, 0.9073]\n",
      "2025-12-17 21:21:03.484743: Epoch time: 138.1 s\n",
      "2025-12-17 21:21:03.484743: Yayy! New best EMA pseudo Dice: 0.9151\n",
      "2025-12-17 21:21:04.371970: \n",
      "2025-12-17 21:21:04.371970: Epoch 88\n",
      "2025-12-17 21:21:04.371970: Current learning rate: 0.0092\n",
      "2025-12-17 21:23:22.658127: train_loss -0.8022\n",
      "2025-12-17 21:23:22.658127: val_loss -0.8159\n",
      "2025-12-17 21:23:22.660129: Pseudo dice [0.9065, 0.9396, 0.9024]\n",
      "2025-12-17 21:23:22.662132: Epoch time: 138.29 s\n",
      "2025-12-17 21:23:22.664135: Yayy! New best EMA pseudo Dice: 0.9152\n",
      "2025-12-17 21:23:23.656408: \n",
      "2025-12-17 21:23:23.656408: Epoch 89\n",
      "2025-12-17 21:23:23.656408: Current learning rate: 0.0092\n",
      "2025-12-17 21:25:41.867697: train_loss -0.8125\n",
      "2025-12-17 21:25:41.869700: val_loss -0.8127\n",
      "2025-12-17 21:25:41.871706: Pseudo dice [0.9048, 0.9375, 0.9052]\n",
      "2025-12-17 21:25:41.873708: Epoch time: 138.21 s\n",
      "2025-12-17 21:25:41.877718: Yayy! New best EMA pseudo Dice: 0.9152\n",
      "2025-12-17 21:25:42.959726: \n",
      "2025-12-17 21:25:42.959726: Epoch 90\n",
      "2025-12-17 21:25:42.959726: Current learning rate: 0.00919\n",
      "2025-12-17 21:28:01.104345: train_loss -0.7987\n",
      "2025-12-17 21:28:01.106348: val_loss -0.8176\n",
      "2025-12-17 21:28:01.110359: Pseudo dice [0.9062, 0.9404, 0.9095]\n",
      "2025-12-17 21:28:01.112363: Epoch time: 138.15 s\n",
      "2025-12-17 21:28:01.112363: Yayy! New best EMA pseudo Dice: 0.9156\n",
      "2025-12-17 21:28:02.011821: \n",
      "2025-12-17 21:28:02.013661: Epoch 91\n",
      "2025-12-17 21:28:02.013661: Current learning rate: 0.00918\n",
      "2025-12-17 21:30:20.124694: train_loss -0.8019\n",
      "2025-12-17 21:30:20.124694: val_loss -0.826\n",
      "2025-12-17 21:30:20.124694: Pseudo dice [0.9077, 0.9441, 0.9102]\n",
      "2025-12-17 21:30:20.124694: Epoch time: 138.11 s\n",
      "2025-12-17 21:30:20.124694: Yayy! New best EMA pseudo Dice: 0.9161\n",
      "2025-12-17 21:30:21.028756: \n",
      "2025-12-17 21:30:21.028756: Epoch 92\n",
      "2025-12-17 21:30:21.028756: Current learning rate: 0.00917\n",
      "2025-12-17 21:32:39.257913: train_loss -0.8092\n",
      "2025-12-17 21:32:39.257913: val_loss -0.8324\n",
      "2025-12-17 21:32:39.257913: Pseudo dice [0.9157, 0.9429, 0.9119]\n",
      "2025-12-17 21:32:39.257913: Epoch time: 138.23 s\n",
      "2025-12-17 21:32:39.257913: Yayy! New best EMA pseudo Dice: 0.9168\n",
      "2025-12-17 21:32:40.149892: \n",
      "2025-12-17 21:32:40.149892: Epoch 93\n",
      "2025-12-17 21:32:40.149892: Current learning rate: 0.00916\n",
      "2025-12-17 21:34:58.264530: train_loss -0.8068\n",
      "2025-12-17 21:34:58.264530: val_loss -0.8249\n",
      "2025-12-17 21:34:58.264530: Pseudo dice [0.9116, 0.9451, 0.896]\n",
      "2025-12-17 21:34:58.264530: Epoch time: 138.11 s\n",
      "2025-12-17 21:34:58.264530: Yayy! New best EMA pseudo Dice: 0.9169\n",
      "2025-12-17 21:34:59.138565: \n",
      "2025-12-17 21:34:59.138565: Epoch 94\n",
      "2025-12-17 21:34:59.138565: Current learning rate: 0.00915\n",
      "2025-12-17 21:37:17.350604: train_loss -0.8064\n",
      "2025-12-17 21:37:17.350604: val_loss -0.8201\n",
      "2025-12-17 21:37:17.352606: Pseudo dice [0.9063, 0.9418, 0.9162]\n",
      "2025-12-17 21:37:17.354609: Epoch time: 138.21 s\n",
      "2025-12-17 21:37:17.354609: Yayy! New best EMA pseudo Dice: 0.9174\n",
      "2025-12-17 21:37:18.252181: \n",
      "2025-12-17 21:37:18.252181: Epoch 95\n",
      "2025-12-17 21:37:18.252181: Current learning rate: 0.00914\n",
      "2025-12-17 21:39:36.295223: train_loss -0.8017\n",
      "2025-12-17 21:39:36.295223: val_loss -0.8075\n",
      "2025-12-17 21:39:36.311015: Pseudo dice [0.8952, 0.9369, 0.9088]\n",
      "2025-12-17 21:39:36.311015: Epoch time: 138.04 s\n",
      "2025-12-17 21:39:37.085853: \n",
      "2025-12-17 21:39:37.085853: Epoch 96\n",
      "2025-12-17 21:39:37.085853: Current learning rate: 0.00913\n",
      "2025-12-17 21:41:55.275710: train_loss -0.7986\n",
      "2025-12-17 21:41:55.275710: val_loss -0.8233\n",
      "2025-12-17 21:41:55.275710: Pseudo dice [0.9097, 0.9379, 0.915]\n",
      "2025-12-17 21:41:55.275710: Epoch time: 138.19 s\n",
      "2025-12-17 21:41:55.275710: Yayy! New best EMA pseudo Dice: 0.9174\n",
      "2025-12-17 21:41:56.164639: \n",
      "2025-12-17 21:41:56.164639: Epoch 97\n",
      "2025-12-17 21:41:56.164639: Current learning rate: 0.00912\n",
      "2025-12-17 21:44:14.418317: train_loss -0.804\n",
      "2025-12-17 21:44:14.418317: val_loss -0.8253\n",
      "2025-12-17 21:44:14.421818: Pseudo dice [0.9086, 0.9408, 0.9118]\n",
      "2025-12-17 21:44:14.423821: Epoch time: 138.26 s\n",
      "2025-12-17 21:44:14.425823: Yayy! New best EMA pseudo Dice: 0.9177\n",
      "2025-12-17 21:44:15.309513: \n",
      "2025-12-17 21:44:15.309513: Epoch 98\n",
      "2025-12-17 21:44:15.309513: Current learning rate: 0.00911\n",
      "2025-12-17 21:46:33.459279: train_loss -0.8055\n",
      "2025-12-17 21:46:33.459279: val_loss -0.8362\n",
      "2025-12-17 21:46:33.461282: Pseudo dice [0.9169, 0.9474, 0.9129]\n",
      "2025-12-17 21:46:33.461282: Epoch time: 138.15 s\n",
      "2025-12-17 21:46:33.461282: Yayy! New best EMA pseudo Dice: 0.9185\n",
      "2025-12-17 21:46:34.326551: \n",
      "2025-12-17 21:46:34.326551: Epoch 99\n",
      "2025-12-17 21:46:34.326551: Current learning rate: 0.0091\n",
      "2025-12-17 21:48:52.389966: train_loss -0.8101\n",
      "2025-12-17 21:48:52.389966: val_loss -0.8351\n",
      "2025-12-17 21:48:52.393059: Pseudo dice [0.9143, 0.9495, 0.918]\n",
      "2025-12-17 21:48:52.393059: Epoch time: 138.06 s\n",
      "2025-12-17 21:48:52.633056: Yayy! New best EMA pseudo Dice: 0.9194\n",
      "2025-12-17 21:48:53.530475: \n",
      "2025-12-17 21:48:53.530475: Epoch 100\n",
      "2025-12-17 21:48:53.536836: Current learning rate: 0.0091\n",
      "2025-12-17 21:51:12.568431: train_loss -0.8173\n",
      "2025-12-17 21:51:12.568431: val_loss -0.8273\n",
      "2025-12-17 21:51:12.573129: Pseudo dice [0.9092, 0.9456, 0.9169]\n",
      "2025-12-17 21:51:12.573129: Epoch time: 139.04 s\n",
      "2025-12-17 21:51:12.575132: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2025-12-17 21:51:13.462317: \n",
      "2025-12-17 21:51:13.462317: Epoch 101\n",
      "2025-12-17 21:51:13.464509: Current learning rate: 0.00909\n",
      "2025-12-17 21:53:32.295670: train_loss -0.8092\n",
      "2025-12-17 21:53:32.295670: val_loss -0.8212\n",
      "2025-12-17 21:53:32.297673: Pseudo dice [0.9073, 0.9422, 0.9115]\n",
      "2025-12-17 21:53:32.299675: Epoch time: 138.83 s\n",
      "2025-12-17 21:53:32.301678: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2025-12-17 21:53:33.508891: \n",
      "2025-12-17 21:53:33.508891: Epoch 102\n",
      "2025-12-17 21:53:33.510570: Current learning rate: 0.00908\n",
      "2025-12-17 21:55:52.040271: train_loss -0.8013\n",
      "2025-12-17 21:55:52.040271: val_loss -0.8206\n",
      "2025-12-17 21:55:52.052499: Pseudo dice [0.9092, 0.9416, 0.9107]\n",
      "2025-12-17 21:55:52.054399: Epoch time: 138.55 s\n",
      "2025-12-17 21:55:52.054399: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2025-12-17 21:55:52.950288: \n",
      "2025-12-17 21:55:52.950288: Epoch 103\n",
      "2025-12-17 21:55:52.950288: Current learning rate: 0.00907\n",
      "2025-12-17 21:58:11.699275: train_loss -0.8062\n",
      "2025-12-17 21:58:11.699275: val_loss -0.8409\n",
      "2025-12-17 21:58:11.706929: Pseudo dice [0.9195, 0.9443, 0.919]\n",
      "2025-12-17 21:58:11.706929: Epoch time: 138.75 s\n",
      "2025-12-17 21:58:11.708932: Yayy! New best EMA pseudo Dice: 0.9207\n",
      "2025-12-17 21:58:12.603006: \n",
      "2025-12-17 21:58:12.605008: Epoch 104\n",
      "2025-12-17 21:58:12.605008: Current learning rate: 0.00906\n",
      "2025-12-17 22:00:31.192122: train_loss -0.8124\n",
      "2025-12-17 22:00:31.192122: val_loss -0.8294\n",
      "2025-12-17 22:00:31.197030: Pseudo dice [0.9119, 0.9442, 0.9099]\n",
      "2025-12-17 22:00:31.198033: Epoch time: 138.59 s\n",
      "2025-12-17 22:00:31.200036: Yayy! New best EMA pseudo Dice: 0.9208\n",
      "2025-12-17 22:00:32.133892: \n",
      "2025-12-17 22:00:32.133892: Epoch 105\n",
      "2025-12-17 22:00:32.135765: Current learning rate: 0.00905\n",
      "2025-12-17 22:02:50.325681: train_loss -0.8079\n",
      "2025-12-17 22:02:50.325681: val_loss -0.816\n",
      "2025-12-17 22:02:50.327683: Pseudo dice [0.9047, 0.9359, 0.9102]\n",
      "2025-12-17 22:02:50.329424: Epoch time: 138.19 s\n",
      "2025-12-17 22:02:50.967064: \n",
      "2025-12-17 22:02:50.967064: Epoch 106\n",
      "2025-12-17 22:02:50.969066: Current learning rate: 0.00904\n",
      "2025-12-17 22:05:09.252465: train_loss -0.8038\n",
      "2025-12-17 22:05:09.252465: val_loss -0.821\n",
      "2025-12-17 22:05:09.256469: Pseudo dice [0.9069, 0.9441, 0.9098]\n",
      "2025-12-17 22:05:09.256469: Epoch time: 138.29 s\n",
      "2025-12-17 22:05:09.900927: \n",
      "2025-12-17 22:05:09.900927: Epoch 107\n",
      "2025-12-17 22:05:09.900927: Current learning rate: 0.00903\n",
      "2025-12-17 22:07:28.077469: train_loss -0.8035\n",
      "2025-12-17 22:07:28.077469: val_loss -0.8207\n",
      "2025-12-17 22:07:28.077469: Pseudo dice [0.9024, 0.9396, 0.9177]\n",
      "2025-12-17 22:07:28.081964: Epoch time: 138.18 s\n",
      "2025-12-17 22:07:28.893426: \n",
      "2025-12-17 22:07:28.893426: Epoch 108\n",
      "2025-12-17 22:07:28.895893: Current learning rate: 0.00902\n",
      "2025-12-17 22:09:47.422853: train_loss -0.808\n",
      "2025-12-17 22:09:47.422853: val_loss -0.8282\n",
      "2025-12-17 22:09:47.424855: Pseudo dice [0.9061, 0.9446, 0.9204]\n",
      "2025-12-17 22:09:47.426857: Epoch time: 138.53 s\n",
      "2025-12-17 22:09:48.048701: \n",
      "2025-12-17 22:09:48.048701: Epoch 109\n",
      "2025-12-17 22:09:48.064799: Current learning rate: 0.00901\n",
      "2025-12-17 22:12:06.197486: train_loss -0.8095\n",
      "2025-12-17 22:12:06.197486: val_loss -0.828\n",
      "2025-12-17 22:12:06.199488: Pseudo dice [0.9109, 0.94, 0.9067]\n",
      "2025-12-17 22:12:06.201490: Epoch time: 138.15 s\n",
      "2025-12-17 22:12:06.825292: \n",
      "2025-12-17 22:12:06.825292: Epoch 110\n",
      "2025-12-17 22:12:06.825292: Current learning rate: 0.009\n",
      "2025-12-17 22:14:24.994057: train_loss -0.8116\n",
      "2025-12-17 22:14:24.994057: val_loss -0.8271\n",
      "2025-12-17 22:14:24.997802: Pseudo dice [0.9056, 0.9445, 0.9235]\n",
      "2025-12-17 22:14:24.999805: Epoch time: 138.17 s\n",
      "2025-12-17 22:14:24.999805: Yayy! New best EMA pseudo Dice: 0.9209\n",
      "2025-12-17 22:14:25.883312: \n",
      "2025-12-17 22:14:25.883312: Epoch 111\n",
      "2025-12-17 22:14:25.883312: Current learning rate: 0.009\n",
      "2025-12-17 22:16:44.044858: train_loss -0.8171\n",
      "2025-12-17 22:16:44.046860: val_loss -0.8336\n",
      "2025-12-17 22:16:44.048862: Pseudo dice [0.9141, 0.9426, 0.9085]\n",
      "2025-12-17 22:16:44.048862: Epoch time: 138.16 s\n",
      "2025-12-17 22:16:44.048862: Yayy! New best EMA pseudo Dice: 0.921\n",
      "2025-12-17 22:16:44.951194: \n",
      "2025-12-17 22:16:44.951194: Epoch 112\n",
      "2025-12-17 22:16:44.951194: Current learning rate: 0.00899\n",
      "2025-12-17 22:19:03.291094: train_loss -0.8183\n",
      "2025-12-17 22:19:03.291094: val_loss -0.8332\n",
      "2025-12-17 22:19:03.291094: Pseudo dice [0.9112, 0.9434, 0.9145]\n",
      "2025-12-17 22:19:03.306903: Epoch time: 138.34 s\n",
      "2025-12-17 22:19:03.306903: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2025-12-17 22:19:04.180902: \n",
      "2025-12-17 22:19:04.180902: Epoch 113\n",
      "2025-12-17 22:19:04.180902: Current learning rate: 0.00898\n",
      "2025-12-17 22:21:22.420536: train_loss -0.8197\n",
      "2025-12-17 22:21:22.420536: val_loss -0.8316\n",
      "2025-12-17 22:21:22.422538: Pseudo dice [0.9163, 0.9448, 0.8994]\n",
      "2025-12-17 22:21:22.424540: Epoch time: 138.24 s\n",
      "2025-12-17 22:21:23.084414: \n",
      "2025-12-17 22:21:23.084414: Epoch 114\n",
      "2025-12-17 22:21:23.090223: Current learning rate: 0.00897\n",
      "2025-12-17 22:23:41.363015: train_loss -0.8128\n",
      "2025-12-17 22:23:41.363015: val_loss -0.8445\n",
      "2025-12-17 22:23:41.365017: Pseudo dice [0.9182, 0.9492, 0.9176]\n",
      "2025-12-17 22:23:41.367019: Epoch time: 138.28 s\n",
      "2025-12-17 22:23:41.367019: Yayy! New best EMA pseudo Dice: 0.9218\n",
      "2025-12-17 22:23:42.434572: \n",
      "2025-12-17 22:23:42.434572: Epoch 115\n",
      "2025-12-17 22:23:42.434572: Current learning rate: 0.00896\n",
      "2025-12-17 22:26:00.711435: train_loss -0.8128\n",
      "2025-12-17 22:26:00.713437: val_loss -0.8254\n",
      "2025-12-17 22:26:00.713437: Pseudo dice [0.9071, 0.9365, 0.908]\n",
      "2025-12-17 22:26:00.713437: Epoch time: 138.28 s\n",
      "2025-12-17 22:26:01.331272: \n",
      "2025-12-17 22:26:01.331272: Epoch 116\n",
      "2025-12-17 22:26:01.347122: Current learning rate: 0.00895\n",
      "2025-12-17 22:28:19.416281: train_loss -0.8171\n",
      "2025-12-17 22:28:19.418263: val_loss -0.828\n",
      "2025-12-17 22:28:19.420265: Pseudo dice [0.9065, 0.9425, 0.9209]\n",
      "2025-12-17 22:28:19.420265: Epoch time: 138.09 s\n",
      "2025-12-17 22:28:20.074162: \n",
      "2025-12-17 22:28:20.074162: Epoch 117\n",
      "2025-12-17 22:28:20.074162: Current learning rate: 0.00894\n",
      "2025-12-17 22:30:38.180514: train_loss -0.8142\n",
      "2025-12-17 22:30:38.180514: val_loss -0.8302\n",
      "2025-12-17 22:30:38.184517: Pseudo dice [0.9127, 0.9406, 0.9068]\n",
      "2025-12-17 22:30:38.184517: Epoch time: 138.12 s\n",
      "2025-12-17 22:30:38.833505: \n",
      "2025-12-17 22:30:38.835246: Epoch 118\n",
      "2025-12-17 22:30:38.835246: Current learning rate: 0.00893\n",
      "2025-12-17 22:32:57.007070: train_loss -0.8144\n",
      "2025-12-17 22:32:57.007070: val_loss -0.8353\n",
      "2025-12-17 22:32:57.007070: Pseudo dice [0.9164, 0.9458, 0.9122]\n",
      "2025-12-17 22:32:57.010980: Epoch time: 138.17 s\n",
      "2025-12-17 22:32:57.656555: \n",
      "2025-12-17 22:32:57.656555: Epoch 119\n",
      "2025-12-17 22:32:57.656555: Current learning rate: 0.00892\n",
      "2025-12-17 22:35:15.794872: train_loss -0.8106\n",
      "2025-12-17 22:35:15.794872: val_loss -0.8345\n",
      "2025-12-17 22:35:15.794872: Pseudo dice [0.9154, 0.9466, 0.9124]\n",
      "2025-12-17 22:35:15.794872: Epoch time: 138.14 s\n",
      "2025-12-17 22:35:15.810730: Yayy! New best EMA pseudo Dice: 0.9221\n",
      "2025-12-17 22:35:16.851010: \n",
      "2025-12-17 22:35:16.851010: Epoch 120\n",
      "2025-12-17 22:35:16.851010: Current learning rate: 0.00891\n",
      "2025-12-17 22:37:35.020705: train_loss -0.8171\n",
      "2025-12-17 22:37:35.020705: val_loss -0.8403\n",
      "2025-12-17 22:37:35.034006: Pseudo dice [0.9123, 0.9503, 0.9181]\n",
      "2025-12-17 22:37:35.036008: Epoch time: 138.17 s\n",
      "2025-12-17 22:37:35.036008: Yayy! New best EMA pseudo Dice: 0.9225\n",
      "2025-12-17 22:37:35.954183: \n",
      "2025-12-17 22:37:35.954183: Epoch 121\n",
      "2025-12-17 22:37:35.970030: Current learning rate: 0.0089\n",
      "2025-12-17 22:39:54.069604: train_loss -0.8159\n",
      "2025-12-17 22:39:54.069604: val_loss -0.8424\n",
      "2025-12-17 22:39:54.071605: Pseudo dice [0.9191, 0.9556, 0.9168]\n",
      "2025-12-17 22:39:54.073607: Epoch time: 138.12 s\n",
      "2025-12-17 22:39:54.075609: Yayy! New best EMA pseudo Dice: 0.9233\n",
      "2025-12-17 22:39:54.974600: \n",
      "2025-12-17 22:39:54.976445: Epoch 122\n",
      "2025-12-17 22:39:54.976445: Current learning rate: 0.00889\n",
      "2025-12-17 22:42:13.120478: train_loss -0.8152\n",
      "2025-12-17 22:42:13.120478: val_loss -0.831\n",
      "2025-12-17 22:42:13.120478: Pseudo dice [0.9118, 0.9394, 0.9162]\n",
      "2025-12-17 22:42:13.120478: Epoch time: 138.15 s\n",
      "2025-12-17 22:42:13.761649: \n",
      "2025-12-17 22:42:13.763652: Epoch 123\n",
      "2025-12-17 22:42:13.763652: Current learning rate: 0.00889\n",
      "2025-12-17 22:44:31.954501: train_loss -0.8138\n",
      "2025-12-17 22:44:31.954501: val_loss -0.8286\n",
      "2025-12-17 22:44:31.954501: Pseudo dice [0.9077, 0.944, 0.9185]\n",
      "2025-12-17 22:44:31.958002: Epoch time: 138.19 s\n",
      "2025-12-17 22:44:32.631158: \n",
      "2025-12-17 22:44:32.631158: Epoch 124\n",
      "2025-12-17 22:44:32.631158: Current learning rate: 0.00888\n",
      "2025-12-17 22:46:50.611531: train_loss -0.8146\n",
      "2025-12-17 22:46:50.611531: val_loss -0.8331\n",
      "2025-12-17 22:46:50.613534: Pseudo dice [0.9145, 0.9421, 0.9147]\n",
      "2025-12-17 22:46:50.617543: Epoch time: 137.98 s\n",
      "2025-12-17 22:46:51.260091: \n",
      "2025-12-17 22:46:51.260091: Epoch 125\n",
      "2025-12-17 22:46:51.260091: Current learning rate: 0.00887\n",
      "2025-12-17 22:49:09.243903: train_loss -0.809\n",
      "2025-12-17 22:49:09.243903: val_loss -0.8266\n",
      "2025-12-17 22:49:09.243903: Pseudo dice [0.907, 0.9428, 0.9159]\n",
      "2025-12-17 22:49:09.243903: Epoch time: 137.98 s\n",
      "2025-12-17 22:49:10.056740: \n",
      "2025-12-17 22:49:10.072567: Epoch 126\n",
      "2025-12-17 22:49:10.072567: Current learning rate: 0.00886\n",
      "2025-12-17 22:51:28.170131: train_loss -0.8173\n",
      "2025-12-17 22:51:28.170131: val_loss -0.8326\n",
      "2025-12-17 22:51:28.170131: Pseudo dice [0.908, 0.9433, 0.9208]\n",
      "2025-12-17 22:51:28.170131: Epoch time: 138.11 s\n",
      "2025-12-17 22:51:28.836521: \n",
      "2025-12-17 22:51:28.836521: Epoch 127\n",
      "2025-12-17 22:51:28.836521: Current learning rate: 0.00885\n",
      "2025-12-17 22:53:47.004789: train_loss -0.8219\n",
      "2025-12-17 22:53:47.004789: val_loss -0.8419\n",
      "2025-12-17 22:53:47.004789: Pseudo dice [0.9128, 0.9486, 0.9287]\n",
      "2025-12-17 22:53:47.004789: Epoch time: 138.18 s\n",
      "2025-12-17 22:53:47.011342: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2025-12-17 22:53:47.884196: \n",
      "2025-12-17 22:53:47.884196: Epoch 128\n",
      "2025-12-17 22:53:47.884196: Current learning rate: 0.00884\n",
      "2025-12-17 22:56:06.164153: train_loss -0.8207\n",
      "2025-12-17 22:56:06.164153: val_loss -0.8416\n",
      "2025-12-17 22:56:06.164153: Pseudo dice [0.9098, 0.9455, 0.9329]\n",
      "2025-12-17 22:56:06.164153: Epoch time: 138.28 s\n",
      "2025-12-17 22:56:06.180032: Yayy! New best EMA pseudo Dice: 0.9245\n",
      "2025-12-17 22:56:07.078601: \n",
      "2025-12-17 22:56:07.078601: Epoch 129\n",
      "2025-12-17 22:56:07.078601: Current learning rate: 0.00883\n",
      "2025-12-17 22:58:25.172385: train_loss -0.8153\n",
      "2025-12-17 22:58:25.172385: val_loss -0.8348\n",
      "2025-12-17 22:58:25.172385: Pseudo dice [0.9114, 0.9452, 0.9144]\n",
      "2025-12-17 22:58:25.188420: Epoch time: 138.1 s\n",
      "2025-12-17 22:58:25.841398: \n",
      "2025-12-17 22:58:25.841398: Epoch 130\n",
      "2025-12-17 22:58:25.841398: Current learning rate: 0.00882\n",
      "2025-12-17 23:00:44.065592: train_loss -0.8117\n",
      "2025-12-17 23:00:44.065592: val_loss -0.8427\n",
      "2025-12-17 23:00:44.073161: Pseudo dice [0.9172, 0.9509, 0.9214]\n",
      "2025-12-17 23:00:44.073161: Epoch time: 138.23 s\n",
      "2025-12-17 23:00:44.075164: Yayy! New best EMA pseudo Dice: 0.9249\n",
      "2025-12-17 23:00:44.968640: \n",
      "2025-12-17 23:00:44.984591: Epoch 131\n",
      "2025-12-17 23:00:44.986007: Current learning rate: 0.00881\n",
      "2025-12-17 23:03:03.057322: train_loss -0.8167\n",
      "2025-12-17 23:03:03.057322: val_loss -0.8216\n",
      "2025-12-17 23:03:03.064282: Pseudo dice [0.9033, 0.9383, 0.9147]\n",
      "2025-12-17 23:03:03.064282: Epoch time: 138.09 s\n",
      "2025-12-17 23:03:03.866241: \n",
      "2025-12-17 23:03:03.866241: Epoch 132\n",
      "2025-12-17 23:03:03.866241: Current learning rate: 0.0088\n",
      "2025-12-17 23:05:22.044165: train_loss -0.822\n",
      "2025-12-17 23:05:22.044165: val_loss -0.8482\n",
      "2025-12-17 23:05:22.057915: Pseudo dice [0.9245, 0.9526, 0.9178]\n",
      "2025-12-17 23:05:22.059919: Epoch time: 138.18 s\n",
      "2025-12-17 23:05:22.059919: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2025-12-17 23:05:22.948775: \n",
      "2025-12-17 23:05:22.948775: Epoch 133\n",
      "2025-12-17 23:05:22.948775: Current learning rate: 0.00879\n",
      "2025-12-17 23:07:41.185489: train_loss -0.8141\n",
      "2025-12-17 23:07:41.185489: val_loss -0.8434\n",
      "2025-12-17 23:07:41.201278: Pseudo dice [0.917, 0.9511, 0.9108]\n",
      "2025-12-17 23:07:41.201278: Epoch time: 138.24 s\n",
      "2025-12-17 23:07:41.201278: Yayy! New best EMA pseudo Dice: 0.9252\n",
      "2025-12-17 23:07:42.091630: \n",
      "2025-12-17 23:07:42.091630: Epoch 134\n",
      "2025-12-17 23:07:42.091630: Current learning rate: 0.00879\n",
      "2025-12-17 23:10:00.469954: train_loss -0.8246\n",
      "2025-12-17 23:10:00.469954: val_loss -0.836\n",
      "2025-12-17 23:10:00.469954: Pseudo dice [0.9083, 0.946, 0.9209]\n",
      "2025-12-17 23:10:00.469954: Epoch time: 138.38 s\n",
      "2025-12-17 23:10:01.108570: \n",
      "2025-12-17 23:10:01.108570: Epoch 135\n",
      "2025-12-17 23:10:01.108570: Current learning rate: 0.00878\n",
      "2025-12-17 23:12:19.294837: train_loss -0.8185\n",
      "2025-12-17 23:12:19.296839: val_loss -0.8317\n",
      "2025-12-17 23:12:19.300848: Pseudo dice [0.9058, 0.941, 0.916]\n",
      "2025-12-17 23:12:19.302850: Epoch time: 138.19 s\n",
      "2025-12-17 23:12:20.035038: \n",
      "2025-12-17 23:12:20.035038: Epoch 136\n",
      "2025-12-17 23:12:20.035038: Current learning rate: 0.00877\n",
      "2025-12-17 23:14:38.365242: train_loss -0.8169\n",
      "2025-12-17 23:14:38.365242: val_loss -0.8366\n",
      "2025-12-17 23:14:38.365242: Pseudo dice [0.915, 0.9454, 0.9212]\n",
      "2025-12-17 23:14:38.365242: Epoch time: 138.33 s\n",
      "2025-12-17 23:14:39.029853: \n",
      "2025-12-17 23:14:39.029853: Epoch 137\n",
      "2025-12-17 23:14:39.029853: Current learning rate: 0.00876\n",
      "2025-12-17 23:16:57.172191: train_loss -0.816\n",
      "2025-12-17 23:16:57.172191: val_loss -0.84\n",
      "2025-12-17 23:16:57.172191: Pseudo dice [0.9202, 0.952, 0.9075]\n",
      "2025-12-17 23:16:57.188027: Epoch time: 138.14 s\n",
      "2025-12-17 23:16:58.013986: \n",
      "2025-12-17 23:16:58.013986: Epoch 138\n",
      "2025-12-17 23:16:58.013986: Current learning rate: 0.00875\n",
      "2025-12-17 23:19:16.090378: train_loss -0.8156\n",
      "2025-12-17 23:19:16.092380: val_loss -0.8376\n",
      "2025-12-17 23:19:16.096387: Pseudo dice [0.9099, 0.9465, 0.9291]\n",
      "2025-12-17 23:19:16.098390: Epoch time: 138.08 s\n",
      "2025-12-17 23:19:16.100392: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2025-12-17 23:19:17.084874: \n",
      "2025-12-17 23:19:17.084874: Epoch 139\n",
      "2025-12-17 23:19:17.086707: Current learning rate: 0.00874\n",
      "2025-12-17 23:21:35.159609: train_loss -0.8222\n",
      "2025-12-17 23:21:35.159609: val_loss -0.8478\n",
      "2025-12-17 23:21:35.159609: Pseudo dice [0.9238, 0.9497, 0.9166]\n",
      "2025-12-17 23:21:35.159609: Epoch time: 138.08 s\n",
      "2025-12-17 23:21:35.159609: Yayy! New best EMA pseudo Dice: 0.9259\n",
      "2025-12-17 23:21:36.060701: \n",
      "2025-12-17 23:21:36.076604: Epoch 140\n",
      "2025-12-17 23:21:36.076604: Current learning rate: 0.00873\n",
      "2025-12-17 23:23:54.442318: train_loss -0.8191\n",
      "2025-12-17 23:23:54.442318: val_loss -0.8313\n",
      "2025-12-17 23:23:54.442318: Pseudo dice [0.9067, 0.9406, 0.9207]\n",
      "2025-12-17 23:23:54.442318: Epoch time: 138.38 s\n",
      "2025-12-17 23:23:55.096109: \n",
      "2025-12-17 23:23:55.096109: Epoch 141\n",
      "2025-12-17 23:23:55.096109: Current learning rate: 0.00872\n",
      "2025-12-17 23:26:13.238771: train_loss -0.8236\n",
      "2025-12-17 23:26:13.238771: val_loss -0.8515\n",
      "2025-12-17 23:26:13.242777: Pseudo dice [0.9234, 0.9518, 0.9252]\n",
      "2025-12-17 23:26:13.244780: Epoch time: 138.14 s\n",
      "2025-12-17 23:26:13.246781: Yayy! New best EMA pseudo Dice: 0.9264\n",
      "2025-12-17 23:26:14.282981: \n",
      "2025-12-17 23:26:14.282981: Epoch 142\n",
      "2025-12-17 23:26:14.282981: Current learning rate: 0.00871\n",
      "2025-12-17 23:28:32.519789: train_loss -0.821\n",
      "2025-12-17 23:28:32.519789: val_loss -0.8273\n",
      "2025-12-17 23:28:32.535807: Pseudo dice [0.9086, 0.9447, 0.9113]\n",
      "2025-12-17 23:28:32.537638: Epoch time: 138.24 s\n",
      "2025-12-17 23:28:33.199569: \n",
      "2025-12-17 23:28:33.199569: Epoch 143\n",
      "2025-12-17 23:28:33.199569: Current learning rate: 0.0087\n",
      "2025-12-17 23:30:51.365877: train_loss -0.8242\n",
      "2025-12-17 23:30:51.365877: val_loss -0.8351\n",
      "2025-12-17 23:30:51.367880: Pseudo dice [0.9069, 0.9494, 0.9241]\n",
      "2025-12-17 23:30:51.369882: Epoch time: 138.17 s\n",
      "2025-12-17 23:30:52.188912: \n",
      "2025-12-17 23:30:52.188912: Epoch 144\n",
      "2025-12-17 23:30:52.188912: Current learning rate: 0.00869\n",
      "2025-12-17 23:33:10.386069: train_loss -0.8257\n",
      "2025-12-17 23:33:10.386069: val_loss -0.8434\n",
      "2025-12-17 23:33:10.386069: Pseudo dice [0.9181, 0.9479, 0.9227]\n",
      "2025-12-17 23:33:10.386069: Epoch time: 138.2 s\n",
      "2025-12-17 23:33:11.147591: \n",
      "2025-12-17 23:33:11.149594: Epoch 145\n",
      "2025-12-17 23:33:11.149594: Current learning rate: 0.00868\n",
      "2025-12-17 23:35:29.571842: train_loss -0.8231\n",
      "2025-12-17 23:35:29.571842: val_loss -0.8592\n",
      "2025-12-17 23:35:29.573845: Pseudo dice [0.9237, 0.9553, 0.93]\n",
      "2025-12-17 23:35:29.576349: Epoch time: 138.42 s\n",
      "2025-12-17 23:35:29.578351: Yayy! New best EMA pseudo Dice: 0.9274\n",
      "2025-12-17 23:35:30.462646: \n",
      "2025-12-17 23:35:30.462646: Epoch 146\n",
      "2025-12-17 23:35:30.462646: Current learning rate: 0.00868\n",
      "2025-12-17 23:37:48.780363: train_loss -0.8219\n",
      "2025-12-17 23:37:48.780363: val_loss -0.844\n",
      "2025-12-17 23:37:48.780363: Pseudo dice [0.9151, 0.9509, 0.9153]\n",
      "2025-12-17 23:37:48.784313: Epoch time: 138.32 s\n",
      "2025-12-17 23:37:49.426381: \n",
      "2025-12-17 23:37:49.426381: Epoch 147\n",
      "2025-12-17 23:37:49.426381: Current learning rate: 0.00867\n",
      "2025-12-17 23:40:07.531331: train_loss -0.817\n",
      "2025-12-17 23:40:07.531331: val_loss -0.8415\n",
      "2025-12-17 23:40:07.531331: Pseudo dice [0.9179, 0.9482, 0.9194]\n",
      "2025-12-17 23:40:07.536093: Epoch time: 138.1 s\n",
      "2025-12-17 23:40:07.538095: Yayy! New best EMA pseudo Dice: 0.9274\n",
      "2025-12-17 23:40:08.647348: \n",
      "2025-12-17 23:40:08.647348: Epoch 148\n",
      "2025-12-17 23:40:08.655018: Current learning rate: 0.00866\n",
      "2025-12-17 23:42:26.719163: train_loss -0.818\n",
      "2025-12-17 23:42:26.719163: val_loss -0.836\n",
      "2025-12-17 23:42:26.721166: Pseudo dice [0.9138, 0.9466, 0.9146]\n",
      "2025-12-17 23:42:26.723167: Epoch time: 138.07 s\n",
      "2025-12-17 23:42:27.388326: \n",
      "2025-12-17 23:42:27.388326: Epoch 149\n",
      "2025-12-17 23:42:27.391009: Current learning rate: 0.00865\n",
      "2025-12-17 23:44:45.571566: train_loss -0.8083\n",
      "2025-12-17 23:44:45.571566: val_loss -0.8282\n",
      "2025-12-17 23:44:45.571566: Pseudo dice [0.9111, 0.9419, 0.9071]\n",
      "2025-12-17 23:44:45.571566: Epoch time: 138.18 s\n",
      "2025-12-17 23:44:46.655419: \n",
      "2025-12-17 23:44:46.655419: Epoch 150\n",
      "2025-12-17 23:44:46.655419: Current learning rate: 0.00864\n",
      "2025-12-17 23:47:04.929481: train_loss -0.8163\n",
      "2025-12-17 23:47:04.931484: val_loss -0.8311\n",
      "2025-12-17 23:47:04.935256: Pseudo dice [0.908, 0.9418, 0.9327]\n",
      "2025-12-17 23:47:04.935256: Epoch time: 138.28 s\n",
      "2025-12-17 23:47:05.726356: \n",
      "2025-12-17 23:47:05.726356: Epoch 151\n",
      "2025-12-17 23:47:05.726356: Current learning rate: 0.00863\n",
      "2025-12-17 23:49:23.830533: train_loss -0.8201\n",
      "2025-12-17 23:49:23.830533: val_loss -0.8494\n",
      "2025-12-17 23:49:23.830533: Pseudo dice [0.9215, 0.9532, 0.9144]\n",
      "2025-12-17 23:49:23.846377: Epoch time: 138.1 s\n",
      "2025-12-17 23:49:24.495798: \n",
      "2025-12-17 23:49:24.495798: Epoch 152\n",
      "2025-12-17 23:49:24.495798: Current learning rate: 0.00862\n",
      "2025-12-17 23:51:42.774410: train_loss -0.8234\n",
      "2025-12-17 23:51:42.774410: val_loss -0.8327\n",
      "2025-12-17 23:51:42.774410: Pseudo dice [0.9072, 0.9438, 0.9266]\n",
      "2025-12-17 23:51:42.774410: Epoch time: 138.28 s\n",
      "2025-12-17 23:51:43.426806: \n",
      "2025-12-17 23:51:43.426806: Epoch 153\n",
      "2025-12-17 23:51:43.426806: Current learning rate: 0.00861\n",
      "2025-12-17 23:54:01.607428: train_loss -0.8088\n",
      "2025-12-17 23:54:01.607428: val_loss -0.8286\n",
      "2025-12-17 23:54:01.607428: Pseudo dice [0.9061, 0.9422, 0.9168]\n",
      "2025-12-17 23:54:01.623460: Epoch time: 138.18 s\n",
      "2025-12-17 23:54:02.398185: \n",
      "2025-12-17 23:54:02.398185: Epoch 154\n",
      "2025-12-17 23:54:02.398185: Current learning rate: 0.0086\n",
      "2025-12-17 23:56:20.413197: train_loss -0.8175\n",
      "2025-12-17 23:56:20.413197: val_loss -0.8223\n",
      "2025-12-17 23:56:20.415200: Pseudo dice [0.9029, 0.9416, 0.9072]\n",
      "2025-12-17 23:56:20.417202: Epoch time: 138.02 s\n",
      "2025-12-17 23:56:21.083896: \n",
      "2025-12-17 23:56:21.083896: Epoch 155\n",
      "2025-12-17 23:56:21.083896: Current learning rate: 0.00859\n",
      "2025-12-17 23:58:39.136252: train_loss -0.8152\n",
      "2025-12-17 23:58:39.138254: val_loss -0.8324\n",
      "2025-12-17 23:58:39.140257: Pseudo dice [0.9077, 0.9451, 0.9251]\n",
      "2025-12-17 23:58:39.142260: Epoch time: 138.05 s\n",
      "2025-12-17 23:58:39.957623: \n",
      "2025-12-17 23:58:39.973550: Epoch 156\n",
      "2025-12-17 23:58:39.975484: Current learning rate: 0.00858\n",
      "2025-12-18 00:00:58.277033: train_loss -0.8156\n",
      "2025-12-18 00:00:58.277033: val_loss -0.8393\n",
      "2025-12-18 00:00:58.277033: Pseudo dice [0.9116, 0.9448, 0.9268]\n",
      "2025-12-18 00:00:58.277033: Epoch time: 138.32 s\n",
      "2025-12-18 00:00:59.018421: \n",
      "2025-12-18 00:00:59.018421: Epoch 157\n",
      "2025-12-18 00:00:59.018421: Current learning rate: 0.00858\n",
      "2025-12-18 00:03:17.085826: train_loss -0.824\n",
      "2025-12-18 00:03:17.085826: val_loss -0.8409\n",
      "2025-12-18 00:03:17.092225: Pseudo dice [0.9163, 0.9477, 0.9159]\n",
      "2025-12-18 00:03:17.094434: Epoch time: 138.07 s\n",
      "2025-12-18 00:03:17.760301: \n",
      "2025-12-18 00:03:17.760301: Epoch 158\n",
      "2025-12-18 00:03:17.760301: Current learning rate: 0.00857\n",
      "2025-12-18 00:05:36.191634: train_loss -0.8172\n",
      "2025-12-18 00:05:36.193638: val_loss -0.8404\n",
      "2025-12-18 00:05:36.197644: Pseudo dice [0.9151, 0.946, 0.921]\n",
      "2025-12-18 00:05:36.199647: Epoch time: 138.43 s\n",
      "2025-12-18 00:05:36.863498: \n",
      "2025-12-18 00:05:36.863498: Epoch 159\n",
      "2025-12-18 00:05:36.865241: Current learning rate: 0.00856\n",
      "2025-12-18 00:07:55.307315: train_loss -0.8162\n",
      "2025-12-18 00:07:55.307315: val_loss -0.832\n",
      "2025-12-18 00:07:55.310388: Pseudo dice [0.9112, 0.9402, 0.9205]\n",
      "2025-12-18 00:07:55.310388: Epoch time: 138.45 s\n",
      "2025-12-18 00:07:56.117045: \n",
      "2025-12-18 00:07:56.117045: Epoch 160\n",
      "2025-12-18 00:07:56.117045: Current learning rate: 0.00855\n",
      "2025-12-18 00:10:14.633581: train_loss -0.8236\n",
      "2025-12-18 00:10:14.635583: val_loss -0.8302\n",
      "2025-12-18 00:10:14.637585: Pseudo dice [0.9071, 0.9416, 0.9204]\n",
      "2025-12-18 00:10:14.639587: Epoch time: 138.52 s\n",
      "2025-12-18 00:10:15.300294: \n",
      "2025-12-18 00:10:15.302034: Epoch 161\n",
      "2025-12-18 00:10:15.302034: Current learning rate: 0.00854\n",
      "2025-12-18 00:12:33.423639: train_loss -0.8239\n",
      "2025-12-18 00:12:33.423639: val_loss -0.8466\n",
      "2025-12-18 00:12:33.425642: Pseudo dice [0.9116, 0.9498, 0.9298]\n",
      "2025-12-18 00:12:33.425642: Epoch time: 138.12 s\n",
      "2025-12-18 00:12:34.263499: \n",
      "2025-12-18 00:12:34.263499: Epoch 162\n",
      "2025-12-18 00:12:34.263499: Current learning rate: 0.00853\n",
      "2025-12-18 00:14:52.423560: train_loss -0.8249\n",
      "2025-12-18 00:14:52.423560: val_loss -0.8598\n",
      "2025-12-18 00:14:52.427421: Pseudo dice [0.9267, 0.952, 0.9263]\n",
      "2025-12-18 00:14:52.429423: Epoch time: 138.16 s\n",
      "2025-12-18 00:14:53.087988: \n",
      "2025-12-18 00:14:53.087988: Epoch 163\n",
      "2025-12-18 00:14:53.087988: Current learning rate: 0.00852\n",
      "2025-12-18 00:17:11.391472: train_loss -0.7912\n",
      "2025-12-18 00:17:11.391472: val_loss -0.808\n",
      "2025-12-18 00:17:11.394047: Pseudo dice [0.8964, 0.9378, 0.9187]\n",
      "2025-12-18 00:17:11.396050: Epoch time: 138.3 s\n",
      "2025-12-18 00:17:12.058330: \n",
      "2025-12-18 00:17:12.058330: Epoch 164\n",
      "2025-12-18 00:17:12.058330: Current learning rate: 0.00851\n",
      "2025-12-18 00:19:30.377658: train_loss -0.8015\n",
      "2025-12-18 00:19:30.377658: val_loss -0.8288\n",
      "2025-12-18 00:19:30.381855: Pseudo dice [0.9092, 0.9411, 0.9161]\n",
      "2025-12-18 00:19:30.384542: Epoch time: 138.32 s\n",
      "2025-12-18 00:19:31.025715: \n",
      "2025-12-18 00:19:31.025715: Epoch 165\n",
      "2025-12-18 00:19:31.025715: Current learning rate: 0.0085\n",
      "2025-12-18 00:21:49.113511: train_loss -0.8075\n",
      "2025-12-18 00:21:49.115444: val_loss -0.824\n",
      "2025-12-18 00:21:49.117450: Pseudo dice [0.9044, 0.9418, 0.9126]\n",
      "2025-12-18 00:21:49.119453: Epoch time: 138.1 s\n",
      "2025-12-18 00:21:49.778971: \n",
      "2025-12-18 00:21:49.778971: Epoch 166\n",
      "2025-12-18 00:21:49.778971: Current learning rate: 0.00849\n",
      "2025-12-18 00:24:07.872613: train_loss -0.7858\n",
      "2025-12-18 00:24:07.872613: val_loss -0.8181\n",
      "2025-12-18 00:24:07.876199: Pseudo dice [0.9144, 0.9431, 0.8995]\n",
      "2025-12-18 00:24:07.876199: Epoch time: 138.09 s\n",
      "2025-12-18 00:24:08.522684: \n",
      "2025-12-18 00:24:08.522684: Epoch 167\n",
      "2025-12-18 00:24:08.522684: Current learning rate: 0.00848\n",
      "2025-12-18 00:26:26.804133: train_loss -0.7891\n",
      "2025-12-18 00:26:26.806134: val_loss -0.805\n",
      "2025-12-18 00:26:26.808136: Pseudo dice [0.8963, 0.9386, 0.9054]\n",
      "2025-12-18 00:26:26.810138: Epoch time: 138.28 s\n",
      "2025-12-18 00:26:27.639692: \n",
      "2025-12-18 00:26:27.639692: Epoch 168\n",
      "2025-12-18 00:26:27.643573: Current learning rate: 0.00847\n",
      "2025-12-18 00:28:45.763939: train_loss -0.7893\n",
      "2025-12-18 00:28:45.765940: val_loss -0.8294\n",
      "2025-12-18 00:28:45.767943: Pseudo dice [0.901, 0.946, 0.9303]\n",
      "2025-12-18 00:28:45.769930: Epoch time: 138.12 s\n",
      "2025-12-18 00:28:46.436838: \n",
      "2025-12-18 00:28:46.436838: Epoch 169\n",
      "2025-12-18 00:28:46.436838: Current learning rate: 0.00847\n",
      "2025-12-18 00:31:04.636634: train_loss -0.8017\n",
      "2025-12-18 00:31:04.636634: val_loss -0.8325\n",
      "2025-12-18 00:31:04.636634: Pseudo dice [0.9115, 0.9453, 0.9156]\n",
      "2025-12-18 00:31:04.636634: Epoch time: 138.2 s\n",
      "2025-12-18 00:31:05.291632: \n",
      "2025-12-18 00:31:05.291632: Epoch 170\n",
      "2025-12-18 00:31:05.291632: Current learning rate: 0.00846\n",
      "2025-12-18 00:33:23.595224: train_loss -0.7981\n",
      "2025-12-18 00:33:23.595224: val_loss -0.8026\n",
      "2025-12-18 00:33:23.595224: Pseudo dice [0.8905, 0.935, 0.9099]\n",
      "2025-12-18 00:33:23.604466: Epoch time: 138.31 s\n",
      "2025-12-18 00:33:24.275604: \n",
      "2025-12-18 00:33:24.275604: Epoch 171\n",
      "2025-12-18 00:33:24.275604: Current learning rate: 0.00845\n",
      "2025-12-18 00:35:42.466499: train_loss -0.796\n",
      "2025-12-18 00:35:42.468502: val_loss -0.823\n",
      "2025-12-18 00:35:42.472508: Pseudo dice [0.9006, 0.946, 0.918]\n",
      "2025-12-18 00:35:42.472508: Epoch time: 138.19 s\n",
      "2025-12-18 00:35:43.133423: \n",
      "2025-12-18 00:35:43.133423: Epoch 172\n",
      "2025-12-18 00:35:43.133423: Current learning rate: 0.00844\n",
      "2025-12-18 00:38:01.435011: train_loss -0.8084\n",
      "2025-12-18 00:38:01.435011: val_loss -0.8307\n",
      "2025-12-18 00:38:01.437014: Pseudo dice [0.9066, 0.9444, 0.9247]\n",
      "2025-12-18 00:38:01.440513: Epoch time: 138.3 s\n",
      "2025-12-18 00:38:02.104754: \n",
      "2025-12-18 00:38:02.104754: Epoch 173\n",
      "2025-12-18 00:38:02.106757: Current learning rate: 0.00843\n",
      "2025-12-18 00:40:20.237557: train_loss -0.8202\n",
      "2025-12-18 00:40:20.237557: val_loss -0.8363\n",
      "2025-12-18 00:40:20.237557: Pseudo dice [0.9134, 0.9487, 0.9218]\n",
      "2025-12-18 00:40:20.237557: Epoch time: 138.13 s\n",
      "2025-12-18 00:40:21.076229: \n",
      "2025-12-18 00:40:21.076229: Epoch 174\n",
      "2025-12-18 00:40:21.076229: Current learning rate: 0.00842\n",
      "2025-12-18 00:42:39.155380: train_loss -0.8229\n",
      "2025-12-18 00:42:39.155380: val_loss -0.8408\n",
      "2025-12-18 00:42:39.157382: Pseudo dice [0.9139, 0.9494, 0.9183]\n",
      "2025-12-18 00:42:39.159384: Epoch time: 138.08 s\n",
      "2025-12-18 00:42:39.819479: \n",
      "2025-12-18 00:42:39.819479: Epoch 175\n",
      "2025-12-18 00:42:39.819479: Current learning rate: 0.00841\n",
      "2025-12-18 00:44:58.078092: train_loss -0.8161\n",
      "2025-12-18 00:44:58.078092: val_loss -0.8389\n",
      "2025-12-18 00:44:58.081693: Pseudo dice [0.9129, 0.947, 0.9275]\n",
      "2025-12-18 00:44:58.081693: Epoch time: 138.26 s\n",
      "2025-12-18 00:44:58.727802: \n",
      "2025-12-18 00:44:58.727802: Epoch 176\n",
      "2025-12-18 00:44:58.743724: Current learning rate: 0.0084\n",
      "2025-12-18 00:47:16.999396: train_loss -0.8056\n",
      "2025-12-18 00:47:16.999396: val_loss -0.8276\n",
      "2025-12-18 00:47:16.999396: Pseudo dice [0.9064, 0.9428, 0.914]\n",
      "2025-12-18 00:47:16.999396: Epoch time: 138.27 s\n",
      "2025-12-18 00:47:17.649245: \n",
      "2025-12-18 00:47:17.649245: Epoch 177\n",
      "2025-12-18 00:47:17.649245: Current learning rate: 0.00839\n",
      "2025-12-18 00:49:35.723659: train_loss -0.8032\n",
      "2025-12-18 00:49:35.723659: val_loss -0.8253\n",
      "2025-12-18 00:49:35.723659: Pseudo dice [0.9078, 0.9426, 0.9136]\n",
      "2025-12-18 00:49:35.723659: Epoch time: 138.07 s\n",
      "2025-12-18 00:49:36.373114: \n",
      "2025-12-18 00:49:36.373114: Epoch 178\n",
      "2025-12-18 00:49:36.373114: Current learning rate: 0.00838\n",
      "2025-12-18 00:51:54.688044: train_loss -0.8186\n",
      "2025-12-18 00:51:54.688044: val_loss -0.8307\n",
      "2025-12-18 00:51:54.691785: Pseudo dice [0.9067, 0.9438, 0.9199]\n",
      "2025-12-18 00:51:54.693787: Epoch time: 138.31 s\n",
      "2025-12-18 00:51:55.343860: \n",
      "2025-12-18 00:51:55.343860: Epoch 179\n",
      "2025-12-18 00:51:55.343860: Current learning rate: 0.00837\n",
      "2025-12-18 00:54:13.464649: train_loss -0.8209\n",
      "2025-12-18 00:54:13.466654: val_loss -0.835\n",
      "2025-12-18 00:54:13.468656: Pseudo dice [0.9094, 0.9437, 0.9189]\n",
      "2025-12-18 00:54:13.470659: Epoch time: 138.12 s\n",
      "2025-12-18 00:54:14.286985: \n",
      "2025-12-18 00:54:14.286985: Epoch 180\n",
      "2025-12-18 00:54:14.303075: Current learning rate: 0.00836\n",
      "2025-12-18 00:56:32.437552: train_loss -0.8235\n",
      "2025-12-18 00:56:32.439554: val_loss -0.8403\n",
      "2025-12-18 00:56:32.443558: Pseudo dice [0.9102, 0.9483, 0.9231]\n",
      "2025-12-18 00:56:32.445560: Epoch time: 138.15 s\n",
      "2025-12-18 00:56:33.134716: \n",
      "2025-12-18 00:56:33.134716: Epoch 181\n",
      "2025-12-18 00:56:33.134716: Current learning rate: 0.00836\n",
      "2025-12-18 00:58:51.283196: train_loss -0.8293\n",
      "2025-12-18 00:58:51.283196: val_loss -0.842\n",
      "2025-12-18 00:58:51.287200: Pseudo dice [0.9134, 0.9517, 0.922]\n",
      "2025-12-18 00:58:51.289202: Epoch time: 138.15 s\n",
      "2025-12-18 00:58:51.940264: \n",
      "2025-12-18 00:58:51.940264: Epoch 182\n",
      "2025-12-18 00:58:51.940264: Current learning rate: 0.00835\n",
      "2025-12-18 01:01:10.278159: train_loss -0.82\n",
      "2025-12-18 01:01:10.278159: val_loss -0.8383\n",
      "2025-12-18 01:01:10.282163: Pseudo dice [0.9073, 0.9425, 0.9286]\n",
      "2025-12-18 01:01:10.284165: Epoch time: 138.34 s\n",
      "2025-12-18 01:01:10.934543: \n",
      "2025-12-18 01:01:10.934543: Epoch 183\n",
      "2025-12-18 01:01:10.934543: Current learning rate: 0.00834\n",
      "2025-12-18 01:03:29.276942: train_loss -0.8236\n",
      "2025-12-18 01:03:29.276942: val_loss -0.8374\n",
      "2025-12-18 01:03:29.280946: Pseudo dice [0.9122, 0.946, 0.9181]\n",
      "2025-12-18 01:03:29.282948: Epoch time: 138.36 s\n",
      "2025-12-18 01:03:29.929180: \n",
      "2025-12-18 01:03:29.929180: Epoch 184\n",
      "2025-12-18 01:03:29.929180: Current learning rate: 0.00833\n",
      "2025-12-18 01:05:48.121102: train_loss -0.829\n",
      "2025-12-18 01:05:48.121102: val_loss -0.8496\n",
      "2025-12-18 01:05:48.136927: Pseudo dice [0.9187, 0.9504, 0.9234]\n",
      "2025-12-18 01:05:48.136927: Epoch time: 138.19 s\n",
      "2025-12-18 01:05:48.787672: \n",
      "2025-12-18 01:05:48.787672: Epoch 185\n",
      "2025-12-18 01:05:48.787672: Current learning rate: 0.00832\n",
      "2025-12-18 01:08:06.934641: train_loss -0.8326\n",
      "2025-12-18 01:08:06.934641: val_loss -0.8511\n",
      "2025-12-18 01:08:06.934641: Pseudo dice [0.9169, 0.9505, 0.9301]\n",
      "2025-12-18 01:08:06.950408: Epoch time: 138.15 s\n",
      "2025-12-18 01:08:07.758630: \n",
      "2025-12-18 01:08:07.758630: Epoch 186\n",
      "2025-12-18 01:08:07.758630: Current learning rate: 0.00831\n",
      "2025-12-18 01:10:26.316653: train_loss -0.8297\n",
      "2025-12-18 01:10:26.316653: val_loss -0.8443\n",
      "2025-12-18 01:10:26.320397: Pseudo dice [0.9161, 0.9453, 0.9235]\n",
      "2025-12-18 01:10:26.322400: Epoch time: 138.56 s\n",
      "2025-12-18 01:10:26.986014: \n",
      "2025-12-18 01:10:26.986014: Epoch 187\n",
      "2025-12-18 01:10:26.986014: Current learning rate: 0.0083\n",
      "2025-12-18 01:12:45.017153: train_loss -0.8262\n",
      "2025-12-18 01:12:45.017153: val_loss -0.8461\n",
      "2025-12-18 01:12:45.031650: Pseudo dice [0.9143, 0.9513, 0.9259]\n",
      "2025-12-18 01:12:45.033152: Epoch time: 138.03 s\n",
      "2025-12-18 01:12:45.681413: \n",
      "2025-12-18 01:12:45.681413: Epoch 188\n",
      "2025-12-18 01:12:45.681413: Current learning rate: 0.00829\n",
      "2025-12-18 01:15:03.867854: train_loss -0.8258\n",
      "2025-12-18 01:15:03.867854: val_loss -0.8417\n",
      "2025-12-18 01:15:03.873600: Pseudo dice [0.9142, 0.9463, 0.9196]\n",
      "2025-12-18 01:15:03.875602: Epoch time: 138.19 s\n",
      "2025-12-18 01:15:04.540978: \n",
      "2025-12-18 01:15:04.540978: Epoch 189\n",
      "2025-12-18 01:15:04.540978: Current learning rate: 0.00828\n",
      "2025-12-18 01:17:22.680119: train_loss -0.8322\n",
      "2025-12-18 01:17:22.680119: val_loss -0.8501\n",
      "2025-12-18 01:17:22.682122: Pseudo dice [0.9163, 0.9537, 0.9273]\n",
      "2025-12-18 01:17:22.682122: Epoch time: 138.14 s\n",
      "2025-12-18 01:17:23.387363: \n",
      "2025-12-18 01:17:23.387363: Epoch 190\n",
      "2025-12-18 01:17:23.387363: Current learning rate: 0.00827\n",
      "2025-12-18 01:19:41.645643: train_loss -0.8277\n",
      "2025-12-18 01:19:41.647645: val_loss -0.8527\n",
      "2025-12-18 01:19:41.649648: Pseudo dice [0.9235, 0.9537, 0.9175]\n",
      "2025-12-18 01:19:41.651650: Epoch time: 138.26 s\n",
      "2025-12-18 01:19:41.653390: Yayy! New best EMA pseudo Dice: 0.9277\n",
      "2025-12-18 01:19:42.535215: \n",
      "2025-12-18 01:19:42.535215: Epoch 191\n",
      "2025-12-18 01:19:42.549057: Current learning rate: 0.00826\n",
      "2025-12-18 01:22:00.666121: train_loss -0.8262\n",
      "2025-12-18 01:22:00.666121: val_loss -0.847\n",
      "2025-12-18 01:22:00.666121: Pseudo dice [0.9158, 0.9461, 0.919]\n",
      "2025-12-18 01:22:00.681772: Epoch time: 138.13 s\n",
      "2025-12-18 01:22:01.508416: \n",
      "2025-12-18 01:22:01.508416: Epoch 192\n",
      "2025-12-18 01:22:01.522283: Current learning rate: 0.00825\n",
      "2025-12-18 01:24:19.792330: train_loss -0.8248\n",
      "2025-12-18 01:24:19.794333: val_loss -0.8489\n",
      "2025-12-18 01:24:19.796336: Pseudo dice [0.9189, 0.9486, 0.9303]\n",
      "2025-12-18 01:24:19.800343: Epoch time: 138.28 s\n",
      "2025-12-18 01:24:19.802345: Yayy! New best EMA pseudo Dice: 0.9281\n",
      "2025-12-18 01:24:20.756201: \n",
      "2025-12-18 01:24:20.756201: Epoch 193\n",
      "2025-12-18 01:24:20.756201: Current learning rate: 0.00824\n",
      "2025-12-18 01:26:39.021779: train_loss -0.8291\n",
      "2025-12-18 01:26:39.021779: val_loss -0.8533\n",
      "2025-12-18 01:26:39.037714: Pseudo dice [0.9202, 0.9536, 0.9255]\n",
      "2025-12-18 01:26:39.040176: Epoch time: 138.27 s\n",
      "2025-12-18 01:26:39.040176: Yayy! New best EMA pseudo Dice: 0.9286\n",
      "2025-12-18 01:26:39.979590: \n",
      "2025-12-18 01:26:39.979590: Epoch 194\n",
      "2025-12-18 01:26:39.979590: Current learning rate: 0.00824\n",
      "2025-12-18 01:28:58.180995: train_loss -0.8287\n",
      "2025-12-18 01:28:58.180995: val_loss -0.8337\n",
      "2025-12-18 01:28:58.184999: Pseudo dice [0.9079, 0.9423, 0.9255]\n",
      "2025-12-18 01:28:58.187001: Epoch time: 138.2 s\n",
      "2025-12-18 01:28:58.880368: \n",
      "2025-12-18 01:28:58.880368: Epoch 195\n",
      "2025-12-18 01:28:58.882371: Current learning rate: 0.00823\n",
      "2025-12-18 01:31:17.106886: train_loss -0.8283\n",
      "2025-12-18 01:31:17.106886: val_loss -0.8412\n",
      "2025-12-18 01:31:17.111807: Pseudo dice [0.9083, 0.9474, 0.9342]\n",
      "2025-12-18 01:31:17.111807: Epoch time: 138.23 s\n",
      "2025-12-18 01:31:17.784384: \n",
      "2025-12-18 01:31:17.784384: Epoch 196\n",
      "2025-12-18 01:31:17.784384: Current learning rate: 0.00822\n",
      "2025-12-18 01:33:36.092687: train_loss -0.8337\n",
      "2025-12-18 01:33:36.092687: val_loss -0.8544\n",
      "2025-12-18 01:33:36.092687: Pseudo dice [0.9223, 0.9515, 0.9284]\n",
      "2025-12-18 01:33:36.105171: Epoch time: 138.31 s\n",
      "2025-12-18 01:33:36.107174: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2025-12-18 01:33:37.031168: \n",
      "2025-12-18 01:33:37.031168: Epoch 197\n",
      "2025-12-18 01:33:37.031168: Current learning rate: 0.00821\n",
      "2025-12-18 01:35:55.339782: train_loss -0.8278\n",
      "2025-12-18 01:35:55.339782: val_loss -0.8424\n",
      "2025-12-18 01:35:55.342074: Pseudo dice [0.9108, 0.945, 0.9227]\n",
      "2025-12-18 01:35:55.342074: Epoch time: 138.31 s\n",
      "2025-12-18 01:35:56.157884: \n",
      "2025-12-18 01:35:56.157884: Epoch 198\n",
      "2025-12-18 01:35:56.157884: Current learning rate: 0.0082\n",
      "2025-12-18 01:38:14.236281: train_loss -0.828\n",
      "2025-12-18 01:38:14.238101: val_loss -0.8542\n",
      "2025-12-18 01:38:14.240104: Pseudo dice [0.9213, 0.9513, 0.9286]\n",
      "2025-12-18 01:38:14.244107: Epoch time: 138.08 s\n",
      "2025-12-18 01:38:14.246110: Yayy! New best EMA pseudo Dice: 0.9292\n",
      "2025-12-18 01:38:15.178135: \n",
      "2025-12-18 01:38:15.178135: Epoch 199\n",
      "2025-12-18 01:38:15.178135: Current learning rate: 0.00819\n",
      "2025-12-18 01:40:33.357751: train_loss -0.8379\n",
      "2025-12-18 01:40:33.357751: val_loss -0.8513\n",
      "2025-12-18 01:40:33.359754: Pseudo dice [0.917, 0.9514, 0.929]\n",
      "2025-12-18 01:40:33.365311: Epoch time: 138.18 s\n",
      "2025-12-18 01:40:33.609794: Yayy! New best EMA pseudo Dice: 0.9295\n",
      "2025-12-18 01:40:34.544866: \n",
      "2025-12-18 01:40:34.544866: Epoch 200\n",
      "2025-12-18 01:40:34.544866: Current learning rate: 0.00818\n",
      "2025-12-18 01:42:52.860301: train_loss -0.8306\n",
      "2025-12-18 01:42:52.860301: val_loss -0.8513\n",
      "2025-12-18 01:42:52.866314: Pseudo dice [0.9176, 0.9475, 0.9278]\n",
      "2025-12-18 01:42:52.866314: Epoch time: 138.32 s\n",
      "2025-12-18 01:42:52.870142: Yayy! New best EMA pseudo Dice: 0.9297\n",
      "2025-12-18 01:42:53.796718: \n",
      "2025-12-18 01:42:53.796718: Epoch 201\n",
      "2025-12-18 01:42:53.796718: Current learning rate: 0.00817\n",
      "2025-12-18 01:45:12.136860: train_loss -0.8332\n",
      "2025-12-18 01:45:12.136860: val_loss -0.8423\n",
      "2025-12-18 01:45:12.141251: Pseudo dice [0.912, 0.9474, 0.9335]\n",
      "2025-12-18 01:45:12.141251: Epoch time: 138.34 s\n",
      "2025-12-18 01:45:12.146264: Yayy! New best EMA pseudo Dice: 0.9298\n",
      "2025-12-18 01:45:13.087829: \n",
      "2025-12-18 01:45:13.087829: Epoch 202\n",
      "2025-12-18 01:45:13.097553: Current learning rate: 0.00816\n",
      "2025-12-18 01:47:31.335215: train_loss -0.8294\n",
      "2025-12-18 01:47:31.335215: val_loss -0.845\n",
      "2025-12-18 01:47:31.335215: Pseudo dice [0.9137, 0.9468, 0.9314]\n",
      "2025-12-18 01:47:31.335215: Epoch time: 138.25 s\n",
      "2025-12-18 01:47:31.335215: Yayy! New best EMA pseudo Dice: 0.9299\n",
      "2025-12-18 01:47:32.610779: \n",
      "2025-12-18 01:47:32.610779: Epoch 203\n",
      "2025-12-18 01:47:32.612782: Current learning rate: 0.00815\n",
      "2025-12-18 01:49:50.685613: train_loss -0.8322\n",
      "2025-12-18 01:49:50.687615: val_loss -0.8496\n",
      "2025-12-18 01:49:50.689618: Pseudo dice [0.9187, 0.95, 0.9271]\n",
      "2025-12-18 01:49:50.691620: Epoch time: 138.08 s\n",
      "2025-12-18 01:49:50.691620: Yayy! New best EMA pseudo Dice: 0.9301\n",
      "2025-12-18 01:49:51.628066: \n",
      "2025-12-18 01:49:51.628066: Epoch 204\n",
      "2025-12-18 01:49:51.628066: Current learning rate: 0.00814\n",
      "2025-12-18 01:52:09.864851: train_loss -0.8302\n",
      "2025-12-18 01:52:09.866856: val_loss -0.85\n",
      "2025-12-18 01:52:09.870864: Pseudo dice [0.9134, 0.9473, 0.9286]\n",
      "2025-12-18 01:52:09.872605: Epoch time: 138.24 s\n",
      "2025-12-18 01:52:10.540146: \n",
      "2025-12-18 01:52:10.542149: Epoch 205\n",
      "2025-12-18 01:52:10.542149: Current learning rate: 0.00813\n",
      "2025-12-18 01:54:28.812549: train_loss -0.8306\n",
      "2025-12-18 01:54:28.812549: val_loss -0.8508\n",
      "2025-12-18 01:54:28.815754: Pseudo dice [0.9174, 0.9503, 0.9244]\n",
      "2025-12-18 01:54:28.817756: Epoch time: 138.27 s\n",
      "2025-12-18 01:54:28.819758: Yayy! New best EMA pseudo Dice: 0.9301\n",
      "2025-12-18 01:54:29.802261: \n",
      "2025-12-18 01:54:29.802261: Epoch 206\n",
      "2025-12-18 01:54:29.802261: Current learning rate: 0.00813\n",
      "2025-12-18 01:56:48.022148: train_loss -0.835\n",
      "2025-12-18 01:56:48.022148: val_loss -0.8499\n",
      "2025-12-18 01:56:48.024151: Pseudo dice [0.9132, 0.9466, 0.9352]\n",
      "2025-12-18 01:56:48.026153: Epoch time: 138.22 s\n",
      "2025-12-18 01:56:48.029655: Yayy! New best EMA pseudo Dice: 0.9303\n",
      "2025-12-18 01:56:48.946030: \n",
      "2025-12-18 01:56:48.948032: Epoch 207\n",
      "2025-12-18 01:56:48.949774: Current learning rate: 0.00812\n",
      "2025-12-18 01:59:07.076473: train_loss -0.8287\n",
      "2025-12-18 01:59:07.076473: val_loss -0.8573\n",
      "2025-12-18 01:59:07.076473: Pseudo dice [0.9192, 0.9508, 0.9352]\n",
      "2025-12-18 01:59:07.090416: Epoch time: 138.13 s\n",
      "2025-12-18 01:59:07.092155: Yayy! New best EMA pseudo Dice: 0.9308\n",
      "2025-12-18 01:59:07.986284: \n",
      "2025-12-18 01:59:07.988024: Epoch 208\n",
      "2025-12-18 01:59:07.988024: Current learning rate: 0.00811\n",
      "2025-12-18 02:01:25.988534: train_loss -0.8337\n",
      "2025-12-18 02:01:25.988534: val_loss -0.839\n",
      "2025-12-18 02:01:25.992540: Pseudo dice [0.9075, 0.9465, 0.9293]\n",
      "2025-12-18 02:01:25.994543: Epoch time: 138.0 s\n",
      "2025-12-18 02:01:26.963345: \n",
      "2025-12-18 02:01:26.963345: Epoch 209\n",
      "2025-12-18 02:01:26.970593: Current learning rate: 0.0081\n",
      "2025-12-18 02:03:45.400727: train_loss -0.8291\n",
      "2025-12-18 02:03:45.400727: val_loss -0.8448\n",
      "2025-12-18 02:03:45.413901: Pseudo dice [0.9118, 0.9534, 0.9251]\n",
      "2025-12-18 02:03:45.416419: Epoch time: 138.44 s\n",
      "2025-12-18 02:03:46.050780: \n",
      "2025-12-18 02:03:46.050780: Epoch 210\n",
      "2025-12-18 02:03:46.050780: Current learning rate: 0.00809\n",
      "2025-12-18 02:06:04.206289: train_loss -0.822\n",
      "2025-12-18 02:06:04.206289: val_loss -0.8466\n",
      "2025-12-18 02:06:04.210294: Pseudo dice [0.919, 0.949, 0.9143]\n",
      "2025-12-18 02:06:04.212297: Epoch time: 138.16 s\n",
      "2025-12-18 02:06:04.844678: \n",
      "2025-12-18 02:06:04.844678: Epoch 211\n",
      "2025-12-18 02:06:04.844678: Current learning rate: 0.00808\n",
      "2025-12-18 02:08:22.962981: train_loss -0.827\n",
      "2025-12-18 02:08:22.962981: val_loss -0.8501\n",
      "2025-12-18 02:08:22.975032: Pseudo dice [0.9204, 0.9497, 0.9171]\n",
      "2025-12-18 02:08:22.977036: Epoch time: 138.12 s\n",
      "2025-12-18 02:08:23.770472: \n",
      "2025-12-18 02:08:23.770472: Epoch 212\n",
      "2025-12-18 02:08:23.770472: Current learning rate: 0.00807\n",
      "2025-12-18 02:10:42.191570: train_loss -0.8299\n",
      "2025-12-18 02:10:42.193472: val_loss -0.8605\n",
      "2025-12-18 02:10:42.195474: Pseudo dice [0.9251, 0.9511, 0.9283]\n",
      "2025-12-18 02:10:42.197477: Epoch time: 138.42 s\n",
      "2025-12-18 02:10:42.843462: \n",
      "2025-12-18 02:10:42.843462: Epoch 213\n",
      "2025-12-18 02:10:42.843462: Current learning rate: 0.00806\n",
      "2025-12-18 02:13:01.169363: train_loss -0.8323\n",
      "2025-12-18 02:13:01.169363: val_loss -0.8576\n",
      "2025-12-18 02:13:01.173457: Pseudo dice [0.9212, 0.9582, 0.9247]\n",
      "2025-12-18 02:13:01.175460: Epoch time: 138.33 s\n",
      "2025-12-18 02:13:01.177315: Yayy! New best EMA pseudo Dice: 0.9309\n",
      "2025-12-18 02:13:02.042335: \n",
      "2025-12-18 02:13:02.042335: Epoch 214\n",
      "2025-12-18 02:13:02.042335: Current learning rate: 0.00805\n",
      "2025-12-18 02:15:20.154467: train_loss -0.8359\n",
      "2025-12-18 02:15:20.156207: val_loss -0.8464\n",
      "2025-12-18 02:15:20.160212: Pseudo dice [0.9146, 0.9462, 0.9271]\n",
      "2025-12-18 02:15:20.160212: Epoch time: 138.11 s\n",
      "2025-12-18 02:15:21.100969: \n",
      "2025-12-18 02:15:21.100969: Epoch 215\n",
      "2025-12-18 02:15:21.102710: Current learning rate: 0.00804\n",
      "2025-12-18 02:17:39.209533: train_loss -0.8343\n",
      "2025-12-18 02:17:39.209533: val_loss -0.8545\n",
      "2025-12-18 02:17:39.211535: Pseudo dice [0.9163, 0.95, 0.936]\n",
      "2025-12-18 02:17:39.211535: Epoch time: 138.11 s\n",
      "2025-12-18 02:17:39.217866: Yayy! New best EMA pseudo Dice: 0.9311\n",
      "2025-12-18 02:17:40.120801: \n",
      "2025-12-18 02:17:40.122542: Epoch 216\n",
      "2025-12-18 02:17:40.122542: Current learning rate: 0.00803\n",
      "2025-12-18 02:19:58.240687: train_loss -0.8321\n",
      "2025-12-18 02:19:58.242693: val_loss -0.8614\n",
      "2025-12-18 02:19:58.248706: Pseudo dice [0.9247, 0.9539, 0.9315]\n",
      "2025-12-18 02:19:58.250709: Epoch time: 138.12 s\n",
      "2025-12-18 02:19:58.252712: Yayy! New best EMA pseudo Dice: 0.9317\n",
      "2025-12-18 02:19:59.148846: \n",
      "2025-12-18 02:19:59.148846: Epoch 217\n",
      "2025-12-18 02:19:59.150588: Current learning rate: 0.00802\n",
      "2025-12-18 02:22:17.343737: train_loss -0.8322\n",
      "2025-12-18 02:22:17.343737: val_loss -0.8434\n",
      "2025-12-18 02:22:17.347480: Pseudo dice [0.9128, 0.9451, 0.9303]\n",
      "2025-12-18 02:22:17.347480: Epoch time: 138.2 s\n",
      "2025-12-18 02:22:18.073368: \n",
      "2025-12-18 02:22:18.073368: Epoch 218\n",
      "2025-12-18 02:22:18.073368: Current learning rate: 0.00801\n",
      "2025-12-18 02:24:36.331615: train_loss -0.8295\n",
      "2025-12-18 02:24:36.336169: val_loss -0.8512\n",
      "2025-12-18 02:24:36.338171: Pseudo dice [0.9252, 0.9516, 0.9171]\n",
      "2025-12-18 02:24:36.340173: Epoch time: 138.26 s\n",
      "2025-12-18 02:24:36.975518: \n",
      "2025-12-18 02:24:36.975518: Epoch 219\n",
      "2025-12-18 02:24:36.975518: Current learning rate: 0.00801\n",
      "2025-12-18 02:26:55.283211: train_loss -0.8258\n",
      "2025-12-18 02:26:55.283211: val_loss -0.8542\n",
      "2025-12-18 02:26:55.298995: Pseudo dice [0.9209, 0.9548, 0.9315]\n",
      "2025-12-18 02:26:55.298995: Epoch time: 138.31 s\n",
      "2025-12-18 02:26:55.298995: Yayy! New best EMA pseudo Dice: 0.9318\n",
      "2025-12-18 02:26:56.162812: \n",
      "2025-12-18 02:26:56.162812: Epoch 220\n",
      "2025-12-18 02:26:56.164554: Current learning rate: 0.008\n",
      "2025-12-18 02:29:14.296799: train_loss -0.8395\n",
      "2025-12-18 02:29:14.296799: val_loss -0.8554\n",
      "2025-12-18 02:29:14.300804: Pseudo dice [0.9201, 0.9496, 0.9287]\n",
      "2025-12-18 02:29:14.303808: Epoch time: 138.14 s\n",
      "2025-12-18 02:29:14.305811: Yayy! New best EMA pseudo Dice: 0.9319\n",
      "2025-12-18 02:29:15.440096: \n",
      "2025-12-18 02:29:15.440096: Epoch 221\n",
      "2025-12-18 02:29:15.440096: Current learning rate: 0.00799\n",
      "2025-12-18 02:31:33.624535: train_loss -0.8372\n",
      "2025-12-18 02:31:33.624535: val_loss -0.858\n",
      "2025-12-18 02:31:33.640523: Pseudo dice [0.9215, 0.9549, 0.9247]\n",
      "2025-12-18 02:31:33.642526: Epoch time: 138.2 s\n",
      "2025-12-18 02:31:33.644529: Yayy! New best EMA pseudo Dice: 0.9321\n",
      "2025-12-18 02:31:34.542986: \n",
      "2025-12-18 02:31:34.544727: Epoch 222\n",
      "2025-12-18 02:31:34.544727: Current learning rate: 0.00798\n",
      "2025-12-18 02:33:52.773261: train_loss -0.8223\n",
      "2025-12-18 02:33:52.773261: val_loss -0.8277\n",
      "2025-12-18 02:33:52.777184: Pseudo dice [0.8999, 0.9407, 0.9214]\n",
      "2025-12-18 02:33:52.779187: Epoch time: 138.23 s\n",
      "2025-12-18 02:33:53.422716: \n",
      "2025-12-18 02:33:53.422716: Epoch 223\n",
      "2025-12-18 02:33:53.422716: Current learning rate: 0.00797\n",
      "2025-12-18 02:36:11.614858: train_loss -0.8245\n",
      "2025-12-18 02:36:11.614858: val_loss -0.8375\n",
      "2025-12-18 02:36:11.614858: Pseudo dice [0.9126, 0.9458, 0.9118]\n",
      "2025-12-18 02:36:11.620356: Epoch time: 138.2 s\n",
      "2025-12-18 02:36:12.256657: \n",
      "2025-12-18 02:36:12.256657: Epoch 224\n",
      "2025-12-18 02:36:12.256657: Current learning rate: 0.00796\n",
      "2025-12-18 02:38:30.438572: train_loss -0.8309\n",
      "2025-12-18 02:38:30.438572: val_loss -0.8574\n",
      "2025-12-18 02:38:30.440574: Pseudo dice [0.9203, 0.9492, 0.9373]\n",
      "2025-12-18 02:38:30.442576: Epoch time: 138.18 s\n",
      "2025-12-18 02:38:31.090357: \n",
      "2025-12-18 02:38:31.090357: Epoch 225\n",
      "2025-12-18 02:38:31.094042: Current learning rate: 0.00795\n",
      "2025-12-18 02:40:49.380696: train_loss -0.8293\n",
      "2025-12-18 02:40:49.380696: val_loss -0.8617\n",
      "2025-12-18 02:40:49.383700: Pseudo dice [0.9264, 0.956, 0.923]\n",
      "2025-12-18 02:40:49.385555: Epoch time: 138.29 s\n",
      "2025-12-18 02:40:50.019321: \n",
      "2025-12-18 02:40:50.019321: Epoch 226\n",
      "2025-12-18 02:40:50.019321: Current learning rate: 0.00794\n",
      "2025-12-18 02:43:08.287088: train_loss -0.8346\n",
      "2025-12-18 02:43:08.287088: val_loss -0.8556\n",
      "2025-12-18 02:43:08.289620: Pseudo dice [0.9224, 0.9503, 0.9281]\n",
      "2025-12-18 02:43:08.289620: Epoch time: 138.27 s\n",
      "2025-12-18 02:43:08.919828: \n",
      "2025-12-18 02:43:08.919828: Epoch 227\n",
      "2025-12-18 02:43:08.919828: Current learning rate: 0.00793\n",
      "2025-12-18 02:45:27.140873: train_loss -0.8263\n",
      "2025-12-18 02:45:27.142876: val_loss -0.8373\n",
      "2025-12-18 02:45:27.142876: Pseudo dice [0.9077, 0.9463, 0.9262]\n",
      "2025-12-18 02:45:27.142876: Epoch time: 138.22 s\n",
      "2025-12-18 02:45:27.941510: \n",
      "2025-12-18 02:45:27.941510: Epoch 228\n",
      "2025-12-18 02:45:27.941510: Current learning rate: 0.00792\n",
      "2025-12-18 02:47:46.179110: train_loss -0.8239\n",
      "2025-12-18 02:47:46.179110: val_loss -0.8463\n",
      "2025-12-18 02:47:46.191572: Pseudo dice [0.9152, 0.9416, 0.9216]\n",
      "2025-12-18 02:47:46.193574: Epoch time: 138.24 s\n",
      "2025-12-18 02:47:46.816622: \n",
      "2025-12-18 02:47:46.816622: Epoch 229\n",
      "2025-12-18 02:47:46.816622: Current learning rate: 0.00791\n",
      "2025-12-18 02:50:05.065478: train_loss -0.8298\n",
      "2025-12-18 02:50:05.067481: val_loss -0.8527\n",
      "2025-12-18 02:50:05.071300: Pseudo dice [0.9219, 0.9531, 0.9247]\n",
      "2025-12-18 02:50:05.073251: Epoch time: 138.25 s\n",
      "2025-12-18 02:50:05.695813: \n",
      "2025-12-18 02:50:05.695813: Epoch 230\n",
      "2025-12-18 02:50:05.711534: Current learning rate: 0.0079\n",
      "2025-12-18 02:52:23.837546: train_loss -0.8241\n",
      "2025-12-18 02:52:23.837546: val_loss -0.858\n",
      "2025-12-18 02:52:23.838547: Pseudo dice [0.9232, 0.9518, 0.9304]\n",
      "2025-12-18 02:52:23.838547: Epoch time: 138.14 s\n",
      "2025-12-18 02:52:24.457712: \n",
      "2025-12-18 02:52:24.457712: Epoch 231\n",
      "2025-12-18 02:52:24.457712: Current learning rate: 0.00789\n",
      "2025-12-18 02:54:42.634085: train_loss -0.832\n",
      "2025-12-18 02:54:42.634085: val_loss -0.8458\n",
      "2025-12-18 02:54:42.637803: Pseudo dice [0.9141, 0.943, 0.9343]\n",
      "2025-12-18 02:54:42.637803: Epoch time: 138.18 s\n",
      "2025-12-18 02:54:43.264429: \n",
      "2025-12-18 02:54:43.264429: Epoch 232\n",
      "2025-12-18 02:54:43.264429: Current learning rate: 0.00789\n",
      "2025-12-18 02:57:01.481192: train_loss -0.8283\n",
      "2025-12-18 02:57:01.483194: val_loss -0.8513\n",
      "2025-12-18 02:57:01.486938: Pseudo dice [0.9199, 0.9521, 0.9301]\n",
      "2025-12-18 02:57:01.488940: Epoch time: 138.22 s\n",
      "2025-12-18 02:57:02.162681: \n",
      "2025-12-18 02:57:02.162681: Epoch 233\n",
      "2025-12-18 02:57:02.162681: Current learning rate: 0.00788\n",
      "2025-12-18 02:59:20.370405: train_loss -0.8327\n",
      "2025-12-18 02:59:20.372408: val_loss -0.8486\n",
      "2025-12-18 02:59:20.374411: Pseudo dice [0.9133, 0.9504, 0.9297]\n",
      "2025-12-18 02:59:20.376413: Epoch time: 138.21 s\n",
      "2025-12-18 02:59:21.173671: \n",
      "2025-12-18 02:59:21.173671: Epoch 234\n",
      "2025-12-18 02:59:21.173671: Current learning rate: 0.00787\n",
      "2025-12-18 03:01:39.133242: train_loss -0.8356\n",
      "2025-12-18 03:01:39.133242: val_loss -0.844\n",
      "2025-12-18 03:01:39.135245: Pseudo dice [0.9148, 0.9442, 0.9217]\n",
      "2025-12-18 03:01:39.137248: Epoch time: 137.96 s\n",
      "2025-12-18 03:01:39.768304: \n",
      "2025-12-18 03:01:39.768304: Epoch 235\n",
      "2025-12-18 03:01:39.768304: Current learning rate: 0.00786\n",
      "2025-12-18 03:03:57.977259: train_loss -0.8356\n",
      "2025-12-18 03:03:57.977259: val_loss -0.8472\n",
      "2025-12-18 03:03:57.980336: Pseudo dice [0.9151, 0.9478, 0.9255]\n",
      "2025-12-18 03:03:57.980336: Epoch time: 138.21 s\n",
      "2025-12-18 03:03:58.604481: \n",
      "2025-12-18 03:03:58.604481: Epoch 236\n",
      "2025-12-18 03:03:58.606222: Current learning rate: 0.00785\n",
      "2025-12-18 03:06:16.786683: train_loss -0.8281\n",
      "2025-12-18 03:06:16.786683: val_loss -0.8463\n",
      "2025-12-18 03:06:16.797055: Pseudo dice [0.9134, 0.9457, 0.9283]\n",
      "2025-12-18 03:06:16.797055: Epoch time: 138.18 s\n",
      "2025-12-18 03:06:17.419768: \n",
      "2025-12-18 03:06:17.419768: Epoch 237\n",
      "2025-12-18 03:06:17.435747: Current learning rate: 0.00784\n",
      "2025-12-18 03:08:35.734929: train_loss -0.8306\n",
      "2025-12-18 03:08:35.734929: val_loss -0.8519\n",
      "2025-12-18 03:08:35.750737: Pseudo dice [0.9188, 0.9527, 0.9249]\n",
      "2025-12-18 03:08:35.750737: Epoch time: 138.32 s\n",
      "2025-12-18 03:08:36.365867: \n",
      "2025-12-18 03:08:36.365867: Epoch 238\n",
      "2025-12-18 03:08:36.365867: Current learning rate: 0.00783\n",
      "2025-12-18 03:10:54.940591: train_loss -0.8254\n",
      "2025-12-18 03:10:54.942594: val_loss -0.8548\n",
      "2025-12-18 03:10:54.944539: Pseudo dice [0.9174, 0.9484, 0.9385]\n",
      "2025-12-18 03:10:54.948138: Epoch time: 138.57 s\n",
      "2025-12-18 03:10:55.570498: \n",
      "2025-12-18 03:10:55.570498: Epoch 239\n",
      "2025-12-18 03:10:55.570498: Current learning rate: 0.00782\n",
      "2025-12-18 03:13:13.706127: train_loss -0.8294\n",
      "2025-12-18 03:13:13.706127: val_loss -0.8381\n",
      "2025-12-18 03:13:13.722194: Pseudo dice [0.9134, 0.9453, 0.9168]\n",
      "2025-12-18 03:13:13.722194: Epoch time: 138.14 s\n",
      "2025-12-18 03:13:14.513741: \n",
      "2025-12-18 03:13:14.513741: Epoch 240\n",
      "2025-12-18 03:13:14.513741: Current learning rate: 0.00781\n",
      "2025-12-18 03:15:32.758488: train_loss -0.82\n",
      "2025-12-18 03:15:32.758488: val_loss -0.8169\n",
      "2025-12-18 03:15:32.762445: Pseudo dice [0.8998, 0.9449, 0.9126]\n",
      "2025-12-18 03:15:32.764447: Epoch time: 138.24 s\n",
      "2025-12-18 03:15:33.399741: \n",
      "2025-12-18 03:15:33.401744: Epoch 241\n",
      "2025-12-18 03:15:33.401744: Current learning rate: 0.0078\n",
      "2025-12-18 03:17:51.628690: train_loss -0.8047\n",
      "2025-12-18 03:17:51.628690: val_loss -0.8499\n",
      "2025-12-18 03:17:51.628690: Pseudo dice [0.9224, 0.9536, 0.9247]\n",
      "2025-12-18 03:17:51.644549: Epoch time: 138.23 s\n",
      "2025-12-18 03:17:52.281847: \n",
      "2025-12-18 03:17:52.281847: Epoch 242\n",
      "2025-12-18 03:17:52.281847: Current learning rate: 0.00779\n",
      "2025-12-18 03:20:10.203177: train_loss -0.8258\n",
      "2025-12-18 03:20:10.203177: val_loss -0.8377\n",
      "2025-12-18 03:20:10.205179: Pseudo dice [0.9164, 0.9481, 0.9111]\n",
      "2025-12-18 03:20:10.209183: Epoch time: 137.92 s\n",
      "2025-12-18 03:20:10.844949: \n",
      "2025-12-18 03:20:10.844949: Epoch 243\n",
      "2025-12-18 03:20:10.844949: Current learning rate: 0.00778\n",
      "2025-12-18 03:22:29.169134: train_loss -0.8281\n",
      "2025-12-18 03:22:29.169134: val_loss -0.8512\n",
      "2025-12-18 03:22:29.172876: Pseudo dice [0.9201, 0.9528, 0.9237]\n",
      "2025-12-18 03:22:29.172876: Epoch time: 138.33 s\n",
      "2025-12-18 03:22:29.820083: \n",
      "2025-12-18 03:22:29.820083: Epoch 244\n",
      "2025-12-18 03:22:29.820083: Current learning rate: 0.00777\n",
      "2025-12-18 03:24:47.996979: train_loss -0.8342\n",
      "2025-12-18 03:24:47.996979: val_loss -0.8555\n",
      "2025-12-18 03:24:48.010334: Pseudo dice [0.9174, 0.9504, 0.9353]\n",
      "2025-12-18 03:24:48.012853: Epoch time: 138.18 s\n",
      "2025-12-18 03:24:48.647569: \n",
      "2025-12-18 03:24:48.647569: Epoch 245\n",
      "2025-12-18 03:24:48.647569: Current learning rate: 0.00777\n",
      "2025-12-18 03:27:06.781179: train_loss -0.8296\n",
      "2025-12-18 03:27:06.783182: val_loss -0.8376\n",
      "2025-12-18 03:27:06.789876: Pseudo dice [0.917, 0.9512, 0.9118]\n",
      "2025-12-18 03:27:06.791879: Epoch time: 138.13 s\n",
      "2025-12-18 03:27:07.410798: \n",
      "2025-12-18 03:27:07.410798: Epoch 246\n",
      "2025-12-18 03:27:07.428512: Current learning rate: 0.00776\n",
      "2025-12-18 03:29:25.630988: train_loss -0.8164\n",
      "2025-12-18 03:29:25.630988: val_loss -0.8414\n",
      "2025-12-18 03:29:25.644752: Pseudo dice [0.9149, 0.9498, 0.9183]\n",
      "2025-12-18 03:29:25.646756: Epoch time: 138.22 s\n",
      "2025-12-18 03:29:26.438773: \n",
      "2025-12-18 03:29:26.438773: Epoch 247\n",
      "2025-12-18 03:29:26.451975: Current learning rate: 0.00775\n",
      "2025-12-18 03:31:44.478928: train_loss -0.8232\n",
      "2025-12-18 03:31:44.478928: val_loss -0.8431\n",
      "2025-12-18 03:31:44.494690: Pseudo dice [0.9162, 0.9448, 0.9089]\n",
      "2025-12-18 03:31:44.497643: Epoch time: 138.04 s\n",
      "2025-12-18 03:31:45.112055: \n",
      "2025-12-18 03:31:45.112055: Epoch 248\n",
      "2025-12-18 03:31:45.112055: Current learning rate: 0.00774\n",
      "2025-12-18 03:34:03.303936: train_loss -0.8213\n",
      "2025-12-18 03:34:03.303936: val_loss -0.8352\n",
      "2025-12-18 03:34:03.321898: Pseudo dice [0.9132, 0.9484, 0.9105]\n",
      "2025-12-18 03:34:03.321898: Epoch time: 138.19 s\n",
      "2025-12-18 03:34:03.952578: \n",
      "2025-12-18 03:34:03.952578: Epoch 249\n",
      "2025-12-18 03:34:03.967106: Current learning rate: 0.00773\n",
      "2025-12-18 03:36:22.106467: train_loss -0.8286\n",
      "2025-12-18 03:36:22.108469: val_loss -0.8525\n",
      "2025-12-18 03:36:22.110653: Pseudo dice [0.9154, 0.9494, 0.9341]\n",
      "2025-12-18 03:36:22.114659: Epoch time: 138.15 s\n",
      "2025-12-18 03:36:23.005071: \n",
      "2025-12-18 03:36:23.005071: Epoch 250\n",
      "2025-12-18 03:36:23.005071: Current learning rate: 0.00772\n",
      "2025-12-18 03:38:40.977467: train_loss -0.834\n",
      "2025-12-18 03:38:40.978469: val_loss -0.8426\n",
      "2025-12-18 03:38:40.982475: Pseudo dice [0.909, 0.947, 0.9352]\n",
      "2025-12-18 03:38:40.984477: Epoch time: 137.97 s\n",
      "2025-12-18 03:38:41.643461: \n",
      "2025-12-18 03:38:41.643461: Epoch 251\n",
      "2025-12-18 03:38:41.643461: Current learning rate: 0.00771\n",
      "2025-12-18 03:40:59.745132: train_loss -0.835\n",
      "2025-12-18 03:40:59.745132: val_loss -0.8614\n",
      "2025-12-18 03:40:59.745132: Pseudo dice [0.9212, 0.9533, 0.9336]\n",
      "2025-12-18 03:40:59.745132: Epoch time: 138.1 s\n",
      "2025-12-18 03:41:00.364020: \n",
      "2025-12-18 03:41:00.364020: Epoch 252\n",
      "2025-12-18 03:41:00.380049: Current learning rate: 0.0077\n",
      "2025-12-18 03:43:18.458658: train_loss -0.8355\n",
      "2025-12-18 03:43:18.458658: val_loss -0.8516\n",
      "2025-12-18 03:43:18.460660: Pseudo dice [0.9161, 0.9499, 0.9239]\n",
      "2025-12-18 03:43:18.460660: Epoch time: 138.09 s\n",
      "2025-12-18 03:43:19.254565: \n",
      "2025-12-18 03:43:19.254565: Epoch 253\n",
      "2025-12-18 03:43:19.254565: Current learning rate: 0.00769\n",
      "2025-12-18 03:45:37.533048: train_loss -0.8342\n",
      "2025-12-18 03:45:37.533048: val_loss -0.8447\n",
      "2025-12-18 03:45:37.537219: Pseudo dice [0.9178, 0.9544, 0.9134]\n",
      "2025-12-18 03:45:37.537219: Epoch time: 138.28 s\n",
      "2025-12-18 03:45:38.276940: \n",
      "2025-12-18 03:45:38.276940: Epoch 254\n",
      "2025-12-18 03:45:38.276940: Current learning rate: 0.00768\n",
      "2025-12-18 03:47:56.373682: train_loss -0.8347\n",
      "2025-12-18 03:47:56.373682: val_loss -0.8478\n",
      "2025-12-18 03:47:56.373682: Pseudo dice [0.9154, 0.9483, 0.9257]\n",
      "2025-12-18 03:47:56.389320: Epoch time: 138.1 s\n",
      "2025-12-18 03:47:57.024531: \n",
      "2025-12-18 03:47:57.024531: Epoch 255\n",
      "2025-12-18 03:47:57.026835: Current learning rate: 0.00767\n",
      "2025-12-18 03:50:15.282107: train_loss -0.8281\n",
      "2025-12-18 03:50:15.282107: val_loss -0.8419\n",
      "2025-12-18 03:50:15.286114: Pseudo dice [0.9113, 0.9496, 0.9273]\n",
      "2025-12-18 03:50:15.289315: Epoch time: 138.27 s\n",
      "2025-12-18 03:50:15.923887: \n",
      "2025-12-18 03:50:15.923887: Epoch 256\n",
      "2025-12-18 03:50:15.923887: Current learning rate: 0.00766\n",
      "2025-12-18 03:52:34.139557: train_loss -0.8271\n",
      "2025-12-18 03:52:34.139557: val_loss -0.8554\n",
      "2025-12-18 03:52:34.143564: Pseudo dice [0.9216, 0.9533, 0.926]\n",
      "2025-12-18 03:52:34.147308: Epoch time: 138.22 s\n",
      "2025-12-18 03:52:34.936422: \n",
      "2025-12-18 03:52:34.936422: Epoch 257\n",
      "2025-12-18 03:52:34.936422: Current learning rate: 0.00765\n",
      "2025-12-18 03:54:53.242930: train_loss -0.8198\n",
      "2025-12-18 03:54:53.244933: val_loss -0.846\n",
      "2025-12-18 03:54:53.246934: Pseudo dice [0.9185, 0.9512, 0.9125]\n",
      "2025-12-18 03:54:53.249942: Epoch time: 138.31 s\n",
      "2025-12-18 03:54:53.883170: \n",
      "2025-12-18 03:54:53.883170: Epoch 258\n",
      "2025-12-18 03:54:53.883170: Current learning rate: 0.00764\n",
      "2025-12-18 03:57:11.966203: train_loss -0.8143\n",
      "2025-12-18 03:57:11.966203: val_loss -0.8575\n",
      "2025-12-18 03:57:11.966203: Pseudo dice [0.9273, 0.9571, 0.9257]\n",
      "2025-12-18 03:57:11.966203: Epoch time: 138.08 s\n",
      "2025-12-18 03:57:12.615515: \n",
      "2025-12-18 03:57:12.615515: Epoch 259\n",
      "2025-12-18 03:57:12.615515: Current learning rate: 0.00764\n",
      "2025-12-18 03:59:30.737421: train_loss -0.8325\n",
      "2025-12-18 03:59:30.737421: val_loss -0.8587\n",
      "2025-12-18 03:59:30.744153: Pseudo dice [0.924, 0.955, 0.9233]\n",
      "2025-12-18 03:59:30.744153: Epoch time: 138.12 s\n",
      "2025-12-18 03:59:31.716585: \n",
      "2025-12-18 03:59:31.716585: Epoch 260\n",
      "2025-12-18 03:59:31.716585: Current learning rate: 0.00763\n",
      "2025-12-18 04:01:49.858484: train_loss -0.8323\n",
      "2025-12-18 04:01:49.858484: val_loss -0.8459\n",
      "2025-12-18 04:01:49.858484: Pseudo dice [0.918, 0.9462, 0.9243]\n",
      "2025-12-18 04:01:49.858484: Epoch time: 138.14 s\n",
      "2025-12-18 04:01:50.492549: \n",
      "2025-12-18 04:01:50.492549: Epoch 261\n",
      "2025-12-18 04:01:50.506171: Current learning rate: 0.00762\n",
      "2025-12-18 04:04:08.729200: train_loss -0.8266\n",
      "2025-12-18 04:04:08.731202: val_loss -0.8467\n",
      "2025-12-18 04:04:08.731202: Pseudo dice [0.9167, 0.9494, 0.9251]\n",
      "2025-12-18 04:04:08.731202: Epoch time: 138.24 s\n",
      "2025-12-18 04:04:09.365771: \n",
      "2025-12-18 04:04:09.365771: Epoch 262\n",
      "2025-12-18 04:04:09.381752: Current learning rate: 0.00761\n",
      "2025-12-18 04:06:27.560616: train_loss -0.8325\n",
      "2025-12-18 04:06:27.560616: val_loss -0.8553\n",
      "2025-12-18 04:06:27.564624: Pseudo dice [0.9239, 0.9545, 0.9181]\n",
      "2025-12-18 04:06:27.568629: Epoch time: 138.19 s\n",
      "2025-12-18 04:06:28.316255: \n",
      "2025-12-18 04:06:28.316255: Epoch 263\n",
      "2025-12-18 04:06:28.316255: Current learning rate: 0.0076\n",
      "2025-12-18 04:08:46.711056: train_loss -0.8348\n",
      "2025-12-18 04:08:46.711056: val_loss -0.8487\n",
      "2025-12-18 04:08:46.715060: Pseudo dice [0.9092, 0.9487, 0.9353]\n",
      "2025-12-18 04:08:46.717062: Epoch time: 138.39 s\n",
      "2025-12-18 04:08:47.355702: \n",
      "2025-12-18 04:08:47.355702: Epoch 264\n",
      "2025-12-18 04:08:47.361829: Current learning rate: 0.00759\n",
      "2025-12-18 04:11:05.835442: train_loss -0.8379\n",
      "2025-12-18 04:11:05.835442: val_loss -0.8616\n",
      "2025-12-18 04:11:05.838593: Pseudo dice [0.9246, 0.9536, 0.9331]\n",
      "2025-12-18 04:11:05.838593: Epoch time: 138.48 s\n",
      "2025-12-18 04:11:06.470406: \n",
      "2025-12-18 04:11:06.470406: Epoch 265\n",
      "2025-12-18 04:11:06.470406: Current learning rate: 0.00758\n",
      "2025-12-18 04:13:24.665246: train_loss -0.8372\n",
      "2025-12-18 04:13:24.666987: val_loss -0.848\n",
      "2025-12-18 04:13:24.668989: Pseudo dice [0.9174, 0.9477, 0.9168]\n",
      "2025-12-18 04:13:24.672995: Epoch time: 138.19 s\n",
      "2025-12-18 04:13:25.601285: \n",
      "2025-12-18 04:13:25.601285: Epoch 266\n",
      "2025-12-18 04:13:25.601285: Current learning rate: 0.00757\n",
      "2025-12-18 04:15:43.809216: train_loss -0.8401\n",
      "2025-12-18 04:15:43.809216: val_loss -0.8535\n",
      "2025-12-18 04:15:43.811218: Pseudo dice [0.9182, 0.9497, 0.9279]\n",
      "2025-12-18 04:15:43.816236: Epoch time: 138.21 s\n",
      "2025-12-18 04:15:44.434994: \n",
      "2025-12-18 04:15:44.434994: Epoch 267\n",
      "2025-12-18 04:15:44.434994: Current learning rate: 0.00756\n",
      "2025-12-18 04:18:02.621475: train_loss -0.8403\n",
      "2025-12-18 04:18:02.621475: val_loss -0.8554\n",
      "2025-12-18 04:18:02.621475: Pseudo dice [0.9125, 0.951, 0.937]\n",
      "2025-12-18 04:18:02.637343: Epoch time: 138.19 s\n",
      "2025-12-18 04:18:03.271564: \n",
      "2025-12-18 04:18:03.271564: Epoch 268\n",
      "2025-12-18 04:18:03.285336: Current learning rate: 0.00755\n",
      "2025-12-18 04:20:21.656622: train_loss -0.8332\n",
      "2025-12-18 04:20:21.656622: val_loss -0.8545\n",
      "2025-12-18 04:20:21.663615: Pseudo dice [0.9178, 0.9553, 0.9285]\n",
      "2025-12-18 04:20:21.663615: Epoch time: 138.39 s\n",
      "2025-12-18 04:20:22.432390: \n",
      "2025-12-18 04:20:22.432390: Epoch 269\n",
      "2025-12-18 04:20:22.432390: Current learning rate: 0.00754\n",
      "2025-12-18 04:22:40.667888: train_loss -0.8349\n",
      "2025-12-18 04:22:40.667888: val_loss -0.8525\n",
      "2025-12-18 04:22:40.667888: Pseudo dice [0.9158, 0.9499, 0.9317]\n",
      "2025-12-18 04:22:40.667888: Epoch time: 138.24 s\n",
      "2025-12-18 04:22:41.302767: \n",
      "2025-12-18 04:22:41.302767: Epoch 270\n",
      "2025-12-18 04:22:41.302767: Current learning rate: 0.00753\n",
      "2025-12-18 04:24:59.567640: train_loss -0.8375\n",
      "2025-12-18 04:24:59.567640: val_loss -0.8608\n",
      "2025-12-18 04:24:59.567640: Pseudo dice [0.9229, 0.9549, 0.9333]\n",
      "2025-12-18 04:24:59.567640: Epoch time: 138.26 s\n",
      "2025-12-18 04:24:59.567640: Yayy! New best EMA pseudo Dice: 0.9322\n",
      "2025-12-18 04:25:00.448307: \n",
      "2025-12-18 04:25:00.448307: Epoch 271\n",
      "2025-12-18 04:25:00.448307: Current learning rate: 0.00752\n",
      "2025-12-18 04:27:18.597434: train_loss -0.8289\n",
      "2025-12-18 04:27:18.599438: val_loss -0.8542\n",
      "2025-12-18 04:27:18.603442: Pseudo dice [0.9232, 0.9536, 0.9186]\n",
      "2025-12-18 04:27:18.609452: Epoch time: 138.15 s\n",
      "2025-12-18 04:27:19.529603: \n",
      "2025-12-18 04:27:19.529603: Epoch 272\n",
      "2025-12-18 04:27:19.532482: Current learning rate: 0.00751\n",
      "2025-12-18 04:29:37.748800: train_loss -0.8355\n",
      "2025-12-18 04:29:37.748800: val_loss -0.8579\n",
      "2025-12-18 04:29:37.764523: Pseudo dice [0.9247, 0.9536, 0.9299]\n",
      "2025-12-18 04:29:37.764523: Epoch time: 138.22 s\n",
      "2025-12-18 04:29:37.764523: Yayy! New best EMA pseudo Dice: 0.9326\n",
      "2025-12-18 04:29:38.653132: \n",
      "2025-12-18 04:29:38.653132: Epoch 273\n",
      "2025-12-18 04:29:38.655134: Current learning rate: 0.00751\n",
      "2025-12-18 04:31:57.019149: train_loss -0.8342\n",
      "2025-12-18 04:31:57.021152: val_loss -0.8622\n",
      "2025-12-18 04:31:57.026908: Pseudo dice [0.9233, 0.9546, 0.9318]\n",
      "2025-12-18 04:31:57.030830: Epoch time: 138.37 s\n",
      "2025-12-18 04:31:57.036837: Yayy! New best EMA pseudo Dice: 0.933\n",
      "2025-12-18 04:31:57.953636: \n",
      "2025-12-18 04:31:57.953636: Epoch 274\n",
      "2025-12-18 04:31:57.955756: Current learning rate: 0.0075\n",
      "2025-12-18 04:34:16.219425: train_loss -0.8303\n",
      "2025-12-18 04:34:16.219425: val_loss -0.8469\n",
      "2025-12-18 04:34:16.223429: Pseudo dice [0.9134, 0.9488, 0.9331]\n",
      "2025-12-18 04:34:16.225430: Epoch time: 138.27 s\n",
      "2025-12-18 04:34:16.859320: \n",
      "2025-12-18 04:34:16.859320: Epoch 275\n",
      "2025-12-18 04:34:16.859320: Current learning rate: 0.00749\n",
      "2025-12-18 04:36:35.212887: train_loss -0.8321\n",
      "2025-12-18 04:36:35.212887: val_loss -0.8529\n",
      "2025-12-18 04:36:35.212887: Pseudo dice [0.9177, 0.9464, 0.924]\n",
      "2025-12-18 04:36:35.212887: Epoch time: 138.35 s\n",
      "2025-12-18 04:36:35.848289: \n",
      "2025-12-18 04:36:35.848289: Epoch 276\n",
      "2025-12-18 04:36:35.848289: Current learning rate: 0.00748\n",
      "2025-12-18 04:38:53.830814: train_loss -0.8388\n",
      "2025-12-18 04:38:53.830814: val_loss -0.8645\n",
      "2025-12-18 04:38:53.846494: Pseudo dice [0.9232, 0.9537, 0.9331]\n",
      "2025-12-18 04:38:53.846494: Epoch time: 137.98 s\n",
      "2025-12-18 04:38:54.463698: \n",
      "2025-12-18 04:38:54.463698: Epoch 277\n",
      "2025-12-18 04:38:54.463698: Current learning rate: 0.00747\n",
      "2025-12-18 04:41:12.832875: train_loss -0.8353\n",
      "2025-12-18 04:41:12.832875: val_loss -0.8673\n",
      "2025-12-18 04:41:12.832875: Pseudo dice [0.9237, 0.9514, 0.9462]\n",
      "2025-12-18 04:41:12.832875: Epoch time: 138.37 s\n",
      "2025-12-18 04:41:12.832875: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-18 04:41:13.885533: \n",
      "2025-12-18 04:41:13.885533: Epoch 278\n",
      "2025-12-18 04:41:13.885533: Current learning rate: 0.00746\n",
      "2025-12-18 04:43:32.241750: train_loss -0.8398\n",
      "2025-12-18 04:43:32.241750: val_loss -0.8577\n",
      "2025-12-18 04:43:32.243753: Pseudo dice [0.9197, 0.9483, 0.9339]\n",
      "2025-12-18 04:43:32.245756: Epoch time: 138.36 s\n",
      "2025-12-18 04:43:32.249723: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-18 04:43:33.164614: \n",
      "2025-12-18 04:43:33.166616: Epoch 279\n",
      "2025-12-18 04:43:33.168849: Current learning rate: 0.00745\n",
      "2025-12-18 04:45:51.287563: train_loss -0.8409\n",
      "2025-12-18 04:45:51.287563: val_loss -0.8605\n",
      "2025-12-18 04:45:51.303579: Pseudo dice [0.921, 0.9561, 0.924]\n",
      "2025-12-18 04:45:51.307152: Epoch time: 138.12 s\n",
      "2025-12-18 04:45:51.309154: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-18 04:45:52.228904: \n",
      "2025-12-18 04:45:52.230906: Epoch 280\n",
      "2025-12-18 04:45:52.230906: Current learning rate: 0.00744\n",
      "2025-12-18 04:48:10.367281: train_loss -0.8392\n",
      "2025-12-18 04:48:10.367281: val_loss -0.8589\n",
      "2025-12-18 04:48:10.370308: Pseudo dice [0.9202, 0.9538, 0.9317]\n",
      "2025-12-18 04:48:10.374313: Epoch time: 138.14 s\n",
      "2025-12-18 04:48:10.376316: Yayy! New best EMA pseudo Dice: 0.9339\n",
      "2025-12-18 04:48:11.287009: \n",
      "2025-12-18 04:48:11.287009: Epoch 281\n",
      "2025-12-18 04:48:11.287009: Current learning rate: 0.00743\n",
      "2025-12-18 04:50:29.301627: train_loss -0.8372\n",
      "2025-12-18 04:50:29.301627: val_loss -0.8597\n",
      "2025-12-18 04:50:29.301627: Pseudo dice [0.9222, 0.954, 0.9329]\n",
      "2025-12-18 04:50:29.317315: Epoch time: 138.01 s\n",
      "2025-12-18 04:50:29.317315: Yayy! New best EMA pseudo Dice: 0.9341\n",
      "2025-12-18 04:50:30.221248: \n",
      "2025-12-18 04:50:30.221248: Epoch 282\n",
      "2025-12-18 04:50:30.221248: Current learning rate: 0.00742\n",
      "2025-12-18 04:52:48.241205: train_loss -0.8372\n",
      "2025-12-18 04:52:48.241205: val_loss -0.8582\n",
      "2025-12-18 04:52:48.245212: Pseudo dice [0.9209, 0.9533, 0.9334]\n",
      "2025-12-18 04:52:48.249216: Epoch time: 138.02 s\n",
      "2025-12-18 04:52:48.251218: Yayy! New best EMA pseudo Dice: 0.9343\n",
      "2025-12-18 04:52:49.158318: \n",
      "2025-12-18 04:52:49.158318: Epoch 283\n",
      "2025-12-18 04:52:49.158318: Current learning rate: 0.00741\n",
      "2025-12-18 04:55:07.314352: train_loss -0.8381\n",
      "2025-12-18 04:55:07.316354: val_loss -0.8575\n",
      "2025-12-18 04:55:07.318356: Pseudo dice [0.922, 0.9521, 0.9292]\n",
      "2025-12-18 04:55:07.322171: Epoch time: 138.16 s\n",
      "2025-12-18 04:55:07.324174: Yayy! New best EMA pseudo Dice: 0.9343\n",
      "2025-12-18 04:55:08.424545: \n",
      "2025-12-18 04:55:08.424545: Epoch 284\n",
      "2025-12-18 04:55:08.424545: Current learning rate: 0.0074\n",
      "2025-12-18 04:57:26.442607: train_loss -0.8373\n",
      "2025-12-18 04:57:26.444610: val_loss -0.8577\n",
      "2025-12-18 04:57:26.444610: Pseudo dice [0.9145, 0.9492, 0.9415]\n",
      "2025-12-18 04:57:26.444610: Epoch time: 138.02 s\n",
      "2025-12-18 04:57:26.450885: Yayy! New best EMA pseudo Dice: 0.9344\n",
      "2025-12-18 04:57:27.345732: \n",
      "2025-12-18 04:57:27.345732: Epoch 285\n",
      "2025-12-18 04:57:27.348505: Current learning rate: 0.00739\n",
      "2025-12-18 04:59:45.537323: train_loss -0.8385\n",
      "2025-12-18 04:59:45.537323: val_loss -0.8657\n",
      "2025-12-18 04:59:45.555169: Pseudo dice [0.9196, 0.9575, 0.9405]\n",
      "2025-12-18 04:59:45.559173: Epoch time: 138.19 s\n",
      "2025-12-18 04:59:45.561175: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2025-12-18 04:59:46.464338: \n",
      "2025-12-18 04:59:46.464338: Epoch 286\n",
      "2025-12-18 04:59:46.464338: Current learning rate: 0.00738\n",
      "2025-12-18 05:02:04.791461: train_loss -0.8352\n",
      "2025-12-18 05:02:04.793463: val_loss -0.8684\n",
      "2025-12-18 05:02:04.797468: Pseudo dice [0.9282, 0.9557, 0.9358]\n",
      "2025-12-18 05:02:04.799916: Epoch time: 138.33 s\n",
      "2025-12-18 05:02:04.801919: Yayy! New best EMA pseudo Dice: 0.9354\n",
      "2025-12-18 05:02:05.712045: \n",
      "2025-12-18 05:02:05.712045: Epoch 287\n",
      "2025-12-18 05:02:05.712045: Current learning rate: 0.00738\n",
      "2025-12-18 05:04:23.816587: train_loss -0.8366\n",
      "2025-12-18 05:04:23.816587: val_loss -0.8568\n",
      "2025-12-18 05:04:23.816587: Pseudo dice [0.92, 0.9533, 0.9247]\n",
      "2025-12-18 05:04:23.816587: Epoch time: 138.1 s\n",
      "2025-12-18 05:04:24.575810: \n",
      "2025-12-18 05:04:24.575810: Epoch 288\n",
      "2025-12-18 05:04:24.575810: Current learning rate: 0.00737\n",
      "2025-12-18 05:06:42.690735: train_loss -0.838\n",
      "2025-12-18 05:06:42.690735: val_loss -0.8599\n",
      "2025-12-18 05:06:42.690735: Pseudo dice [0.9232, 0.9516, 0.9267]\n",
      "2025-12-18 05:06:42.690735: Epoch time: 138.12 s\n",
      "2025-12-18 05:06:43.324012: \n",
      "2025-12-18 05:06:43.324012: Epoch 289\n",
      "2025-12-18 05:06:43.324012: Current learning rate: 0.00736\n",
      "2025-12-18 05:09:01.684222: train_loss -0.8345\n",
      "2025-12-18 05:09:01.684222: val_loss -0.8511\n",
      "2025-12-18 05:09:01.692290: Pseudo dice [0.9134, 0.9497, 0.9384]\n",
      "2025-12-18 05:09:01.694293: Epoch time: 138.36 s\n",
      "2025-12-18 05:09:02.538897: \n",
      "2025-12-18 05:09:02.538897: Epoch 290\n",
      "2025-12-18 05:09:02.538897: Current learning rate: 0.00735\n",
      "2025-12-18 05:11:20.871157: train_loss -0.8383\n",
      "2025-12-18 05:11:20.873159: val_loss -0.8605\n",
      "2025-12-18 05:11:20.877163: Pseudo dice [0.9217, 0.9559, 0.9313]\n",
      "2025-12-18 05:11:20.880906: Epoch time: 138.33 s\n",
      "2025-12-18 05:11:21.621911: \n",
      "2025-12-18 05:11:21.621911: Epoch 291\n",
      "2025-12-18 05:11:21.621911: Current learning rate: 0.00734\n",
      "2025-12-18 05:13:39.903814: train_loss -0.8378\n",
      "2025-12-18 05:13:39.903814: val_loss -0.8675\n",
      "2025-12-18 05:13:39.921462: Pseudo dice [0.9266, 0.9534, 0.9358]\n",
      "2025-12-18 05:13:39.927225: Epoch time: 138.28 s\n",
      "2025-12-18 05:13:40.568856: \n",
      "2025-12-18 05:13:40.568856: Epoch 292\n",
      "2025-12-18 05:13:40.568856: Current learning rate: 0.00733\n",
      "2025-12-18 05:15:58.784716: train_loss -0.8428\n",
      "2025-12-18 05:15:58.786718: val_loss -0.8478\n",
      "2025-12-18 05:15:58.790723: Pseudo dice [0.9163, 0.9508, 0.927]\n",
      "2025-12-18 05:15:58.794732: Epoch time: 138.22 s\n",
      "2025-12-18 05:15:59.448152: \n",
      "2025-12-18 05:15:59.448152: Epoch 293\n",
      "2025-12-18 05:15:59.448152: Current learning rate: 0.00732\n",
      "2025-12-18 05:18:17.599099: train_loss -0.8406\n",
      "2025-12-18 05:18:17.599099: val_loss -0.8645\n",
      "2025-12-18 05:18:17.605467: Pseudo dice [0.9257, 0.9521, 0.9313]\n",
      "2025-12-18 05:18:17.607469: Epoch time: 138.15 s\n",
      "2025-12-18 05:18:18.335919: \n",
      "2025-12-18 05:18:18.335919: Epoch 294\n",
      "2025-12-18 05:18:18.335919: Current learning rate: 0.00731\n",
      "2025-12-18 05:20:36.520703: train_loss -0.8424\n",
      "2025-12-18 05:20:36.522705: val_loss -0.8641\n",
      "2025-12-18 05:20:36.525281: Pseudo dice [0.926, 0.9545, 0.9308]\n",
      "2025-12-18 05:20:36.529286: Epoch time: 138.19 s\n",
      "2025-12-18 05:20:37.178478: \n",
      "2025-12-18 05:20:37.178478: Epoch 295\n",
      "2025-12-18 05:20:37.178478: Current learning rate: 0.0073\n",
      "2025-12-18 05:22:55.145949: train_loss -0.8456\n",
      "2025-12-18 05:22:55.147690: val_loss -0.8579\n",
      "2025-12-18 05:22:55.151512: Pseudo dice [0.9177, 0.9498, 0.9346]\n",
      "2025-12-18 05:22:55.153514: Epoch time: 137.97 s\n",
      "2025-12-18 05:22:55.936924: \n",
      "2025-12-18 05:22:55.936924: Epoch 296\n",
      "2025-12-18 05:22:55.952666: Current learning rate: 0.00729\n",
      "2025-12-18 05:25:14.145038: train_loss -0.8413\n",
      "2025-12-18 05:25:14.145038: val_loss -0.8567\n",
      "2025-12-18 05:25:14.146778: Pseudo dice [0.9196, 0.9454, 0.9347]\n",
      "2025-12-18 05:25:14.153167: Epoch time: 138.21 s\n",
      "2025-12-18 05:25:14.918313: \n",
      "2025-12-18 05:25:14.918313: Epoch 297\n",
      "2025-12-18 05:25:14.922196: Current learning rate: 0.00728\n",
      "2025-12-18 05:27:32.915194: train_loss -0.8413\n",
      "2025-12-18 05:27:32.930985: val_loss -0.8654\n",
      "2025-12-18 05:27:32.930985: Pseudo dice [0.9277, 0.9575, 0.9306]\n",
      "2025-12-18 05:27:32.930985: Epoch time: 138.0 s\n",
      "2025-12-18 05:27:33.575350: \n",
      "2025-12-18 05:27:33.575350: Epoch 298\n",
      "2025-12-18 05:27:33.575350: Current learning rate: 0.00727\n",
      "2025-12-18 05:29:51.575381: train_loss -0.8419\n",
      "2025-12-18 05:29:51.575381: val_loss -0.8558\n",
      "2025-12-18 05:29:51.575381: Pseudo dice [0.9202, 0.9505, 0.9249]\n",
      "2025-12-18 05:29:51.582628: Epoch time: 138.0 s\n",
      "2025-12-18 05:29:52.223804: \n",
      "2025-12-18 05:29:52.223804: Epoch 299\n",
      "2025-12-18 05:29:52.223804: Current learning rate: 0.00726\n",
      "2025-12-18 05:32:10.551772: train_loss -0.8381\n",
      "2025-12-18 05:32:10.551772: val_loss -0.8527\n",
      "2025-12-18 05:32:10.555778: Pseudo dice [0.9175, 0.9507, 0.931]\n",
      "2025-12-18 05:32:10.559014: Epoch time: 138.33 s\n",
      "2025-12-18 05:32:11.579002: \n",
      "2025-12-18 05:32:11.579002: Epoch 300\n",
      "2025-12-18 05:32:11.580744: Current learning rate: 0.00725\n",
      "2025-12-18 05:34:30.028788: train_loss -0.8347\n",
      "2025-12-18 05:34:30.028788: val_loss -0.8519\n",
      "2025-12-18 05:34:30.041509: Pseudo dice [0.9163, 0.9487, 0.9331]\n",
      "2025-12-18 05:34:30.043511: Epoch time: 138.45 s\n",
      "2025-12-18 05:34:30.688238: \n",
      "2025-12-18 05:34:30.688238: Epoch 301\n",
      "2025-12-18 05:34:30.691957: Current learning rate: 0.00724\n",
      "2025-12-18 05:36:48.736782: train_loss -0.8388\n",
      "2025-12-18 05:36:48.736782: val_loss -0.8598\n",
      "2025-12-18 05:36:48.736782: Pseudo dice [0.9221, 0.9523, 0.93]\n",
      "2025-12-18 05:36:48.736782: Epoch time: 138.05 s\n",
      "2025-12-18 05:36:49.544726: \n",
      "2025-12-18 05:36:49.544726: Epoch 302\n",
      "2025-12-18 05:36:49.544726: Current learning rate: 0.00724\n",
      "2025-12-18 05:39:07.624338: train_loss -0.8389\n",
      "2025-12-18 05:39:07.626340: val_loss -0.8672\n",
      "2025-12-18 05:39:07.630083: Pseudo dice [0.9244, 0.9522, 0.9422]\n",
      "2025-12-18 05:39:07.632085: Epoch time: 138.08 s\n",
      "2025-12-18 05:39:08.435336: \n",
      "2025-12-18 05:39:08.435336: Epoch 303\n",
      "2025-12-18 05:39:08.435336: Current learning rate: 0.00723\n",
      "2025-12-18 05:41:26.593545: train_loss -0.8351\n",
      "2025-12-18 05:41:26.593545: val_loss -0.8605\n",
      "2025-12-18 05:41:26.593545: Pseudo dice [0.9225, 0.9533, 0.9221]\n",
      "2025-12-18 05:41:26.593545: Epoch time: 138.17 s\n",
      "2025-12-18 05:41:27.242346: \n",
      "2025-12-18 05:41:27.242346: Epoch 304\n",
      "2025-12-18 05:41:27.248607: Current learning rate: 0.00722\n",
      "2025-12-18 05:43:45.325446: train_loss -0.8377\n",
      "2025-12-18 05:43:45.325446: val_loss -0.8535\n",
      "2025-12-18 05:43:45.325446: Pseudo dice [0.9185, 0.9474, 0.925]\n",
      "2025-12-18 05:43:45.331307: Epoch time: 138.08 s\n",
      "2025-12-18 05:43:45.974624: \n",
      "2025-12-18 05:43:45.974624: Epoch 305\n",
      "2025-12-18 05:43:45.974624: Current learning rate: 0.00721\n",
      "2025-12-18 05:46:04.081658: train_loss -0.8391\n",
      "2025-12-18 05:46:04.081658: val_loss -0.8638\n",
      "2025-12-18 05:46:04.081658: Pseudo dice [0.9265, 0.955, 0.9225]\n",
      "2025-12-18 05:46:04.081658: Epoch time: 138.11 s\n",
      "2025-12-18 05:46:04.905035: \n",
      "2025-12-18 05:46:04.905035: Epoch 306\n",
      "2025-12-18 05:46:04.909513: Current learning rate: 0.0072\n",
      "2025-12-18 05:48:22.990741: train_loss -0.8423\n",
      "2025-12-18 05:48:22.990741: val_loss -0.8635\n",
      "2025-12-18 05:48:22.990741: Pseudo dice [0.92, 0.9539, 0.9372]\n",
      "2025-12-18 05:48:23.002834: Epoch time: 138.09 s\n",
      "2025-12-18 05:48:23.655672: \n",
      "2025-12-18 05:48:23.655672: Epoch 307\n",
      "2025-12-18 05:48:23.662254: Current learning rate: 0.00719\n",
      "2025-12-18 05:50:41.815435: train_loss -0.8359\n",
      "2025-12-18 05:50:41.815435: val_loss -0.852\n",
      "2025-12-18 05:50:41.823186: Pseudo dice [0.9205, 0.9563, 0.9166]\n",
      "2025-12-18 05:50:41.825191: Epoch time: 138.16 s\n",
      "2025-12-18 05:50:42.624784: \n",
      "2025-12-18 05:50:42.624784: Epoch 308\n",
      "2025-12-18 05:50:42.624784: Current learning rate: 0.00718\n",
      "2025-12-18 05:53:00.726053: train_loss -0.8375\n",
      "2025-12-18 05:53:00.726053: val_loss -0.8501\n",
      "2025-12-18 05:53:00.730059: Pseudo dice [0.9179, 0.9475, 0.9232]\n",
      "2025-12-18 05:53:00.731799: Epoch time: 138.1 s\n",
      "2025-12-18 05:53:01.369816: \n",
      "2025-12-18 05:53:01.369816: Epoch 309\n",
      "2025-12-18 05:53:01.379412: Current learning rate: 0.00717\n",
      "2025-12-18 05:55:19.456342: train_loss -0.8422\n",
      "2025-12-18 05:55:19.456342: val_loss -0.8526\n",
      "2025-12-18 05:55:19.472422: Pseudo dice [0.9171, 0.9515, 0.9345]\n",
      "2025-12-18 05:55:19.476678: Epoch time: 138.09 s\n",
      "2025-12-18 05:55:20.120754: \n",
      "2025-12-18 05:55:20.120754: Epoch 310\n",
      "2025-12-18 05:55:20.122758: Current learning rate: 0.00716\n",
      "2025-12-18 05:57:38.289045: train_loss -0.8342\n",
      "2025-12-18 05:57:38.289045: val_loss -0.8606\n",
      "2025-12-18 05:57:38.293372: Pseudo dice [0.9213, 0.9515, 0.9258]\n",
      "2025-12-18 05:57:38.296571: Epoch time: 138.18 s\n",
      "2025-12-18 05:57:38.947980: \n",
      "2025-12-18 05:57:38.947980: Epoch 311\n",
      "2025-12-18 05:57:38.963794: Current learning rate: 0.00715\n",
      "2025-12-18 05:59:57.047709: train_loss -0.8367\n",
      "2025-12-18 05:59:57.047709: val_loss -0.8503\n",
      "2025-12-18 05:59:57.047709: Pseudo dice [0.914, 0.9483, 0.9295]\n",
      "2025-12-18 05:59:57.047709: Epoch time: 138.1 s\n",
      "2025-12-18 05:59:57.681543: \n",
      "2025-12-18 05:59:57.681543: Epoch 312\n",
      "2025-12-18 05:59:57.681543: Current learning rate: 0.00714\n",
      "2025-12-18 06:02:16.019722: train_loss -0.8366\n",
      "2025-12-18 06:02:16.019722: val_loss -0.8647\n",
      "2025-12-18 06:02:16.031598: Pseudo dice [0.9229, 0.9567, 0.937]\n",
      "2025-12-18 06:02:16.034866: Epoch time: 138.34 s\n",
      "2025-12-18 06:02:16.668300: \n",
      "2025-12-18 06:02:16.668300: Epoch 313\n",
      "2025-12-18 06:02:16.673734: Current learning rate: 0.00713\n",
      "2025-12-18 06:04:35.166101: train_loss -0.8281\n",
      "2025-12-18 06:04:35.168107: val_loss -0.8507\n",
      "2025-12-18 06:04:35.170109: Pseudo dice [0.915, 0.9511, 0.9263]\n",
      "2025-12-18 06:04:35.174118: Epoch time: 138.5 s\n",
      "2025-12-18 06:04:35.807641: \n",
      "2025-12-18 06:04:35.807641: Epoch 314\n",
      "2025-12-18 06:04:35.822056: Current learning rate: 0.00712\n",
      "2025-12-18 06:06:53.928446: train_loss -0.833\n",
      "2025-12-18 06:06:53.928446: val_loss -0.8462\n",
      "2025-12-18 06:06:53.928446: Pseudo dice [0.9091, 0.9478, 0.9403]\n",
      "2025-12-18 06:06:53.938540: Epoch time: 138.12 s\n",
      "2025-12-18 06:06:54.564477: \n",
      "2025-12-18 06:06:54.564477: Epoch 315\n",
      "2025-12-18 06:06:54.578826: Current learning rate: 0.00711\n",
      "2025-12-18 06:09:13.073445: train_loss -0.836\n",
      "2025-12-18 06:09:13.073445: val_loss -0.8584\n",
      "2025-12-18 06:09:13.078205: Pseudo dice [0.92, 0.9533, 0.9205]\n",
      "2025-12-18 06:09:13.078205: Epoch time: 138.51 s\n",
      "2025-12-18 06:09:13.722925: \n",
      "2025-12-18 06:09:13.722925: Epoch 316\n",
      "2025-12-18 06:09:13.738933: Current learning rate: 0.0071\n",
      "2025-12-18 06:11:32.250634: train_loss -0.8382\n",
      "2025-12-18 06:11:32.252640: val_loss -0.8656\n",
      "2025-12-18 06:11:32.260405: Pseudo dice [0.924, 0.9504, 0.9364]\n",
      "2025-12-18 06:11:32.260405: Epoch time: 138.53 s\n",
      "2025-12-18 06:11:32.905731: \n",
      "2025-12-18 06:11:32.905731: Epoch 317\n",
      "2025-12-18 06:11:32.910980: Current learning rate: 0.0071\n",
      "2025-12-18 06:13:51.046622: train_loss -0.838\n",
      "2025-12-18 06:13:51.046622: val_loss -0.8564\n",
      "2025-12-18 06:13:51.046622: Pseudo dice [0.9137, 0.9521, 0.9374]\n",
      "2025-12-18 06:13:51.046622: Epoch time: 138.14 s\n",
      "2025-12-18 06:13:51.696360: \n",
      "2025-12-18 06:13:51.696360: Epoch 318\n",
      "2025-12-18 06:13:51.696360: Current learning rate: 0.00709\n",
      "2025-12-18 06:16:09.759957: train_loss -0.8378\n",
      "2025-12-18 06:16:09.761965: val_loss -0.8618\n",
      "2025-12-18 06:16:09.765974: Pseudo dice [0.9201, 0.9564, 0.9277]\n",
      "2025-12-18 06:16:09.769719: Epoch time: 138.06 s\n",
      "2025-12-18 06:16:10.559338: \n",
      "2025-12-18 06:16:10.575116: Epoch 319\n",
      "2025-12-18 06:16:10.577384: Current learning rate: 0.00708\n",
      "2025-12-18 06:18:28.662036: train_loss -0.8409\n",
      "2025-12-18 06:18:28.662036: val_loss -0.8619\n",
      "2025-12-18 06:18:28.662036: Pseudo dice [0.9236, 0.952, 0.9336]\n",
      "2025-12-18 06:18:28.677974: Epoch time: 138.1 s\n",
      "2025-12-18 06:18:29.327970: \n",
      "2025-12-18 06:18:29.327970: Epoch 320\n",
      "2025-12-18 06:18:29.343950: Current learning rate: 0.00707\n",
      "2025-12-18 06:20:47.593770: train_loss -0.8419\n",
      "2025-12-18 06:20:47.593770: val_loss -0.852\n",
      "2025-12-18 06:20:47.593770: Pseudo dice [0.916, 0.9477, 0.9219]\n",
      "2025-12-18 06:20:47.593770: Epoch time: 138.27 s\n",
      "2025-12-18 06:20:48.228295: \n",
      "2025-12-18 06:20:48.228295: Epoch 321\n",
      "2025-12-18 06:20:48.244219: Current learning rate: 0.00706\n",
      "2025-12-18 06:23:06.352314: train_loss -0.8436\n",
      "2025-12-18 06:23:06.367946: val_loss -0.8637\n",
      "2025-12-18 06:23:06.367946: Pseudo dice [0.9208, 0.9533, 0.9343]\n",
      "2025-12-18 06:23:06.367946: Epoch time: 138.12 s\n",
      "2025-12-18 06:23:07.018935: \n",
      "2025-12-18 06:23:07.018935: Epoch 322\n",
      "2025-12-18 06:23:07.018935: Current learning rate: 0.00705\n",
      "2025-12-18 06:25:25.315794: train_loss -0.8427\n",
      "2025-12-18 06:25:25.317626: val_loss -0.8644\n",
      "2025-12-18 06:25:25.319628: Pseudo dice [0.9231, 0.9532, 0.9372]\n",
      "2025-12-18 06:25:25.321630: Epoch time: 138.3 s\n",
      "2025-12-18 06:25:25.964980: \n",
      "2025-12-18 06:25:25.964980: Epoch 323\n",
      "2025-12-18 06:25:25.964980: Current learning rate: 0.00704\n",
      "2025-12-18 06:27:44.074519: train_loss -0.8469\n",
      "2025-12-18 06:27:44.074519: val_loss -0.8613\n",
      "2025-12-18 06:27:44.090606: Pseudo dice [0.9222, 0.9505, 0.9317]\n",
      "2025-12-18 06:27:44.090606: Epoch time: 138.11 s\n",
      "2025-12-18 06:27:44.725591: \n",
      "2025-12-18 06:27:44.741693: Epoch 324\n",
      "2025-12-18 06:27:44.744164: Current learning rate: 0.00703\n",
      "2025-12-18 06:30:02.796249: train_loss -0.844\n",
      "2025-12-18 06:30:02.796249: val_loss -0.8582\n",
      "2025-12-18 06:30:02.812174: Pseudo dice [0.9167, 0.9515, 0.934]\n",
      "2025-12-18 06:30:02.812174: Epoch time: 138.07 s\n",
      "2025-12-18 06:30:03.446736: \n",
      "2025-12-18 06:30:03.446736: Epoch 325\n",
      "2025-12-18 06:30:03.446736: Current learning rate: 0.00702\n",
      "2025-12-18 06:32:21.636598: train_loss -0.8473\n",
      "2025-12-18 06:32:21.636598: val_loss -0.8575\n",
      "2025-12-18 06:32:21.644357: Pseudo dice [0.9182, 0.9493, 0.9354]\n",
      "2025-12-18 06:32:21.648367: Epoch time: 138.19 s\n",
      "2025-12-18 06:32:22.468777: \n",
      "2025-12-18 06:32:22.468777: Epoch 326\n",
      "2025-12-18 06:32:22.482960: Current learning rate: 0.00701\n",
      "2025-12-18 06:34:40.604934: train_loss -0.8465\n",
      "2025-12-18 06:34:40.604934: val_loss -0.8653\n",
      "2025-12-18 06:34:40.620585: Pseudo dice [0.9263, 0.9563, 0.9333]\n",
      "2025-12-18 06:34:40.622588: Epoch time: 138.14 s\n",
      "2025-12-18 06:34:41.255115: \n",
      "2025-12-18 06:34:41.255115: Epoch 327\n",
      "2025-12-18 06:34:41.267320: Current learning rate: 0.007\n",
      "2025-12-18 06:36:59.482067: train_loss -0.8401\n",
      "2025-12-18 06:36:59.482067: val_loss -0.8683\n",
      "2025-12-18 06:36:59.495504: Pseudo dice [0.9259, 0.9558, 0.9314]\n",
      "2025-12-18 06:36:59.498008: Epoch time: 138.23 s\n",
      "2025-12-18 06:37:00.131052: \n",
      "2025-12-18 06:37:00.131052: Epoch 328\n",
      "2025-12-18 06:37:00.131052: Current learning rate: 0.00699\n",
      "2025-12-18 06:39:18.247422: train_loss -0.8435\n",
      "2025-12-18 06:39:18.247422: val_loss -0.8553\n",
      "2025-12-18 06:39:18.247422: Pseudo dice [0.924, 0.951, 0.9147]\n",
      "2025-12-18 06:39:18.257808: Epoch time: 138.12 s\n",
      "2025-12-18 06:39:18.911515: \n",
      "2025-12-18 06:39:18.911515: Epoch 329\n",
      "2025-12-18 06:39:18.914138: Current learning rate: 0.00698\n",
      "2025-12-18 06:41:37.028405: train_loss -0.8479\n",
      "2025-12-18 06:41:37.028405: val_loss -0.8671\n",
      "2025-12-18 06:41:37.036156: Pseudo dice [0.9221, 0.956, 0.937]\n",
      "2025-12-18 06:41:37.040162: Epoch time: 138.12 s\n",
      "2025-12-18 06:41:37.681396: \n",
      "2025-12-18 06:41:37.681396: Epoch 330\n",
      "2025-12-18 06:41:37.697150: Current learning rate: 0.00697\n",
      "2025-12-18 06:43:55.753249: train_loss -0.8458\n",
      "2025-12-18 06:43:55.753249: val_loss -0.8554\n",
      "2025-12-18 06:43:55.759267: Pseudo dice [0.9141, 0.947, 0.9338]\n",
      "2025-12-18 06:43:55.761270: Epoch time: 138.07 s\n",
      "2025-12-18 06:43:56.402542: \n",
      "2025-12-18 06:43:56.402542: Epoch 331\n",
      "2025-12-18 06:43:56.402542: Current learning rate: 0.00696\n",
      "2025-12-18 06:46:14.490772: train_loss -0.8371\n",
      "2025-12-18 06:46:14.492774: val_loss -0.8556\n",
      "2025-12-18 06:46:14.494514: Pseudo dice [0.9175, 0.9548, 0.9249]\n",
      "2025-12-18 06:46:14.494514: Epoch time: 138.09 s\n",
      "2025-12-18 06:46:15.335419: \n",
      "2025-12-18 06:46:15.335419: Epoch 332\n",
      "2025-12-18 06:46:15.339847: Current learning rate: 0.00696\n",
      "2025-12-18 06:48:33.350021: train_loss -0.8422\n",
      "2025-12-18 06:48:33.350021: val_loss -0.8563\n",
      "2025-12-18 06:48:33.353969: Pseudo dice [0.9154, 0.9517, 0.9331]\n",
      "2025-12-18 06:48:33.355971: Epoch time: 138.01 s\n",
      "2025-12-18 06:48:33.991657: \n",
      "2025-12-18 06:48:33.991657: Epoch 333\n",
      "2025-12-18 06:48:34.007671: Current learning rate: 0.00695\n",
      "2025-12-18 06:50:52.150496: train_loss -0.841\n",
      "2025-12-18 06:50:52.150496: val_loss -0.857\n",
      "2025-12-18 06:50:52.155095: Pseudo dice [0.9176, 0.9589, 0.929]\n",
      "2025-12-18 06:50:52.158392: Epoch time: 138.16 s\n",
      "2025-12-18 06:50:52.785375: \n",
      "2025-12-18 06:50:52.785375: Epoch 334\n",
      "2025-12-18 06:50:52.785375: Current learning rate: 0.00694\n",
      "2025-12-18 06:53:10.874062: train_loss -0.8461\n",
      "2025-12-18 06:53:10.876065: val_loss -0.865\n",
      "2025-12-18 06:53:10.880070: Pseudo dice [0.9251, 0.9562, 0.9322]\n",
      "2025-12-18 06:53:10.883551: Epoch time: 138.09 s\n",
      "2025-12-18 06:53:11.638711: \n",
      "2025-12-18 06:53:11.638711: Epoch 335\n",
      "2025-12-18 06:53:11.638711: Current learning rate: 0.00693\n",
      "2025-12-18 06:55:29.931825: train_loss -0.8447\n",
      "2025-12-18 06:55:29.931825: val_loss -0.8721\n",
      "2025-12-18 06:55:29.935269: Pseudo dice [0.9255, 0.9581, 0.9359]\n",
      "2025-12-18 06:55:29.938780: Epoch time: 138.29 s\n",
      "2025-12-18 06:55:30.581539: \n",
      "2025-12-18 06:55:30.581539: Epoch 336\n",
      "2025-12-18 06:55:30.597653: Current learning rate: 0.00692\n",
      "2025-12-18 06:57:48.717775: train_loss -0.8476\n",
      "2025-12-18 06:57:48.717775: val_loss -0.8694\n",
      "2025-12-18 06:57:48.723781: Pseudo dice [0.9297, 0.9566, 0.9284]\n",
      "2025-12-18 06:57:48.725783: Epoch time: 138.14 s\n",
      "2025-12-18 06:57:48.727578: Yayy! New best EMA pseudo Dice: 0.9355\n",
      "2025-12-18 06:57:49.620593: \n",
      "2025-12-18 06:57:49.621595: Epoch 337\n",
      "2025-12-18 06:57:49.622805: Current learning rate: 0.00691\n",
      "2025-12-18 07:00:07.742284: train_loss -0.8444\n",
      "2025-12-18 07:00:07.742284: val_loss -0.875\n",
      "2025-12-18 07:00:07.745174: Pseudo dice [0.9287, 0.9581, 0.938]\n",
      "2025-12-18 07:00:07.745174: Epoch time: 138.12 s\n",
      "2025-12-18 07:00:07.745174: Yayy! New best EMA pseudo Dice: 0.9361\n",
      "2025-12-18 07:00:08.677601: \n",
      "2025-12-18 07:00:08.693518: Epoch 338\n",
      "2025-12-18 07:00:08.693518: Current learning rate: 0.0069\n",
      "2025-12-18 07:02:26.766067: train_loss -0.8455\n",
      "2025-12-18 07:02:26.766067: val_loss -0.8585\n",
      "2025-12-18 07:02:26.766067: Pseudo dice [0.9189, 0.9525, 0.9311]\n",
      "2025-12-18 07:02:26.766067: Epoch time: 138.09 s\n",
      "2025-12-18 07:02:27.416602: \n",
      "2025-12-18 07:02:27.416602: Epoch 339\n",
      "2025-12-18 07:02:27.416602: Current learning rate: 0.00689\n",
      "2025-12-18 07:04:45.567483: train_loss -0.8427\n",
      "2025-12-18 07:04:45.569486: val_loss -0.8591\n",
      "2025-12-18 07:04:45.575501: Pseudo dice [0.9213, 0.9514, 0.9305]\n",
      "2025-12-18 07:04:45.577242: Epoch time: 138.15 s\n",
      "2025-12-18 07:04:46.225266: \n",
      "2025-12-18 07:04:46.225266: Epoch 340\n",
      "2025-12-18 07:04:46.225266: Current learning rate: 0.00688\n",
      "2025-12-18 07:07:04.316972: train_loss -0.8393\n",
      "2025-12-18 07:07:04.318974: val_loss -0.8606\n",
      "2025-12-18 07:07:04.322979: Pseudo dice [0.9215, 0.9485, 0.9315]\n",
      "2025-12-18 07:07:04.324981: Epoch time: 138.09 s\n",
      "2025-12-18 07:07:05.089422: \n",
      "2025-12-18 07:07:05.089422: Epoch 341\n",
      "2025-12-18 07:07:05.105053: Current learning rate: 0.00687\n",
      "2025-12-18 07:09:23.494699: train_loss -0.8391\n",
      "2025-12-18 07:09:23.494699: val_loss -0.8562\n",
      "2025-12-18 07:09:23.510636: Pseudo dice [0.9186, 0.9554, 0.927]\n",
      "2025-12-18 07:09:23.515078: Epoch time: 138.41 s\n",
      "2025-12-18 07:09:24.178516: \n",
      "2025-12-18 07:09:24.178516: Epoch 342\n",
      "2025-12-18 07:09:24.178516: Current learning rate: 0.00686\n",
      "2025-12-18 07:11:42.598178: train_loss -0.8279\n",
      "2025-12-18 07:11:42.598178: val_loss -0.8559\n",
      "2025-12-18 07:11:42.598178: Pseudo dice [0.9209, 0.9559, 0.93]\n",
      "2025-12-18 07:11:42.614204: Epoch time: 138.42 s\n",
      "2025-12-18 07:11:43.422872: \n",
      "2025-12-18 07:11:43.422872: Epoch 343\n",
      "2025-12-18 07:11:43.422872: Current learning rate: 0.00685\n",
      "2025-12-18 07:14:01.587417: train_loss -0.8231\n",
      "2025-12-18 07:14:01.587417: val_loss -0.8431\n",
      "2025-12-18 07:14:01.591421: Pseudo dice [0.9142, 0.9456, 0.9229]\n",
      "2025-12-18 07:14:01.593423: Epoch time: 138.16 s\n",
      "2025-12-18 07:14:02.242966: \n",
      "2025-12-18 07:14:02.242966: Epoch 344\n",
      "2025-12-18 07:14:02.258883: Current learning rate: 0.00684\n",
      "2025-12-18 07:16:20.444466: train_loss -0.8367\n",
      "2025-12-18 07:16:20.444466: val_loss -0.8535\n",
      "2025-12-18 07:16:20.444466: Pseudo dice [0.9148, 0.9533, 0.9298]\n",
      "2025-12-18 07:16:20.444466: Epoch time: 138.2 s\n",
      "2025-12-18 07:16:21.109885: \n",
      "2025-12-18 07:16:21.109885: Epoch 345\n",
      "2025-12-18 07:16:21.109885: Current learning rate: 0.00683\n",
      "2025-12-18 07:18:39.242187: train_loss -0.8381\n",
      "2025-12-18 07:18:39.242187: val_loss -0.8606\n",
      "2025-12-18 07:18:39.242187: Pseudo dice [0.9256, 0.952, 0.9256]\n",
      "2025-12-18 07:18:39.242187: Epoch time: 138.13 s\n",
      "2025-12-18 07:18:39.923226: \n",
      "2025-12-18 07:18:39.923226: Epoch 346\n",
      "2025-12-18 07:18:39.923226: Current learning rate: 0.00682\n",
      "2025-12-18 07:20:58.019140: train_loss -0.8331\n",
      "2025-12-18 07:20:58.019140: val_loss -0.8611\n",
      "2025-12-18 07:20:58.024885: Pseudo dice [0.9235, 0.955, 0.93]\n",
      "2025-12-18 07:20:58.026889: Epoch time: 138.1 s\n",
      "2025-12-18 07:20:58.752945: \n",
      "2025-12-18 07:20:58.752945: Epoch 347\n",
      "2025-12-18 07:20:58.752945: Current learning rate: 0.00681\n",
      "2025-12-18 07:23:16.893118: train_loss -0.8418\n",
      "2025-12-18 07:23:16.893118: val_loss -0.8571\n",
      "2025-12-18 07:23:16.909106: Pseudo dice [0.9188, 0.9478, 0.9313]\n",
      "2025-12-18 07:23:16.909106: Epoch time: 138.14 s\n",
      "2025-12-18 07:23:17.542037: \n",
      "2025-12-18 07:23:17.542037: Epoch 348\n",
      "2025-12-18 07:23:17.557811: Current learning rate: 0.0068\n",
      "2025-12-18 07:25:35.781590: train_loss -0.8354\n",
      "2025-12-18 07:25:35.781590: val_loss -0.8619\n",
      "2025-12-18 07:25:35.781590: Pseudo dice [0.9262, 0.9516, 0.9256]\n",
      "2025-12-18 07:25:35.797466: Epoch time: 138.24 s\n",
      "2025-12-18 07:25:36.621384: \n",
      "2025-12-18 07:25:36.621384: Epoch 349\n",
      "2025-12-18 07:25:36.621384: Current learning rate: 0.0068\n",
      "2025-12-18 07:27:54.662800: train_loss -0.84\n",
      "2025-12-18 07:27:54.664301: val_loss -0.851\n",
      "2025-12-18 07:27:54.668306: Pseudo dice [0.9168, 0.9453, 0.9273]\n",
      "2025-12-18 07:27:54.672310: Epoch time: 138.04 s\n",
      "2025-12-18 07:27:55.615239: \n",
      "2025-12-18 07:27:55.615239: Epoch 350\n",
      "2025-12-18 07:27:55.629534: Current learning rate: 0.00679\n",
      "2025-12-18 07:30:13.767298: train_loss -0.8432\n",
      "2025-12-18 07:30:13.767298: val_loss -0.8693\n",
      "2025-12-18 07:30:13.773792: Pseudo dice [0.9267, 0.9553, 0.9365]\n",
      "2025-12-18 07:30:13.773792: Epoch time: 138.15 s\n",
      "2025-12-18 07:30:14.417469: \n",
      "2025-12-18 07:30:14.417469: Epoch 351\n",
      "2025-12-18 07:30:14.417469: Current learning rate: 0.00678\n",
      "2025-12-18 07:32:32.472370: train_loss -0.8459\n",
      "2025-12-18 07:32:32.472370: val_loss -0.8563\n",
      "2025-12-18 07:32:32.484290: Pseudo dice [0.9184, 0.9488, 0.928]\n",
      "2025-12-18 07:32:32.486244: Epoch time: 138.05 s\n",
      "2025-12-18 07:32:33.122431: \n",
      "2025-12-18 07:32:33.122431: Epoch 352\n",
      "2025-12-18 07:32:33.133881: Current learning rate: 0.00677\n",
      "2025-12-18 07:34:51.518716: train_loss -0.8402\n",
      "2025-12-18 07:34:51.518716: val_loss -0.8698\n",
      "2025-12-18 07:34:51.524227: Pseudo dice [0.9306, 0.9577, 0.9286]\n",
      "2025-12-18 07:34:51.526229: Epoch time: 138.4 s\n",
      "2025-12-18 07:34:52.298099: \n",
      "2025-12-18 07:34:52.298099: Epoch 353\n",
      "2025-12-18 07:34:52.298099: Current learning rate: 0.00676\n",
      "2025-12-18 07:37:10.474470: train_loss -0.8439\n",
      "2025-12-18 07:37:10.476449: val_loss -0.8677\n",
      "2025-12-18 07:37:10.478451: Pseudo dice [0.9264, 0.9546, 0.9312]\n",
      "2025-12-18 07:37:10.480454: Epoch time: 138.18 s\n",
      "2025-12-18 07:37:11.125333: \n",
      "2025-12-18 07:37:11.125333: Epoch 354\n",
      "2025-12-18 07:37:11.141189: Current learning rate: 0.00675\n",
      "2025-12-18 07:39:29.381271: train_loss -0.8374\n",
      "2025-12-18 07:39:29.381271: val_loss -0.8672\n",
      "2025-12-18 07:39:29.394572: Pseudo dice [0.9281, 0.9582, 0.9238]\n",
      "2025-12-18 07:39:29.397857: Epoch time: 138.26 s\n",
      "2025-12-18 07:39:30.205342: \n",
      "2025-12-18 07:39:30.205342: Epoch 355\n",
      "2025-12-18 07:39:30.218738: Current learning rate: 0.00674\n",
      "2025-12-18 07:41:48.447309: train_loss -0.8386\n",
      "2025-12-18 07:41:48.447309: val_loss -0.8653\n",
      "2025-12-18 07:41:48.453317: Pseudo dice [0.9246, 0.9558, 0.9355]\n",
      "2025-12-18 07:41:48.457322: Epoch time: 138.24 s\n",
      "2025-12-18 07:41:49.173443: \n",
      "2025-12-18 07:41:49.173443: Epoch 356\n",
      "2025-12-18 07:41:49.173443: Current learning rate: 0.00673\n",
      "2025-12-18 07:44:07.417499: train_loss -0.8381\n",
      "2025-12-18 07:44:07.417499: val_loss -0.8604\n",
      "2025-12-18 07:44:07.421342: Pseudo dice [0.9237, 0.9537, 0.921]\n",
      "2025-12-18 07:44:07.423345: Epoch time: 138.24 s\n",
      "2025-12-18 07:44:08.066416: \n",
      "2025-12-18 07:44:08.066416: Epoch 357\n",
      "2025-12-18 07:44:08.066416: Current learning rate: 0.00672\n",
      "2025-12-18 07:46:26.288953: train_loss -0.8421\n",
      "2025-12-18 07:46:26.288953: val_loss -0.8632\n",
      "2025-12-18 07:46:26.306819: Pseudo dice [0.9221, 0.9499, 0.9386]\n",
      "2025-12-18 07:46:26.306819: Epoch time: 138.22 s\n",
      "2025-12-18 07:46:26.953557: \n",
      "2025-12-18 07:46:26.953557: Epoch 358\n",
      "2025-12-18 07:46:26.969343: Current learning rate: 0.00671\n",
      "2025-12-18 07:48:44.823335: train_loss -0.8507\n",
      "2025-12-18 07:48:44.823335: val_loss -0.8621\n",
      "2025-12-18 07:48:44.830253: Pseudo dice [0.9248, 0.9533, 0.9246]\n",
      "2025-12-18 07:48:44.830253: Epoch time: 137.87 s\n",
      "2025-12-18 07:48:45.566365: \n",
      "2025-12-18 07:48:45.566365: Epoch 359\n",
      "2025-12-18 07:48:45.584075: Current learning rate: 0.0067\n",
      "2025-12-18 07:51:03.633013: train_loss -0.8462\n",
      "2025-12-18 07:51:03.633013: val_loss -0.8593\n",
      "2025-12-18 07:51:03.633013: Pseudo dice [0.9198, 0.9477, 0.9349]\n",
      "2025-12-18 07:51:03.641117: Epoch time: 138.07 s\n",
      "2025-12-18 07:51:04.281102: \n",
      "2025-12-18 07:51:04.281102: Epoch 360\n",
      "2025-12-18 07:51:04.281102: Current learning rate: 0.00669\n",
      "2025-12-18 07:53:22.389822: train_loss -0.843\n",
      "2025-12-18 07:53:22.389822: val_loss -0.8571\n",
      "2025-12-18 07:53:22.394305: Pseudo dice [0.9189, 0.9492, 0.9373]\n",
      "2025-12-18 07:53:22.394305: Epoch time: 138.11 s\n",
      "2025-12-18 07:53:23.212912: \n",
      "2025-12-18 07:53:23.212912: Epoch 361\n",
      "2025-12-18 07:53:23.212912: Current learning rate: 0.00668\n",
      "2025-12-18 07:55:41.336091: train_loss -0.8437\n",
      "2025-12-18 07:55:41.336091: val_loss -0.8602\n",
      "2025-12-18 07:55:41.336091: Pseudo dice [0.9148, 0.948, 0.9453]\n",
      "2025-12-18 07:55:41.343121: Epoch time: 138.12 s\n",
      "2025-12-18 07:55:42.095934: \n",
      "2025-12-18 07:55:42.095934: Epoch 362\n",
      "2025-12-18 07:55:42.095934: Current learning rate: 0.00667\n",
      "2025-12-18 07:58:00.210758: train_loss -0.8452\n",
      "2025-12-18 07:58:00.210758: val_loss -0.8702\n",
      "2025-12-18 07:58:00.216769: Pseudo dice [0.9282, 0.9558, 0.9317]\n",
      "2025-12-18 07:58:00.220519: Epoch time: 138.11 s\n",
      "2025-12-18 07:58:00.869749: \n",
      "2025-12-18 07:58:00.869749: Epoch 363\n",
      "2025-12-18 07:58:00.885868: Current learning rate: 0.00666\n",
      "2025-12-18 08:00:19.115682: train_loss -0.845\n",
      "2025-12-18 08:00:19.115682: val_loss -0.8674\n",
      "2025-12-18 08:00:19.129753: Pseudo dice [0.9222, 0.9548, 0.9403]\n",
      "2025-12-18 08:00:19.131493: Epoch time: 138.25 s\n",
      "2025-12-18 08:00:19.796257: \n",
      "2025-12-18 08:00:19.796257: Epoch 364\n",
      "2025-12-18 08:00:19.796257: Current learning rate: 0.00665\n",
      "2025-12-18 08:02:38.127897: train_loss -0.8445\n",
      "2025-12-18 08:02:38.143801: val_loss -0.8576\n",
      "2025-12-18 08:02:38.143801: Pseudo dice [0.9179, 0.9531, 0.93]\n",
      "2025-12-18 08:02:38.143801: Epoch time: 138.33 s\n",
      "2025-12-18 08:02:38.924748: \n",
      "2025-12-18 08:02:38.924748: Epoch 365\n",
      "2025-12-18 08:02:38.931676: Current learning rate: 0.00665\n",
      "2025-12-18 08:04:57.096878: train_loss -0.8448\n",
      "2025-12-18 08:04:57.096878: val_loss -0.8682\n",
      "2025-12-18 08:04:57.096878: Pseudo dice [0.9268, 0.9545, 0.9312]\n",
      "2025-12-18 08:04:57.103899: Epoch time: 138.17 s\n",
      "2025-12-18 08:04:57.761631: \n",
      "2025-12-18 08:04:57.761631: Epoch 366\n",
      "2025-12-18 08:04:57.761631: Current learning rate: 0.00664\n",
      "2025-12-18 08:07:16.059107: train_loss -0.8441\n",
      "2025-12-18 08:07:16.061110: val_loss -0.8697\n",
      "2025-12-18 08:07:16.064114: Pseudo dice [0.9252, 0.9574, 0.9387]\n",
      "2025-12-18 08:07:16.068119: Epoch time: 138.3 s\n",
      "2025-12-18 08:07:16.070121: Yayy! New best EMA pseudo Dice: 0.9363\n",
      "2025-12-18 08:07:17.172543: \n",
      "2025-12-18 08:07:17.172543: Epoch 367\n",
      "2025-12-18 08:07:17.174546: Current learning rate: 0.00663\n",
      "2025-12-18 08:09:35.613229: train_loss -0.8469\n",
      "2025-12-18 08:09:35.613229: val_loss -0.8686\n",
      "2025-12-18 08:09:35.617234: Pseudo dice [0.9234, 0.9585, 0.9312]\n",
      "2025-12-18 08:09:35.619237: Epoch time: 138.44 s\n",
      "2025-12-18 08:09:35.623242: Yayy! New best EMA pseudo Dice: 0.9365\n",
      "2025-12-18 08:09:36.599250: \n",
      "2025-12-18 08:09:36.599250: Epoch 368\n",
      "2025-12-18 08:09:36.599250: Current learning rate: 0.00662\n",
      "2025-12-18 08:11:54.978170: train_loss -0.8465\n",
      "2025-12-18 08:11:54.978170: val_loss -0.8685\n",
      "2025-12-18 08:11:54.984181: Pseudo dice [0.9224, 0.9608, 0.9323]\n",
      "2025-12-18 08:11:54.984181: Epoch time: 138.38 s\n",
      "2025-12-18 08:11:54.984181: Yayy! New best EMA pseudo Dice: 0.9367\n",
      "2025-12-18 08:11:55.919393: \n",
      "2025-12-18 08:11:55.919393: Epoch 369\n",
      "2025-12-18 08:11:55.935180: Current learning rate: 0.00661\n",
      "2025-12-18 08:14:14.060630: train_loss -0.8404\n",
      "2025-12-18 08:14:14.060630: val_loss -0.8665\n",
      "2025-12-18 08:14:14.076717: Pseudo dice [0.9216, 0.9541, 0.9399]\n",
      "2025-12-18 08:14:14.076717: Epoch time: 138.14 s\n",
      "2025-12-18 08:14:14.076717: Yayy! New best EMA pseudo Dice: 0.9369\n",
      "2025-12-18 08:14:14.997230: \n",
      "2025-12-18 08:14:14.997230: Epoch 370\n",
      "2025-12-18 08:14:14.997230: Current learning rate: 0.0066\n",
      "2025-12-18 08:16:33.143183: train_loss -0.8456\n",
      "2025-12-18 08:16:33.145185: val_loss -0.8545\n",
      "2025-12-18 08:16:33.148537: Pseudo dice [0.9164, 0.9525, 0.9293]\n",
      "2025-12-18 08:16:33.150539: Epoch time: 138.15 s\n",
      "2025-12-18 08:16:33.804541: \n",
      "2025-12-18 08:16:33.804541: Epoch 371\n",
      "2025-12-18 08:16:33.804541: Current learning rate: 0.00659\n",
      "2025-12-18 08:18:52.008504: train_loss -0.8467\n",
      "2025-12-18 08:18:52.008504: val_loss -0.862\n",
      "2025-12-18 08:18:52.008504: Pseudo dice [0.9204, 0.9527, 0.9343]\n",
      "2025-12-18 08:18:52.024139: Epoch time: 138.2 s\n",
      "2025-12-18 08:18:52.848393: \n",
      "2025-12-18 08:18:52.848393: Epoch 372\n",
      "2025-12-18 08:18:52.848393: Current learning rate: 0.00658\n",
      "2025-12-18 08:21:11.056624: train_loss -0.8459\n",
      "2025-12-18 08:21:11.056624: val_loss -0.8614\n",
      "2025-12-18 08:21:11.056624: Pseudo dice [0.9193, 0.9488, 0.9374]\n",
      "2025-12-18 08:21:11.069668: Epoch time: 138.21 s\n",
      "2025-12-18 08:21:11.723720: \n",
      "2025-12-18 08:21:11.723720: Epoch 373\n",
      "2025-12-18 08:21:11.738018: Current learning rate: 0.00657\n",
      "2025-12-18 08:23:29.872459: train_loss -0.8453\n",
      "2025-12-18 08:23:29.872459: val_loss -0.869\n",
      "2025-12-18 08:23:29.876463: Pseudo dice [0.927, 0.9542, 0.9321]\n",
      "2025-12-18 08:23:29.876463: Epoch time: 138.15 s\n",
      "2025-12-18 08:23:30.537098: \n",
      "2025-12-18 08:23:30.537098: Epoch 374\n",
      "2025-12-18 08:23:30.537098: Current learning rate: 0.00656\n",
      "2025-12-18 08:25:48.700679: train_loss -0.8441\n",
      "2025-12-18 08:25:48.702681: val_loss -0.8645\n",
      "2025-12-18 08:25:48.704683: Pseudo dice [0.922, 0.9512, 0.937]\n",
      "2025-12-18 08:25:48.704683: Epoch time: 138.16 s\n",
      "2025-12-18 08:25:49.361226: \n",
      "2025-12-18 08:25:49.361226: Epoch 375\n",
      "2025-12-18 08:25:49.361226: Current learning rate: 0.00655\n",
      "2025-12-18 08:28:07.499615: train_loss -0.8418\n",
      "2025-12-18 08:28:07.499615: val_loss -0.8556\n",
      "2025-12-18 08:28:07.509038: Pseudo dice [0.9138, 0.9488, 0.9371]\n",
      "2025-12-18 08:28:07.509038: Epoch time: 138.14 s\n",
      "2025-12-18 08:28:08.166097: \n",
      "2025-12-18 08:28:08.166097: Epoch 376\n",
      "2025-12-18 08:28:08.175506: Current learning rate: 0.00654\n",
      "2025-12-18 08:30:26.371494: train_loss -0.8524\n",
      "2025-12-18 08:30:26.371494: val_loss -0.8653\n",
      "2025-12-18 08:30:26.377387: Pseudo dice [0.9225, 0.9568, 0.9359]\n",
      "2025-12-18 08:30:26.381391: Epoch time: 138.21 s\n",
      "2025-12-18 08:30:27.037890: \n",
      "2025-12-18 08:30:27.037890: Epoch 377\n",
      "2025-12-18 08:30:27.037890: Current learning rate: 0.00653\n",
      "2025-12-18 08:32:45.233422: train_loss -0.8369\n",
      "2025-12-18 08:32:45.233422: val_loss -0.8583\n",
      "2025-12-18 08:32:45.233422: Pseudo dice [0.9253, 0.9531, 0.9313]\n",
      "2025-12-18 08:32:45.233422: Epoch time: 138.2 s\n",
      "2025-12-18 08:32:46.058246: \n",
      "2025-12-18 08:32:46.058246: Epoch 378\n",
      "2025-12-18 08:32:46.074354: Current learning rate: 0.00652\n",
      "2025-12-18 08:35:04.254485: train_loss -0.8143\n",
      "2025-12-18 08:35:04.254485: val_loss -0.8376\n",
      "2025-12-18 08:35:04.270389: Pseudo dice [0.9149, 0.9516, 0.9078]\n",
      "2025-12-18 08:35:04.270389: Epoch time: 138.2 s\n",
      "2025-12-18 08:35:04.920031: \n",
      "2025-12-18 08:35:04.920031: Epoch 379\n",
      "2025-12-18 08:35:04.920031: Current learning rate: 0.00651\n",
      "2025-12-18 08:37:22.882127: train_loss -0.7941\n",
      "2025-12-18 08:37:22.882127: val_loss -0.8247\n",
      "2025-12-18 08:37:22.882127: Pseudo dice [0.9141, 0.9443, 0.9086]\n",
      "2025-12-18 08:37:22.897795: Epoch time: 137.96 s\n",
      "2025-12-18 08:37:23.612346: \n",
      "2025-12-18 08:37:23.612346: Epoch 380\n",
      "2025-12-18 08:37:23.612346: Current learning rate: 0.0065\n",
      "2025-12-18 08:39:41.722084: train_loss -0.8121\n",
      "2025-12-18 08:39:41.738007: val_loss -0.8428\n",
      "2025-12-18 08:39:41.738007: Pseudo dice [0.9141, 0.9469, 0.9225]\n",
      "2025-12-18 08:39:41.738007: Epoch time: 138.11 s\n",
      "2025-12-18 08:39:42.386945: \n",
      "2025-12-18 08:39:42.386945: Epoch 381\n",
      "2025-12-18 08:39:42.386945: Current learning rate: 0.00649\n",
      "2025-12-18 08:42:00.528599: train_loss -0.8236\n",
      "2025-12-18 08:42:00.528599: val_loss -0.8496\n",
      "2025-12-18 08:42:00.530601: Pseudo dice [0.9153, 0.9503, 0.9208]\n",
      "2025-12-18 08:42:00.530601: Epoch time: 138.14 s\n",
      "2025-12-18 08:42:01.191204: \n",
      "2025-12-18 08:42:01.191204: Epoch 382\n",
      "2025-12-18 08:42:01.191204: Current learning rate: 0.00648\n",
      "2025-12-18 08:44:19.369262: train_loss -0.8402\n",
      "2025-12-18 08:44:19.371264: val_loss -0.8478\n",
      "2025-12-18 08:44:19.373266: Pseudo dice [0.9134, 0.9452, 0.9336]\n",
      "2025-12-18 08:44:19.373266: Epoch time: 138.18 s\n",
      "2025-12-18 08:44:20.190029: \n",
      "2025-12-18 08:44:20.190029: Epoch 383\n",
      "2025-12-18 08:44:20.190029: Current learning rate: 0.00648\n",
      "2025-12-18 08:46:38.980486: train_loss -0.8407\n",
      "2025-12-18 08:46:38.982489: val_loss -0.8564\n",
      "2025-12-18 08:46:38.988504: Pseudo dice [0.9146, 0.9523, 0.9398]\n",
      "2025-12-18 08:46:38.992511: Epoch time: 138.81 s\n",
      "2025-12-18 08:46:39.815726: \n",
      "2025-12-18 08:46:39.815726: Epoch 384\n",
      "2025-12-18 08:46:39.822271: Current learning rate: 0.00647\n",
      "2025-12-18 08:48:58.445284: train_loss -0.8305\n",
      "2025-12-18 08:48:58.445284: val_loss -0.8459\n",
      "2025-12-18 08:48:58.445284: Pseudo dice [0.9081, 0.9451, 0.9433]\n",
      "2025-12-18 08:48:58.445284: Epoch time: 138.63 s\n",
      "2025-12-18 08:48:59.127500: \n",
      "2025-12-18 08:48:59.127500: Epoch 385\n",
      "2025-12-18 08:48:59.127500: Current learning rate: 0.00646\n",
      "2025-12-18 08:51:17.767040: train_loss -0.8332\n",
      "2025-12-18 08:51:17.767040: val_loss -0.8557\n",
      "2025-12-18 08:51:17.785001: Pseudo dice [0.922, 0.9483, 0.928]\n",
      "2025-12-18 08:51:17.787112: Epoch time: 138.64 s\n",
      "2025-12-18 08:51:18.433321: \n",
      "2025-12-18 08:51:18.433321: Epoch 386\n",
      "2025-12-18 08:51:18.444551: Current learning rate: 0.00645\n",
      "2025-12-18 08:53:36.834226: train_loss -0.8431\n",
      "2025-12-18 08:53:36.834226: val_loss -0.8657\n",
      "2025-12-18 08:53:36.850323: Pseudo dice [0.9255, 0.9578, 0.9359]\n",
      "2025-12-18 08:53:36.850323: Epoch time: 138.4 s\n",
      "2025-12-18 08:53:37.521487: \n",
      "2025-12-18 08:53:37.521487: Epoch 387\n",
      "2025-12-18 08:53:37.523250: Current learning rate: 0.00644\n",
      "2025-12-18 08:55:56.208943: train_loss -0.8431\n",
      "2025-12-18 08:55:56.208943: val_loss -0.8596\n",
      "2025-12-18 08:55:56.215189: Pseudo dice [0.9255, 0.9524, 0.9222]\n",
      "2025-12-18 08:55:56.219291: Epoch time: 138.69 s\n",
      "2025-12-18 08:55:56.887196: \n",
      "2025-12-18 08:55:56.887196: Epoch 388\n",
      "2025-12-18 08:55:56.887196: Current learning rate: 0.00643\n",
      "2025-12-18 08:58:15.084941: train_loss -0.8508\n",
      "2025-12-18 08:58:15.084941: val_loss -0.853\n",
      "2025-12-18 08:58:15.091307: Pseudo dice [0.9137, 0.9489, 0.9432]\n",
      "2025-12-18 08:58:15.091307: Epoch time: 138.2 s\n",
      "2025-12-18 08:58:15.749232: \n",
      "2025-12-18 08:58:15.749232: Epoch 389\n",
      "2025-12-18 08:58:15.751530: Current learning rate: 0.00642\n",
      "2025-12-18 09:00:33.678299: train_loss -0.8352\n",
      "2025-12-18 09:00:33.680039: val_loss -0.8489\n",
      "2025-12-18 09:00:33.684044: Pseudo dice [0.9174, 0.9465, 0.9346]\n",
      "2025-12-18 09:00:33.686837: Epoch time: 137.94 s\n",
      "2025-12-18 09:00:34.695935: \n",
      "2025-12-18 09:00:34.695935: Epoch 390\n",
      "2025-12-18 09:00:34.698939: Current learning rate: 0.00641\n",
      "2025-12-18 09:02:52.636653: train_loss -0.8388\n",
      "2025-12-18 09:02:52.636653: val_loss -0.8704\n",
      "2025-12-18 09:02:52.652706: Pseudo dice [0.9271, 0.9578, 0.9342]\n",
      "2025-12-18 09:02:52.652706: Epoch time: 137.94 s\n",
      "2025-12-18 09:02:53.304405: \n",
      "2025-12-18 09:02:53.304405: Epoch 391\n",
      "2025-12-18 09:02:53.304405: Current learning rate: 0.0064\n",
      "2025-12-18 09:05:11.429917: train_loss -0.8433\n",
      "2025-12-18 09:05:11.431658: val_loss -0.8602\n",
      "2025-12-18 09:05:11.435663: Pseudo dice [0.9175, 0.9531, 0.9433]\n",
      "2025-12-18 09:05:11.438958: Epoch time: 138.13 s\n",
      "2025-12-18 09:05:12.109201: \n",
      "2025-12-18 09:05:12.109201: Epoch 392\n",
      "2025-12-18 09:05:12.113254: Current learning rate: 0.00639\n",
      "2025-12-18 09:07:30.134917: train_loss -0.8481\n",
      "2025-12-18 09:07:30.134917: val_loss -0.8616\n",
      "2025-12-18 09:07:30.138923: Pseudo dice [0.9205, 0.958, 0.9323]\n",
      "2025-12-18 09:07:30.140926: Epoch time: 138.03 s\n",
      "2025-12-18 09:07:30.917986: \n",
      "2025-12-18 09:07:30.917986: Epoch 393\n",
      "2025-12-18 09:07:30.928683: Current learning rate: 0.00638\n",
      "2025-12-18 09:09:49.239489: train_loss -0.8441\n",
      "2025-12-18 09:09:49.239489: val_loss -0.8586\n",
      "2025-12-18 09:09:49.255505: Pseudo dice [0.916, 0.9504, 0.9392]\n",
      "2025-12-18 09:09:49.255505: Epoch time: 138.32 s\n",
      "2025-12-18 09:09:49.922928: \n",
      "2025-12-18 09:09:49.922928: Epoch 394\n",
      "2025-12-18 09:09:49.922928: Current learning rate: 0.00637\n",
      "2025-12-18 09:12:07.945711: train_loss -0.8473\n",
      "2025-12-18 09:12:07.947713: val_loss -0.862\n",
      "2025-12-18 09:12:07.951718: Pseudo dice [0.9245, 0.9519, 0.9228]\n",
      "2025-12-18 09:12:07.951718: Epoch time: 138.02 s\n",
      "2025-12-18 09:12:08.613420: \n",
      "2025-12-18 09:12:08.613420: Epoch 395\n",
      "2025-12-18 09:12:08.619807: Current learning rate: 0.00636\n",
      "2025-12-18 09:14:26.764771: train_loss -0.8451\n",
      "2025-12-18 09:14:26.764771: val_loss -0.8567\n",
      "2025-12-18 09:14:26.770523: Pseudo dice [0.9139, 0.9484, 0.9322]\n",
      "2025-12-18 09:14:26.774489: Epoch time: 138.15 s\n",
      "2025-12-18 09:14:27.767956: \n",
      "2025-12-18 09:14:27.767956: Epoch 396\n",
      "2025-12-18 09:14:27.780061: Current learning rate: 0.00635\n",
      "2025-12-18 09:16:45.788673: train_loss -0.8446\n",
      "2025-12-18 09:16:45.790676: val_loss -0.8597\n",
      "2025-12-18 09:16:45.794458: Pseudo dice [0.9165, 0.9497, 0.9351]\n",
      "2025-12-18 09:16:45.796324: Epoch time: 138.02 s\n",
      "2025-12-18 09:16:46.474770: \n",
      "2025-12-18 09:16:46.474770: Epoch 397\n",
      "2025-12-18 09:16:46.486630: Current learning rate: 0.00634\n",
      "2025-12-18 09:19:04.654323: train_loss -0.8426\n",
      "2025-12-18 09:19:04.654323: val_loss -0.868\n",
      "2025-12-18 09:19:04.656325: Pseudo dice [0.9235, 0.9547, 0.9399]\n",
      "2025-12-18 09:19:04.656325: Epoch time: 138.18 s\n",
      "2025-12-18 09:19:05.319729: \n",
      "2025-12-18 09:19:05.319729: Epoch 398\n",
      "2025-12-18 09:19:05.319729: Current learning rate: 0.00633\n",
      "2025-12-18 09:21:23.412613: train_loss -0.8458\n",
      "2025-12-18 09:21:23.414616: val_loss -0.8689\n",
      "2025-12-18 09:21:23.418622: Pseudo dice [0.929, 0.9601, 0.9322]\n",
      "2025-12-18 09:21:23.420447: Epoch time: 138.09 s\n",
      "2025-12-18 09:21:24.207358: \n",
      "2025-12-18 09:21:24.207358: Epoch 399\n",
      "2025-12-18 09:21:24.207358: Current learning rate: 0.00632\n",
      "2025-12-18 09:23:42.260605: train_loss -0.8468\n",
      "2025-12-18 09:23:42.260605: val_loss -0.8647\n",
      "2025-12-18 09:23:42.264610: Pseudo dice [0.9189, 0.9565, 0.936]\n",
      "2025-12-18 09:23:42.268614: Epoch time: 138.07 s\n",
      "2025-12-18 09:23:43.172451: \n",
      "2025-12-18 09:23:43.172451: Epoch 400\n",
      "2025-12-18 09:23:43.172451: Current learning rate: 0.00631\n",
      "2025-12-18 09:26:01.203505: train_loss -0.8501\n",
      "2025-12-18 09:26:01.203505: val_loss -0.8699\n",
      "2025-12-18 09:26:01.207509: Pseudo dice [0.924, 0.9526, 0.9501]\n",
      "2025-12-18 09:26:01.211513: Epoch time: 138.03 s\n",
      "2025-12-18 09:26:01.873899: \n",
      "2025-12-18 09:26:01.873899: Epoch 401\n",
      "2025-12-18 09:26:01.873899: Current learning rate: 0.0063\n",
      "2025-12-18 09:28:20.081867: train_loss -0.8464\n",
      "2025-12-18 09:28:20.081867: val_loss -0.8747\n",
      "2025-12-18 09:28:20.085606: Pseudo dice [0.93, 0.9569, 0.9325]\n",
      "2025-12-18 09:28:20.089610: Epoch time: 138.21 s\n",
      "2025-12-18 09:28:21.001310: \n",
      "2025-12-18 09:28:21.001310: Epoch 402\n",
      "2025-12-18 09:28:21.001310: Current learning rate: 0.0063\n",
      "2025-12-18 09:30:39.091674: train_loss -0.8472\n",
      "2025-12-18 09:30:39.091674: val_loss -0.8596\n",
      "2025-12-18 09:30:39.095678: Pseudo dice [0.9176, 0.9531, 0.932]\n",
      "2025-12-18 09:30:39.097680: Epoch time: 138.09 s\n",
      "2025-12-18 09:30:39.763455: \n",
      "2025-12-18 09:30:39.763455: Epoch 403\n",
      "2025-12-18 09:30:39.763455: Current learning rate: 0.00629\n",
      "2025-12-18 09:32:57.957752: train_loss -0.8467\n",
      "2025-12-18 09:32:57.957752: val_loss -0.8754\n",
      "2025-12-18 09:32:57.963602: Pseudo dice [0.9299, 0.9574, 0.932]\n",
      "2025-12-18 09:32:57.967105: Epoch time: 138.2 s\n",
      "2025-12-18 09:32:58.623829: \n",
      "2025-12-18 09:32:58.623829: Epoch 404\n",
      "2025-12-18 09:32:58.623829: Current learning rate: 0.00628\n",
      "2025-12-18 09:35:16.830253: train_loss -0.8435\n",
      "2025-12-18 09:35:16.830253: val_loss -0.8688\n",
      "2025-12-18 09:35:16.830253: Pseudo dice [0.924, 0.9554, 0.9371]\n",
      "2025-12-18 09:35:16.837717: Epoch time: 138.21 s\n",
      "2025-12-18 09:35:16.839718: Yayy! New best EMA pseudo Dice: 0.9369\n",
      "2025-12-18 09:35:17.731452: \n",
      "2025-12-18 09:35:17.731452: Epoch 405\n",
      "2025-12-18 09:35:17.747368: Current learning rate: 0.00627\n",
      "2025-12-18 09:37:35.930191: train_loss -0.8487\n",
      "2025-12-18 09:37:35.930191: val_loss -0.8726\n",
      "2025-12-18 09:37:35.932193: Pseudo dice [0.9252, 0.9581, 0.9362]\n",
      "2025-12-18 09:37:35.936416: Epoch time: 138.2 s\n",
      "2025-12-18 09:37:35.940421: Yayy! New best EMA pseudo Dice: 0.9372\n",
      "2025-12-18 09:37:36.871311: \n",
      "2025-12-18 09:37:36.873313: Epoch 406\n",
      "2025-12-18 09:37:36.873313: Current learning rate: 0.00626\n",
      "2025-12-18 09:39:54.986423: train_loss -0.845\n",
      "2025-12-18 09:39:54.986423: val_loss -0.8607\n",
      "2025-12-18 09:39:55.002408: Pseudo dice [0.9214, 0.9535, 0.9365]\n",
      "2025-12-18 09:39:55.006247: Epoch time: 138.12 s\n",
      "2025-12-18 09:39:55.834795: \n",
      "2025-12-18 09:39:55.834795: Epoch 407\n",
      "2025-12-18 09:39:55.839379: Current learning rate: 0.00625\n",
      "2025-12-18 09:42:13.939198: train_loss -0.8398\n",
      "2025-12-18 09:42:13.939198: val_loss -0.8629\n",
      "2025-12-18 09:42:13.955256: Pseudo dice [0.9274, 0.9564, 0.9189]\n",
      "2025-12-18 09:42:13.955256: Epoch time: 138.1 s\n",
      "2025-12-18 09:42:14.621200: \n",
      "2025-12-18 09:42:14.621200: Epoch 408\n",
      "2025-12-18 09:42:14.621200: Current learning rate: 0.00624\n",
      "2025-12-18 09:44:32.854575: train_loss -0.8438\n",
      "2025-12-18 09:44:32.854575: val_loss -0.8672\n",
      "2025-12-18 09:44:32.872320: Pseudo dice [0.9266, 0.9592, 0.9255]\n",
      "2025-12-18 09:44:32.872320: Epoch time: 138.23 s\n",
      "2025-12-18 09:44:33.519120: \n",
      "2025-12-18 09:44:33.519120: Epoch 409\n",
      "2025-12-18 09:44:33.519120: Current learning rate: 0.00623\n",
      "2025-12-18 09:46:51.721697: train_loss -0.8459\n",
      "2025-12-18 09:46:51.721697: val_loss -0.8666\n",
      "2025-12-18 09:46:51.721697: Pseudo dice [0.9268, 0.9585, 0.9414]\n",
      "2025-12-18 09:46:51.721697: Epoch time: 138.2 s\n",
      "2025-12-18 09:46:51.721697: Yayy! New best EMA pseudo Dice: 0.9374\n",
      "2025-12-18 09:46:52.638462: \n",
      "2025-12-18 09:46:52.638462: Epoch 410\n",
      "2025-12-18 09:46:52.642205: Current learning rate: 0.00622\n",
      "2025-12-18 09:49:10.857196: train_loss -0.8515\n",
      "2025-12-18 09:49:10.857196: val_loss -0.8669\n",
      "2025-12-18 09:49:10.868399: Pseudo dice [0.9246, 0.958, 0.9321]\n",
      "2025-12-18 09:49:10.873021: Epoch time: 138.23 s\n",
      "2025-12-18 09:49:10.873021: Yayy! New best EMA pseudo Dice: 0.9375\n",
      "2025-12-18 09:49:11.780176: \n",
      "2025-12-18 09:49:11.780176: Epoch 411\n",
      "2025-12-18 09:49:11.780176: Current learning rate: 0.00621\n",
      "2025-12-18 09:51:29.976914: train_loss -0.8447\n",
      "2025-12-18 09:51:29.976914: val_loss -0.8578\n",
      "2025-12-18 09:51:29.978916: Pseudo dice [0.9201, 0.9521, 0.9257]\n",
      "2025-12-18 09:51:29.978916: Epoch time: 138.2 s\n",
      "2025-12-18 09:51:30.713343: \n",
      "2025-12-18 09:51:30.713343: Epoch 412\n",
      "2025-12-18 09:51:30.717085: Current learning rate: 0.0062\n",
      "2025-12-18 09:53:48.984291: train_loss -0.8089\n",
      "2025-12-18 09:53:48.986294: val_loss -0.7681\n",
      "2025-12-18 09:53:48.989385: Pseudo dice [0.8842, 0.9309, 0.8699]\n",
      "2025-12-18 09:53:48.993645: Epoch time: 138.27 s\n",
      "2025-12-18 09:53:49.775891: \n",
      "2025-12-18 09:53:49.775891: Epoch 413\n",
      "2025-12-18 09:53:49.775891: Current learning rate: 0.00619\n",
      "2025-12-18 09:56:07.971212: train_loss -0.7721\n",
      "2025-12-18 09:56:07.971212: val_loss -0.8163\n",
      "2025-12-18 09:56:07.975217: Pseudo dice [0.9048, 0.9365, 0.9147]\n",
      "2025-12-18 09:56:07.979221: Epoch time: 138.2 s\n",
      "2025-12-18 09:56:08.613591: \n",
      "2025-12-18 09:56:08.613591: Epoch 414\n",
      "2025-12-18 09:56:08.613591: Current learning rate: 0.00618\n",
      "2025-12-18 09:58:26.635371: train_loss -0.8086\n",
      "2025-12-18 09:58:26.635371: val_loss -0.8423\n",
      "2025-12-18 09:58:26.639377: Pseudo dice [0.9155, 0.9494, 0.919]\n",
      "2025-12-18 09:58:26.643121: Epoch time: 138.02 s\n",
      "2025-12-18 09:58:27.368550: \n",
      "2025-12-18 09:58:27.368550: Epoch 415\n",
      "2025-12-18 09:58:27.368550: Current learning rate: 0.00617\n",
      "2025-12-18 10:00:45.597861: train_loss -0.8259\n",
      "2025-12-18 10:00:45.597861: val_loss -0.8513\n",
      "2025-12-18 10:00:45.610104: Pseudo dice [0.9216, 0.9494, 0.9252]\n",
      "2025-12-18 10:00:45.613847: Epoch time: 138.23 s\n",
      "2025-12-18 10:00:46.247593: \n",
      "2025-12-18 10:00:46.247593: Epoch 416\n",
      "2025-12-18 10:00:46.247593: Current learning rate: 0.00616\n",
      "2025-12-18 10:03:04.346490: train_loss -0.8191\n",
      "2025-12-18 10:03:04.346490: val_loss -0.8443\n",
      "2025-12-18 10:03:04.352578: Pseudo dice [0.9157, 0.9517, 0.922]\n",
      "2025-12-18 10:03:04.354589: Epoch time: 138.1 s\n",
      "2025-12-18 10:03:04.997726: \n",
      "2025-12-18 10:03:04.997726: Epoch 417\n",
      "2025-12-18 10:03:05.006238: Current learning rate: 0.00615\n",
      "2025-12-18 10:05:23.199174: train_loss -0.8259\n",
      "2025-12-18 10:05:23.215136: val_loss -0.8564\n",
      "2025-12-18 10:05:23.215136: Pseudo dice [0.9196, 0.9517, 0.9303]\n",
      "2025-12-18 10:05:23.215136: Epoch time: 138.2 s\n",
      "2025-12-18 10:05:23.990579: \n",
      "2025-12-18 10:05:23.990579: Epoch 418\n",
      "2025-12-18 10:05:24.000811: Current learning rate: 0.00614\n",
      "2025-12-18 10:07:42.016842: train_loss -0.8354\n",
      "2025-12-18 10:07:42.018844: val_loss -0.8501\n",
      "2025-12-18 10:07:42.022850: Pseudo dice [0.9121, 0.9531, 0.9314]\n",
      "2025-12-18 10:07:42.028200: Epoch time: 138.03 s\n",
      "2025-12-18 10:07:42.664452: \n",
      "2025-12-18 10:07:42.664452: Epoch 419\n",
      "2025-12-18 10:07:42.664452: Current learning rate: 0.00613\n",
      "2025-12-18 10:10:01.283057: train_loss -0.8412\n",
      "2025-12-18 10:10:01.285060: val_loss -0.8466\n",
      "2025-12-18 10:10:01.285060: Pseudo dice [0.9117, 0.9486, 0.924]\n",
      "2025-12-18 10:10:01.285060: Epoch time: 138.62 s\n",
      "2025-12-18 10:10:02.093388: \n",
      "2025-12-18 10:10:02.093388: Epoch 420\n",
      "2025-12-18 10:10:02.093388: Current learning rate: 0.00612\n",
      "2025-12-18 10:12:20.216114: train_loss -0.8374\n",
      "2025-12-18 10:12:20.216114: val_loss -0.8607\n",
      "2025-12-18 10:12:20.220120: Pseudo dice [0.9202, 0.9533, 0.9327]\n",
      "2025-12-18 10:12:20.224127: Epoch time: 138.12 s\n",
      "2025-12-18 10:12:20.987951: \n",
      "2025-12-18 10:12:20.987951: Epoch 421\n",
      "2025-12-18 10:12:20.993251: Current learning rate: 0.00612\n",
      "2025-12-18 10:14:39.127162: train_loss -0.8407\n",
      "2025-12-18 10:14:39.127162: val_loss -0.8568\n",
      "2025-12-18 10:14:39.127162: Pseudo dice [0.9175, 0.9508, 0.931]\n",
      "2025-12-18 10:14:39.127162: Epoch time: 138.14 s\n",
      "2025-12-18 10:14:39.761517: \n",
      "2025-12-18 10:14:39.761517: Epoch 422\n",
      "2025-12-18 10:14:39.761517: Current learning rate: 0.00611\n",
      "2025-12-18 10:16:57.842940: train_loss -0.8362\n",
      "2025-12-18 10:16:57.842940: val_loss -0.8609\n",
      "2025-12-18 10:16:57.858672: Pseudo dice [0.9193, 0.9532, 0.9356]\n",
      "2025-12-18 10:16:57.858672: Epoch time: 138.08 s\n",
      "2025-12-18 10:16:58.492320: \n",
      "2025-12-18 10:16:58.492320: Epoch 423\n",
      "2025-12-18 10:16:58.508333: Current learning rate: 0.0061\n",
      "2025-12-18 10:19:16.651973: train_loss -0.8376\n",
      "2025-12-18 10:19:16.651973: val_loss -0.8591\n",
      "2025-12-18 10:19:16.657981: Pseudo dice [0.9221, 0.9526, 0.9296]\n",
      "2025-12-18 10:19:16.661724: Epoch time: 138.16 s\n",
      "2025-12-18 10:19:17.404715: \n",
      "2025-12-18 10:19:17.404715: Epoch 424\n",
      "2025-12-18 10:19:17.404715: Current learning rate: 0.00609\n",
      "2025-12-18 10:21:35.601597: train_loss -0.8423\n",
      "2025-12-18 10:21:35.601597: val_loss -0.8571\n",
      "2025-12-18 10:21:35.603599: Pseudo dice [0.9188, 0.9532, 0.9292]\n",
      "2025-12-18 10:21:35.603599: Epoch time: 138.2 s\n",
      "2025-12-18 10:21:36.249660: \n",
      "2025-12-18 10:21:36.249660: Epoch 425\n",
      "2025-12-18 10:21:36.249660: Current learning rate: 0.00608\n",
      "2025-12-18 10:23:54.271398: train_loss -0.845\n",
      "2025-12-18 10:23:54.273139: val_loss -0.8608\n",
      "2025-12-18 10:23:54.281157: Pseudo dice [0.9201, 0.9574, 0.9323]\n",
      "2025-12-18 10:23:54.283159: Epoch time: 138.04 s\n",
      "2025-12-18 10:23:55.112036: \n",
      "2025-12-18 10:23:55.112036: Epoch 426\n",
      "2025-12-18 10:23:55.112036: Current learning rate: 0.00607\n",
      "2025-12-18 10:26:13.226671: train_loss -0.8458\n",
      "2025-12-18 10:26:13.228673: val_loss -0.8691\n",
      "2025-12-18 10:26:13.234421: Pseudo dice [0.9279, 0.9568, 0.9353]\n",
      "2025-12-18 10:26:13.240169: Epoch time: 138.13 s\n",
      "2025-12-18 10:26:13.879282: \n",
      "2025-12-18 10:26:13.879282: Epoch 427\n",
      "2025-12-18 10:26:13.879282: Current learning rate: 0.00606\n",
      "2025-12-18 10:28:31.997320: train_loss -0.8419\n",
      "2025-12-18 10:28:31.999322: val_loss -0.8674\n",
      "2025-12-18 10:28:32.004328: Pseudo dice [0.9259, 0.9526, 0.9366]\n",
      "2025-12-18 10:28:32.006330: Epoch time: 138.12 s\n",
      "2025-12-18 10:28:32.637733: \n",
      "2025-12-18 10:28:32.637733: Epoch 428\n",
      "2025-12-18 10:28:32.653596: Current learning rate: 0.00605\n",
      "2025-12-18 10:30:50.612858: train_loss -0.8507\n",
      "2025-12-18 10:30:50.612858: val_loss -0.8686\n",
      "2025-12-18 10:30:50.612858: Pseudo dice [0.9264, 0.9554, 0.9342]\n",
      "2025-12-18 10:30:50.612858: Epoch time: 137.98 s\n",
      "2025-12-18 10:30:51.246716: \n",
      "2025-12-18 10:30:51.246716: Epoch 429\n",
      "2025-12-18 10:30:51.246716: Current learning rate: 0.00604\n",
      "2025-12-18 10:33:09.268438: train_loss -0.8506\n",
      "2025-12-18 10:33:09.270440: val_loss -0.8726\n",
      "2025-12-18 10:33:09.273941: Pseudo dice [0.9237, 0.9542, 0.9495]\n",
      "2025-12-18 10:33:09.277945: Epoch time: 138.02 s\n",
      "2025-12-18 10:33:09.913430: \n",
      "2025-12-18 10:33:09.913430: Epoch 430\n",
      "2025-12-18 10:33:09.913430: Current learning rate: 0.00603\n",
      "2025-12-18 10:35:28.154698: train_loss -0.8432\n",
      "2025-12-18 10:35:28.160511: val_loss -0.8574\n",
      "2025-12-18 10:35:28.164301: Pseudo dice [0.9182, 0.9497, 0.9269]\n",
      "2025-12-18 10:35:28.166303: Epoch time: 138.24 s\n",
      "2025-12-18 10:35:28.826666: \n",
      "2025-12-18 10:35:28.826666: Epoch 431\n",
      "2025-12-18 10:35:28.826666: Current learning rate: 0.00602\n",
      "2025-12-18 10:37:46.712497: train_loss -0.8413\n",
      "2025-12-18 10:37:46.714504: val_loss -0.8656\n",
      "2025-12-18 10:37:46.722529: Pseudo dice [0.9202, 0.9532, 0.942]\n",
      "2025-12-18 10:37:46.724532: Epoch time: 137.89 s\n",
      "2025-12-18 10:37:47.522300: \n",
      "2025-12-18 10:37:47.522300: Epoch 432\n",
      "2025-12-18 10:37:47.522300: Current learning rate: 0.00601\n",
      "2025-12-18 10:40:05.507398: train_loss -0.8445\n",
      "2025-12-18 10:40:05.509400: val_loss -0.8534\n",
      "2025-12-18 10:40:05.515144: Pseudo dice [0.915, 0.9511, 0.9298]\n",
      "2025-12-18 10:40:05.519148: Epoch time: 137.99 s\n",
      "2025-12-18 10:40:06.151946: \n",
      "2025-12-18 10:40:06.151946: Epoch 433\n",
      "2025-12-18 10:40:06.151946: Current learning rate: 0.006\n",
      "2025-12-18 10:42:24.244630: train_loss -0.8445\n",
      "2025-12-18 10:42:24.244630: val_loss -0.8703\n",
      "2025-12-18 10:42:24.248634: Pseudo dice [0.924, 0.9543, 0.9445]\n",
      "2025-12-18 10:42:24.252639: Epoch time: 138.09 s\n",
      "2025-12-18 10:42:24.880161: \n",
      "2025-12-18 10:42:24.880161: Epoch 434\n",
      "2025-12-18 10:42:24.896139: Current learning rate: 0.00599\n",
      "2025-12-18 10:44:43.080672: train_loss -0.8483\n",
      "2025-12-18 10:44:43.080672: val_loss -0.8691\n",
      "2025-12-18 10:44:43.080672: Pseudo dice [0.926, 0.9576, 0.9369]\n",
      "2025-12-18 10:44:43.086492: Epoch time: 138.2 s\n",
      "2025-12-18 10:44:43.722371: \n",
      "2025-12-18 10:44:43.722371: Epoch 435\n",
      "2025-12-18 10:44:43.736137: Current learning rate: 0.00598\n",
      "2025-12-18 10:47:01.792627: train_loss -0.8446\n",
      "2025-12-18 10:47:01.792627: val_loss -0.8679\n",
      "2025-12-18 10:47:01.803255: Pseudo dice [0.9255, 0.9537, 0.9297]\n",
      "2025-12-18 10:47:01.806630: Epoch time: 138.07 s\n",
      "2025-12-18 10:47:02.439448: \n",
      "2025-12-18 10:47:02.439448: Epoch 436\n",
      "2025-12-18 10:47:02.451903: Current learning rate: 0.00597\n",
      "2025-12-18 10:49:20.448446: train_loss -0.8492\n",
      "2025-12-18 10:49:20.448446: val_loss -0.8619\n",
      "2025-12-18 10:49:20.452450: Pseudo dice [0.916, 0.9503, 0.9438]\n",
      "2025-12-18 10:49:20.456454: Epoch time: 138.01 s\n",
      "2025-12-18 10:49:21.095295: \n",
      "2025-12-18 10:49:21.095295: Epoch 437\n",
      "2025-12-18 10:49:21.108652: Current learning rate: 0.00596\n",
      "2025-12-18 10:51:38.973546: train_loss -0.849\n",
      "2025-12-18 10:51:38.973546: val_loss -0.8696\n",
      "2025-12-18 10:51:38.977550: Pseudo dice [0.9226, 0.9538, 0.9368]\n",
      "2025-12-18 10:51:38.981292: Epoch time: 137.88 s\n",
      "2025-12-18 10:51:39.616805: \n",
      "2025-12-18 10:51:39.616805: Epoch 438\n",
      "2025-12-18 10:51:39.616805: Current learning rate: 0.00595\n",
      "2025-12-18 10:53:57.612839: train_loss -0.8451\n",
      "2025-12-18 10:53:57.612839: val_loss -0.8616\n",
      "2025-12-18 10:53:57.618370: Pseudo dice [0.9212, 0.95, 0.9364]\n",
      "2025-12-18 10:53:57.618370: Epoch time: 138.0 s\n",
      "2025-12-18 10:53:58.422051: \n",
      "2025-12-18 10:53:58.422051: Epoch 439\n",
      "2025-12-18 10:53:58.422051: Current learning rate: 0.00594\n",
      "2025-12-18 10:56:16.507634: train_loss -0.8432\n",
      "2025-12-18 10:56:16.507634: val_loss -0.8601\n",
      "2025-12-18 10:56:16.511848: Pseudo dice [0.9173, 0.9509, 0.9359]\n",
      "2025-12-18 10:56:16.513851: Epoch time: 138.09 s\n",
      "2025-12-18 10:56:17.140599: \n",
      "2025-12-18 10:56:17.140599: Epoch 440\n",
      "2025-12-18 10:56:17.156240: Current learning rate: 0.00593\n",
      "2025-12-18 10:58:35.381214: train_loss -0.8428\n",
      "2025-12-18 10:58:35.381214: val_loss -0.8769\n",
      "2025-12-18 10:58:35.381214: Pseudo dice [0.9336, 0.9641, 0.9337]\n",
      "2025-12-18 10:58:35.381214: Epoch time: 138.24 s\n",
      "2025-12-18 10:58:36.017851: \n",
      "2025-12-18 10:58:36.017851: Epoch 441\n",
      "2025-12-18 10:58:36.017851: Current learning rate: 0.00592\n",
      "2025-12-18 11:00:54.207352: train_loss -0.8433\n",
      "2025-12-18 11:00:54.209356: val_loss -0.8652\n",
      "2025-12-18 11:00:54.215366: Pseudo dice [0.9223, 0.9557, 0.9371]\n",
      "2025-12-18 11:00:54.219371: Epoch time: 138.19 s\n",
      "2025-12-18 11:00:54.846083: \n",
      "2025-12-18 11:00:54.846083: Epoch 442\n",
      "2025-12-18 11:00:54.846083: Current learning rate: 0.00592\n",
      "2025-12-18 11:03:13.025616: train_loss -0.842\n",
      "2025-12-18 11:03:13.025616: val_loss -0.8714\n",
      "2025-12-18 11:03:13.029394: Pseudo dice [0.9287, 0.957, 0.9347]\n",
      "2025-12-18 11:03:13.034797: Epoch time: 138.18 s\n",
      "2025-12-18 11:03:13.732346: \n",
      "2025-12-18 11:03:13.732346: Epoch 443\n",
      "2025-12-18 11:03:13.732346: Current learning rate: 0.00591\n",
      "2025-12-18 11:05:32.012022: train_loss -0.8451\n",
      "2025-12-18 11:05:32.014024: val_loss -0.8611\n",
      "2025-12-18 11:05:32.018028: Pseudo dice [0.9161, 0.9515, 0.9396]\n",
      "2025-12-18 11:05:32.018028: Epoch time: 138.28 s\n",
      "2025-12-18 11:05:32.638608: \n",
      "2025-12-18 11:05:32.638608: Epoch 444\n",
      "2025-12-18 11:05:32.638608: Current learning rate: 0.0059\n",
      "2025-12-18 11:07:50.648018: train_loss -0.8481\n",
      "2025-12-18 11:07:50.648018: val_loss -0.8621\n",
      "2025-12-18 11:07:50.652022: Pseudo dice [0.9167, 0.9514, 0.9415]\n",
      "2025-12-18 11:07:50.656026: Epoch time: 138.01 s\n",
      "2025-12-18 11:07:51.475727: \n",
      "2025-12-18 11:07:51.475727: Epoch 445\n",
      "2025-12-18 11:07:51.491704: Current learning rate: 0.00589\n",
      "2025-12-18 11:10:10.015862: train_loss -0.8429\n",
      "2025-12-18 11:10:10.015862: val_loss -0.8643\n",
      "2025-12-18 11:10:10.015862: Pseudo dice [0.9232, 0.9545, 0.9316]\n",
      "2025-12-18 11:10:10.015862: Epoch time: 138.54 s\n",
      "2025-12-18 11:10:10.634641: \n",
      "2025-12-18 11:10:10.634641: Epoch 446\n",
      "2025-12-18 11:10:10.634641: Current learning rate: 0.00588\n",
      "2025-12-18 11:12:28.936885: train_loss -0.8481\n",
      "2025-12-18 11:12:28.936885: val_loss -0.8656\n",
      "2025-12-18 11:12:28.942389: Pseudo dice [0.9229, 0.952, 0.9319]\n",
      "2025-12-18 11:12:28.944392: Epoch time: 138.3 s\n",
      "2025-12-18 11:12:29.568880: \n",
      "2025-12-18 11:12:29.568880: Epoch 447\n",
      "2025-12-18 11:12:29.568880: Current learning rate: 0.00587\n",
      "2025-12-18 11:14:47.857288: train_loss -0.8462\n",
      "2025-12-18 11:14:47.857288: val_loss -0.8738\n",
      "2025-12-18 11:14:47.857288: Pseudo dice [0.9257, 0.9599, 0.9391]\n",
      "2025-12-18 11:14:47.857288: Epoch time: 138.29 s\n",
      "2025-12-18 11:14:48.489811: \n",
      "2025-12-18 11:14:48.489811: Epoch 448\n",
      "2025-12-18 11:14:48.489811: Current learning rate: 0.00586\n",
      "2025-12-18 11:17:06.577017: train_loss -0.851\n",
      "2025-12-18 11:17:06.577017: val_loss -0.8658\n",
      "2025-12-18 11:17:06.577017: Pseudo dice [0.9227, 0.9541, 0.9383]\n",
      "2025-12-18 11:17:06.577017: Epoch time: 138.09 s\n",
      "2025-12-18 11:17:07.305127: \n",
      "2025-12-18 11:17:07.305127: Epoch 449\n",
      "2025-12-18 11:17:07.305127: Current learning rate: 0.00585\n",
      "2025-12-18 11:19:25.524264: train_loss -0.8485\n",
      "2025-12-18 11:19:25.524264: val_loss -0.8699\n",
      "2025-12-18 11:19:25.524264: Pseudo dice [0.9243, 0.9544, 0.9371]\n",
      "2025-12-18 11:19:25.524264: Epoch time: 138.22 s\n",
      "2025-12-18 11:19:25.777805: Yayy! New best EMA pseudo Dice: 0.9376\n",
      "2025-12-18 11:19:26.699834: \n",
      "2025-12-18 11:19:26.699834: Epoch 450\n",
      "2025-12-18 11:19:26.699834: Current learning rate: 0.00584\n",
      "2025-12-18 11:21:44.900062: train_loss -0.8475\n",
      "2025-12-18 11:21:44.900062: val_loss -0.8525\n",
      "2025-12-18 11:21:44.900062: Pseudo dice [0.915, 0.9519, 0.9302]\n",
      "2025-12-18 11:21:44.907980: Epoch time: 138.2 s\n",
      "2025-12-18 11:21:45.682548: \n",
      "2025-12-18 11:21:45.682548: Epoch 451\n",
      "2025-12-18 11:21:45.682548: Current learning rate: 0.00583\n",
      "2025-12-18 11:24:03.967228: train_loss -0.8409\n",
      "2025-12-18 11:24:03.967228: val_loss -0.8724\n",
      "2025-12-18 11:24:03.971232: Pseudo dice [0.924, 0.9538, 0.9453]\n",
      "2025-12-18 11:24:03.974974: Epoch time: 138.28 s\n",
      "2025-12-18 11:24:04.724457: \n",
      "2025-12-18 11:24:04.724457: Epoch 452\n",
      "2025-12-18 11:24:04.724457: Current learning rate: 0.00582\n",
      "2025-12-18 11:26:23.126930: train_loss -0.8496\n",
      "2025-12-18 11:26:23.126930: val_loss -0.867\n",
      "2025-12-18 11:26:23.126930: Pseudo dice [0.9251, 0.9515, 0.9408]\n",
      "2025-12-18 11:26:23.136832: Epoch time: 138.4 s\n",
      "2025-12-18 11:26:23.138834: Yayy! New best EMA pseudo Dice: 0.9376\n",
      "2025-12-18 11:26:23.989047: \n",
      "2025-12-18 11:26:23.989047: Epoch 453\n",
      "2025-12-18 11:26:24.004971: Current learning rate: 0.00581\n",
      "2025-12-18 11:28:42.089580: train_loss -0.8505\n",
      "2025-12-18 11:28:42.089580: val_loss -0.8649\n",
      "2025-12-18 11:28:42.089580: Pseudo dice [0.9251, 0.9565, 0.9257]\n",
      "2025-12-18 11:28:42.103578: Epoch time: 138.1 s\n",
      "2025-12-18 11:28:42.769293: \n",
      "2025-12-18 11:28:42.769293: Epoch 454\n",
      "2025-12-18 11:28:42.785058: Current learning rate: 0.0058\n",
      "2025-12-18 11:31:01.134575: train_loss -0.8489\n",
      "2025-12-18 11:31:01.134575: val_loss -0.8641\n",
      "2025-12-18 11:31:01.150278: Pseudo dice [0.9191, 0.9522, 0.9379]\n",
      "2025-12-18 11:31:01.150278: Epoch time: 138.37 s\n",
      "2025-12-18 11:31:01.860000: \n",
      "2025-12-18 11:31:01.860000: Epoch 455\n",
      "2025-12-18 11:31:01.860000: Current learning rate: 0.00579\n",
      "2025-12-18 11:33:19.980186: train_loss -0.8397\n",
      "2025-12-18 11:33:19.980186: val_loss -0.868\n",
      "2025-12-18 11:33:19.980186: Pseudo dice [0.9239, 0.9572, 0.9322]\n",
      "2025-12-18 11:33:19.990115: Epoch time: 138.12 s\n",
      "2025-12-18 11:33:20.610359: \n",
      "2025-12-18 11:33:20.610359: Epoch 456\n",
      "2025-12-18 11:33:20.610359: Current learning rate: 0.00578\n",
      "2025-12-18 11:35:38.782939: train_loss -0.8464\n",
      "2025-12-18 11:35:38.782939: val_loss -0.8621\n",
      "2025-12-18 11:35:38.798629: Pseudo dice [0.9196, 0.954, 0.9296]\n",
      "2025-12-18 11:35:38.798629: Epoch time: 138.17 s\n",
      "2025-12-18 11:35:39.415000: \n",
      "2025-12-18 11:35:39.415000: Epoch 457\n",
      "2025-12-18 11:35:39.415000: Current learning rate: 0.00577\n",
      "2025-12-18 11:37:57.955194: train_loss -0.8447\n",
      "2025-12-18 11:37:57.955194: val_loss -0.8744\n",
      "2025-12-18 11:37:57.955194: Pseudo dice [0.9282, 0.9605, 0.9361]\n",
      "2025-12-18 11:37:57.960891: Epoch time: 138.54 s\n",
      "2025-12-18 11:37:58.578360: \n",
      "2025-12-18 11:37:58.578360: Epoch 458\n",
      "2025-12-18 11:37:58.594135: Current learning rate: 0.00576\n",
      "2025-12-18 11:40:16.725323: train_loss -0.8471\n",
      "2025-12-18 11:40:16.725323: val_loss -0.8761\n",
      "2025-12-18 11:40:16.727326: Pseudo dice [0.9303, 0.9577, 0.9413]\n",
      "2025-12-18 11:40:16.735666: Epoch time: 138.15 s\n",
      "2025-12-18 11:40:16.739408: Yayy! New best EMA pseudo Dice: 0.9381\n",
      "2025-12-18 11:40:17.625828: \n",
      "2025-12-18 11:40:17.625828: Epoch 459\n",
      "2025-12-18 11:40:17.625828: Current learning rate: 0.00575\n",
      "2025-12-18 11:42:35.749142: train_loss -0.8311\n",
      "2025-12-18 11:42:35.749142: val_loss -0.8355\n",
      "2025-12-18 11:42:35.751144: Pseudo dice [0.9131, 0.9472, 0.9057]\n",
      "2025-12-18 11:42:35.757598: Epoch time: 138.12 s\n",
      "2025-12-18 11:42:36.373256: \n",
      "2025-12-18 11:42:36.373256: Epoch 460\n",
      "2025-12-18 11:42:36.373256: Current learning rate: 0.00574\n",
      "2025-12-18 11:44:54.650226: train_loss -0.8128\n",
      "2025-12-18 11:44:54.650226: val_loss -0.8495\n",
      "2025-12-18 11:44:54.650226: Pseudo dice [0.9224, 0.9504, 0.9216]\n",
      "2025-12-18 11:44:54.650226: Epoch time: 138.28 s\n",
      "2025-12-18 11:44:55.279124: \n",
      "2025-12-18 11:44:55.279124: Epoch 461\n",
      "2025-12-18 11:44:55.279124: Current learning rate: 0.00573\n",
      "2025-12-18 11:47:13.387806: train_loss -0.8219\n",
      "2025-12-18 11:47:13.387806: val_loss -0.8517\n",
      "2025-12-18 11:47:13.391810: Pseudo dice [0.9148, 0.955, 0.9285]\n",
      "2025-12-18 11:47:13.395814: Epoch time: 138.11 s\n",
      "2025-12-18 11:47:14.006574: \n",
      "2025-12-18 11:47:14.006574: Epoch 462\n",
      "2025-12-18 11:47:14.006574: Current learning rate: 0.00572\n",
      "2025-12-18 11:49:31.995778: train_loss -0.8205\n",
      "2025-12-18 11:49:31.995778: val_loss -0.8457\n",
      "2025-12-18 11:49:31.995778: Pseudo dice [0.9136, 0.9467, 0.927]\n",
      "2025-12-18 11:49:32.011506: Epoch time: 137.99 s\n",
      "2025-12-18 11:49:32.630095: \n",
      "2025-12-18 11:49:32.630095: Epoch 463\n",
      "2025-12-18 11:49:32.630095: Current learning rate: 0.00571\n",
      "2025-12-18 11:51:50.910917: train_loss -0.8273\n",
      "2025-12-18 11:51:50.910917: val_loss -0.8569\n",
      "2025-12-18 11:51:50.912919: Pseudo dice [0.9206, 0.9543, 0.9256]\n",
      "2025-12-18 11:51:50.919971: Epoch time: 138.3 s\n",
      "2025-12-18 11:51:51.711695: \n",
      "2025-12-18 11:51:51.711695: Epoch 464\n",
      "2025-12-18 11:51:51.711695: Current learning rate: 0.0057\n",
      "2025-12-18 11:54:09.737303: train_loss -0.8412\n",
      "2025-12-18 11:54:09.737303: val_loss -0.8532\n",
      "2025-12-18 11:54:09.753033: Pseudo dice [0.9129, 0.9492, 0.934]\n",
      "2025-12-18 11:54:09.753033: Epoch time: 138.04 s\n",
      "2025-12-18 11:54:10.370485: \n",
      "2025-12-18 11:54:10.370485: Epoch 465\n",
      "2025-12-18 11:54:10.370485: Current learning rate: 0.0057\n",
      "2025-12-18 11:56:28.398163: train_loss -0.8467\n",
      "2025-12-18 11:56:28.400164: val_loss -0.8697\n",
      "2025-12-18 11:56:28.403906: Pseudo dice [0.9247, 0.9556, 0.937]\n",
      "2025-12-18 11:56:28.405909: Epoch time: 138.03 s\n",
      "2025-12-18 11:56:29.020868: \n",
      "2025-12-18 11:56:29.036650: Epoch 466\n",
      "2025-12-18 11:56:29.036650: Current learning rate: 0.00569\n",
      "2025-12-18 11:58:47.089376: train_loss -0.8431\n",
      "2025-12-18 11:58:47.089376: val_loss -0.8499\n",
      "2025-12-18 11:58:47.105304: Pseudo dice [0.9093, 0.9465, 0.9371]\n",
      "2025-12-18 11:58:47.105304: Epoch time: 138.07 s\n",
      "2025-12-18 11:58:47.722813: \n",
      "2025-12-18 11:58:47.722813: Epoch 467\n",
      "2025-12-18 11:58:47.722813: Current learning rate: 0.00568\n",
      "2025-12-18 12:01:05.925377: train_loss -0.842\n",
      "2025-12-18 12:01:05.925377: val_loss -0.8616\n",
      "2025-12-18 12:01:05.943363: Pseudo dice [0.9258, 0.9525, 0.9199]\n",
      "2025-12-18 12:01:05.943363: Epoch time: 138.2 s\n",
      "2025-12-18 12:01:06.574481: \n",
      "2025-12-18 12:01:06.574481: Epoch 468\n",
      "2025-12-18 12:01:06.574481: Current learning rate: 0.00567\n",
      "2025-12-18 12:03:24.563176: train_loss -0.838\n",
      "2025-12-18 12:03:24.563176: val_loss -0.8594\n",
      "2025-12-18 12:03:24.579154: Pseudo dice [0.921, 0.9489, 0.9288]\n",
      "2025-12-18 12:03:24.579154: Epoch time: 137.99 s\n",
      "2025-12-18 12:03:25.198149: \n",
      "2025-12-18 12:03:25.198149: Epoch 469\n",
      "2025-12-18 12:03:25.198149: Current learning rate: 0.00566\n",
      "2025-12-18 12:05:43.238272: train_loss -0.8388\n",
      "2025-12-18 12:05:43.240274: val_loss -0.866\n",
      "2025-12-18 12:05:43.244016: Pseudo dice [0.9244, 0.9559, 0.9369]\n",
      "2025-12-18 12:05:43.246018: Epoch time: 138.04 s\n",
      "2025-12-18 12:05:44.035948: \n",
      "2025-12-18 12:05:44.035948: Epoch 470\n",
      "2025-12-18 12:05:44.035948: Current learning rate: 0.00565\n",
      "2025-12-18 12:08:01.977583: train_loss -0.8447\n",
      "2025-12-18 12:08:01.977583: val_loss -0.8714\n",
      "2025-12-18 12:08:01.977583: Pseudo dice [0.927, 0.958, 0.9353]\n",
      "2025-12-18 12:08:01.977583: Epoch time: 137.94 s\n",
      "2025-12-18 12:08:02.627016: \n",
      "2025-12-18 12:08:02.627016: Epoch 471\n",
      "2025-12-18 12:08:02.627016: Current learning rate: 0.00564\n",
      "2025-12-18 12:10:21.119295: train_loss -0.8354\n",
      "2025-12-18 12:10:21.119295: val_loss -0.8719\n",
      "2025-12-18 12:10:21.119295: Pseudo dice [0.9318, 0.9575, 0.9316]\n",
      "2025-12-18 12:10:21.128544: Epoch time: 138.49 s\n",
      "2025-12-18 12:10:21.752741: \n",
      "2025-12-18 12:10:21.752741: Epoch 472\n",
      "2025-12-18 12:10:21.752741: Current learning rate: 0.00563\n",
      "2025-12-18 12:12:39.962618: train_loss -0.8452\n",
      "2025-12-18 12:12:39.962618: val_loss -0.8637\n",
      "2025-12-18 12:12:39.978352: Pseudo dice [0.9203, 0.9515, 0.9338]\n",
      "2025-12-18 12:12:39.978352: Epoch time: 138.21 s\n",
      "2025-12-18 12:12:40.596178: \n",
      "2025-12-18 12:12:40.596178: Epoch 473\n",
      "2025-12-18 12:12:40.611933: Current learning rate: 0.00562\n",
      "2025-12-18 12:14:58.799303: train_loss -0.8482\n",
      "2025-12-18 12:14:58.799303: val_loss -0.8644\n",
      "2025-12-18 12:14:58.805049: Pseudo dice [0.9171, 0.9492, 0.947]\n",
      "2025-12-18 12:14:58.809052: Epoch time: 138.2 s\n",
      "2025-12-18 12:14:59.499346: \n",
      "2025-12-18 12:14:59.499346: Epoch 474\n",
      "2025-12-18 12:14:59.501350: Current learning rate: 0.00561\n",
      "2025-12-18 12:17:17.757367: train_loss -0.8474\n",
      "2025-12-18 12:17:17.757367: val_loss -0.8672\n",
      "2025-12-18 12:17:17.764473: Pseudo dice [0.922, 0.9541, 0.9374]\n",
      "2025-12-18 12:17:17.769014: Epoch time: 138.26 s\n",
      "2025-12-18 12:17:18.393505: \n",
      "2025-12-18 12:17:18.393505: Epoch 475\n",
      "2025-12-18 12:17:18.393505: Current learning rate: 0.0056\n",
      "2025-12-18 12:19:36.533004: train_loss -0.8449\n",
      "2025-12-18 12:19:36.533004: val_loss -0.869\n",
      "2025-12-18 12:19:36.548755: Pseudo dice [0.9237, 0.9582, 0.9402]\n",
      "2025-12-18 12:19:36.548755: Epoch time: 138.14 s\n",
      "2025-12-18 12:19:37.182140: \n",
      "2025-12-18 12:19:37.182140: Epoch 476\n",
      "2025-12-18 12:19:37.182140: Current learning rate: 0.00559\n",
      "2025-12-18 12:21:55.128664: train_loss -0.8479\n",
      "2025-12-18 12:21:55.128664: val_loss -0.8568\n",
      "2025-12-18 12:21:55.130666: Pseudo dice [0.918, 0.9517, 0.9233]\n",
      "2025-12-18 12:21:55.130666: Epoch time: 137.95 s\n",
      "2025-12-18 12:21:56.073988: \n",
      "2025-12-18 12:21:56.073988: Epoch 477\n",
      "2025-12-18 12:21:56.073988: Current learning rate: 0.00558\n",
      "2025-12-18 12:24:14.144975: train_loss -0.8506\n",
      "2025-12-18 12:24:14.144975: val_loss -0.8561\n",
      "2025-12-18 12:24:14.144975: Pseudo dice [0.9158, 0.9502, 0.9358]\n",
      "2025-12-18 12:24:14.144975: Epoch time: 138.07 s\n",
      "2025-12-18 12:24:14.788019: \n",
      "2025-12-18 12:24:14.788019: Epoch 478\n",
      "2025-12-18 12:24:14.788019: Current learning rate: 0.00557\n",
      "2025-12-18 12:26:33.001388: train_loss -0.8449\n",
      "2025-12-18 12:26:33.001388: val_loss -0.8653\n",
      "2025-12-18 12:26:33.001388: Pseudo dice [0.9286, 0.9537, 0.9269]\n",
      "2025-12-18 12:26:33.001388: Epoch time: 138.21 s\n",
      "2025-12-18 12:26:33.633402: \n",
      "2025-12-18 12:26:33.649306: Epoch 479\n",
      "2025-12-18 12:26:33.649306: Current learning rate: 0.00556\n",
      "2025-12-18 12:28:51.744061: train_loss -0.8494\n",
      "2025-12-18 12:28:51.744061: val_loss -0.8567\n",
      "2025-12-18 12:28:51.759710: Pseudo dice [0.9235, 0.9466, 0.9171]\n",
      "2025-12-18 12:28:51.759710: Epoch time: 138.11 s\n",
      "2025-12-18 12:28:52.473194: \n",
      "2025-12-18 12:28:52.473194: Epoch 480\n",
      "2025-12-18 12:28:52.473194: Current learning rate: 0.00555\n",
      "2025-12-18 12:31:10.806619: train_loss -0.8509\n",
      "2025-12-18 12:31:10.806619: val_loss -0.8638\n",
      "2025-12-18 12:31:10.810363: Pseudo dice [0.9229, 0.9516, 0.9391]\n",
      "2025-12-18 12:31:10.817991: Epoch time: 138.33 s\n",
      "2025-12-18 12:31:11.457309: \n",
      "2025-12-18 12:31:11.457309: Epoch 481\n",
      "2025-12-18 12:31:11.457309: Current learning rate: 0.00554\n",
      "2025-12-18 12:33:29.610444: train_loss -0.8449\n",
      "2025-12-18 12:33:29.610444: val_loss -0.8672\n",
      "2025-12-18 12:33:29.610444: Pseudo dice [0.9214, 0.9525, 0.94]\n",
      "2025-12-18 12:33:29.610444: Epoch time: 138.17 s\n",
      "2025-12-18 12:33:30.241639: \n",
      "2025-12-18 12:33:30.241639: Epoch 482\n",
      "2025-12-18 12:33:30.241639: Current learning rate: 0.00553\n",
      "2025-12-18 12:35:48.384059: train_loss -0.8477\n",
      "2025-12-18 12:35:48.384059: val_loss -0.8687\n",
      "2025-12-18 12:35:48.391844: Pseudo dice [0.9283, 0.9579, 0.9312]\n",
      "2025-12-18 12:35:48.391844: Epoch time: 138.14 s\n",
      "2025-12-18 12:35:49.143054: \n",
      "2025-12-18 12:35:49.143054: Epoch 483\n",
      "2025-12-18 12:35:49.143054: Current learning rate: 0.00552\n",
      "2025-12-18 12:38:07.461284: train_loss -0.8465\n",
      "2025-12-18 12:38:07.463024: val_loss -0.8719\n",
      "2025-12-18 12:38:07.466876: Pseudo dice [0.9263, 0.9575, 0.9395]\n",
      "2025-12-18 12:38:07.470880: Epoch time: 138.32 s\n",
      "2025-12-18 12:38:08.271016: \n",
      "2025-12-18 12:38:08.271016: Epoch 484\n",
      "2025-12-18 12:38:08.271016: Current learning rate: 0.00551\n",
      "2025-12-18 12:40:26.369801: train_loss -0.849\n",
      "2025-12-18 12:40:26.369801: val_loss -0.8683\n",
      "2025-12-18 12:40:26.374567: Pseudo dice [0.9251, 0.9512, 0.9352]\n",
      "2025-12-18 12:40:26.377571: Epoch time: 138.1 s\n",
      "2025-12-18 12:40:27.010922: \n",
      "2025-12-18 12:40:27.010922: Epoch 485\n",
      "2025-12-18 12:40:27.026876: Current learning rate: 0.0055\n",
      "2025-12-18 12:42:45.213652: train_loss -0.8462\n",
      "2025-12-18 12:42:45.213652: val_loss -0.8585\n",
      "2025-12-18 12:42:45.213652: Pseudo dice [0.919, 0.9506, 0.9307]\n",
      "2025-12-18 12:42:45.213652: Epoch time: 138.2 s\n",
      "2025-12-18 12:42:45.971911: \n",
      "2025-12-18 12:42:45.971911: Epoch 486\n",
      "2025-12-18 12:42:45.987784: Current learning rate: 0.00549\n",
      "2025-12-18 12:45:04.288637: train_loss -0.8473\n",
      "2025-12-18 12:45:04.288637: val_loss -0.87\n",
      "2025-12-18 12:45:04.294780: Pseudo dice [0.927, 0.9534, 0.9368]\n",
      "2025-12-18 12:45:04.296782: Epoch time: 138.32 s\n",
      "2025-12-18 12:45:04.926422: \n",
      "2025-12-18 12:45:04.926422: Epoch 487\n",
      "2025-12-18 12:45:04.926422: Current learning rate: 0.00548\n",
      "2025-12-18 12:47:23.113395: train_loss -0.8524\n",
      "2025-12-18 12:47:23.113395: val_loss -0.8658\n",
      "2025-12-18 12:47:23.117180: Pseudo dice [0.9278, 0.9528, 0.9265]\n",
      "2025-12-18 12:47:23.121184: Epoch time: 138.19 s\n",
      "2025-12-18 12:47:23.742224: \n",
      "2025-12-18 12:47:23.742224: Epoch 488\n",
      "2025-12-18 12:47:23.758248: Current learning rate: 0.00547\n",
      "2025-12-18 12:49:41.921139: train_loss -0.849\n",
      "2025-12-18 12:49:41.921139: val_loss -0.8668\n",
      "2025-12-18 12:49:41.921139: Pseudo dice [0.9246, 0.9542, 0.9365]\n",
      "2025-12-18 12:49:41.931485: Epoch time: 138.18 s\n",
      "2025-12-18 12:49:42.571343: \n",
      "2025-12-18 12:49:42.571343: Epoch 489\n",
      "2025-12-18 12:49:42.571343: Current learning rate: 0.00546\n",
      "2025-12-18 12:52:00.787195: train_loss -0.8516\n",
      "2025-12-18 12:52:00.789197: val_loss -0.8676\n",
      "2025-12-18 12:52:00.793201: Pseudo dice [0.9258, 0.9531, 0.9254]\n",
      "2025-12-18 12:52:00.796700: Epoch time: 138.22 s\n",
      "2025-12-18 12:52:01.585509: \n",
      "2025-12-18 12:52:01.585509: Epoch 490\n",
      "2025-12-18 12:52:01.585509: Current learning rate: 0.00546\n",
      "2025-12-18 12:54:19.858731: train_loss -0.851\n",
      "2025-12-18 12:54:19.858731: val_loss -0.8674\n",
      "2025-12-18 12:54:19.864829: Pseudo dice [0.9211, 0.9554, 0.9389]\n",
      "2025-12-18 12:54:19.868571: Epoch time: 138.27 s\n",
      "2025-12-18 12:54:20.509961: \n",
      "2025-12-18 12:54:20.509961: Epoch 491\n",
      "2025-12-18 12:54:20.509961: Current learning rate: 0.00545\n",
      "2025-12-18 12:56:38.610074: train_loss -0.8462\n",
      "2025-12-18 12:56:38.610074: val_loss -0.8673\n",
      "2025-12-18 12:56:38.610074: Pseudo dice [0.9249, 0.9546, 0.9355]\n",
      "2025-12-18 12:56:38.610074: Epoch time: 138.1 s\n",
      "2025-12-18 12:56:39.245599: \n",
      "2025-12-18 12:56:39.247601: Epoch 492\n",
      "2025-12-18 12:56:39.247601: Current learning rate: 0.00544\n",
      "2025-12-18 12:58:57.339191: train_loss -0.846\n",
      "2025-12-18 12:58:57.339191: val_loss -0.8726\n",
      "2025-12-18 12:58:57.344799: Pseudo dice [0.9291, 0.9552, 0.9407]\n",
      "2025-12-18 12:58:57.348803: Epoch time: 138.09 s\n",
      "2025-12-18 12:58:58.034593: \n",
      "2025-12-18 12:58:58.034593: Epoch 493\n",
      "2025-12-18 12:58:58.034593: Current learning rate: 0.00543\n",
      "2025-12-18 13:01:16.168197: train_loss -0.8483\n",
      "2025-12-18 13:01:16.168197: val_loss -0.8695\n",
      "2025-12-18 13:01:16.177702: Pseudo dice [0.9246, 0.955, 0.9386]\n",
      "2025-12-18 13:01:16.181706: Epoch time: 138.13 s\n",
      "2025-12-18 13:01:16.812365: \n",
      "2025-12-18 13:01:16.812365: Epoch 494\n",
      "2025-12-18 13:01:16.828229: Current learning rate: 0.00542\n",
      "2025-12-18 13:03:34.973056: train_loss -0.8502\n",
      "2025-12-18 13:03:34.975059: val_loss -0.8632\n",
      "2025-12-18 13:03:34.977625: Pseudo dice [0.9177, 0.9485, 0.9379]\n",
      "2025-12-18 13:03:34.981630: Epoch time: 138.16 s\n",
      "2025-12-18 13:03:35.620454: \n",
      "2025-12-18 13:03:35.620454: Epoch 495\n",
      "2025-12-18 13:03:35.620454: Current learning rate: 0.00541\n",
      "2025-12-18 13:05:53.790537: train_loss -0.8481\n",
      "2025-12-18 13:05:53.790537: val_loss -0.8771\n",
      "2025-12-18 13:05:53.794541: Pseudo dice [0.9304, 0.9573, 0.9341]\n",
      "2025-12-18 13:05:53.796543: Epoch time: 138.17 s\n",
      "2025-12-18 13:05:54.599492: \n",
      "2025-12-18 13:05:54.599492: Epoch 496\n",
      "2025-12-18 13:05:54.599492: Current learning rate: 0.0054\n",
      "2025-12-18 13:08:12.819928: train_loss -0.8449\n",
      "2025-12-18 13:08:12.819928: val_loss -0.8675\n",
      "2025-12-18 13:08:12.830725: Pseudo dice [0.9193, 0.9576, 0.9472]\n",
      "2025-12-18 13:08:12.834729: Epoch time: 138.22 s\n",
      "2025-12-18 13:08:13.464091: \n",
      "2025-12-18 13:08:13.464091: Epoch 497\n",
      "2025-12-18 13:08:13.464091: Current learning rate: 0.00539\n",
      "2025-12-18 13:10:32.109277: train_loss -0.85\n",
      "2025-12-18 13:10:32.109277: val_loss -0.8555\n",
      "2025-12-18 13:10:32.113282: Pseudo dice [0.9168, 0.9497, 0.9294]\n",
      "2025-12-18 13:10:32.115285: Epoch time: 138.65 s\n",
      "2025-12-18 13:10:32.800131: \n",
      "2025-12-18 13:10:32.800131: Epoch 498\n",
      "2025-12-18 13:10:32.800131: Current learning rate: 0.00538\n",
      "2025-12-18 13:12:51.132421: train_loss -0.8476\n",
      "2025-12-18 13:12:51.134423: val_loss -0.8741\n",
      "2025-12-18 13:12:51.141578: Pseudo dice [0.9286, 0.9559, 0.942]\n",
      "2025-12-18 13:12:51.147588: Epoch time: 138.33 s\n",
      "2025-12-18 13:12:51.804003: \n",
      "2025-12-18 13:12:51.804003: Epoch 499\n",
      "2025-12-18 13:12:51.819917: Current learning rate: 0.00537\n",
      "2025-12-18 13:15:10.007685: train_loss -0.8523\n",
      "2025-12-18 13:15:10.007685: val_loss -0.8706\n",
      "2025-12-18 13:15:10.023394: Pseudo dice [0.9263, 0.9556, 0.9325]\n",
      "2025-12-18 13:15:10.028986: Epoch time: 138.2 s\n",
      "2025-12-18 13:15:10.911959: \n",
      "2025-12-18 13:15:10.911959: Epoch 500\n",
      "2025-12-18 13:15:10.911959: Current learning rate: 0.00536\n",
      "2025-12-18 13:17:29.049465: train_loss -0.8525\n",
      "2025-12-18 13:17:29.049465: val_loss -0.8734\n",
      "2025-12-18 13:17:29.049465: Pseudo dice [0.9286, 0.9557, 0.9325]\n",
      "2025-12-18 13:17:29.065250: Epoch time: 138.15 s\n",
      "2025-12-18 13:17:29.683789: \n",
      "2025-12-18 13:17:29.683789: Epoch 501\n",
      "2025-12-18 13:17:29.683789: Current learning rate: 0.00535\n",
      "2025-12-18 13:19:47.867064: train_loss -0.8532\n",
      "2025-12-18 13:19:47.867064: val_loss -0.8704\n",
      "2025-12-18 13:19:47.872813: Pseudo dice [0.9255, 0.955, 0.9373]\n",
      "2025-12-18 13:19:47.874816: Epoch time: 138.18 s\n",
      "2025-12-18 13:19:47.878822: Yayy! New best EMA pseudo Dice: 0.9381\n",
      "2025-12-18 13:19:49.103336: \n",
      "2025-12-18 13:19:49.103336: Epoch 502\n",
      "2025-12-18 13:19:49.108587: Current learning rate: 0.00534\n",
      "2025-12-18 13:22:07.393409: train_loss -0.8514\n",
      "2025-12-18 13:22:07.395150: val_loss -0.8661\n",
      "2025-12-18 13:22:07.399154: Pseudo dice [0.9213, 0.9525, 0.9405]\n",
      "2025-12-18 13:22:07.403158: Epoch time: 138.29 s\n",
      "2025-12-18 13:22:08.060032: \n",
      "2025-12-18 13:22:08.060032: Epoch 503\n",
      "2025-12-18 13:22:08.060032: Current learning rate: 0.00533\n",
      "2025-12-18 13:24:26.204594: train_loss -0.8516\n",
      "2025-12-18 13:24:26.204594: val_loss -0.8658\n",
      "2025-12-18 13:24:26.209810: Pseudo dice [0.9202, 0.9514, 0.9406]\n",
      "2025-12-18 13:24:26.213814: Epoch time: 138.14 s\n",
      "2025-12-18 13:24:26.854803: \n",
      "2025-12-18 13:24:26.856543: Epoch 504\n",
      "2025-12-18 13:24:26.856543: Current learning rate: 0.00532\n",
      "2025-12-18 13:26:45.159741: train_loss -0.8443\n",
      "2025-12-18 13:26:45.159741: val_loss -0.8642\n",
      "2025-12-18 13:26:45.163751: Pseudo dice [0.9241, 0.9568, 0.9367]\n",
      "2025-12-18 13:26:45.167762: Epoch time: 138.3 s\n",
      "2025-12-18 13:26:45.171770: Yayy! New best EMA pseudo Dice: 0.9382\n",
      "2025-12-18 13:26:46.209778: \n",
      "2025-12-18 13:26:46.209778: Epoch 505\n",
      "2025-12-18 13:26:46.214870: Current learning rate: 0.00531\n",
      "2025-12-18 13:29:04.527879: train_loss -0.8258\n",
      "2025-12-18 13:29:04.527879: val_loss -0.8415\n",
      "2025-12-18 13:29:04.543537: Pseudo dice [0.9123, 0.945, 0.9142]\n",
      "2025-12-18 13:29:04.543537: Epoch time: 138.32 s\n",
      "2025-12-18 13:29:05.181072: \n",
      "2025-12-18 13:29:05.181072: Epoch 506\n",
      "2025-12-18 13:29:05.185454: Current learning rate: 0.0053\n",
      "2025-12-18 13:31:23.417490: train_loss -0.8285\n",
      "2025-12-18 13:31:23.421012: val_loss -0.863\n",
      "2025-12-18 13:31:23.425016: Pseudo dice [0.9234, 0.9543, 0.9364]\n",
      "2025-12-18 13:31:23.429020: Epoch time: 138.24 s\n",
      "2025-12-18 13:31:24.067226: \n",
      "2025-12-18 13:31:24.067226: Epoch 507\n",
      "2025-12-18 13:31:24.067226: Current learning rate: 0.00529\n",
      "2025-12-18 13:33:42.190571: train_loss -0.8329\n",
      "2025-12-18 13:33:42.192573: val_loss -0.8464\n",
      "2025-12-18 13:33:42.192573: Pseudo dice [0.914, 0.9455, 0.9291]\n",
      "2025-12-18 13:33:42.199195: Epoch time: 138.13 s\n",
      "2025-12-18 13:33:43.020185: \n",
      "2025-12-18 13:33:43.020185: Epoch 508\n",
      "2025-12-18 13:33:43.020185: Current learning rate: 0.00528\n",
      "2025-12-18 13:36:01.180406: train_loss -0.8401\n",
      "2025-12-18 13:36:01.180406: val_loss -0.8633\n",
      "2025-12-18 13:36:01.186191: Pseudo dice [0.9249, 0.9531, 0.9311]\n",
      "2025-12-18 13:36:01.190194: Epoch time: 138.16 s\n",
      "2025-12-18 13:36:01.814685: \n",
      "2025-12-18 13:36:01.814685: Epoch 509\n",
      "2025-12-18 13:36:01.830492: Current learning rate: 0.00527\n",
      "2025-12-18 13:38:19.929430: train_loss -0.85\n",
      "2025-12-18 13:38:19.929430: val_loss -0.8623\n",
      "2025-12-18 13:38:19.945282: Pseudo dice [0.92, 0.9548, 0.9339]\n",
      "2025-12-18 13:38:19.945282: Epoch time: 138.11 s\n",
      "2025-12-18 13:38:20.579248: \n",
      "2025-12-18 13:38:20.579248: Epoch 510\n",
      "2025-12-18 13:38:20.579248: Current learning rate: 0.00526\n",
      "2025-12-18 13:40:38.915920: train_loss -0.8361\n",
      "2025-12-18 13:40:38.915920: val_loss -0.8565\n",
      "2025-12-18 13:40:38.931718: Pseudo dice [0.9186, 0.9481, 0.9283]\n",
      "2025-12-18 13:40:38.935724: Epoch time: 138.34 s\n",
      "2025-12-18 13:40:39.566089: \n",
      "2025-12-18 13:40:39.566089: Epoch 511\n",
      "2025-12-18 13:40:39.579645: Current learning rate: 0.00525\n",
      "2025-12-18 13:42:57.833025: train_loss -0.8369\n",
      "2025-12-18 13:42:57.833025: val_loss -0.867\n",
      "2025-12-18 13:42:57.850519: Pseudo dice [0.9268, 0.9539, 0.9375]\n",
      "2025-12-18 13:42:57.854524: Epoch time: 138.27 s\n",
      "2025-12-18 13:42:58.491547: \n",
      "2025-12-18 13:42:58.491547: Epoch 512\n",
      "2025-12-18 13:42:58.497220: Current learning rate: 0.00524\n",
      "2025-12-18 13:45:16.843328: train_loss -0.8398\n",
      "2025-12-18 13:45:16.843328: val_loss -0.8679\n",
      "2025-12-18 13:45:16.843328: Pseudo dice [0.9221, 0.9575, 0.9376]\n",
      "2025-12-18 13:45:16.843328: Epoch time: 138.35 s\n",
      "2025-12-18 13:45:17.491565: \n",
      "2025-12-18 13:45:17.491565: Epoch 513\n",
      "2025-12-18 13:45:17.491565: Current learning rate: 0.00523\n",
      "2025-12-18 13:47:35.650197: train_loss -0.8415\n",
      "2025-12-18 13:47:35.650197: val_loss -0.865\n",
      "2025-12-18 13:47:35.654201: Pseudo dice [0.9237, 0.953, 0.9368]\n",
      "2025-12-18 13:47:35.659542: Epoch time: 138.16 s\n",
      "2025-12-18 13:47:36.297035: \n",
      "2025-12-18 13:47:36.297035: Epoch 514\n",
      "2025-12-18 13:47:36.299945: Current learning rate: 0.00522\n",
      "2025-12-18 13:49:54.445931: train_loss -0.8436\n",
      "2025-12-18 13:49:54.445931: val_loss -0.8717\n",
      "2025-12-18 13:49:54.451431: Pseudo dice [0.9313, 0.9603, 0.9328]\n",
      "2025-12-18 13:49:54.455435: Epoch time: 138.15 s\n",
      "2025-12-18 13:49:55.264134: \n",
      "2025-12-18 13:49:55.264134: Epoch 515\n",
      "2025-12-18 13:49:55.264134: Current learning rate: 0.00521\n",
      "2025-12-18 13:52:13.400393: train_loss -0.846\n",
      "2025-12-18 13:52:13.400393: val_loss -0.8757\n",
      "2025-12-18 13:52:13.404397: Pseudo dice [0.9298, 0.9579, 0.9437]\n",
      "2025-12-18 13:52:13.408401: Epoch time: 138.14 s\n",
      "2025-12-18 13:52:14.031824: \n",
      "2025-12-18 13:52:14.031824: Epoch 516\n",
      "2025-12-18 13:52:14.047890: Current learning rate: 0.0052\n",
      "2025-12-18 13:54:32.090332: train_loss -0.8504\n",
      "2025-12-18 13:54:32.090332: val_loss -0.8693\n",
      "2025-12-18 13:54:32.094337: Pseudo dice [0.9238, 0.9564, 0.9362]\n",
      "2025-12-18 13:54:32.098078: Epoch time: 138.06 s\n",
      "2025-12-18 13:54:32.743400: \n",
      "2025-12-18 13:54:32.743400: Epoch 517\n",
      "2025-12-18 13:54:32.743400: Current learning rate: 0.00519\n",
      "2025-12-18 13:56:50.897816: train_loss -0.846\n",
      "2025-12-18 13:56:50.897816: val_loss -0.8676\n",
      "2025-12-18 13:56:50.897816: Pseudo dice [0.9228, 0.956, 0.9371]\n",
      "2025-12-18 13:56:50.897816: Epoch time: 138.16 s\n",
      "2025-12-18 13:56:51.531254: \n",
      "2025-12-18 13:56:51.531254: Epoch 518\n",
      "2025-12-18 13:56:51.531254: Current learning rate: 0.00518\n",
      "2025-12-18 13:59:09.426850: train_loss -0.853\n",
      "2025-12-18 13:59:09.426850: val_loss -0.8729\n",
      "2025-12-18 13:59:09.426850: Pseudo dice [0.9276, 0.955, 0.938]\n",
      "2025-12-18 13:59:09.426850: Epoch time: 137.9 s\n",
      "2025-12-18 13:59:10.063828: \n",
      "2025-12-18 13:59:10.063828: Epoch 519\n",
      "2025-12-18 13:59:10.063828: Current learning rate: 0.00518\n",
      "2025-12-18 14:01:28.462538: train_loss -0.8446\n",
      "2025-12-18 14:01:28.462538: val_loss -0.872\n",
      "2025-12-18 14:01:28.462538: Pseudo dice [0.9261, 0.9552, 0.9442]\n",
      "2025-12-18 14:01:28.476547: Epoch time: 138.4 s\n",
      "2025-12-18 14:01:28.476547: Yayy! New best EMA pseudo Dice: 0.9385\n",
      "2025-12-18 14:01:29.381099: \n",
      "2025-12-18 14:01:29.381099: Epoch 520\n",
      "2025-12-18 14:01:29.381099: Current learning rate: 0.00517\n",
      "2025-12-18 14:03:47.724641: train_loss -0.8444\n",
      "2025-12-18 14:03:47.726384: val_loss -0.8698\n",
      "2025-12-18 14:03:47.728390: Pseudo dice [0.9263, 0.9581, 0.9403]\n",
      "2025-12-18 14:03:47.736358: Epoch time: 138.34 s\n",
      "2025-12-18 14:03:47.740364: Yayy! New best EMA pseudo Dice: 0.9388\n",
      "2025-12-18 14:03:48.976796: \n",
      "2025-12-18 14:03:48.976796: Epoch 521\n",
      "2025-12-18 14:03:48.976796: Current learning rate: 0.00516\n",
      "2025-12-18 14:06:07.329633: train_loss -0.8432\n",
      "2025-12-18 14:06:07.331636: val_loss -0.8597\n",
      "2025-12-18 14:06:07.335380: Pseudo dice [0.9184, 0.9482, 0.9415]\n",
      "2025-12-18 14:06:07.339122: Epoch time: 138.35 s\n",
      "2025-12-18 14:06:07.968460: \n",
      "2025-12-18 14:06:07.968460: Epoch 522\n",
      "2025-12-18 14:06:07.982478: Current learning rate: 0.00515\n",
      "2025-12-18 14:08:26.087900: train_loss -0.8502\n",
      "2025-12-18 14:08:26.087900: val_loss -0.8659\n",
      "2025-12-18 14:08:26.103620: Pseudo dice [0.9219, 0.9551, 0.9378]\n",
      "2025-12-18 14:08:26.103620: Epoch time: 138.12 s\n",
      "2025-12-18 14:08:26.734394: \n",
      "2025-12-18 14:08:26.734394: Epoch 523\n",
      "2025-12-18 14:08:26.734394: Current learning rate: 0.00514\n",
      "2025-12-18 14:10:45.381907: train_loss -0.8496\n",
      "2025-12-18 14:10:45.381907: val_loss -0.8693\n",
      "2025-12-18 14:10:45.381907: Pseudo dice [0.9298, 0.9562, 0.9314]\n",
      "2025-12-18 14:10:45.381907: Epoch time: 138.65 s\n",
      "2025-12-18 14:10:46.142076: \n",
      "2025-12-18 14:10:46.142076: Epoch 524\n",
      "2025-12-18 14:10:46.142076: Current learning rate: 0.00513\n",
      "2025-12-18 14:13:04.377200: train_loss -0.8392\n",
      "2025-12-18 14:13:04.377200: val_loss -0.865\n",
      "2025-12-18 14:13:04.382848: Pseudo dice [0.9216, 0.9534, 0.9321]\n",
      "2025-12-18 14:13:04.384850: Epoch time: 138.24 s\n",
      "2025-12-18 14:13:05.013309: \n",
      "2025-12-18 14:13:05.013309: Epoch 525\n",
      "2025-12-18 14:13:05.013309: Current learning rate: 0.00512\n",
      "2025-12-18 14:15:23.263728: train_loss -0.8533\n",
      "2025-12-18 14:15:23.263728: val_loss -0.8706\n",
      "2025-12-18 14:15:23.263728: Pseudo dice [0.9267, 0.9545, 0.9433]\n",
      "2025-12-18 14:15:23.263728: Epoch time: 138.25 s\n",
      "2025-12-18 14:15:23.910851: \n",
      "2025-12-18 14:15:23.910851: Epoch 526\n",
      "2025-12-18 14:15:23.910851: Current learning rate: 0.00511\n",
      "2025-12-18 14:17:42.058389: train_loss -0.8478\n",
      "2025-12-18 14:17:42.058389: val_loss -0.8732\n",
      "2025-12-18 14:17:42.063394: Pseudo dice [0.9275, 0.9585, 0.9425]\n",
      "2025-12-18 14:17:42.067398: Epoch time: 138.15 s\n",
      "2025-12-18 14:17:42.071402: Yayy! New best EMA pseudo Dice: 0.939\n",
      "2025-12-18 14:17:43.187655: \n",
      "2025-12-18 14:17:43.187655: Epoch 527\n",
      "2025-12-18 14:17:43.187655: Current learning rate: 0.0051\n",
      "2025-12-18 14:20:01.500448: train_loss -0.8516\n",
      "2025-12-18 14:20:01.500448: val_loss -0.8743\n",
      "2025-12-18 14:20:01.506481: Pseudo dice [0.9258, 0.9589, 0.9384]\n",
      "2025-12-18 14:20:01.508972: Epoch time: 138.31 s\n",
      "2025-12-18 14:20:01.508972: Yayy! New best EMA pseudo Dice: 0.9392\n",
      "2025-12-18 14:20:02.421916: \n",
      "2025-12-18 14:20:02.421916: Epoch 528\n",
      "2025-12-18 14:20:02.424615: Current learning rate: 0.00509\n",
      "2025-12-18 14:22:20.614163: train_loss -0.8526\n",
      "2025-12-18 14:22:20.614163: val_loss -0.8739\n",
      "2025-12-18 14:22:20.619543: Pseudo dice [0.9265, 0.9576, 0.9493]\n",
      "2025-12-18 14:22:20.623547: Epoch time: 138.19 s\n",
      "2025-12-18 14:22:20.625549: Yayy! New best EMA pseudo Dice: 0.9397\n",
      "2025-12-18 14:22:21.536318: \n",
      "2025-12-18 14:22:21.536318: Epoch 529\n",
      "2025-12-18 14:22:21.538059: Current learning rate: 0.00508\n",
      "2025-12-18 14:24:39.640200: train_loss -0.8542\n",
      "2025-12-18 14:24:39.640200: val_loss -0.868\n",
      "2025-12-18 14:24:39.647951: Pseudo dice [0.9218, 0.9531, 0.9446]\n",
      "2025-12-18 14:24:39.653957: Epoch time: 138.11 s\n",
      "2025-12-18 14:24:39.657700: Yayy! New best EMA pseudo Dice: 0.9397\n",
      "2025-12-18 14:24:40.600073: \n",
      "2025-12-18 14:24:40.600073: Epoch 530\n",
      "2025-12-18 14:24:40.606395: Current learning rate: 0.00507\n",
      "2025-12-18 14:26:58.867301: train_loss -0.8502\n",
      "2025-12-18 14:26:58.867301: val_loss -0.8693\n",
      "2025-12-18 14:26:58.867301: Pseudo dice [0.9241, 0.9538, 0.9351]\n",
      "2025-12-18 14:26:58.867301: Epoch time: 138.27 s\n",
      "2025-12-18 14:26:59.516491: \n",
      "2025-12-18 14:26:59.516491: Epoch 531\n",
      "2025-12-18 14:26:59.516491: Current learning rate: 0.00506\n",
      "2025-12-18 14:29:17.599818: train_loss -0.8505\n",
      "2025-12-18 14:29:17.599818: val_loss -0.867\n",
      "2025-12-18 14:29:17.605824: Pseudo dice [0.9234, 0.9555, 0.9335]\n",
      "2025-12-18 14:29:17.609630: Epoch time: 138.09 s\n",
      "2025-12-18 14:29:18.398371: \n",
      "2025-12-18 14:29:18.398371: Epoch 532\n",
      "2025-12-18 14:29:18.414258: Current learning rate: 0.00505\n",
      "2025-12-18 14:31:36.467502: train_loss -0.8554\n",
      "2025-12-18 14:31:36.467502: val_loss -0.8686\n",
      "2025-12-18 14:31:36.471507: Pseudo dice [0.9214, 0.9535, 0.9342]\n",
      "2025-12-18 14:31:36.475511: Epoch time: 138.07 s\n",
      "2025-12-18 14:31:37.147798: \n",
      "2025-12-18 14:31:37.147798: Epoch 533\n",
      "2025-12-18 14:31:37.153806: Current learning rate: 0.00504\n",
      "2025-12-18 14:33:55.340987: train_loss -0.85\n",
      "2025-12-18 14:33:55.340987: val_loss -0.8697\n",
      "2025-12-18 14:33:55.345988: Pseudo dice [0.926, 0.9547, 0.9418]\n",
      "2025-12-18 14:33:55.349992: Epoch time: 138.2 s\n",
      "2025-12-18 14:33:55.975237: \n",
      "2025-12-18 14:33:55.975237: Epoch 534\n",
      "2025-12-18 14:33:55.975237: Current learning rate: 0.00503\n",
      "2025-12-18 14:36:14.119441: train_loss -0.852\n",
      "2025-12-18 14:36:14.119441: val_loss -0.8728\n",
      "2025-12-18 14:36:14.125447: Pseudo dice [0.9253, 0.9548, 0.9398]\n",
      "2025-12-18 14:36:14.127187: Epoch time: 138.14 s\n",
      "2025-12-18 14:36:14.760217: \n",
      "2025-12-18 14:36:14.760217: Epoch 535\n",
      "2025-12-18 14:36:14.776296: Current learning rate: 0.00502\n",
      "2025-12-18 14:38:32.923566: train_loss -0.8525\n",
      "2025-12-18 14:38:32.923566: val_loss -0.8752\n",
      "2025-12-18 14:38:32.923566: Pseudo dice [0.9301, 0.9623, 0.9261]\n",
      "2025-12-18 14:38:32.923566: Epoch time: 138.16 s\n",
      "2025-12-18 14:38:33.586439: \n",
      "2025-12-18 14:38:33.586439: Epoch 536\n",
      "2025-12-18 14:38:33.586439: Current learning rate: 0.00501\n",
      "2025-12-18 14:40:51.964427: train_loss -0.8525\n",
      "2025-12-18 14:40:51.964427: val_loss -0.8726\n",
      "2025-12-18 14:40:51.964427: Pseudo dice [0.9272, 0.9544, 0.9352]\n",
      "2025-12-18 14:40:51.982119: Epoch time: 138.39 s\n",
      "2025-12-18 14:40:52.660733: \n",
      "2025-12-18 14:40:52.660733: Epoch 537\n",
      "2025-12-18 14:40:52.666740: Current learning rate: 0.005\n",
      "2025-12-18 14:43:10.958302: train_loss -0.8531\n",
      "2025-12-18 14:43:10.958302: val_loss -0.8683\n",
      "2025-12-18 14:43:10.964041: Pseudo dice [0.9258, 0.9556, 0.9339]\n",
      "2025-12-18 14:43:10.966588: Epoch time: 138.3 s\n",
      "2025-12-18 14:43:11.787082: \n",
      "2025-12-18 14:43:11.787082: Epoch 538\n",
      "2025-12-18 14:43:11.787082: Current learning rate: 0.00499\n",
      "2025-12-18 14:45:30.127215: train_loss -0.8506\n",
      "2025-12-18 14:45:30.129217: val_loss -0.8728\n",
      "2025-12-18 14:45:30.134962: Pseudo dice [0.9236, 0.9514, 0.9419]\n",
      "2025-12-18 14:45:30.138960: Epoch time: 138.34 s\n",
      "2025-12-18 14:45:30.798331: \n",
      "2025-12-18 14:45:30.798331: Epoch 539\n",
      "2025-12-18 14:45:30.813230: Current learning rate: 0.00498\n",
      "2025-12-18 14:47:49.043811: train_loss -0.8476\n",
      "2025-12-18 14:47:49.043811: val_loss -0.8636\n",
      "2025-12-18 14:47:49.043811: Pseudo dice [0.9252, 0.9526, 0.9238]\n",
      "2025-12-18 14:47:49.057525: Epoch time: 138.25 s\n",
      "2025-12-18 14:47:49.819394: \n",
      "2025-12-18 14:47:49.819394: Epoch 540\n",
      "2025-12-18 14:47:49.826931: Current learning rate: 0.00497\n",
      "2025-12-18 14:50:07.989081: train_loss -0.8575\n",
      "2025-12-18 14:50:07.989081: val_loss -0.8667\n",
      "2025-12-18 14:50:07.993367: Pseudo dice [0.9233, 0.9557, 0.9318]\n",
      "2025-12-18 14:50:07.998061: Epoch time: 138.17 s\n",
      "2025-12-18 14:50:08.673465: \n",
      "2025-12-18 14:50:08.673465: Epoch 541\n",
      "2025-12-18 14:50:08.679472: Current learning rate: 0.00496\n",
      "2025-12-18 14:52:26.855009: train_loss -0.8517\n",
      "2025-12-18 14:52:26.855009: val_loss -0.8772\n",
      "2025-12-18 14:52:26.863911: Pseudo dice [0.9311, 0.9543, 0.9366]\n",
      "2025-12-18 14:52:26.866827: Epoch time: 138.18 s\n",
      "2025-12-18 14:52:27.503072: \n",
      "2025-12-18 14:52:27.503072: Epoch 542\n",
      "2025-12-18 14:52:27.503072: Current learning rate: 0.00495\n",
      "2025-12-18 14:54:45.799654: train_loss -0.849\n",
      "2025-12-18 14:54:45.801656: val_loss -0.86\n",
      "2025-12-18 14:54:45.807663: Pseudo dice [0.9193, 0.95, 0.936]\n",
      "2025-12-18 14:54:45.811667: Epoch time: 138.3 s\n",
      "2025-12-18 14:54:46.637610: \n",
      "2025-12-18 14:54:46.638613: Epoch 543\n",
      "2025-12-18 14:54:46.638613: Current learning rate: 0.00494\n",
      "2025-12-18 14:57:04.901243: train_loss -0.8525\n",
      "2025-12-18 14:57:04.901243: val_loss -0.8691\n",
      "2025-12-18 14:57:04.903246: Pseudo dice [0.9266, 0.9588, 0.929]\n",
      "2025-12-18 14:57:04.903246: Epoch time: 138.28 s\n",
      "2025-12-18 14:57:05.752933: \n",
      "2025-12-18 14:57:05.752933: Epoch 544\n",
      "2025-12-18 14:57:05.761597: Current learning rate: 0.00493\n",
      "2025-12-18 14:59:23.772689: train_loss -0.8527\n",
      "2025-12-18 14:59:23.772689: val_loss -0.873\n",
      "2025-12-18 14:59:23.777026: Pseudo dice [0.926, 0.9532, 0.9393]\n",
      "2025-12-18 14:59:23.777026: Epoch time: 138.02 s\n",
      "2025-12-18 14:59:24.405889: \n",
      "2025-12-18 14:59:24.405889: Epoch 545\n",
      "2025-12-18 14:59:24.405889: Current learning rate: 0.00492\n",
      "2025-12-18 15:01:42.607708: train_loss -0.8526\n",
      "2025-12-18 15:01:42.607708: val_loss -0.875\n",
      "2025-12-18 15:01:42.613577: Pseudo dice [0.9307, 0.9549, 0.9333]\n",
      "2025-12-18 15:01:42.617283: Epoch time: 138.2 s\n",
      "2025-12-18 15:01:43.353938: \n",
      "2025-12-18 15:01:43.353938: Epoch 546\n",
      "2025-12-18 15:01:43.353938: Current learning rate: 0.00491\n",
      "2025-12-18 15:04:01.693153: train_loss -0.8487\n",
      "2025-12-18 15:04:01.693153: val_loss -0.8718\n",
      "2025-12-18 15:04:01.703167: Pseudo dice [0.9254, 0.9597, 0.9378]\n",
      "2025-12-18 15:04:01.708914: Epoch time: 138.34 s\n",
      "2025-12-18 15:04:02.353001: \n",
      "2025-12-18 15:04:02.353001: Epoch 547\n",
      "2025-12-18 15:04:02.353001: Current learning rate: 0.0049\n",
      "2025-12-18 15:06:20.527568: train_loss -0.8503\n",
      "2025-12-18 15:06:20.527568: val_loss -0.8691\n",
      "2025-12-18 15:06:20.527568: Pseudo dice [0.9256, 0.9568, 0.9377]\n",
      "2025-12-18 15:06:20.535102: Epoch time: 138.19 s\n",
      "2025-12-18 15:06:21.168416: \n",
      "2025-12-18 15:06:21.168416: Epoch 548\n",
      "2025-12-18 15:06:21.181495: Current learning rate: 0.00489\n",
      "2025-12-18 15:08:39.414988: train_loss -0.8519\n",
      "2025-12-18 15:08:39.414988: val_loss -0.8698\n",
      "2025-12-18 15:08:39.424865: Pseudo dice [0.9268, 0.9573, 0.9259]\n",
      "2025-12-18 15:08:39.424865: Epoch time: 138.25 s\n",
      "2025-12-18 15:08:40.074930: \n",
      "2025-12-18 15:08:40.074930: Epoch 549\n",
      "2025-12-18 15:08:40.074930: Current learning rate: 0.00488\n",
      "2025-12-18 15:10:58.500413: train_loss -0.8526\n",
      "2025-12-18 15:10:58.500413: val_loss -0.8631\n",
      "2025-12-18 15:10:58.500413: Pseudo dice [0.9197, 0.9521, 0.938]\n",
      "2025-12-18 15:10:58.516202: Epoch time: 138.43 s\n",
      "2025-12-18 15:10:59.578264: \n",
      "2025-12-18 15:10:59.578264: Epoch 550\n",
      "2025-12-18 15:10:59.592236: Current learning rate: 0.00487\n",
      "2025-12-18 15:13:17.880475: train_loss -0.852\n",
      "2025-12-18 15:13:17.880475: val_loss -0.8703\n",
      "2025-12-18 15:13:17.880475: Pseudo dice [0.928, 0.9559, 0.9263]\n",
      "2025-12-18 15:13:17.896307: Epoch time: 138.3 s\n",
      "2025-12-18 15:13:18.529600: \n",
      "2025-12-18 15:13:18.529600: Epoch 551\n",
      "2025-12-18 15:13:18.529600: Current learning rate: 0.00486\n",
      "2025-12-18 15:15:36.644338: train_loss -0.8507\n",
      "2025-12-18 15:15:36.646341: val_loss -0.8653\n",
      "2025-12-18 15:15:36.650344: Pseudo dice [0.9264, 0.9531, 0.9254]\n",
      "2025-12-18 15:15:36.654349: Epoch time: 138.11 s\n",
      "2025-12-18 15:15:37.288036: \n",
      "2025-12-18 15:15:37.288036: Epoch 552\n",
      "2025-12-18 15:15:37.288036: Current learning rate: 0.00485\n",
      "2025-12-18 15:17:55.421947: train_loss -0.8499\n",
      "2025-12-18 15:17:55.423949: val_loss -0.8727\n",
      "2025-12-18 15:17:55.427691: Pseudo dice [0.9269, 0.9586, 0.9309]\n",
      "2025-12-18 15:17:55.432932: Epoch time: 138.13 s\n",
      "2025-12-18 15:17:56.061899: \n",
      "2025-12-18 15:17:56.061899: Epoch 553\n",
      "2025-12-18 15:17:56.077992: Current learning rate: 0.00484\n",
      "2025-12-18 15:20:14.181849: train_loss -0.8575\n",
      "2025-12-18 15:20:14.181849: val_loss -0.8765\n",
      "2025-12-18 15:20:14.187593: Pseudo dice [0.931, 0.9584, 0.9408]\n",
      "2025-12-18 15:20:14.193606: Epoch time: 138.12 s\n",
      "2025-12-18 15:20:14.826824: \n",
      "2025-12-18 15:20:14.826824: Epoch 554\n",
      "2025-12-18 15:20:14.826824: Current learning rate: 0.00484\n",
      "2025-12-18 15:22:33.078112: train_loss -0.8512\n",
      "2025-12-18 15:22:33.080115: val_loss -0.8647\n",
      "2025-12-18 15:22:33.086120: Pseudo dice [0.92, 0.9521, 0.9426]\n",
      "2025-12-18 15:22:33.089601: Epoch time: 138.25 s\n",
      "2025-12-18 15:22:33.785987: \n",
      "2025-12-18 15:22:33.785987: Epoch 555\n",
      "2025-12-18 15:22:33.785987: Current learning rate: 0.00483\n",
      "2025-12-18 15:24:51.896732: train_loss -0.8508\n",
      "2025-12-18 15:24:51.896732: val_loss -0.8709\n",
      "2025-12-18 15:24:51.912844: Pseudo dice [0.9234, 0.9532, 0.943]\n",
      "2025-12-18 15:24:51.912844: Epoch time: 138.11 s\n",
      "2025-12-18 15:24:52.548045: \n",
      "2025-12-18 15:24:52.548045: Epoch 556\n",
      "2025-12-18 15:24:52.548045: Current learning rate: 0.00482\n",
      "2025-12-18 15:27:10.649963: train_loss -0.8537\n",
      "2025-12-18 15:27:10.651967: val_loss -0.8795\n",
      "2025-12-18 15:27:10.657712: Pseudo dice [0.9326, 0.9595, 0.9392]\n",
      "2025-12-18 15:27:10.661717: Epoch time: 138.1 s\n",
      "2025-12-18 15:27:11.475682: \n",
      "2025-12-18 15:27:11.475682: Epoch 557\n",
      "2025-12-18 15:27:11.475682: Current learning rate: 0.00481\n",
      "2025-12-18 15:29:29.639265: train_loss -0.8545\n",
      "2025-12-18 15:29:29.639265: val_loss -0.8786\n",
      "2025-12-18 15:29:29.655029: Pseudo dice [0.9338, 0.9589, 0.9376]\n",
      "2025-12-18 15:29:29.658154: Epoch time: 138.18 s\n",
      "2025-12-18 15:29:30.272498: \n",
      "2025-12-18 15:29:30.272498: Epoch 558\n",
      "2025-12-18 15:29:30.290245: Current learning rate: 0.0048\n",
      "2025-12-18 15:31:48.448303: train_loss -0.8582\n",
      "2025-12-18 15:31:48.448303: val_loss -0.871\n",
      "2025-12-18 15:31:48.463979: Pseudo dice [0.9249, 0.9523, 0.9405]\n",
      "2025-12-18 15:31:48.463979: Epoch time: 138.18 s\n",
      "2025-12-18 15:31:49.210170: \n",
      "2025-12-18 15:31:49.210170: Epoch 559\n",
      "2025-12-18 15:31:49.229824: Current learning rate: 0.00479\n",
      "2025-12-18 15:34:07.459706: train_loss -0.8502\n",
      "2025-12-18 15:34:07.459706: val_loss -0.8849\n",
      "2025-12-18 15:34:07.471808: Pseudo dice [0.9358, 0.959, 0.9405]\n",
      "2025-12-18 15:34:07.475640: Epoch time: 138.25 s\n",
      "2025-12-18 15:34:07.481122: Yayy! New best EMA pseudo Dice: 0.9401\n",
      "2025-12-18 15:34:08.362849: \n",
      "2025-12-18 15:34:08.362849: Epoch 560\n",
      "2025-12-18 15:34:08.367888: Current learning rate: 0.00478\n",
      "2025-12-18 15:36:26.433963: train_loss -0.8534\n",
      "2025-12-18 15:36:26.433963: val_loss -0.872\n",
      "2025-12-18 15:36:26.433963: Pseudo dice [0.9278, 0.9586, 0.933]\n",
      "2025-12-18 15:36:26.445263: Epoch time: 138.09 s\n",
      "2025-12-18 15:36:27.069426: \n",
      "2025-12-18 15:36:27.069426: Epoch 561\n",
      "2025-12-18 15:36:27.080712: Current learning rate: 0.00477\n",
      "2025-12-18 15:38:45.289845: train_loss -0.852\n",
      "2025-12-18 15:38:45.291847: val_loss -0.8776\n",
      "2025-12-18 15:38:45.297857: Pseudo dice [0.9279, 0.9584, 0.9394]\n",
      "2025-12-18 15:38:45.303864: Epoch time: 138.22 s\n",
      "2025-12-18 15:38:45.307610: Yayy! New best EMA pseudo Dice: 0.9403\n",
      "2025-12-18 15:38:46.413704: \n",
      "2025-12-18 15:38:46.413704: Epoch 562\n",
      "2025-12-18 15:38:46.423188: Current learning rate: 0.00476\n",
      "2025-12-18 15:41:04.511634: train_loss -0.8495\n",
      "2025-12-18 15:41:04.511634: val_loss -0.8744\n",
      "2025-12-18 15:41:04.527485: Pseudo dice [0.9281, 0.958, 0.9359]\n",
      "2025-12-18 15:41:04.527485: Epoch time: 138.1 s\n",
      "2025-12-18 15:41:04.527485: Yayy! New best EMA pseudo Dice: 0.9403\n",
      "2025-12-18 15:41:05.638316: \n",
      "2025-12-18 15:41:05.638316: Epoch 563\n",
      "2025-12-18 15:41:05.638316: Current learning rate: 0.00475\n",
      "2025-12-18 15:43:24.008212: train_loss -0.8433\n",
      "2025-12-18 15:43:24.008212: val_loss -0.8597\n",
      "2025-12-18 15:43:24.008212: Pseudo dice [0.9205, 0.9474, 0.9361]\n",
      "2025-12-18 15:43:24.021045: Epoch time: 138.37 s\n",
      "2025-12-18 15:43:24.642272: \n",
      "2025-12-18 15:43:24.642272: Epoch 564\n",
      "2025-12-18 15:43:24.642272: Current learning rate: 0.00474\n",
      "2025-12-18 15:45:42.819368: train_loss -0.8492\n",
      "2025-12-18 15:45:42.821370: val_loss -0.8595\n",
      "2025-12-18 15:45:42.827115: Pseudo dice [0.9163, 0.9517, 0.9338]\n",
      "2025-12-18 15:45:42.833125: Epoch time: 138.18 s\n",
      "2025-12-18 15:45:43.585644: \n",
      "2025-12-18 15:45:43.585644: Epoch 565\n",
      "2025-12-18 15:45:43.585644: Current learning rate: 0.00473\n",
      "2025-12-18 15:48:01.842137: train_loss -0.8487\n",
      "2025-12-18 15:48:01.842137: val_loss -0.8614\n",
      "2025-12-18 15:48:01.851400: Pseudo dice [0.9149, 0.9522, 0.9497]\n",
      "2025-12-18 15:48:01.854003: Epoch time: 138.27 s\n",
      "2025-12-18 15:48:02.477058: \n",
      "2025-12-18 15:48:02.477058: Epoch 566\n",
      "2025-12-18 15:48:02.477058: Current learning rate: 0.00472\n",
      "2025-12-18 15:50:20.579287: train_loss -0.8524\n",
      "2025-12-18 15:50:20.579287: val_loss -0.8745\n",
      "2025-12-18 15:50:20.579287: Pseudo dice [0.927, 0.9588, 0.9392]\n",
      "2025-12-18 15:50:20.595210: Epoch time: 138.1 s\n",
      "2025-12-18 15:50:21.213186: \n",
      "2025-12-18 15:50:21.213186: Epoch 567\n",
      "2025-12-18 15:50:21.213186: Current learning rate: 0.00471\n",
      "2025-12-18 15:52:39.355666: train_loss -0.8525\n",
      "2025-12-18 15:52:39.355666: val_loss -0.8645\n",
      "2025-12-18 15:52:39.361515: Pseudo dice [0.9201, 0.9514, 0.9259]\n",
      "2025-12-18 15:52:39.365519: Epoch time: 138.14 s\n",
      "2025-12-18 15:52:40.020193: \n",
      "2025-12-18 15:52:40.020193: Epoch 568\n",
      "2025-12-18 15:52:40.020193: Current learning rate: 0.0047\n",
      "2025-12-18 15:54:58.098520: train_loss -0.8477\n",
      "2025-12-18 15:54:58.100523: val_loss -0.8741\n",
      "2025-12-18 15:54:58.106527: Pseudo dice [0.9265, 0.9578, 0.9431]\n",
      "2025-12-18 15:54:58.110271: Epoch time: 138.08 s\n",
      "2025-12-18 15:54:58.898498: \n",
      "2025-12-18 15:54:58.898498: Epoch 569\n",
      "2025-12-18 15:54:58.898498: Current learning rate: 0.00469\n",
      "2025-12-18 15:57:16.996345: train_loss -0.8489\n",
      "2025-12-18 15:57:16.996345: val_loss -0.876\n",
      "2025-12-18 15:57:16.996345: Pseudo dice [0.9268, 0.9574, 0.9417]\n",
      "2025-12-18 15:57:16.996345: Epoch time: 138.1 s\n",
      "2025-12-18 15:57:17.645946: \n",
      "2025-12-18 15:57:17.645946: Epoch 570\n",
      "2025-12-18 15:57:17.645946: Current learning rate: 0.00468\n",
      "2025-12-18 15:59:35.683884: train_loss -0.8524\n",
      "2025-12-18 15:59:35.683884: val_loss -0.8747\n",
      "2025-12-18 15:59:35.690356: Pseudo dice [0.9264, 0.9557, 0.9401]\n",
      "2025-12-18 15:59:35.690356: Epoch time: 138.04 s\n",
      "2025-12-18 15:59:36.317224: \n",
      "2025-12-18 15:59:36.317224: Epoch 571\n",
      "2025-12-18 15:59:36.317224: Current learning rate: 0.00467\n",
      "2025-12-18 16:01:54.157398: train_loss -0.8499\n",
      "2025-12-18 16:01:54.157398: val_loss -0.8732\n",
      "2025-12-18 16:01:54.163941: Pseudo dice [0.9248, 0.9553, 0.9431]\n",
      "2025-12-18 16:01:54.165944: Epoch time: 137.84 s\n",
      "2025-12-18 16:01:54.812905: \n",
      "2025-12-18 16:01:54.812905: Epoch 572\n",
      "2025-12-18 16:01:54.814740: Current learning rate: 0.00466\n",
      "2025-12-18 16:04:13.026212: train_loss -0.8522\n",
      "2025-12-18 16:04:13.042257: val_loss -0.8771\n",
      "2025-12-18 16:04:13.046263: Pseudo dice [0.9258, 0.9608, 0.9398]\n",
      "2025-12-18 16:04:13.046263: Epoch time: 138.23 s\n",
      "2025-12-18 16:04:13.690636: \n",
      "2025-12-18 16:04:13.690636: Epoch 573\n",
      "2025-12-18 16:04:13.690636: Current learning rate: 0.00465\n",
      "2025-12-18 16:06:31.865442: train_loss -0.8535\n",
      "2025-12-18 16:06:31.865442: val_loss -0.8785\n",
      "2025-12-18 16:06:31.865442: Pseudo dice [0.9297, 0.9582, 0.9424]\n",
      "2025-12-18 16:06:31.865442: Epoch time: 138.17 s\n",
      "2025-12-18 16:06:32.513530: \n",
      "2025-12-18 16:06:32.513530: Epoch 574\n",
      "2025-12-18 16:06:32.513530: Current learning rate: 0.00464\n",
      "2025-12-18 16:08:50.761085: train_loss -0.8505\n",
      "2025-12-18 16:08:50.761085: val_loss -0.8694\n",
      "2025-12-18 16:08:50.776826: Pseudo dice [0.9274, 0.9566, 0.9361]\n",
      "2025-12-18 16:08:50.776826: Epoch time: 138.25 s\n",
      "2025-12-18 16:08:51.587600: \n",
      "2025-12-18 16:08:51.587600: Epoch 575\n",
      "2025-12-18 16:08:51.587600: Current learning rate: 0.00463\n",
      "2025-12-18 16:11:10.014717: train_loss -0.8486\n",
      "2025-12-18 16:11:10.015719: val_loss -0.8732\n",
      "2025-12-18 16:11:10.022081: Pseudo dice [0.9308, 0.9563, 0.9319]\n",
      "2025-12-18 16:11:10.024084: Epoch time: 138.43 s\n",
      "2025-12-18 16:11:10.665179: \n",
      "2025-12-18 16:11:10.665179: Epoch 576\n",
      "2025-12-18 16:11:10.680902: Current learning rate: 0.00462\n",
      "2025-12-18 16:13:28.811864: train_loss -0.85\n",
      "2025-12-18 16:13:28.811864: val_loss -0.8753\n",
      "2025-12-18 16:13:28.811864: Pseudo dice [0.9284, 0.9562, 0.9379]\n",
      "2025-12-18 16:13:28.811864: Epoch time: 138.15 s\n",
      "2025-12-18 16:13:29.460743: \n",
      "2025-12-18 16:13:29.460743: Epoch 577\n",
      "2025-12-18 16:13:29.460743: Current learning rate: 0.00461\n",
      "2025-12-18 16:15:47.634049: train_loss -0.8594\n",
      "2025-12-18 16:15:47.634049: val_loss -0.8806\n",
      "2025-12-18 16:15:47.639795: Pseudo dice [0.9308, 0.9577, 0.9415]\n",
      "2025-12-18 16:15:47.643799: Epoch time: 138.17 s\n",
      "2025-12-18 16:15:47.649805: Yayy! New best EMA pseudo Dice: 0.9406\n",
      "2025-12-18 16:15:48.537421: \n",
      "2025-12-18 16:15:48.537421: Epoch 578\n",
      "2025-12-18 16:15:48.537421: Current learning rate: 0.0046\n",
      "2025-12-18 16:18:06.750077: train_loss -0.8564\n",
      "2025-12-18 16:18:06.752080: val_loss -0.8709\n",
      "2025-12-18 16:18:06.756426: Pseudo dice [0.9287, 0.9562, 0.9284]\n",
      "2025-12-18 16:18:06.756426: Epoch time: 138.21 s\n",
      "2025-12-18 16:18:07.385510: \n",
      "2025-12-18 16:18:07.385510: Epoch 579\n",
      "2025-12-18 16:18:07.401151: Current learning rate: 0.00459\n",
      "2025-12-18 16:20:25.528809: train_loss -0.8515\n",
      "2025-12-18 16:20:25.528809: val_loss -0.8739\n",
      "2025-12-18 16:20:25.534554: Pseudo dice [0.9304, 0.9574, 0.9352]\n",
      "2025-12-18 16:20:25.540564: Epoch time: 138.14 s\n",
      "2025-12-18 16:20:26.183773: \n",
      "2025-12-18 16:20:26.183773: Epoch 580\n",
      "2025-12-18 16:20:26.183773: Current learning rate: 0.00458\n",
      "2025-12-18 16:22:44.388247: train_loss -0.8492\n",
      "2025-12-18 16:22:44.388247: val_loss -0.8715\n",
      "2025-12-18 16:22:44.394253: Pseudo dice [0.9237, 0.957, 0.9374]\n",
      "2025-12-18 16:22:44.394253: Epoch time: 138.2 s\n",
      "2025-12-18 16:22:45.297508: \n",
      "2025-12-18 16:22:45.297508: Epoch 581\n",
      "2025-12-18 16:22:45.299510: Current learning rate: 0.00457\n",
      "2025-12-18 16:25:04.772915: train_loss -0.8561\n",
      "2025-12-18 16:25:04.772915: val_loss -0.8693\n",
      "2025-12-18 16:25:04.779443: Pseudo dice [0.9202, 0.956, 0.9419]\n",
      "2025-12-18 16:25:04.783447: Epoch time: 139.48 s\n",
      "2025-12-18 16:25:05.434590: \n",
      "2025-12-18 16:25:05.434590: Epoch 582\n",
      "2025-12-18 16:25:05.436592: Current learning rate: 0.00456\n",
      "2025-12-18 16:27:24.124233: train_loss -0.8553\n",
      "2025-12-18 16:27:24.124233: val_loss -0.8673\n",
      "2025-12-18 16:27:24.127975: Pseudo dice [0.9239, 0.9522, 0.9336]\n",
      "2025-12-18 16:27:24.132423: Epoch time: 138.69 s\n",
      "2025-12-18 16:27:24.838799: \n",
      "2025-12-18 16:27:24.838799: Epoch 583\n",
      "2025-12-18 16:27:24.838799: Current learning rate: 0.00455\n",
      "2025-12-18 16:29:43.441510: train_loss -0.8495\n",
      "2025-12-18 16:29:43.441510: val_loss -0.8753\n",
      "2025-12-18 16:29:43.457572: Pseudo dice [0.9287, 0.956, 0.9406]\n",
      "2025-12-18 16:29:43.457572: Epoch time: 138.6 s\n",
      "2025-12-18 16:29:44.146452: \n",
      "2025-12-18 16:29:44.146452: Epoch 584\n",
      "2025-12-18 16:29:44.146452: Current learning rate: 0.00454\n",
      "2025-12-18 16:32:02.561142: train_loss -0.8516\n",
      "2025-12-18 16:32:02.563145: val_loss -0.8671\n",
      "2025-12-18 16:32:02.568152: Pseudo dice [0.9237, 0.9532, 0.934]\n",
      "2025-12-18 16:32:02.574095: Epoch time: 138.41 s\n",
      "2025-12-18 16:32:03.218064: \n",
      "2025-12-18 16:32:03.218064: Epoch 585\n",
      "2025-12-18 16:32:03.220066: Current learning rate: 0.00453\n",
      "2025-12-18 16:34:22.006697: train_loss -0.8558\n",
      "2025-12-18 16:34:22.006697: val_loss -0.872\n",
      "2025-12-18 16:34:22.012291: Pseudo dice [0.9279, 0.9559, 0.9359]\n",
      "2025-12-18 16:34:22.018297: Epoch time: 138.79 s\n",
      "2025-12-18 16:34:22.673552: \n",
      "2025-12-18 16:34:22.675554: Epoch 586\n",
      "2025-12-18 16:34:22.678272: Current learning rate: 0.00452\n",
      "2025-12-18 16:36:40.838150: train_loss -0.8522\n",
      "2025-12-18 16:36:40.840152: val_loss -0.867\n",
      "2025-12-18 16:36:40.844156: Pseudo dice [0.9224, 0.956, 0.9339]\n",
      "2025-12-18 16:36:40.848160: Epoch time: 138.16 s\n",
      "2025-12-18 16:36:41.524133: \n",
      "2025-12-18 16:36:41.526136: Epoch 587\n",
      "2025-12-18 16:36:41.528140: Current learning rate: 0.00451\n",
      "2025-12-18 16:38:59.654215: train_loss -0.8538\n",
      "2025-12-18 16:38:59.656217: val_loss -0.8752\n",
      "2025-12-18 16:38:59.659960: Pseudo dice [0.9284, 0.957, 0.9344]\n",
      "2025-12-18 16:38:59.664934: Epoch time: 138.13 s\n",
      "2025-12-18 16:39:00.308227: \n",
      "2025-12-18 16:39:00.308227: Epoch 588\n",
      "2025-12-18 16:39:00.308227: Current learning rate: 0.0045\n",
      "2025-12-18 16:41:18.583876: train_loss -0.851\n",
      "2025-12-18 16:41:18.583876: val_loss -0.875\n",
      "2025-12-18 16:41:18.591861: Pseudo dice [0.9286, 0.9556, 0.9326]\n",
      "2025-12-18 16:41:18.595865: Epoch time: 138.28 s\n",
      "2025-12-18 16:41:19.295109: \n",
      "2025-12-18 16:41:19.295109: Epoch 589\n",
      "2025-12-18 16:41:19.299054: Current learning rate: 0.00449\n",
      "2025-12-18 16:43:37.575994: train_loss -0.8468\n",
      "2025-12-18 16:43:37.577996: val_loss -0.8696\n",
      "2025-12-18 16:43:37.585748: Pseudo dice [0.9254, 0.9562, 0.9339]\n",
      "2025-12-18 16:43:37.591761: Epoch time: 138.28 s\n",
      "2025-12-18 16:43:38.232454: \n",
      "2025-12-18 16:43:38.232454: Epoch 590\n",
      "2025-12-18 16:43:38.232454: Current learning rate: 0.00448\n",
      "2025-12-18 16:45:56.607162: train_loss -0.851\n",
      "2025-12-18 16:45:56.607162: val_loss -0.8704\n",
      "2025-12-18 16:45:56.613168: Pseudo dice [0.924, 0.9569, 0.9393]\n",
      "2025-12-18 16:45:56.617172: Epoch time: 138.37 s\n",
      "2025-12-18 16:45:57.312593: \n",
      "2025-12-18 16:45:57.312593: Epoch 591\n",
      "2025-12-18 16:45:57.312593: Current learning rate: 0.00447\n",
      "2025-12-18 16:48:15.516424: train_loss -0.8531\n",
      "2025-12-18 16:48:15.518426: val_loss -0.8713\n",
      "2025-12-18 16:48:15.522429: Pseudo dice [0.9243, 0.9552, 0.9393]\n",
      "2025-12-18 16:48:15.528090: Epoch time: 138.2 s\n",
      "2025-12-18 16:48:16.330054: \n",
      "2025-12-18 16:48:16.330054: Epoch 592\n",
      "2025-12-18 16:48:16.330054: Current learning rate: 0.00446\n",
      "2025-12-18 16:50:34.576110: train_loss -0.8379\n",
      "2025-12-18 16:50:34.576110: val_loss -0.8518\n",
      "2025-12-18 16:50:34.582118: Pseudo dice [0.919, 0.9471, 0.9212]\n",
      "2025-12-18 16:50:34.584120: Epoch time: 138.25 s\n",
      "2025-12-18 16:50:35.223254: \n",
      "2025-12-18 16:50:35.223254: Epoch 593\n",
      "2025-12-18 16:50:35.239221: Current learning rate: 0.00445\n",
      "2025-12-18 16:52:53.391540: train_loss -0.833\n",
      "2025-12-18 16:52:53.391540: val_loss -0.859\n",
      "2025-12-18 16:52:53.407254: Pseudo dice [0.9213, 0.9528, 0.9313]\n",
      "2025-12-18 16:52:53.407254: Epoch time: 138.17 s\n",
      "2025-12-18 16:52:54.163949: \n",
      "2025-12-18 16:52:54.163949: Epoch 594\n",
      "2025-12-18 16:52:54.179737: Current learning rate: 0.00444\n",
      "2025-12-18 16:55:12.265541: train_loss -0.8446\n",
      "2025-12-18 16:55:12.265541: val_loss -0.8726\n",
      "2025-12-18 16:55:12.271547: Pseudo dice [0.9271, 0.9567, 0.9351]\n",
      "2025-12-18 16:55:12.275551: Epoch time: 138.1 s\n",
      "2025-12-18 16:55:12.918669: \n",
      "2025-12-18 16:55:12.918669: Epoch 595\n",
      "2025-12-18 16:55:12.918669: Current learning rate: 0.00443\n",
      "2025-12-18 16:57:31.132443: train_loss -0.8484\n",
      "2025-12-18 16:57:31.132443: val_loss -0.8726\n",
      "2025-12-18 16:57:31.132443: Pseudo dice [0.9273, 0.9568, 0.9362]\n",
      "2025-12-18 16:57:31.132443: Epoch time: 138.22 s\n",
      "2025-12-18 16:57:31.780775: \n",
      "2025-12-18 16:57:31.780775: Epoch 596\n",
      "2025-12-18 16:57:31.782778: Current learning rate: 0.00442\n",
      "2025-12-18 16:59:49.877289: train_loss -0.8454\n",
      "2025-12-18 16:59:49.877289: val_loss -0.8768\n",
      "2025-12-18 16:59:49.887307: Pseudo dice [0.9309, 0.9549, 0.9416]\n",
      "2025-12-18 16:59:49.892829: Epoch time: 138.1 s\n",
      "2025-12-18 16:59:50.649357: \n",
      "2025-12-18 16:59:50.649357: Epoch 597\n",
      "2025-12-18 16:59:50.651360: Current learning rate: 0.00441\n",
      "2025-12-18 17:02:08.633016: train_loss -0.8533\n",
      "2025-12-18 17:02:08.633016: val_loss -0.871\n",
      "2025-12-18 17:02:08.637020: Pseudo dice [0.9228, 0.9543, 0.9408]\n",
      "2025-12-18 17:02:08.642026: Epoch time: 137.98 s\n",
      "2025-12-18 17:02:09.296490: \n",
      "2025-12-18 17:02:09.296490: Epoch 598\n",
      "2025-12-18 17:02:09.296490: Current learning rate: 0.0044\n",
      "2025-12-18 17:04:27.404639: train_loss -0.8458\n",
      "2025-12-18 17:04:27.404639: val_loss -0.8724\n",
      "2025-12-18 17:04:27.410645: Pseudo dice [0.9278, 0.9562, 0.9367]\n",
      "2025-12-18 17:04:27.412647: Epoch time: 138.11 s\n",
      "2025-12-18 17:04:28.239500: \n",
      "2025-12-18 17:04:28.239500: Epoch 599\n",
      "2025-12-18 17:04:28.239500: Current learning rate: 0.00439\n",
      "2025-12-18 17:06:46.397934: train_loss -0.852\n",
      "2025-12-18 17:06:46.397934: val_loss -0.8718\n",
      "2025-12-18 17:06:46.405437: Pseudo dice [0.9266, 0.9574, 0.9381]\n",
      "2025-12-18 17:06:46.411443: Epoch time: 138.16 s\n",
      "2025-12-18 17:06:47.544082: \n",
      "2025-12-18 17:06:47.546084: Epoch 600\n",
      "2025-12-18 17:06:47.548088: Current learning rate: 0.00438\n",
      "2025-12-18 17:09:05.812137: train_loss -0.8524\n",
      "2025-12-18 17:09:05.812137: val_loss -0.8754\n",
      "2025-12-18 17:09:05.820011: Pseudo dice [0.9265, 0.9594, 0.9424]\n",
      "2025-12-18 17:09:05.824540: Epoch time: 138.27 s\n",
      "2025-12-18 17:09:06.465018: \n",
      "2025-12-18 17:09:06.465018: Epoch 601\n",
      "2025-12-18 17:09:06.465018: Current learning rate: 0.00437\n",
      "2025-12-18 17:11:24.763754: train_loss -0.8538\n",
      "2025-12-18 17:11:24.763754: val_loss -0.8666\n",
      "2025-12-18 17:11:24.767759: Pseudo dice [0.9222, 0.9531, 0.9348]\n",
      "2025-12-18 17:11:24.771763: Epoch time: 138.3 s\n",
      "2025-12-18 17:11:25.414090: \n",
      "2025-12-18 17:11:25.414090: Epoch 602\n",
      "2025-12-18 17:11:25.431491: Current learning rate: 0.00436\n",
      "2025-12-18 17:13:43.582624: train_loss -0.851\n",
      "2025-12-18 17:13:43.582624: val_loss -0.8733\n",
      "2025-12-18 17:13:43.582624: Pseudo dice [0.9292, 0.9565, 0.9376]\n",
      "2025-12-18 17:13:43.582624: Epoch time: 138.17 s\n",
      "2025-12-18 17:13:44.371322: \n",
      "2025-12-18 17:13:44.371322: Epoch 603\n",
      "2025-12-18 17:13:44.382579: Current learning rate: 0.00435\n",
      "2025-12-18 17:16:02.562633: train_loss -0.8567\n",
      "2025-12-18 17:16:02.564635: val_loss -0.8742\n",
      "2025-12-18 17:16:02.570382: Pseudo dice [0.9276, 0.9551, 0.9402]\n",
      "2025-12-18 17:16:02.574386: Epoch time: 138.19 s\n",
      "2025-12-18 17:16:03.230568: \n",
      "2025-12-18 17:16:03.230568: Epoch 604\n",
      "2025-12-18 17:16:03.230568: Current learning rate: 0.00434\n",
      "2025-12-18 17:18:21.325037: train_loss -0.8519\n",
      "2025-12-18 17:18:21.325037: val_loss -0.8759\n",
      "2025-12-18 17:18:21.330908: Pseudo dice [0.9266, 0.957, 0.9442]\n",
      "2025-12-18 17:18:21.334912: Epoch time: 138.09 s\n",
      "2025-12-18 17:18:22.131526: \n",
      "2025-12-18 17:18:22.147400: Epoch 605\n",
      "2025-12-18 17:18:22.150924: Current learning rate: 0.00433\n",
      "2025-12-18 17:20:40.390665: train_loss -0.8534\n",
      "2025-12-18 17:20:40.390665: val_loss -0.8748\n",
      "2025-12-18 17:20:40.396409: Pseudo dice [0.9299, 0.96, 0.9347]\n",
      "2025-12-18 17:20:40.400268: Epoch time: 138.26 s\n",
      "2025-12-18 17:20:41.046563: \n",
      "2025-12-18 17:20:41.048314: Epoch 606\n",
      "2025-12-18 17:20:41.048314: Current learning rate: 0.00432\n",
      "2025-12-18 17:22:59.120938: train_loss -0.8529\n",
      "2025-12-18 17:22:59.120938: val_loss -0.874\n",
      "2025-12-18 17:22:59.120938: Pseudo dice [0.9249, 0.9566, 0.9452]\n",
      "2025-12-18 17:22:59.132158: Epoch time: 138.09 s\n",
      "2025-12-18 17:22:59.831733: \n",
      "2025-12-18 17:22:59.831733: Epoch 607\n",
      "2025-12-18 17:22:59.844785: Current learning rate: 0.00431\n",
      "2025-12-18 17:25:18.016279: train_loss -0.8511\n",
      "2025-12-18 17:25:18.016279: val_loss -0.871\n",
      "2025-12-18 17:25:18.024670: Pseudo dice [0.9264, 0.9572, 0.9359]\n",
      "2025-12-18 17:25:18.027422: Epoch time: 138.18 s\n",
      "2025-12-18 17:25:18.666588: \n",
      "2025-12-18 17:25:18.666588: Epoch 608\n",
      "2025-12-18 17:25:18.672085: Current learning rate: 0.0043\n",
      "2025-12-18 17:27:36.874508: train_loss -0.8533\n",
      "2025-12-18 17:27:36.878599: val_loss -0.8613\n",
      "2025-12-18 17:27:36.885231: Pseudo dice [0.9193, 0.9491, 0.9395]\n",
      "2025-12-18 17:27:36.891878: Epoch time: 138.21 s\n",
      "2025-12-18 17:27:37.543125: \n",
      "2025-12-18 17:27:37.543125: Epoch 609\n",
      "2025-12-18 17:27:37.558901: Current learning rate: 0.00429\n",
      "2025-12-18 17:29:55.870289: train_loss -0.8521\n",
      "2025-12-18 17:29:55.872292: val_loss -0.8617\n",
      "2025-12-18 17:29:55.878298: Pseudo dice [0.9177, 0.953, 0.9404]\n",
      "2025-12-18 17:29:55.882301: Epoch time: 138.33 s\n",
      "2025-12-18 17:29:56.518828: \n",
      "2025-12-18 17:29:56.518828: Epoch 610\n",
      "2025-12-18 17:29:56.536947: Current learning rate: 0.00429\n",
      "2025-12-18 17:32:14.685175: train_loss -0.8581\n",
      "2025-12-18 17:32:14.685175: val_loss -0.8647\n",
      "2025-12-18 17:32:14.691181: Pseudo dice [0.9215, 0.9527, 0.9355]\n",
      "2025-12-18 17:32:14.693183: Epoch time: 138.17 s\n",
      "2025-12-18 17:32:15.513634: \n",
      "2025-12-18 17:32:15.513634: Epoch 611\n",
      "2025-12-18 17:32:15.519201: Current learning rate: 0.00428\n",
      "2025-12-18 17:34:33.686017: train_loss -0.851\n",
      "2025-12-18 17:34:33.686017: val_loss -0.872\n",
      "2025-12-18 17:34:33.700098: Pseudo dice [0.926, 0.9599, 0.9349]\n",
      "2025-12-18 17:34:33.704102: Epoch time: 138.19 s\n",
      "2025-12-18 17:34:34.333177: \n",
      "2025-12-18 17:34:34.333177: Epoch 612\n",
      "2025-12-18 17:34:34.349169: Current learning rate: 0.00427\n",
      "2025-12-18 17:36:52.503854: train_loss -0.8536\n",
      "2025-12-18 17:36:52.503854: val_loss -0.8603\n",
      "2025-12-18 17:36:52.509861: Pseudo dice [0.9193, 0.9517, 0.9333]\n",
      "2025-12-18 17:36:52.513603: Epoch time: 138.17 s\n",
      "2025-12-18 17:36:53.153042: \n",
      "2025-12-18 17:36:53.153042: Epoch 613\n",
      "2025-12-18 17:36:53.153042: Current learning rate: 0.00426\n",
      "2025-12-18 17:39:11.669734: train_loss -0.8526\n",
      "2025-12-18 17:39:11.669734: val_loss -0.8736\n",
      "2025-12-18 17:39:11.673738: Pseudo dice [0.928, 0.958, 0.9368]\n",
      "2025-12-18 17:39:11.675478: Epoch time: 138.52 s\n",
      "2025-12-18 17:39:12.310164: \n",
      "2025-12-18 17:39:12.310164: Epoch 614\n",
      "2025-12-18 17:39:12.328496: Current learning rate: 0.00425\n",
      "2025-12-18 17:41:30.339568: train_loss -0.8528\n",
      "2025-12-18 17:41:30.339568: val_loss -0.8722\n",
      "2025-12-18 17:41:30.355376: Pseudo dice [0.9258, 0.9545, 0.9359]\n",
      "2025-12-18 17:41:30.361430: Epoch time: 138.03 s\n",
      "2025-12-18 17:41:31.005854: \n",
      "2025-12-18 17:41:31.005854: Epoch 615\n",
      "2025-12-18 17:41:31.010733: Current learning rate: 0.00424\n",
      "2025-12-18 17:43:49.291045: train_loss -0.8582\n",
      "2025-12-18 17:43:49.291045: val_loss -0.8689\n",
      "2025-12-18 17:43:49.297051: Pseudo dice [0.9243, 0.9562, 0.9377]\n",
      "2025-12-18 17:43:49.301055: Epoch time: 138.3 s\n",
      "2025-12-18 17:43:50.187105: \n",
      "2025-12-18 17:43:50.187105: Epoch 616\n",
      "2025-12-18 17:43:50.202566: Current learning rate: 0.00423\n",
      "2025-12-18 17:46:08.414448: train_loss -0.8583\n",
      "2025-12-18 17:46:08.416450: val_loss -0.8611\n",
      "2025-12-18 17:46:08.420192: Pseudo dice [0.9154, 0.9548, 0.9381]\n",
      "2025-12-18 17:46:08.424196: Epoch time: 138.23 s\n",
      "2025-12-18 17:46:09.063746: \n",
      "2025-12-18 17:46:09.063746: Epoch 617\n",
      "2025-12-18 17:46:09.077017: Current learning rate: 0.00422\n",
      "2025-12-18 17:48:27.201771: train_loss -0.8499\n",
      "2025-12-18 17:48:27.203773: val_loss -0.8705\n",
      "2025-12-18 17:48:27.209326: Pseudo dice [0.927, 0.9557, 0.9338]\n",
      "2025-12-18 17:48:27.213068: Epoch time: 138.14 s\n",
      "2025-12-18 17:48:27.847880: \n",
      "2025-12-18 17:48:27.847880: Epoch 618\n",
      "2025-12-18 17:48:27.861284: Current learning rate: 0.00421\n",
      "2025-12-18 17:50:46.023331: train_loss -0.8524\n",
      "2025-12-18 17:50:46.023331: val_loss -0.8623\n",
      "2025-12-18 17:50:46.027076: Pseudo dice [0.9228, 0.9519, 0.9349]\n",
      "2025-12-18 17:50:46.033984: Epoch time: 138.18 s\n",
      "2025-12-18 17:50:46.803753: \n",
      "2025-12-18 17:50:46.803753: Epoch 619\n",
      "2025-12-18 17:50:46.803753: Current learning rate: 0.0042\n",
      "2025-12-18 17:53:04.896729: train_loss -0.8516\n",
      "2025-12-18 17:53:04.896729: val_loss -0.8621\n",
      "2025-12-18 17:53:04.896729: Pseudo dice [0.9198, 0.9519, 0.9344]\n",
      "2025-12-18 17:53:04.910469: Epoch time: 138.09 s\n",
      "2025-12-18 17:53:05.558741: \n",
      "2025-12-18 17:53:05.558741: Epoch 620\n",
      "2025-12-18 17:53:05.558741: Current learning rate: 0.00419\n",
      "2025-12-18 17:55:23.737576: train_loss -0.8476\n",
      "2025-12-18 17:55:23.737576: val_loss -0.8787\n",
      "2025-12-18 17:55:23.745906: Pseudo dice [0.9282, 0.9597, 0.9417]\n",
      "2025-12-18 17:55:23.750283: Epoch time: 138.18 s\n",
      "2025-12-18 17:55:24.403185: \n",
      "2025-12-18 17:55:24.403185: Epoch 621\n",
      "2025-12-18 17:55:24.410939: Current learning rate: 0.00418\n",
      "2025-12-18 17:57:42.506253: train_loss -0.849\n",
      "2025-12-18 17:57:42.506253: val_loss -0.8686\n",
      "2025-12-18 17:57:42.512000: Pseudo dice [0.9225, 0.9544, 0.9449]\n",
      "2025-12-18 17:57:42.514003: Epoch time: 138.1 s\n",
      "2025-12-18 17:57:43.254436: \n",
      "2025-12-18 17:57:43.254436: Epoch 622\n",
      "2025-12-18 17:57:43.262636: Current learning rate: 0.00417\n",
      "2025-12-18 18:00:01.542533: train_loss -0.848\n",
      "2025-12-18 18:00:01.542533: val_loss -0.8686\n",
      "2025-12-18 18:00:01.552717: Pseudo dice [0.9248, 0.9557, 0.933]\n",
      "2025-12-18 18:00:01.558465: Epoch time: 138.29 s\n",
      "2025-12-18 18:00:02.384310: \n",
      "2025-12-18 18:00:02.384310: Epoch 623\n",
      "2025-12-18 18:00:02.397081: Current learning rate: 0.00416\n",
      "2025-12-18 18:02:20.491012: train_loss -0.854\n",
      "2025-12-18 18:02:20.491012: val_loss -0.8651\n",
      "2025-12-18 18:02:20.495016: Pseudo dice [0.9188, 0.9551, 0.9375]\n",
      "2025-12-18 18:02:20.498758: Epoch time: 138.1 s\n",
      "2025-12-18 18:02:21.212769: \n",
      "2025-12-18 18:02:21.212769: Epoch 624\n",
      "2025-12-18 18:02:21.222344: Current learning rate: 0.00415\n",
      "2025-12-18 18:04:39.300934: train_loss -0.8541\n",
      "2025-12-18 18:04:39.300934: val_loss -0.8669\n",
      "2025-12-18 18:04:39.304940: Pseudo dice [0.92, 0.9541, 0.9396]\n",
      "2025-12-18 18:04:39.310699: Epoch time: 138.09 s\n",
      "2025-12-18 18:04:40.116988: \n",
      "2025-12-18 18:04:40.116988: Epoch 625\n",
      "2025-12-18 18:04:40.133110: Current learning rate: 0.00414\n",
      "2025-12-18 18:06:58.035605: train_loss -0.8524\n",
      "2025-12-18 18:06:58.035605: val_loss -0.8741\n",
      "2025-12-18 18:06:58.040611: Pseudo dice [0.9264, 0.9583, 0.9377]\n",
      "2025-12-18 18:06:58.040611: Epoch time: 137.92 s\n",
      "2025-12-18 18:06:58.689901: \n",
      "2025-12-18 18:06:58.689901: Epoch 626\n",
      "2025-12-18 18:06:58.689901: Current learning rate: 0.00413\n",
      "2025-12-18 18:09:16.939043: train_loss -0.8516\n",
      "2025-12-18 18:09:16.939043: val_loss -0.8774\n",
      "2025-12-18 18:09:16.939043: Pseudo dice [0.9284, 0.9596, 0.9403]\n",
      "2025-12-18 18:09:16.954770: Epoch time: 138.25 s\n",
      "2025-12-18 18:09:17.654222: \n",
      "2025-12-18 18:09:17.654222: Epoch 627\n",
      "2025-12-18 18:09:17.654222: Current learning rate: 0.00412\n",
      "2025-12-18 18:11:36.036380: train_loss -0.8504\n",
      "2025-12-18 18:11:36.036380: val_loss -0.8773\n",
      "2025-12-18 18:11:36.042386: Pseudo dice [0.9294, 0.9574, 0.9413]\n",
      "2025-12-18 18:11:36.048130: Epoch time: 138.38 s\n",
      "2025-12-18 18:11:36.761787: \n",
      "2025-12-18 18:11:36.761787: Epoch 628\n",
      "2025-12-18 18:11:36.775806: Current learning rate: 0.00411\n",
      "2025-12-18 18:13:54.989088: train_loss -0.8546\n",
      "2025-12-18 18:13:54.989088: val_loss -0.8721\n",
      "2025-12-18 18:13:54.993092: Pseudo dice [0.9252, 0.9599, 0.9356]\n",
      "2025-12-18 18:13:54.999099: Epoch time: 138.23 s\n",
      "2025-12-18 18:13:55.810070: \n",
      "2025-12-18 18:13:55.810070: Epoch 629\n",
      "2025-12-18 18:13:55.810070: Current learning rate: 0.0041\n",
      "2025-12-18 18:16:13.809989: train_loss -0.8531\n",
      "2025-12-18 18:16:13.809989: val_loss -0.8729\n",
      "2025-12-18 18:16:13.809989: Pseudo dice [0.9268, 0.9544, 0.9415]\n",
      "2025-12-18 18:16:13.820776: Epoch time: 138.0 s\n",
      "2025-12-18 18:16:14.472240: \n",
      "2025-12-18 18:16:14.472240: Epoch 630\n",
      "2025-12-18 18:16:14.472240: Current learning rate: 0.00409\n",
      "2025-12-18 18:18:32.645051: train_loss -0.8529\n",
      "2025-12-18 18:18:32.645051: val_loss -0.8607\n",
      "2025-12-18 18:18:32.645051: Pseudo dice [0.9183, 0.9481, 0.9324]\n",
      "2025-12-18 18:18:32.645051: Epoch time: 138.17 s\n",
      "2025-12-18 18:18:33.341950: \n",
      "2025-12-18 18:18:33.341950: Epoch 631\n",
      "2025-12-18 18:18:33.357922: Current learning rate: 0.00408\n",
      "2025-12-18 18:20:51.488249: train_loss -0.8532\n",
      "2025-12-18 18:20:51.488249: val_loss -0.8785\n",
      "2025-12-18 18:20:51.494254: Pseudo dice [0.9301, 0.9581, 0.9387]\n",
      "2025-12-18 18:20:51.498259: Epoch time: 138.15 s\n",
      "2025-12-18 18:20:52.148860: \n",
      "2025-12-18 18:20:52.148860: Epoch 632\n",
      "2025-12-18 18:20:52.148860: Current learning rate: 0.00407\n",
      "2025-12-18 18:23:10.290495: train_loss -0.8474\n",
      "2025-12-18 18:23:10.290495: val_loss -0.8844\n",
      "2025-12-18 18:23:10.298886: Pseudo dice [0.9345, 0.9587, 0.9436]\n",
      "2025-12-18 18:23:10.304391: Epoch time: 138.14 s\n",
      "2025-12-18 18:23:10.955534: \n",
      "2025-12-18 18:23:10.955534: Epoch 633\n",
      "2025-12-18 18:23:10.955534: Current learning rate: 0.00406\n",
      "2025-12-18 18:25:29.246035: train_loss -0.857\n",
      "2025-12-18 18:25:29.246035: val_loss -0.8791\n",
      "2025-12-18 18:25:29.246035: Pseudo dice [0.9308, 0.9537, 0.9429]\n",
      "2025-12-18 18:25:29.246035: Epoch time: 138.29 s\n",
      "2025-12-18 18:25:29.947612: \n",
      "2025-12-18 18:25:29.947612: Epoch 634\n",
      "2025-12-18 18:25:29.947612: Current learning rate: 0.00405\n",
      "2025-12-18 18:27:48.008488: train_loss -0.8543\n",
      "2025-12-18 18:27:48.008488: val_loss -0.865\n",
      "2025-12-18 18:27:48.008488: Pseudo dice [0.9198, 0.954, 0.9416]\n",
      "2025-12-18 18:27:48.024584: Epoch time: 138.06 s\n",
      "2025-12-18 18:27:48.847683: \n",
      "2025-12-18 18:27:48.847683: Epoch 635\n",
      "2025-12-18 18:27:48.847683: Current learning rate: 0.00404\n",
      "2025-12-18 18:30:06.958189: train_loss -0.8529\n",
      "2025-12-18 18:30:06.960191: val_loss -0.8748\n",
      "2025-12-18 18:30:06.965935: Pseudo dice [0.9278, 0.9543, 0.9417]\n",
      "2025-12-18 18:30:06.965935: Epoch time: 138.11 s\n",
      "2025-12-18 18:30:07.613219: \n",
      "2025-12-18 18:30:07.613219: Epoch 636\n",
      "2025-12-18 18:30:07.613219: Current learning rate: 0.00403\n",
      "2025-12-18 18:32:25.696613: train_loss -0.8559\n",
      "2025-12-18 18:32:25.696613: val_loss -0.8791\n",
      "2025-12-18 18:32:25.712358: Pseudo dice [0.9289, 0.9614, 0.9389]\n",
      "2025-12-18 18:32:25.712358: Epoch time: 138.08 s\n",
      "2025-12-18 18:32:26.347086: \n",
      "2025-12-18 18:32:26.347086: Epoch 637\n",
      "2025-12-18 18:32:26.362973: Current learning rate: 0.00402\n",
      "2025-12-18 18:34:44.768497: train_loss -0.8514\n",
      "2025-12-18 18:34:44.768497: val_loss -0.8698\n",
      "2025-12-18 18:34:44.784295: Pseudo dice [0.9258, 0.9501, 0.935]\n",
      "2025-12-18 18:34:44.784295: Epoch time: 138.42 s\n",
      "2025-12-18 18:34:45.418053: \n",
      "2025-12-18 18:34:45.418053: Epoch 638\n",
      "2025-12-18 18:34:45.433880: Current learning rate: 0.00401\n",
      "2025-12-18 18:37:03.591024: train_loss -0.8541\n",
      "2025-12-18 18:37:03.591024: val_loss -0.8689\n",
      "2025-12-18 18:37:03.596769: Pseudo dice [0.9232, 0.9535, 0.9414]\n",
      "2025-12-18 18:37:03.596769: Epoch time: 138.17 s\n",
      "2025-12-18 18:37:04.245264: \n",
      "2025-12-18 18:37:04.245264: Epoch 639\n",
      "2025-12-18 18:37:04.245264: Current learning rate: 0.004\n",
      "2025-12-18 18:39:22.723128: train_loss -0.8554\n",
      "2025-12-18 18:39:22.723128: val_loss -0.8792\n",
      "2025-12-18 18:39:22.738865: Pseudo dice [0.9298, 0.9597, 0.9421]\n",
      "2025-12-18 18:39:22.738865: Epoch time: 138.48 s\n",
      "2025-12-18 18:39:23.371921: \n",
      "2025-12-18 18:39:23.387834: Epoch 640\n",
      "2025-12-18 18:39:23.387834: Current learning rate: 0.00399\n",
      "2025-12-18 18:41:41.494277: train_loss -0.8554\n",
      "2025-12-18 18:41:41.494277: val_loss -0.8803\n",
      "2025-12-18 18:41:41.494277: Pseudo dice [0.9317, 0.9595, 0.9388]\n",
      "2025-12-18 18:41:41.494277: Epoch time: 138.12 s\n",
      "2025-12-18 18:41:41.509986: Yayy! New best EMA pseudo Dice: 0.9407\n",
      "2025-12-18 18:41:42.686694: \n",
      "2025-12-18 18:41:42.686694: Epoch 641\n",
      "2025-12-18 18:41:42.686694: Current learning rate: 0.00398\n",
      "2025-12-18 18:44:00.885975: train_loss -0.8531\n",
      "2025-12-18 18:44:00.885975: val_loss -0.8703\n",
      "2025-12-18 18:44:00.885975: Pseudo dice [0.9226, 0.9586, 0.9384]\n",
      "2025-12-18 18:44:00.885975: Epoch time: 138.2 s\n",
      "2025-12-18 18:44:01.536132: \n",
      "2025-12-18 18:44:01.536132: Epoch 642\n",
      "2025-12-18 18:44:01.536132: Current learning rate: 0.00397\n",
      "2025-12-18 18:46:19.733314: train_loss -0.8511\n",
      "2025-12-18 18:46:19.733314: val_loss -0.8677\n",
      "2025-12-18 18:46:19.751188: Pseudo dice [0.9231, 0.9528, 0.9367]\n",
      "2025-12-18 18:46:19.751188: Epoch time: 138.2 s\n",
      "2025-12-18 18:46:20.401207: \n",
      "2025-12-18 18:46:20.401207: Epoch 643\n",
      "2025-12-18 18:46:20.415401: Current learning rate: 0.00396\n",
      "2025-12-18 18:48:38.582888: train_loss -0.8544\n",
      "2025-12-18 18:48:38.582888: val_loss -0.8778\n",
      "2025-12-18 18:48:38.582888: Pseudo dice [0.9228, 0.9608, 0.9431]\n",
      "2025-12-18 18:48:38.598597: Epoch time: 138.18 s\n",
      "2025-12-18 18:48:39.326511: \n",
      "2025-12-18 18:48:39.326511: Epoch 644\n",
      "2025-12-18 18:48:39.342591: Current learning rate: 0.00395\n",
      "2025-12-18 18:50:57.547943: train_loss -0.8532\n",
      "2025-12-18 18:50:57.547943: val_loss -0.8703\n",
      "2025-12-18 18:50:57.567298: Pseudo dice [0.9302, 0.9541, 0.9237]\n",
      "2025-12-18 18:50:57.571046: Epoch time: 138.22 s\n",
      "2025-12-18 18:50:58.212490: \n",
      "2025-12-18 18:50:58.212490: Epoch 645\n",
      "2025-12-18 18:50:58.228129: Current learning rate: 0.00394\n",
      "2025-12-18 18:53:16.302014: train_loss -0.8543\n",
      "2025-12-18 18:53:16.304016: val_loss -0.8839\n",
      "2025-12-18 18:53:16.310022: Pseudo dice [0.9334, 0.9627, 0.9449]\n",
      "2025-12-18 18:53:16.314026: Epoch time: 138.09 s\n",
      "2025-12-18 18:53:16.317768: Yayy! New best EMA pseudo Dice: 0.9408\n",
      "2025-12-18 18:53:17.212902: \n",
      "2025-12-18 18:53:17.212902: Epoch 646\n",
      "2025-12-18 18:53:17.212902: Current learning rate: 0.00393\n",
      "2025-12-18 18:55:35.428557: train_loss -0.8584\n",
      "2025-12-18 18:55:35.428557: val_loss -0.8761\n",
      "2025-12-18 18:55:35.444234: Pseudo dice [0.9306, 0.9581, 0.935]\n",
      "2025-12-18 18:55:35.444234: Epoch time: 138.22 s\n",
      "2025-12-18 18:55:35.444234: Yayy! New best EMA pseudo Dice: 0.9408\n",
      "2025-12-18 18:55:36.712811: \n",
      "2025-12-18 18:55:36.712811: Epoch 647\n",
      "2025-12-18 18:55:36.712811: Current learning rate: 0.00392\n",
      "2025-12-18 18:57:54.636544: train_loss -0.8545\n",
      "2025-12-18 18:57:54.636544: val_loss -0.876\n",
      "2025-12-18 18:57:54.636544: Pseudo dice [0.9275, 0.9564, 0.9433]\n",
      "2025-12-18 18:57:54.636544: Epoch time: 137.92 s\n",
      "2025-12-18 18:57:54.636544: Yayy! New best EMA pseudo Dice: 0.941\n",
      "2025-12-18 18:57:55.570553: \n",
      "2025-12-18 18:57:55.570553: Epoch 648\n",
      "2025-12-18 18:57:55.586334: Current learning rate: 0.00391\n",
      "2025-12-18 19:00:13.857848: train_loss -0.8539\n",
      "2025-12-18 19:00:13.857848: val_loss -0.8738\n",
      "2025-12-18 19:00:13.863855: Pseudo dice [0.9289, 0.9538, 0.9376]\n",
      "2025-12-18 19:00:13.867859: Epoch time: 138.29 s\n",
      "2025-12-18 19:00:14.547349: \n",
      "2025-12-18 19:00:14.547349: Epoch 649\n",
      "2025-12-18 19:00:14.547349: Current learning rate: 0.0039\n",
      "2025-12-18 19:02:32.774066: train_loss -0.8375\n",
      "2025-12-18 19:02:32.774066: val_loss -0.8527\n",
      "2025-12-18 19:02:32.780074: Pseudo dice [0.9142, 0.9448, 0.9378]\n",
      "2025-12-18 19:02:32.784078: Epoch time: 138.23 s\n",
      "2025-12-18 19:02:33.694475: \n",
      "2025-12-18 19:02:33.696477: Epoch 650\n",
      "2025-12-18 19:02:33.696477: Current learning rate: 0.00389\n",
      "2025-12-18 19:04:51.832348: train_loss -0.8267\n",
      "2025-12-18 19:04:51.832348: val_loss -0.8494\n",
      "2025-12-18 19:04:51.838356: Pseudo dice [0.9134, 0.9504, 0.9304]\n",
      "2025-12-18 19:04:51.842360: Epoch time: 138.14 s\n",
      "2025-12-18 19:04:52.489987: \n",
      "2025-12-18 19:04:52.489987: Epoch 651\n",
      "2025-12-18 19:04:52.489987: Current learning rate: 0.00388\n",
      "2025-12-18 19:07:10.726903: train_loss -0.8265\n",
      "2025-12-18 19:07:10.728905: val_loss -0.8667\n",
      "2025-12-18 19:07:10.728905: Pseudo dice [0.9255, 0.9551, 0.934]\n",
      "2025-12-18 19:07:10.728905: Epoch time: 138.24 s\n",
      "2025-12-18 19:07:11.390021: \n",
      "2025-12-18 19:07:11.390021: Epoch 652\n",
      "2025-12-18 19:07:11.390021: Current learning rate: 0.00387\n",
      "2025-12-18 19:09:29.656803: train_loss -0.8418\n",
      "2025-12-18 19:09:29.656803: val_loss -0.8642\n",
      "2025-12-18 19:09:29.656803: Pseudo dice [0.9218, 0.9528, 0.9331]\n",
      "2025-12-18 19:09:29.656803: Epoch time: 138.27 s\n",
      "2025-12-18 19:09:30.482903: \n",
      "2025-12-18 19:09:30.482903: Epoch 653\n",
      "2025-12-18 19:09:30.482903: Current learning rate: 0.00386\n",
      "2025-12-18 19:11:48.665665: train_loss -0.8434\n",
      "2025-12-18 19:11:48.665665: val_loss -0.8667\n",
      "2025-12-18 19:11:48.676593: Pseudo dice [0.9276, 0.9576, 0.9259]\n",
      "2025-12-18 19:11:48.681599: Epoch time: 138.18 s\n",
      "2025-12-18 19:11:49.332737: \n",
      "2025-12-18 19:11:49.332737: Epoch 654\n",
      "2025-12-18 19:11:49.332737: Current learning rate: 0.00385\n",
      "2025-12-18 19:14:07.474459: train_loss -0.8443\n",
      "2025-12-18 19:14:07.474459: val_loss -0.8596\n",
      "2025-12-18 19:14:07.480200: Pseudo dice [0.918, 0.9509, 0.9274]\n",
      "2025-12-18 19:14:07.486014: Epoch time: 138.16 s\n",
      "2025-12-18 19:14:08.134647: \n",
      "2025-12-18 19:14:08.134647: Epoch 655\n",
      "2025-12-18 19:14:08.134647: Current learning rate: 0.00384\n",
      "2025-12-18 19:16:26.421760: train_loss -0.8472\n",
      "2025-12-18 19:16:26.421760: val_loss -0.8687\n",
      "2025-12-18 19:16:26.432743: Pseudo dice [0.9283, 0.9553, 0.9265]\n",
      "2025-12-18 19:16:26.437749: Epoch time: 138.3 s\n",
      "2025-12-18 19:16:27.149826: \n",
      "2025-12-18 19:16:27.149826: Epoch 656\n",
      "2025-12-18 19:16:27.149826: Current learning rate: 0.00383\n",
      "2025-12-18 19:18:45.219823: train_loss -0.853\n",
      "2025-12-18 19:18:45.219823: val_loss -0.8647\n",
      "2025-12-18 19:18:45.219823: Pseudo dice [0.9195, 0.9499, 0.9439]\n",
      "2025-12-18 19:18:45.219823: Epoch time: 138.07 s\n",
      "2025-12-18 19:18:45.870794: \n",
      "2025-12-18 19:18:45.870794: Epoch 657\n",
      "2025-12-18 19:18:45.870794: Current learning rate: 0.00382\n",
      "2025-12-18 19:21:03.983236: train_loss -0.8542\n",
      "2025-12-18 19:21:03.985238: val_loss -0.8689\n",
      "2025-12-18 19:21:03.991307: Pseudo dice [0.9245, 0.9545, 0.9424]\n",
      "2025-12-18 19:21:03.995311: Epoch time: 138.11 s\n",
      "2025-12-18 19:21:04.808241: \n",
      "2025-12-18 19:21:04.808241: Epoch 658\n",
      "2025-12-18 19:21:04.808241: Current learning rate: 0.00381\n",
      "2025-12-18 19:23:22.981585: train_loss -0.8515\n",
      "2025-12-18 19:23:22.983587: val_loss -0.8676\n",
      "2025-12-18 19:23:22.988921: Pseudo dice [0.9246, 0.955, 0.933]\n",
      "2025-12-18 19:23:22.992925: Epoch time: 138.18 s\n",
      "2025-12-18 19:23:23.644855: \n",
      "2025-12-18 19:23:23.644855: Epoch 659\n",
      "2025-12-18 19:23:23.644855: Current learning rate: 0.0038\n",
      "2025-12-18 19:25:41.954200: train_loss -0.8522\n",
      "2025-12-18 19:25:41.956202: val_loss -0.8757\n",
      "2025-12-18 19:25:41.962208: Pseudo dice [0.9268, 0.9545, 0.9466]\n",
      "2025-12-18 19:25:41.966212: Epoch time: 138.31 s\n",
      "2025-12-18 19:25:42.669757: \n",
      "2025-12-18 19:25:42.670760: Epoch 660\n",
      "2025-12-18 19:25:42.670760: Current learning rate: 0.00379\n",
      "2025-12-18 19:28:00.935483: train_loss -0.8533\n",
      "2025-12-18 19:28:00.937223: val_loss -0.872\n",
      "2025-12-18 19:28:00.937223: Pseudo dice [0.9278, 0.953, 0.9382]\n",
      "2025-12-18 19:28:00.937223: Epoch time: 138.27 s\n",
      "2025-12-18 19:28:01.590999: \n",
      "2025-12-18 19:28:01.590999: Epoch 661\n",
      "2025-12-18 19:28:01.595086: Current learning rate: 0.00378\n",
      "2025-12-18 19:30:19.905821: train_loss -0.8557\n",
      "2025-12-18 19:30:19.907824: val_loss -0.8743\n",
      "2025-12-18 19:30:19.909565: Pseudo dice [0.9279, 0.9522, 0.9389]\n",
      "2025-12-18 19:30:19.917067: Epoch time: 138.32 s\n",
      "2025-12-18 19:30:20.563851: \n",
      "2025-12-18 19:30:20.565853: Epoch 662\n",
      "2025-12-18 19:30:20.570215: Current learning rate: 0.00377\n",
      "2025-12-18 19:32:38.623296: train_loss -0.8552\n",
      "2025-12-18 19:32:38.623296: val_loss -0.8623\n",
      "2025-12-18 19:32:38.627301: Pseudo dice [0.9178, 0.9518, 0.9411]\n",
      "2025-12-18 19:32:38.631305: Epoch time: 138.06 s\n",
      "2025-12-18 19:32:39.363617: \n",
      "2025-12-18 19:32:39.363617: Epoch 663\n",
      "2025-12-18 19:32:39.363617: Current learning rate: 0.00376\n",
      "2025-12-18 19:34:57.497684: train_loss -0.8539\n",
      "2025-12-18 19:34:57.499687: val_loss -0.8784\n",
      "2025-12-18 19:34:57.503692: Pseudo dice [0.9296, 0.9557, 0.9405]\n",
      "2025-12-18 19:34:57.507696: Epoch time: 138.13 s\n",
      "2025-12-18 19:34:58.151712: \n",
      "2025-12-18 19:34:58.151712: Epoch 664\n",
      "2025-12-18 19:34:58.167395: Current learning rate: 0.00375\n",
      "2025-12-18 19:37:16.193043: train_loss -0.8527\n",
      "2025-12-18 19:37:16.193043: val_loss -0.8796\n",
      "2025-12-18 19:37:16.208732: Pseudo dice [0.9298, 0.96, 0.9349]\n",
      "2025-12-18 19:37:16.208732: Epoch time: 138.04 s\n",
      "2025-12-18 19:37:17.081577: \n",
      "2025-12-18 19:37:17.081577: Epoch 665\n",
      "2025-12-18 19:37:17.097353: Current learning rate: 0.00374\n",
      "2025-12-18 19:39:35.253883: train_loss -0.8511\n",
      "2025-12-18 19:39:35.253883: val_loss -0.8643\n",
      "2025-12-18 19:39:35.261630: Pseudo dice [0.9197, 0.9508, 0.9414]\n",
      "2025-12-18 19:39:35.267638: Epoch time: 138.17 s\n",
      "2025-12-18 19:39:36.085967: \n",
      "2025-12-18 19:39:36.085967: Epoch 666\n",
      "2025-12-18 19:39:36.085967: Current learning rate: 0.00373\n",
      "2025-12-18 19:41:54.296722: train_loss -0.8567\n",
      "2025-12-18 19:41:54.296722: val_loss -0.8694\n",
      "2025-12-18 19:41:54.299107: Pseudo dice [0.9215, 0.9558, 0.9409]\n",
      "2025-12-18 19:41:54.299107: Epoch time: 138.21 s\n",
      "2025-12-18 19:41:54.942975: \n",
      "2025-12-18 19:41:54.944716: Epoch 667\n",
      "2025-12-18 19:41:54.949323: Current learning rate: 0.00372\n",
      "2025-12-18 19:44:13.093997: train_loss -0.8529\n",
      "2025-12-18 19:44:13.093997: val_loss -0.8783\n",
      "2025-12-18 19:44:13.096000: Pseudo dice [0.9291, 0.9572, 0.943]\n",
      "2025-12-18 19:44:13.102565: Epoch time: 138.15 s\n",
      "2025-12-18 19:44:13.745330: \n",
      "2025-12-18 19:44:13.745330: Epoch 668\n",
      "2025-12-18 19:44:13.745330: Current learning rate: 0.00371\n",
      "2025-12-18 19:46:32.033293: train_loss -0.8534\n",
      "2025-12-18 19:46:32.033293: val_loss -0.8787\n",
      "2025-12-18 19:46:32.040534: Pseudo dice [0.9291, 0.9579, 0.9452]\n",
      "2025-12-18 19:46:32.046540: Epoch time: 138.29 s\n",
      "2025-12-18 19:46:32.799195: \n",
      "2025-12-18 19:46:32.801198: Epoch 669\n",
      "2025-12-18 19:46:32.802939: Current learning rate: 0.0037\n",
      "2025-12-18 19:48:51.022215: train_loss -0.8489\n",
      "2025-12-18 19:48:51.022215: val_loss -0.8683\n",
      "2025-12-18 19:48:51.028221: Pseudo dice [0.9248, 0.9542, 0.9322]\n",
      "2025-12-18 19:48:51.032225: Epoch time: 138.22 s\n",
      "2025-12-18 19:48:51.690290: \n",
      "2025-12-18 19:48:51.690290: Epoch 670\n",
      "2025-12-18 19:48:51.692293: Current learning rate: 0.00369\n",
      "2025-12-18 19:51:09.767428: train_loss -0.8561\n",
      "2025-12-18 19:51:09.769431: val_loss -0.8772\n",
      "2025-12-18 19:51:09.769431: Pseudo dice [0.9314, 0.9586, 0.9332]\n",
      "2025-12-18 19:51:09.777985: Epoch time: 138.08 s\n",
      "2025-12-18 19:51:10.650417: \n",
      "2025-12-18 19:51:10.650417: Epoch 671\n",
      "2025-12-18 19:51:10.652420: Current learning rate: 0.00368\n",
      "2025-12-18 19:53:28.690007: train_loss -0.8588\n",
      "2025-12-18 19:53:28.690007: val_loss -0.8835\n",
      "2025-12-18 19:53:28.696016: Pseudo dice [0.9313, 0.9599, 0.9431]\n",
      "2025-12-18 19:53:28.702025: Epoch time: 138.04 s\n",
      "2025-12-18 19:53:29.425050: \n",
      "2025-12-18 19:53:29.427052: Epoch 672\n",
      "2025-12-18 19:53:29.427052: Current learning rate: 0.00367\n",
      "2025-12-18 19:55:47.548970: train_loss -0.8538\n",
      "2025-12-18 19:55:47.550973: val_loss -0.8841\n",
      "2025-12-18 19:55:47.554976: Pseudo dice [0.935, 0.9608, 0.9345]\n",
      "2025-12-18 19:55:47.558980: Epoch time: 138.12 s\n",
      "2025-12-18 19:55:48.215036: \n",
      "2025-12-18 19:55:48.215036: Epoch 673\n",
      "2025-12-18 19:55:48.215036: Current learning rate: 0.00366\n",
      "2025-12-18 19:58:06.274676: train_loss -0.8532\n",
      "2025-12-18 19:58:06.276417: val_loss -0.8618\n",
      "2025-12-18 19:58:06.282003: Pseudo dice [0.917, 0.952, 0.9374]\n",
      "2025-12-18 19:58:06.286007: Epoch time: 138.06 s\n",
      "2025-12-18 19:58:06.926131: \n",
      "2025-12-18 19:58:06.926131: Epoch 674\n",
      "2025-12-18 19:58:06.942238: Current learning rate: 0.00365\n",
      "2025-12-18 20:00:25.303010: train_loss -0.8543\n",
      "2025-12-18 20:00:25.303010: val_loss -0.8728\n",
      "2025-12-18 20:00:25.311018: Pseudo dice [0.9235, 0.9581, 0.9371]\n",
      "2025-12-18 20:00:25.311018: Epoch time: 138.38 s\n",
      "2025-12-18 20:00:25.965379: \n",
      "2025-12-18 20:00:25.965379: Epoch 675\n",
      "2025-12-18 20:00:25.981408: Current learning rate: 0.00364\n",
      "2025-12-18 20:02:44.057546: train_loss -0.8551\n",
      "2025-12-18 20:02:44.057546: val_loss -0.8805\n",
      "2025-12-18 20:02:44.063520: Pseudo dice [0.9331, 0.9607, 0.9336]\n",
      "2025-12-18 20:02:44.063520: Epoch time: 138.09 s\n",
      "2025-12-18 20:02:44.871505: \n",
      "2025-12-18 20:02:44.871505: Epoch 676\n",
      "2025-12-18 20:02:44.887526: Current learning rate: 0.00363\n",
      "2025-12-18 20:05:02.901874: train_loss -0.8539\n",
      "2025-12-18 20:05:02.901874: val_loss -0.8725\n",
      "2025-12-18 20:05:02.919621: Pseudo dice [0.9243, 0.9534, 0.9369]\n",
      "2025-12-18 20:05:02.919621: Epoch time: 138.03 s\n",
      "2025-12-18 20:05:03.583141: \n",
      "2025-12-18 20:05:03.583141: Epoch 677\n",
      "2025-12-18 20:05:03.583141: Current learning rate: 0.00362\n",
      "2025-12-18 20:07:21.824923: train_loss -0.8526\n",
      "2025-12-18 20:07:21.824923: val_loss -0.8741\n",
      "2025-12-18 20:07:21.831204: Pseudo dice [0.9269, 0.9599, 0.9399]\n",
      "2025-12-18 20:07:21.834659: Epoch time: 138.24 s\n",
      "2025-12-18 20:07:22.530770: \n",
      "2025-12-18 20:07:22.530770: Epoch 678\n",
      "2025-12-18 20:07:22.537480: Current learning rate: 0.00361\n",
      "2025-12-18 20:09:40.892730: train_loss -0.8525\n",
      "2025-12-18 20:09:40.892730: val_loss -0.873\n",
      "2025-12-18 20:09:40.898739: Pseudo dice [0.9232, 0.9532, 0.9419]\n",
      "2025-12-18 20:09:40.902743: Epoch time: 138.36 s\n",
      "2025-12-18 20:09:41.550484: \n",
      "2025-12-18 20:09:41.550484: Epoch 679\n",
      "2025-12-18 20:09:41.566278: Current learning rate: 0.0036\n",
      "2025-12-18 20:11:59.908893: train_loss -0.8571\n",
      "2025-12-18 20:11:59.908893: val_loss -0.87\n",
      "2025-12-18 20:11:59.924612: Pseudo dice [0.9215, 0.9523, 0.9473]\n",
      "2025-12-18 20:11:59.928394: Epoch time: 138.36 s\n",
      "2025-12-18 20:12:00.575174: \n",
      "2025-12-18 20:12:00.575174: Epoch 680\n",
      "2025-12-18 20:12:00.575174: Current learning rate: 0.00359\n",
      "2025-12-18 20:14:18.776925: train_loss -0.8556\n",
      "2025-12-18 20:14:18.776925: val_loss -0.873\n",
      "2025-12-18 20:14:18.788710: Pseudo dice [0.9247, 0.9583, 0.9392]\n",
      "2025-12-18 20:14:18.792901: Epoch time: 138.2 s\n",
      "2025-12-18 20:14:19.442331: \n",
      "2025-12-18 20:14:19.442331: Epoch 681\n",
      "2025-12-18 20:14:19.442331: Current learning rate: 0.00358\n",
      "2025-12-18 20:16:37.556716: train_loss -0.8569\n",
      "2025-12-18 20:16:37.558720: val_loss -0.8747\n",
      "2025-12-18 20:16:37.568741: Pseudo dice [0.9281, 0.9557, 0.9324]\n",
      "2025-12-18 20:16:37.574496: Epoch time: 138.11 s\n",
      "2025-12-18 20:16:38.394309: \n",
      "2025-12-18 20:16:38.394309: Epoch 682\n",
      "2025-12-18 20:16:38.406205: Current learning rate: 0.00357\n",
      "2025-12-18 20:18:56.491597: train_loss -0.8581\n",
      "2025-12-18 20:18:56.491597: val_loss -0.8701\n",
      "2025-12-18 20:18:56.497048: Pseudo dice [0.9226, 0.9529, 0.9382]\n",
      "2025-12-18 20:18:56.503054: Epoch time: 138.1 s\n",
      "2025-12-18 20:18:57.204409: \n",
      "2025-12-18 20:18:57.204409: Epoch 683\n",
      "2025-12-18 20:18:57.204409: Current learning rate: 0.00356\n",
      "2025-12-18 20:21:15.423657: train_loss -0.8555\n",
      "2025-12-18 20:21:15.425659: val_loss -0.8769\n",
      "2025-12-18 20:21:15.429663: Pseudo dice [0.9255, 0.9576, 0.9442]\n",
      "2025-12-18 20:21:15.434956: Epoch time: 138.22 s\n",
      "2025-12-18 20:21:16.085812: \n",
      "2025-12-18 20:21:16.085812: Epoch 684\n",
      "2025-12-18 20:21:16.085812: Current learning rate: 0.00355\n",
      "2025-12-18 20:23:34.226227: train_loss -0.8504\n",
      "2025-12-18 20:23:34.226227: val_loss -0.8711\n",
      "2025-12-18 20:23:34.233518: Pseudo dice [0.923, 0.9537, 0.9454]\n",
      "2025-12-18 20:23:34.237228: Epoch time: 138.14 s\n",
      "2025-12-18 20:23:34.987000: \n",
      "2025-12-18 20:23:34.987000: Epoch 685\n",
      "2025-12-18 20:23:34.987000: Current learning rate: 0.00354\n",
      "2025-12-18 20:25:53.220989: train_loss -0.8531\n",
      "2025-12-18 20:25:53.220989: val_loss -0.8721\n",
      "2025-12-18 20:25:53.220989: Pseudo dice [0.9236, 0.9608, 0.936]\n",
      "2025-12-18 20:25:53.236756: Epoch time: 138.23 s\n",
      "2025-12-18 20:25:53.870506: \n",
      "2025-12-18 20:25:53.870506: Epoch 686\n",
      "2025-12-18 20:25:53.870506: Current learning rate: 0.00353\n",
      "2025-12-18 20:28:11.886062: train_loss -0.8513\n",
      "2025-12-18 20:28:11.886062: val_loss -0.8684\n",
      "2025-12-18 20:28:11.887802: Pseudo dice [0.9202, 0.9567, 0.9335]\n",
      "2025-12-18 20:28:11.897171: Epoch time: 138.02 s\n",
      "2025-12-18 20:28:12.583949: \n",
      "2025-12-18 20:28:12.583949: Epoch 687\n",
      "2025-12-18 20:28:12.589486: Current learning rate: 0.00352\n",
      "2025-12-18 20:30:30.744659: train_loss -0.8487\n",
      "2025-12-18 20:30:30.744659: val_loss -0.8685\n",
      "2025-12-18 20:30:30.746662: Pseudo dice [0.925, 0.9543, 0.9356]\n",
      "2025-12-18 20:30:30.754078: Epoch time: 138.16 s\n",
      "2025-12-18 20:30:31.556736: \n",
      "2025-12-18 20:30:31.556736: Epoch 688\n",
      "2025-12-18 20:30:31.560482: Current learning rate: 0.00351\n",
      "2025-12-18 20:32:49.763092: train_loss -0.8528\n",
      "2025-12-18 20:32:49.765094: val_loss -0.8663\n",
      "2025-12-18 20:32:49.768836: Pseudo dice [0.9206, 0.9499, 0.9429]\n",
      "2025-12-18 20:32:49.774580: Epoch time: 138.21 s\n",
      "2025-12-18 20:32:50.418642: \n",
      "2025-12-18 20:32:50.418642: Epoch 689\n",
      "2025-12-18 20:32:50.434296: Current learning rate: 0.0035\n",
      "2025-12-18 20:35:08.528455: train_loss -0.8537\n",
      "2025-12-18 20:35:08.528455: val_loss -0.8735\n",
      "2025-12-18 20:35:08.532459: Pseudo dice [0.9271, 0.9566, 0.9326]\n",
      "2025-12-18 20:35:08.539201: Epoch time: 138.11 s\n",
      "2025-12-18 20:35:09.255592: \n",
      "2025-12-18 20:35:09.255592: Epoch 690\n",
      "2025-12-18 20:35:09.255592: Current learning rate: 0.00349\n",
      "2025-12-18 20:37:27.438291: train_loss -0.8543\n",
      "2025-12-18 20:37:27.438291: val_loss -0.8674\n",
      "2025-12-18 20:37:27.439795: Pseudo dice [0.9238, 0.9521, 0.93]\n",
      "2025-12-18 20:37:27.439795: Epoch time: 138.18 s\n",
      "2025-12-18 20:37:28.200392: \n",
      "2025-12-18 20:37:28.200392: Epoch 691\n",
      "2025-12-18 20:37:28.200392: Current learning rate: 0.00348\n",
      "2025-12-18 20:39:46.291001: train_loss -0.8529\n",
      "2025-12-18 20:39:46.306703: val_loss -0.8733\n",
      "2025-12-18 20:39:46.306703: Pseudo dice [0.9276, 0.9544, 0.9386]\n",
      "2025-12-18 20:39:46.306703: Epoch time: 138.11 s\n",
      "2025-12-18 20:39:46.957367: \n",
      "2025-12-18 20:39:46.957367: Epoch 692\n",
      "2025-12-18 20:39:46.973155: Current learning rate: 0.00346\n",
      "2025-12-18 20:42:05.143285: train_loss -0.8536\n",
      "2025-12-18 20:42:05.143285: val_loss -0.8858\n",
      "2025-12-18 20:42:05.143285: Pseudo dice [0.9366, 0.9627, 0.9371]\n",
      "2025-12-18 20:42:05.155164: Epoch time: 138.19 s\n",
      "2025-12-18 20:42:05.806312: \n",
      "2025-12-18 20:42:05.806312: Epoch 693\n",
      "2025-12-18 20:42:05.806312: Current learning rate: 0.00345\n",
      "2025-12-18 20:44:24.190244: train_loss -0.855\n",
      "2025-12-18 20:44:24.190244: val_loss -0.8797\n",
      "2025-12-18 20:44:24.190244: Pseudo dice [0.9328, 0.9601, 0.9455]\n",
      "2025-12-18 20:44:24.203946: Epoch time: 138.38 s\n",
      "2025-12-18 20:44:25.202365: \n",
      "2025-12-18 20:44:25.202365: Epoch 694\n",
      "2025-12-18 20:44:25.202365: Current learning rate: 0.00344\n",
      "2025-12-18 20:46:43.295017: train_loss -0.8554\n",
      "2025-12-18 20:46:43.295017: val_loss -0.8847\n",
      "2025-12-18 20:46:43.301843: Pseudo dice [0.9339, 0.9606, 0.9416]\n",
      "2025-12-18 20:46:43.309861: Epoch time: 138.11 s\n",
      "2025-12-18 20:46:43.959494: \n",
      "2025-12-18 20:46:43.959494: Epoch 695\n",
      "2025-12-18 20:46:43.975577: Current learning rate: 0.00343\n",
      "2025-12-18 20:49:02.048245: train_loss -0.856\n",
      "2025-12-18 20:49:02.048245: val_loss -0.8759\n",
      "2025-12-18 20:49:02.063982: Pseudo dice [0.9274, 0.9541, 0.9421]\n",
      "2025-12-18 20:49:02.063982: Epoch time: 138.09 s\n",
      "2025-12-18 20:49:02.715257: \n",
      "2025-12-18 20:49:02.715257: Epoch 696\n",
      "2025-12-18 20:49:02.731288: Current learning rate: 0.00342\n",
      "2025-12-18 20:51:20.852856: train_loss -0.8549\n",
      "2025-12-18 20:51:20.854858: val_loss -0.8749\n",
      "2025-12-18 20:51:20.862869: Pseudo dice [0.9289, 0.9545, 0.9397]\n",
      "2025-12-18 20:51:20.868613: Epoch time: 138.14 s\n",
      "2025-12-18 20:51:21.628041: \n",
      "2025-12-18 20:51:21.628041: Epoch 697\n",
      "2025-12-18 20:51:21.628041: Current learning rate: 0.00341\n",
      "2025-12-18 20:53:39.775077: train_loss -0.8516\n",
      "2025-12-18 20:53:39.775077: val_loss -0.8705\n",
      "2025-12-18 20:53:39.780821: Pseudo dice [0.9229, 0.9554, 0.9371]\n",
      "2025-12-18 20:53:39.782823: Epoch time: 138.15 s\n",
      "2025-12-18 20:53:40.430262: \n",
      "2025-12-18 20:53:40.430262: Epoch 698\n",
      "2025-12-18 20:53:40.446284: Current learning rate: 0.0034\n",
      "2025-12-18 20:55:58.525635: train_loss -0.8542\n",
      "2025-12-18 20:55:58.525635: val_loss -0.8678\n",
      "2025-12-18 20:55:58.525635: Pseudo dice [0.9215, 0.9597, 0.9293]\n",
      "2025-12-18 20:55:58.541481: Epoch time: 138.1 s\n",
      "2025-12-18 20:55:59.191410: \n",
      "2025-12-18 20:55:59.191410: Epoch 699\n",
      "2025-12-18 20:55:59.191410: Current learning rate: 0.00339\n",
      "2025-12-18 20:58:17.292156: train_loss -0.8574\n",
      "2025-12-18 20:58:17.292156: val_loss -0.8732\n",
      "2025-12-18 20:58:17.292156: Pseudo dice [0.9256, 0.9546, 0.9376]\n",
      "2025-12-18 20:58:17.292156: Epoch time: 138.1 s\n",
      "2025-12-18 20:58:18.432183: \n",
      "2025-12-18 20:58:18.432183: Epoch 700\n",
      "2025-12-18 20:58:18.432183: Current learning rate: 0.00338\n",
      "2025-12-18 21:00:36.700951: train_loss -0.8563\n",
      "2025-12-18 21:00:36.700951: val_loss -0.8626\n",
      "2025-12-18 21:00:36.700951: Pseudo dice [0.9219, 0.9516, 0.9375]\n",
      "2025-12-18 21:00:36.700951: Epoch time: 138.27 s\n",
      "2025-12-18 21:00:37.365386: \n",
      "2025-12-18 21:00:37.365386: Epoch 701\n",
      "2025-12-18 21:00:37.365386: Current learning rate: 0.00337\n",
      "2025-12-18 21:02:55.487955: train_loss -0.8585\n",
      "2025-12-18 21:02:55.489695: val_loss -0.8774\n",
      "2025-12-18 21:02:55.497709: Pseudo dice [0.9307, 0.9569, 0.94]\n",
      "2025-12-18 21:02:55.501713: Epoch time: 138.12 s\n",
      "2025-12-18 21:02:56.156293: \n",
      "2025-12-18 21:02:56.156293: Epoch 702\n",
      "2025-12-18 21:02:56.156293: Current learning rate: 0.00336\n",
      "2025-12-18 21:05:14.325238: train_loss -0.8597\n",
      "2025-12-18 21:05:14.325238: val_loss -0.8731\n",
      "2025-12-18 21:05:14.329242: Pseudo dice [0.9231, 0.955, 0.9423]\n",
      "2025-12-18 21:05:14.329242: Epoch time: 138.17 s\n",
      "2025-12-18 21:05:15.037080: \n",
      "2025-12-18 21:05:15.037080: Epoch 703\n",
      "2025-12-18 21:05:15.037080: Current learning rate: 0.00335\n",
      "2025-12-18 21:07:33.248630: train_loss -0.8521\n",
      "2025-12-18 21:07:33.248630: val_loss -0.8793\n",
      "2025-12-18 21:07:33.248630: Pseudo dice [0.9335, 0.9569, 0.9378]\n",
      "2025-12-18 21:07:33.248630: Epoch time: 138.21 s\n",
      "2025-12-18 21:07:33.896070: \n",
      "2025-12-18 21:07:33.896070: Epoch 704\n",
      "2025-12-18 21:07:33.896070: Current learning rate: 0.00334\n",
      "2025-12-18 21:09:52.467843: train_loss -0.8547\n",
      "2025-12-18 21:09:52.467843: val_loss -0.8751\n",
      "2025-12-18 21:09:52.467843: Pseudo dice [0.9261, 0.9576, 0.9403]\n",
      "2025-12-18 21:09:52.483592: Epoch time: 138.57 s\n",
      "2025-12-18 21:09:53.134297: \n",
      "2025-12-18 21:09:53.134297: Epoch 705\n",
      "2025-12-18 21:09:53.150062: Current learning rate: 0.00333\n",
      "2025-12-18 21:12:11.354235: train_loss -0.8558\n",
      "2025-12-18 21:12:11.354235: val_loss -0.877\n",
      "2025-12-18 21:12:11.369956: Pseudo dice [0.9288, 0.961, 0.9375]\n",
      "2025-12-18 21:12:11.369956: Epoch time: 138.22 s\n",
      "2025-12-18 21:12:12.196383: \n",
      "2025-12-18 21:12:12.196383: Epoch 706\n",
      "2025-12-18 21:12:12.196383: Current learning rate: 0.00332\n",
      "2025-12-18 21:14:30.454174: train_loss -0.8531\n",
      "2025-12-18 21:14:30.454174: val_loss -0.8646\n",
      "2025-12-18 21:14:30.454174: Pseudo dice [0.9198, 0.9532, 0.9365]\n",
      "2025-12-18 21:14:30.454174: Epoch time: 138.26 s\n",
      "2025-12-18 21:14:31.184089: \n",
      "2025-12-18 21:14:31.184089: Epoch 707\n",
      "2025-12-18 21:14:31.199855: Current learning rate: 0.00331\n",
      "2025-12-18 21:16:49.392241: train_loss -0.8586\n",
      "2025-12-18 21:16:49.394243: val_loss -0.8919\n",
      "2025-12-18 21:16:49.399987: Pseudo dice [0.9401, 0.9606, 0.9472]\n",
      "2025-12-18 21:16:49.403991: Epoch time: 138.21 s\n",
      "2025-12-18 21:16:49.407994: Yayy! New best EMA pseudo Dice: 0.9412\n",
      "2025-12-18 21:16:50.332412: \n",
      "2025-12-18 21:16:50.332412: Epoch 708\n",
      "2025-12-18 21:16:50.332412: Current learning rate: 0.0033\n",
      "2025-12-18 21:19:08.576314: train_loss -0.8565\n",
      "2025-12-18 21:19:08.576314: val_loss -0.8752\n",
      "2025-12-18 21:19:08.576314: Pseudo dice [0.9278, 0.9547, 0.9381]\n",
      "2025-12-18 21:19:08.592009: Epoch time: 138.25 s\n",
      "2025-12-18 21:19:09.247569: \n",
      "2025-12-18 21:19:09.247569: Epoch 709\n",
      "2025-12-18 21:19:09.251923: Current learning rate: 0.00329\n",
      "2025-12-18 21:21:27.562418: train_loss -0.8485\n",
      "2025-12-18 21:21:27.564421: val_loss -0.8816\n",
      "2025-12-18 21:21:27.572169: Pseudo dice [0.9317, 0.9566, 0.9395]\n",
      "2025-12-18 21:21:27.578176: Epoch time: 138.31 s\n",
      "2025-12-18 21:21:27.585925: Yayy! New best EMA pseudo Dice: 0.9412\n",
      "2025-12-18 21:21:28.702508: \n",
      "2025-12-18 21:21:28.702508: Epoch 710\n",
      "2025-12-18 21:21:28.702508: Current learning rate: 0.00328\n",
      "2025-12-18 21:23:46.852281: train_loss -0.8577\n",
      "2025-12-18 21:23:46.852281: val_loss -0.8737\n",
      "2025-12-18 21:23:46.854785: Pseudo dice [0.929, 0.959, 0.9353]\n",
      "2025-12-18 21:23:46.854785: Epoch time: 138.15 s\n",
      "2025-12-18 21:23:47.514530: \n",
      "2025-12-18 21:23:47.516533: Epoch 711\n",
      "2025-12-18 21:23:47.516533: Current learning rate: 0.00327\n",
      "2025-12-18 21:26:05.696269: train_loss -0.8537\n",
      "2025-12-18 21:26:05.696269: val_loss -0.8653\n",
      "2025-12-18 21:26:05.702276: Pseudo dice [0.921, 0.9523, 0.9329]\n",
      "2025-12-18 21:26:05.706280: Epoch time: 138.18 s\n",
      "2025-12-18 21:26:06.530283: \n",
      "2025-12-18 21:26:06.530283: Epoch 712\n",
      "2025-12-18 21:26:06.530283: Current learning rate: 0.00326\n",
      "2025-12-18 21:28:24.441483: train_loss -0.86\n",
      "2025-12-18 21:28:24.441483: val_loss -0.875\n",
      "2025-12-18 21:28:24.449492: Pseudo dice [0.9257, 0.9562, 0.9398]\n",
      "2025-12-18 21:28:24.455497: Epoch time: 137.91 s\n",
      "2025-12-18 21:28:25.279593: \n",
      "2025-12-18 21:28:25.279593: Epoch 713\n",
      "2025-12-18 21:28:25.299495: Current learning rate: 0.00325\n",
      "2025-12-18 21:30:43.627196: train_loss -0.8563\n",
      "2025-12-18 21:30:43.627196: val_loss -0.8749\n",
      "2025-12-18 21:30:43.643095: Pseudo dice [0.9258, 0.9557, 0.9404]\n",
      "2025-12-18 21:30:43.643095: Epoch time: 138.35 s\n",
      "2025-12-18 21:30:44.293276: \n",
      "2025-12-18 21:30:44.293276: Epoch 714\n",
      "2025-12-18 21:30:44.293276: Current learning rate: 0.00324\n",
      "2025-12-18 21:33:02.672204: train_loss -0.8539\n",
      "2025-12-18 21:33:02.672204: val_loss -0.8704\n",
      "2025-12-18 21:33:02.678211: Pseudo dice [0.9252, 0.9529, 0.9378]\n",
      "2025-12-18 21:33:02.683932: Epoch time: 138.38 s\n",
      "2025-12-18 21:33:03.346271: \n",
      "2025-12-18 21:33:03.346271: Epoch 715\n",
      "2025-12-18 21:33:03.346271: Current learning rate: 0.00323\n",
      "2025-12-18 21:35:21.656337: train_loss -0.8552\n",
      "2025-12-18 21:35:21.656337: val_loss -0.8775\n",
      "2025-12-18 21:35:21.662344: Pseudo dice [0.928, 0.9596, 0.9395]\n",
      "2025-12-18 21:35:21.666348: Epoch time: 138.31 s\n",
      "2025-12-18 21:35:22.397650: \n",
      "2025-12-18 21:35:22.397650: Epoch 716\n",
      "2025-12-18 21:35:22.397650: Current learning rate: 0.00322\n",
      "2025-12-18 21:37:40.538622: train_loss -0.8572\n",
      "2025-12-18 21:37:40.538622: val_loss -0.8682\n",
      "2025-12-18 21:37:40.548378: Pseudo dice [0.9224, 0.9493, 0.932]\n",
      "2025-12-18 21:37:40.552954: Epoch time: 138.16 s\n",
      "2025-12-18 21:37:41.358923: \n",
      "2025-12-18 21:37:41.358923: Epoch 717\n",
      "2025-12-18 21:37:41.374603: Current learning rate: 0.00321\n",
      "2025-12-18 21:39:59.524228: train_loss -0.8537\n",
      "2025-12-18 21:39:59.524228: val_loss -0.8759\n",
      "2025-12-18 21:39:59.531101: Pseudo dice [0.9299, 0.9587, 0.9338]\n",
      "2025-12-18 21:39:59.535105: Epoch time: 138.17 s\n",
      "2025-12-18 21:40:00.219174: \n",
      "2025-12-18 21:40:00.219174: Epoch 718\n",
      "2025-12-18 21:40:00.219174: Current learning rate: 0.0032\n",
      "2025-12-18 21:42:18.551671: train_loss -0.8551\n",
      "2025-12-18 21:42:18.551671: val_loss -0.8689\n",
      "2025-12-18 21:42:18.553674: Pseudo dice [0.9251, 0.9548, 0.9365]\n",
      "2025-12-18 21:42:18.562901: Epoch time: 138.33 s\n",
      "2025-12-18 21:42:19.224910: \n",
      "2025-12-18 21:42:19.224910: Epoch 719\n",
      "2025-12-18 21:42:19.227426: Current learning rate: 0.00319\n",
      "2025-12-18 21:44:37.645055: train_loss -0.8563\n",
      "2025-12-18 21:44:37.645055: val_loss -0.8715\n",
      "2025-12-18 21:44:37.645055: Pseudo dice [0.9231, 0.9531, 0.9366]\n",
      "2025-12-18 21:44:37.645055: Epoch time: 138.42 s\n",
      "2025-12-18 21:44:38.310062: \n",
      "2025-12-18 21:44:38.310062: Epoch 720\n",
      "2025-12-18 21:44:38.310062: Current learning rate: 0.00318\n",
      "2025-12-18 21:46:56.440247: train_loss -0.8575\n",
      "2025-12-18 21:46:56.440247: val_loss -0.8721\n",
      "2025-12-18 21:46:56.455965: Pseudo dice [0.9271, 0.9524, 0.931]\n",
      "2025-12-18 21:46:56.455965: Epoch time: 138.13 s\n",
      "2025-12-18 21:46:57.102109: \n",
      "2025-12-18 21:46:57.102109: Epoch 721\n",
      "2025-12-18 21:46:57.117889: Current learning rate: 0.00317\n",
      "2025-12-18 21:49:15.146586: train_loss -0.8626\n",
      "2025-12-18 21:49:15.146586: val_loss -0.8832\n",
      "2025-12-18 21:49:15.146586: Pseudo dice [0.9291, 0.9587, 0.9468]\n",
      "2025-12-18 21:49:15.146586: Epoch time: 138.04 s\n",
      "2025-12-18 21:49:15.810167: \n",
      "2025-12-18 21:49:15.812170: Epoch 722\n",
      "2025-12-18 21:49:15.812170: Current learning rate: 0.00316\n",
      "2025-12-18 21:51:34.067221: train_loss -0.8574\n",
      "2025-12-18 21:51:34.067221: val_loss -0.8802\n",
      "2025-12-18 21:51:34.082789: Pseudo dice [0.9343, 0.9598, 0.9295]\n",
      "2025-12-18 21:51:34.082789: Epoch time: 138.26 s\n",
      "2025-12-18 21:51:34.733466: \n",
      "2025-12-18 21:51:34.733466: Epoch 723\n",
      "2025-12-18 21:51:34.733466: Current learning rate: 0.00315\n",
      "2025-12-18 21:53:53.214339: train_loss -0.8532\n",
      "2025-12-18 21:53:53.214339: val_loss -0.8725\n",
      "2025-12-18 21:53:53.216342: Pseudo dice [0.9232, 0.9574, 0.9423]\n",
      "2025-12-18 21:53:53.224022: Epoch time: 138.48 s\n",
      "2025-12-18 21:53:54.046463: \n",
      "2025-12-18 21:53:54.046463: Epoch 724\n",
      "2025-12-18 21:53:54.062301: Current learning rate: 0.00314\n",
      "2025-12-18 21:56:12.103250: train_loss -0.8613\n",
      "2025-12-18 21:56:12.103250: val_loss -0.8707\n",
      "2025-12-18 21:56:12.109255: Pseudo dice [0.9251, 0.9525, 0.934]\n",
      "2025-12-18 21:56:12.113260: Epoch time: 138.06 s\n",
      "2025-12-18 21:56:12.776313: \n",
      "2025-12-18 21:56:12.776313: Epoch 725\n",
      "2025-12-18 21:56:12.776313: Current learning rate: 0.00313\n",
      "2025-12-18 21:58:31.038320: train_loss -0.8581\n",
      "2025-12-18 21:58:31.038320: val_loss -0.8699\n",
      "2025-12-18 21:58:31.044064: Pseudo dice [0.9238, 0.9555, 0.9375]\n",
      "2025-12-18 21:58:31.048068: Epoch time: 138.26 s\n",
      "2025-12-18 21:58:31.856410: \n",
      "2025-12-18 21:58:31.856410: Epoch 726\n",
      "2025-12-18 21:58:31.867908: Current learning rate: 0.00312\n",
      "2025-12-18 22:00:50.127067: train_loss -0.8532\n",
      "2025-12-18 22:00:50.127067: val_loss -0.874\n",
      "2025-12-18 22:00:50.134814: Pseudo dice [0.9253, 0.9594, 0.9414]\n",
      "2025-12-18 22:00:50.138555: Epoch time: 138.27 s\n",
      "2025-12-18 22:00:50.794305: \n",
      "2025-12-18 22:00:50.794305: Epoch 727\n",
      "2025-12-18 22:00:50.794305: Current learning rate: 0.00311\n",
      "2025-12-18 22:03:09.077626: train_loss -0.861\n",
      "2025-12-18 22:03:09.077626: val_loss -0.88\n",
      "2025-12-18 22:03:09.093284: Pseudo dice [0.9305, 0.9608, 0.9406]\n",
      "2025-12-18 22:03:09.093284: Epoch time: 138.28 s\n",
      "2025-12-18 22:03:09.746364: \n",
      "2025-12-18 22:03:09.746364: Epoch 728\n",
      "2025-12-18 22:03:09.746364: Current learning rate: 0.0031\n",
      "2025-12-18 22:05:28.025326: train_loss -0.8552\n",
      "2025-12-18 22:05:28.025326: val_loss -0.8674\n",
      "2025-12-18 22:05:28.031337: Pseudo dice [0.9184, 0.9546, 0.9477]\n",
      "2025-12-18 22:05:28.035343: Epoch time: 138.28 s\n",
      "2025-12-18 22:05:29.005154: \n",
      "2025-12-18 22:05:29.005154: Epoch 729\n",
      "2025-12-18 22:05:29.009160: Current learning rate: 0.00309\n",
      "2025-12-18 22:07:47.159521: train_loss -0.858\n",
      "2025-12-18 22:07:47.159521: val_loss -0.8791\n",
      "2025-12-18 22:07:47.169389: Pseudo dice [0.9307, 0.956, 0.9388]\n",
      "2025-12-18 22:07:47.175199: Epoch time: 138.16 s\n",
      "2025-12-18 22:07:47.827635: \n",
      "2025-12-18 22:07:47.827635: Epoch 730\n",
      "2025-12-18 22:07:47.827635: Current learning rate: 0.00308\n",
      "2025-12-18 22:10:06.276344: train_loss -0.8516\n",
      "2025-12-18 22:10:06.276344: val_loss -0.8734\n",
      "2025-12-18 22:10:06.291152: Pseudo dice [0.9238, 0.9553, 0.9422]\n",
      "2025-12-18 22:10:06.295656: Epoch time: 138.45 s\n",
      "2025-12-18 22:10:06.941626: \n",
      "2025-12-18 22:10:06.941626: Epoch 731\n",
      "2025-12-18 22:10:06.957673: Current learning rate: 0.00307\n",
      "2025-12-18 22:12:25.198093: train_loss -0.8544\n",
      "2025-12-18 22:12:25.198093: val_loss -0.8658\n",
      "2025-12-18 22:12:25.209861: Pseudo dice [0.9227, 0.9501, 0.9289]\n",
      "2025-12-18 22:12:25.215870: Epoch time: 138.26 s\n",
      "2025-12-18 22:12:25.967178: \n",
      "2025-12-18 22:12:25.967178: Epoch 732\n",
      "2025-12-18 22:12:25.967178: Current learning rate: 0.00306\n",
      "2025-12-18 22:14:44.115413: train_loss -0.8562\n",
      "2025-12-18 22:14:44.115413: val_loss -0.8845\n",
      "2025-12-18 22:14:44.115413: Pseudo dice [0.9299, 0.9632, 0.9484]\n",
      "2025-12-18 22:14:44.115413: Epoch time: 138.15 s\n",
      "2025-12-18 22:14:44.780592: \n",
      "2025-12-18 22:14:44.780592: Epoch 733\n",
      "2025-12-18 22:14:44.780592: Current learning rate: 0.00305\n",
      "2025-12-18 22:17:03.061933: train_loss -0.8543\n",
      "2025-12-18 22:17:03.061933: val_loss -0.8856\n",
      "2025-12-18 22:17:03.061933: Pseudo dice [0.9369, 0.9615, 0.9412]\n",
      "2025-12-18 22:17:03.061933: Epoch time: 138.28 s\n",
      "2025-12-18 22:17:03.729586: \n",
      "2025-12-18 22:17:03.729586: Epoch 734\n",
      "2025-12-18 22:17:03.729586: Current learning rate: 0.00304\n",
      "2025-12-18 22:19:21.806354: train_loss -0.8555\n",
      "2025-12-18 22:19:21.808357: val_loss -0.8684\n",
      "2025-12-18 22:19:21.814086: Pseudo dice [0.9238, 0.9521, 0.9375]\n",
      "2025-12-18 22:19:21.818001: Epoch time: 138.08 s\n",
      "2025-12-18 22:19:22.653391: \n",
      "2025-12-18 22:19:22.653391: Epoch 735\n",
      "2025-12-18 22:19:22.659805: Current learning rate: 0.00303\n",
      "2025-12-18 22:21:40.898938: train_loss -0.8562\n",
      "2025-12-18 22:21:40.900939: val_loss -0.8699\n",
      "2025-12-18 22:21:40.906449: Pseudo dice [0.9244, 0.9552, 0.9335]\n",
      "2025-12-18 22:21:40.910322: Epoch time: 138.25 s\n",
      "2025-12-18 22:21:41.557086: \n",
      "2025-12-18 22:21:41.557086: Epoch 736\n",
      "2025-12-18 22:21:41.566250: Current learning rate: 0.00302\n",
      "2025-12-18 22:23:59.771641: train_loss -0.8507\n",
      "2025-12-18 22:23:59.785709: val_loss -0.874\n",
      "2025-12-18 22:23:59.791197: Pseudo dice [0.9289, 0.9572, 0.936]\n",
      "2025-12-18 22:23:59.795369: Epoch time: 138.21 s\n",
      "2025-12-18 22:24:00.452283: \n",
      "2025-12-18 22:24:00.453743: Epoch 737\n",
      "2025-12-18 22:24:00.458262: Current learning rate: 0.00301\n",
      "2025-12-18 22:26:18.886611: train_loss -0.8485\n",
      "2025-12-18 22:26:18.886611: val_loss -0.8828\n",
      "2025-12-18 22:26:18.906182: Pseudo dice [0.9328, 0.9618, 0.9365]\n",
      "2025-12-18 22:26:18.910186: Epoch time: 138.45 s\n",
      "2025-12-18 22:26:19.568329: \n",
      "2025-12-18 22:26:19.568329: Epoch 738\n",
      "2025-12-18 22:26:19.588217: Current learning rate: 0.003\n",
      "2025-12-18 22:28:37.760707: train_loss -0.8572\n",
      "2025-12-18 22:28:37.760707: val_loss -0.8787\n",
      "2025-12-18 22:28:37.760707: Pseudo dice [0.9323, 0.9563, 0.9371]\n",
      "2025-12-18 22:28:37.760707: Epoch time: 138.19 s\n",
      "2025-12-18 22:28:38.426876: \n",
      "2025-12-18 22:28:38.426876: Epoch 739\n",
      "2025-12-18 22:28:38.426876: Current learning rate: 0.00299\n",
      "2025-12-18 22:30:56.683811: train_loss -0.858\n",
      "2025-12-18 22:30:56.683811: val_loss -0.8716\n",
      "2025-12-18 22:30:56.693438: Pseudo dice [0.9241, 0.9532, 0.9403]\n",
      "2025-12-18 22:30:56.697443: Epoch time: 138.26 s\n",
      "2025-12-18 22:30:57.403831: \n",
      "2025-12-18 22:30:57.403831: Epoch 740\n",
      "2025-12-18 22:30:57.419691: Current learning rate: 0.00297\n",
      "2025-12-18 22:33:15.591752: train_loss -0.8586\n",
      "2025-12-18 22:33:15.591752: val_loss -0.878\n",
      "2025-12-18 22:33:15.607541: Pseudo dice [0.9304, 0.9576, 0.9365]\n",
      "2025-12-18 22:33:15.613547: Epoch time: 138.19 s\n",
      "2025-12-18 22:33:16.271230: \n",
      "2025-12-18 22:33:16.271230: Epoch 741\n",
      "2025-12-18 22:33:16.271230: Current learning rate: 0.00296\n",
      "2025-12-18 22:35:34.966139: train_loss -0.8549\n",
      "2025-12-18 22:35:34.966139: val_loss -0.8783\n",
      "2025-12-18 22:35:34.966139: Pseudo dice [0.9294, 0.9584, 0.9415]\n",
      "2025-12-18 22:35:34.977876: Epoch time: 138.69 s\n",
      "2025-12-18 22:35:35.745076: \n",
      "2025-12-18 22:35:35.745076: Epoch 742\n",
      "2025-12-18 22:35:35.751088: Current learning rate: 0.00295\n",
      "2025-12-18 22:37:53.802418: train_loss -0.8604\n",
      "2025-12-18 22:37:53.802418: val_loss -0.8749\n",
      "2025-12-18 22:37:53.810426: Pseudo dice [0.927, 0.9572, 0.937]\n",
      "2025-12-18 22:37:53.816434: Epoch time: 138.06 s\n",
      "2025-12-18 22:37:54.462721: \n",
      "2025-12-18 22:37:54.462721: Epoch 743\n",
      "2025-12-18 22:37:54.476259: Current learning rate: 0.00294\n",
      "2025-12-18 22:40:12.845647: train_loss -0.8612\n",
      "2025-12-18 22:40:12.845647: val_loss -0.8732\n",
      "2025-12-18 22:40:12.845647: Pseudo dice [0.9258, 0.9572, 0.9342]\n",
      "2025-12-18 22:40:12.845647: Epoch time: 138.38 s\n",
      "2025-12-18 22:40:13.557028: \n",
      "2025-12-18 22:40:13.557028: Epoch 744\n",
      "2025-12-18 22:40:13.562513: Current learning rate: 0.00293\n",
      "2025-12-18 22:42:31.802418: train_loss -0.8592\n",
      "2025-12-18 22:42:31.802418: val_loss -0.8788\n",
      "2025-12-18 22:42:31.808424: Pseudo dice [0.9319, 0.9586, 0.9332]\n",
      "2025-12-18 22:42:31.812427: Epoch time: 138.25 s\n",
      "2025-12-18 22:42:32.582920: \n",
      "2025-12-18 22:42:32.582920: Epoch 745\n",
      "2025-12-18 22:42:32.582920: Current learning rate: 0.00292\n",
      "2025-12-18 22:44:50.895994: train_loss -0.8563\n",
      "2025-12-18 22:44:50.895994: val_loss -0.8712\n",
      "2025-12-18 22:44:50.895994: Pseudo dice [0.9252, 0.9547, 0.9325]\n",
      "2025-12-18 22:44:50.911891: Epoch time: 138.31 s\n",
      "2025-12-18 22:44:51.595500: \n",
      "2025-12-18 22:44:51.595500: Epoch 746\n",
      "2025-12-18 22:44:51.595500: Current learning rate: 0.00291\n",
      "2025-12-18 22:47:10.044143: train_loss -0.8535\n",
      "2025-12-18 22:47:10.044143: val_loss -0.8705\n",
      "2025-12-18 22:47:10.044143: Pseudo dice [0.9246, 0.9544, 0.9378]\n",
      "2025-12-18 22:47:10.059998: Epoch time: 138.45 s\n",
      "2025-12-18 22:47:10.931284: \n",
      "2025-12-18 22:47:10.931284: Epoch 747\n",
      "2025-12-18 22:47:10.936771: Current learning rate: 0.0029\n",
      "2025-12-18 22:49:29.135391: train_loss -0.8524\n",
      "2025-12-18 22:49:29.137392: val_loss -0.8733\n",
      "2025-12-18 22:49:29.145139: Pseudo dice [0.9253, 0.9514, 0.9426]\n",
      "2025-12-18 22:49:29.151145: Epoch time: 138.2 s\n",
      "2025-12-18 22:49:29.981476: \n",
      "2025-12-18 22:49:29.981476: Epoch 748\n",
      "2025-12-18 22:49:29.981476: Current learning rate: 0.00289\n",
      "2025-12-18 22:51:48.245332: train_loss -0.8635\n",
      "2025-12-18 22:51:48.245332: val_loss -0.8747\n",
      "2025-12-18 22:51:48.247334: Pseudo dice [0.9263, 0.9562, 0.9399]\n",
      "2025-12-18 22:51:48.257411: Epoch time: 138.28 s\n",
      "2025-12-18 22:51:48.943482: \n",
      "2025-12-18 22:51:48.943482: Epoch 749\n",
      "2025-12-18 22:51:48.945484: Current learning rate: 0.00288\n",
      "2025-12-18 22:54:07.248617: train_loss -0.857\n",
      "2025-12-18 22:54:07.248617: val_loss -0.8745\n",
      "2025-12-18 22:54:07.248617: Pseudo dice [0.9223, 0.9527, 0.9461]\n",
      "2025-12-18 22:54:07.248617: Epoch time: 138.31 s\n",
      "2025-12-18 22:54:08.222988: \n",
      "2025-12-18 22:54:08.222988: Epoch 750\n",
      "2025-12-18 22:54:08.222988: Current learning rate: 0.00287\n",
      "2025-12-18 22:56:26.451109: train_loss -0.8526\n",
      "2025-12-18 22:56:26.451109: val_loss -0.8805\n",
      "2025-12-18 22:56:26.458857: Pseudo dice [0.9291, 0.9623, 0.9392]\n",
      "2025-12-18 22:56:26.462861: Epoch time: 138.23 s\n",
      "2025-12-18 22:56:27.169996: \n",
      "2025-12-18 22:56:27.169996: Epoch 751\n",
      "2025-12-18 22:56:27.169996: Current learning rate: 0.00286\n",
      "2025-12-18 22:58:45.320667: train_loss -0.8608\n",
      "2025-12-18 22:58:45.320667: val_loss -0.8791\n",
      "2025-12-18 22:58:45.326497: Pseudo dice [0.9279, 0.9562, 0.9435]\n",
      "2025-12-18 22:58:45.330501: Epoch time: 138.15 s\n",
      "2025-12-18 22:58:45.997577: \n",
      "2025-12-18 22:58:45.997577: Epoch 752\n",
      "2025-12-18 22:58:45.997577: Current learning rate: 0.00285\n",
      "2025-12-18 23:01:04.198962: train_loss -0.8603\n",
      "2025-12-18 23:01:04.200965: val_loss -0.8704\n",
      "2025-12-18 23:01:04.206972: Pseudo dice [0.92, 0.9516, 0.9432]\n",
      "2025-12-18 23:01:04.212470: Epoch time: 138.2 s\n",
      "2025-12-18 23:01:05.046540: \n",
      "2025-12-18 23:01:05.046540: Epoch 753\n",
      "2025-12-18 23:01:05.050894: Current learning rate: 0.00284\n",
      "2025-12-18 23:03:23.446700: train_loss -0.8608\n",
      "2025-12-18 23:03:23.448702: val_loss -0.8774\n",
      "2025-12-18 23:03:23.460336: Pseudo dice [0.926, 0.9537, 0.9436]\n",
      "2025-12-18 23:03:23.464556: Epoch time: 138.4 s\n",
      "2025-12-18 23:03:24.172228: \n",
      "2025-12-18 23:03:24.172228: Epoch 754\n",
      "2025-12-18 23:03:24.177429: Current learning rate: 0.00283\n",
      "2025-12-18 23:05:42.543805: train_loss -0.855\n",
      "2025-12-18 23:05:42.543805: val_loss -0.8716\n",
      "2025-12-18 23:05:42.548957: Pseudo dice [0.9219, 0.9539, 0.9439]\n",
      "2025-12-18 23:05:42.554456: Epoch time: 138.37 s\n",
      "2025-12-18 23:05:43.214346: \n",
      "2025-12-18 23:05:43.214346: Epoch 755\n",
      "2025-12-18 23:05:43.214346: Current learning rate: 0.00282\n",
      "2025-12-18 23:08:01.233197: train_loss -0.8547\n",
      "2025-12-18 23:08:01.233197: val_loss -0.8814\n",
      "2025-12-18 23:08:01.240514: Pseudo dice [0.9281, 0.959, 0.9404]\n",
      "2025-12-18 23:08:01.244518: Epoch time: 138.02 s\n",
      "2025-12-18 23:08:01.898196: \n",
      "2025-12-18 23:08:01.898196: Epoch 756\n",
      "2025-12-18 23:08:01.914103: Current learning rate: 0.00281\n",
      "2025-12-18 23:10:20.454391: train_loss -0.8551\n",
      "2025-12-18 23:10:20.454391: val_loss -0.8831\n",
      "2025-12-18 23:10:20.470333: Pseudo dice [0.9328, 0.9592, 0.9403]\n",
      "2025-12-18 23:10:20.470333: Epoch time: 138.56 s\n",
      "2025-12-18 23:10:21.167594: \n",
      "2025-12-18 23:10:21.167594: Epoch 757\n",
      "2025-12-18 23:10:21.167594: Current learning rate: 0.0028\n",
      "2025-12-18 23:12:39.461757: train_loss -0.8533\n",
      "2025-12-18 23:12:39.463759: val_loss -0.8674\n",
      "2025-12-18 23:12:39.471769: Pseudo dice [0.9226, 0.9518, 0.9384]\n",
      "2025-12-18 23:12:39.477415: Epoch time: 138.29 s\n",
      "2025-12-18 23:12:40.185024: \n",
      "2025-12-18 23:12:40.185024: Epoch 758\n",
      "2025-12-18 23:12:40.190317: Current learning rate: 0.00279\n",
      "2025-12-18 23:14:58.356048: train_loss -0.8528\n",
      "2025-12-18 23:14:58.358050: val_loss -0.8798\n",
      "2025-12-18 23:14:58.360306: Pseudo dice [0.9267, 0.9608, 0.9449]\n",
      "2025-12-18 23:14:58.360306: Epoch time: 138.17 s\n",
      "2025-12-18 23:14:59.181027: \n",
      "2025-12-18 23:14:59.181027: Epoch 759\n",
      "2025-12-18 23:14:59.196835: Current learning rate: 0.00278\n",
      "2025-12-18 23:17:17.525382: train_loss -0.8517\n",
      "2025-12-18 23:17:17.525382: val_loss -0.8743\n",
      "2025-12-18 23:17:17.538285: Pseudo dice [0.9259, 0.9527, 0.9415]\n",
      "2025-12-18 23:17:17.541288: Epoch time: 138.34 s\n",
      "2025-12-18 23:17:18.277992: \n",
      "2025-12-18 23:17:18.277992: Epoch 760\n",
      "2025-12-18 23:17:18.277992: Current learning rate: 0.00277\n",
      "2025-12-18 23:19:36.442133: train_loss -0.8538\n",
      "2025-12-18 23:19:36.442133: val_loss -0.8648\n",
      "2025-12-18 23:19:36.449881: Pseudo dice [0.9211, 0.9517, 0.9377]\n",
      "2025-12-18 23:19:36.455888: Epoch time: 138.16 s\n",
      "2025-12-18 23:19:37.238498: \n",
      "2025-12-18 23:19:37.240501: Epoch 761\n",
      "2025-12-18 23:19:37.240501: Current learning rate: 0.00276\n",
      "2025-12-18 23:21:55.332176: train_loss -0.8543\n",
      "2025-12-18 23:21:55.332176: val_loss -0.8831\n",
      "2025-12-18 23:21:55.333918: Pseudo dice [0.932, 0.9619, 0.9425]\n",
      "2025-12-18 23:21:55.333918: Epoch time: 138.1 s\n",
      "2025-12-18 23:21:56.014695: \n",
      "2025-12-18 23:21:56.014695: Epoch 762\n",
      "2025-12-18 23:21:56.014695: Current learning rate: 0.00275\n",
      "2025-12-18 23:24:14.211185: train_loss -0.86\n",
      "2025-12-18 23:24:14.211185: val_loss -0.8754\n",
      "2025-12-18 23:24:14.217377: Pseudo dice [0.93, 0.9598, 0.9346]\n",
      "2025-12-18 23:24:14.221381: Epoch time: 138.2 s\n",
      "2025-12-18 23:24:14.889310: \n",
      "2025-12-18 23:24:14.889310: Epoch 763\n",
      "2025-12-18 23:24:14.889310: Current learning rate: 0.00274\n",
      "2025-12-18 23:26:33.243009: train_loss -0.8565\n",
      "2025-12-18 23:26:33.243009: val_loss -0.8802\n",
      "2025-12-18 23:26:33.249015: Pseudo dice [0.9287, 0.9577, 0.9407]\n",
      "2025-12-18 23:26:33.255020: Epoch time: 138.35 s\n",
      "2025-12-18 23:26:34.109185: \n",
      "2025-12-18 23:26:34.109185: Epoch 764\n",
      "2025-12-18 23:26:34.109185: Current learning rate: 0.00273\n",
      "2025-12-18 23:28:52.317981: train_loss -0.8579\n",
      "2025-12-18 23:28:52.317981: val_loss -0.8837\n",
      "2025-12-18 23:28:52.323987: Pseudo dice [0.9331, 0.9587, 0.9388]\n",
      "2025-12-18 23:28:52.329731: Epoch time: 138.21 s\n",
      "2025-12-18 23:28:52.333542: Yayy! New best EMA pseudo Dice: 0.9415\n",
      "2025-12-18 23:28:53.439537: \n",
      "2025-12-18 23:28:53.439537: Epoch 765\n",
      "2025-12-18 23:28:53.447056: Current learning rate: 0.00272\n",
      "2025-12-18 23:31:11.573357: train_loss -0.8631\n",
      "2025-12-18 23:31:11.573357: val_loss -0.8729\n",
      "2025-12-18 23:31:11.589442: Pseudo dice [0.921, 0.9546, 0.9488]\n",
      "2025-12-18 23:31:11.589442: Epoch time: 138.13 s\n",
      "2025-12-18 23:31:11.589442: Yayy! New best EMA pseudo Dice: 0.9415\n",
      "2025-12-18 23:31:12.541756: \n",
      "2025-12-18 23:31:12.541756: Epoch 766\n",
      "2025-12-18 23:31:12.561170: Current learning rate: 0.00271\n",
      "2025-12-18 23:33:30.720073: train_loss -0.8591\n",
      "2025-12-18 23:33:30.720073: val_loss -0.878\n",
      "2025-12-18 23:33:30.729070: Pseudo dice [0.9307, 0.9617, 0.934]\n",
      "2025-12-18 23:33:30.733976: Epoch time: 138.18 s\n",
      "2025-12-18 23:33:30.737980: Yayy! New best EMA pseudo Dice: 0.9415\n",
      "2025-12-18 23:33:31.702812: \n",
      "2025-12-18 23:33:31.702812: Epoch 767\n",
      "2025-12-18 23:33:31.702812: Current learning rate: 0.0027\n",
      "2025-12-18 23:35:49.934917: train_loss -0.8588\n",
      "2025-12-18 23:35:49.934917: val_loss -0.8784\n",
      "2025-12-18 23:35:49.938545: Pseudo dice [0.9297, 0.9572, 0.9408]\n",
      "2025-12-18 23:35:49.938545: Epoch time: 138.23 s\n",
      "2025-12-18 23:35:49.938545: Yayy! New best EMA pseudo Dice: 0.9416\n",
      "2025-12-18 23:35:50.928438: \n",
      "2025-12-18 23:35:50.928438: Epoch 768\n",
      "2025-12-18 23:35:50.932183: Current learning rate: 0.00268\n",
      "2025-12-18 23:38:09.032323: train_loss -0.854\n",
      "2025-12-18 23:38:09.032323: val_loss -0.8758\n",
      "2025-12-18 23:38:09.038329: Pseudo dice [0.9259, 0.9567, 0.9434]\n",
      "2025-12-18 23:38:09.044335: Epoch time: 138.1 s\n",
      "2025-12-18 23:38:09.048339: Yayy! New best EMA pseudo Dice: 0.9417\n",
      "2025-12-18 23:38:09.992738: \n",
      "2025-12-18 23:38:09.992738: Epoch 769\n",
      "2025-12-18 23:38:09.994740: Current learning rate: 0.00267\n",
      "2025-12-18 23:40:28.245404: train_loss -0.8617\n",
      "2025-12-18 23:40:28.245404: val_loss -0.8752\n",
      "2025-12-18 23:40:28.251410: Pseudo dice [0.9263, 0.9579, 0.9449]\n",
      "2025-12-18 23:40:28.256910: Epoch time: 138.25 s\n",
      "2025-12-18 23:40:28.260914: Yayy! New best EMA pseudo Dice: 0.9418\n",
      "2025-12-18 23:40:29.457348: \n",
      "2025-12-18 23:40:29.457348: Epoch 770\n",
      "2025-12-18 23:40:29.461580: Current learning rate: 0.00266\n",
      "2025-12-18 23:42:47.733479: train_loss -0.8625\n",
      "2025-12-18 23:42:47.733479: val_loss -0.8719\n",
      "2025-12-18 23:42:47.733479: Pseudo dice [0.9265, 0.9558, 0.9352]\n",
      "2025-12-18 23:42:47.744103: Epoch time: 138.28 s\n",
      "2025-12-18 23:42:48.447477: \n",
      "2025-12-18 23:42:48.447477: Epoch 771\n",
      "2025-12-18 23:42:48.455227: Current learning rate: 0.00265\n",
      "2025-12-18 23:45:06.603470: train_loss -0.8563\n",
      "2025-12-18 23:45:06.605472: val_loss -0.8744\n",
      "2025-12-18 23:45:06.607635: Pseudo dice [0.927, 0.9582, 0.9355]\n",
      "2025-12-18 23:45:06.616724: Epoch time: 138.16 s\n",
      "2025-12-18 23:45:07.286902: \n",
      "2025-12-18 23:45:07.286902: Epoch 772\n",
      "2025-12-18 23:45:07.286902: Current learning rate: 0.00264\n",
      "2025-12-18 23:47:25.406665: train_loss -0.8595\n",
      "2025-12-18 23:47:25.406665: val_loss -0.8724\n",
      "2025-12-18 23:47:25.412671: Pseudo dice [0.9247, 0.9552, 0.9383]\n",
      "2025-12-18 23:47:25.417630: Epoch time: 138.12 s\n",
      "2025-12-18 23:47:26.109785: \n",
      "2025-12-18 23:47:26.109785: Epoch 773\n",
      "2025-12-18 23:47:26.129067: Current learning rate: 0.00263\n",
      "2025-12-18 23:49:44.111626: train_loss -0.8593\n",
      "2025-12-18 23:49:44.111626: val_loss -0.8774\n",
      "2025-12-18 23:49:44.120857: Pseudo dice [0.9268, 0.9568, 0.9419]\n",
      "2025-12-18 23:49:44.126863: Epoch time: 138.0 s\n",
      "2025-12-18 23:49:44.898424: \n",
      "2025-12-18 23:49:44.898424: Epoch 774\n",
      "2025-12-18 23:49:44.917723: Current learning rate: 0.00262\n",
      "2025-12-18 23:52:03.182278: train_loss -0.8605\n",
      "2025-12-18 23:52:03.182278: val_loss -0.8842\n",
      "2025-12-18 23:52:03.189608: Pseudo dice [0.9326, 0.9603, 0.9435]\n",
      "2025-12-18 23:52:03.193613: Epoch time: 138.28 s\n",
      "2025-12-18 23:52:03.862028: \n",
      "2025-12-18 23:52:03.862028: Epoch 775\n",
      "2025-12-18 23:52:03.872536: Current learning rate: 0.00261\n",
      "2025-12-18 23:54:22.180191: train_loss -0.8584\n",
      "2025-12-18 23:54:22.180191: val_loss -0.8754\n",
      "2025-12-18 23:54:22.180191: Pseudo dice [0.9273, 0.9555, 0.9429]\n",
      "2025-12-18 23:54:22.195994: Epoch time: 138.32 s\n",
      "2025-12-18 23:54:23.019134: \n",
      "2025-12-18 23:54:23.019134: Epoch 776\n",
      "2025-12-18 23:54:23.019134: Current learning rate: 0.0026\n",
      "2025-12-18 23:56:41.275890: train_loss -0.8607\n",
      "2025-12-18 23:56:41.275890: val_loss -0.8756\n",
      "2025-12-18 23:56:41.281896: Pseudo dice [0.9269, 0.9551, 0.9414]\n",
      "2025-12-18 23:56:41.285900: Epoch time: 138.26 s\n",
      "2025-12-18 23:56:42.114562: \n",
      "2025-12-18 23:56:42.114562: Epoch 777\n",
      "2025-12-18 23:56:42.114562: Current learning rate: 0.00259\n",
      "2025-12-18 23:59:00.327641: train_loss -0.8564\n",
      "2025-12-18 23:59:00.329643: val_loss -0.8692\n",
      "2025-12-18 23:59:00.335649: Pseudo dice [0.9217, 0.9568, 0.9377]\n",
      "2025-12-18 23:59:00.341393: Epoch time: 138.21 s\n",
      "2025-12-18 23:59:01.055347: \n",
      "2025-12-18 23:59:01.055347: Epoch 778\n",
      "2025-12-18 23:59:01.071437: Current learning rate: 0.00258\n",
      "2025-12-19 00:01:19.362229: train_loss -0.8586\n",
      "2025-12-19 00:01:19.364232: val_loss -0.8758\n",
      "2025-12-19 00:01:19.371981: Pseudo dice [0.9259, 0.9588, 0.9406]\n",
      "2025-12-19 00:01:19.377987: Epoch time: 138.31 s\n",
      "2025-12-19 00:01:20.064939: \n",
      "2025-12-19 00:01:20.064939: Epoch 779\n",
      "2025-12-19 00:01:20.068960: Current learning rate: 0.00257\n",
      "2025-12-19 00:03:38.294489: train_loss -0.8585\n",
      "2025-12-19 00:03:38.294489: val_loss -0.8821\n",
      "2025-12-19 00:03:38.300495: Pseudo dice [0.9325, 0.9604, 0.9452]\n",
      "2025-12-19 00:03:38.307548: Epoch time: 138.23 s\n",
      "2025-12-19 00:03:38.310551: Yayy! New best EMA pseudo Dice: 0.9419\n",
      "2025-12-19 00:03:39.371040: \n",
      "2025-12-19 00:03:39.371040: Epoch 780\n",
      "2025-12-19 00:03:39.371040: Current learning rate: 0.00256\n",
      "2025-12-19 00:05:57.533255: train_loss -0.8567\n",
      "2025-12-19 00:05:57.533255: val_loss -0.8724\n",
      "2025-12-19 00:05:57.533255: Pseudo dice [0.9248, 0.9558, 0.9406]\n",
      "2025-12-19 00:05:57.549034: Epoch time: 138.16 s\n",
      "2025-12-19 00:05:58.199462: \n",
      "2025-12-19 00:05:58.199462: Epoch 781\n",
      "2025-12-19 00:05:58.215256: Current learning rate: 0.00255\n",
      "2025-12-19 00:08:16.530179: train_loss -0.8573\n",
      "2025-12-19 00:08:16.531967: val_loss -0.8758\n",
      "2025-12-19 00:08:16.531967: Pseudo dice [0.9268, 0.9552, 0.9433]\n",
      "2025-12-19 00:08:16.531967: Epoch time: 138.33 s\n",
      "2025-12-19 00:08:17.372407: \n",
      "2025-12-19 00:08:17.372407: Epoch 782\n",
      "2025-12-19 00:08:17.372407: Current learning rate: 0.00254\n",
      "2025-12-19 00:10:35.874387: train_loss -0.8594\n",
      "2025-12-19 00:10:35.874387: val_loss -0.8775\n",
      "2025-12-19 00:10:35.887468: Pseudo dice [0.9261, 0.9566, 0.9378]\n",
      "2025-12-19 00:10:35.892032: Epoch time: 138.5 s\n",
      "2025-12-19 00:10:36.573892: \n",
      "2025-12-19 00:10:36.573892: Epoch 783\n",
      "2025-12-19 00:10:36.575895: Current learning rate: 0.00253\n",
      "2025-12-19 00:12:54.758232: train_loss -0.8606\n",
      "2025-12-19 00:12:54.758232: val_loss -0.8706\n",
      "2025-12-19 00:12:54.762236: Pseudo dice [0.9227, 0.9558, 0.9367]\n",
      "2025-12-19 00:12:54.762236: Epoch time: 138.19 s\n",
      "2025-12-19 00:12:55.422688: \n",
      "2025-12-19 00:12:55.422688: Epoch 784\n",
      "2025-12-19 00:12:55.438578: Current learning rate: 0.00252\n",
      "2025-12-19 00:15:13.700640: train_loss -0.8619\n",
      "2025-12-19 00:15:13.700640: val_loss -0.875\n",
      "2025-12-19 00:15:13.716350: Pseudo dice [0.9258, 0.9566, 0.9457]\n",
      "2025-12-19 00:15:13.721531: Epoch time: 138.28 s\n",
      "2025-12-19 00:15:14.444734: \n",
      "2025-12-19 00:15:14.444734: Epoch 785\n",
      "2025-12-19 00:15:14.444734: Current learning rate: 0.00251\n",
      "2025-12-19 00:17:32.650534: train_loss -0.8574\n",
      "2025-12-19 00:17:32.650534: val_loss -0.8799\n",
      "2025-12-19 00:17:32.668256: Pseudo dice [0.9283, 0.9614, 0.9435]\n",
      "2025-12-19 00:17:32.668256: Epoch time: 138.21 s\n",
      "2025-12-19 00:17:33.394972: \n",
      "2025-12-19 00:17:33.394972: Epoch 786\n",
      "2025-12-19 00:17:33.394972: Current learning rate: 0.0025\n",
      "2025-12-19 00:19:51.532339: train_loss -0.8606\n",
      "2025-12-19 00:19:51.532339: val_loss -0.8743\n",
      "2025-12-19 00:19:51.540084: Pseudo dice [0.9262, 0.9544, 0.9431]\n",
      "2025-12-19 00:19:51.544088: Epoch time: 138.14 s\n",
      "2025-12-19 00:19:52.207555: \n",
      "2025-12-19 00:19:52.207555: Epoch 787\n",
      "2025-12-19 00:19:52.221574: Current learning rate: 0.00249\n",
      "2025-12-19 00:22:10.545717: train_loss -0.8562\n",
      "2025-12-19 00:22:10.545717: val_loss -0.8739\n",
      "2025-12-19 00:22:10.555730: Pseudo dice [0.9245, 0.9549, 0.9373]\n",
      "2025-12-19 00:22:10.563477: Epoch time: 138.34 s\n",
      "2025-12-19 00:22:11.400148: \n",
      "2025-12-19 00:22:11.400148: Epoch 788\n",
      "2025-12-19 00:22:11.400148: Current learning rate: 0.00248\n",
      "2025-12-19 00:24:29.442208: train_loss -0.8601\n",
      "2025-12-19 00:24:29.457969: val_loss -0.8733\n",
      "2025-12-19 00:24:29.457969: Pseudo dice [0.9259, 0.9568, 0.9352]\n",
      "2025-12-19 00:24:29.457969: Epoch time: 138.04 s\n",
      "2025-12-19 00:24:30.109756: \n",
      "2025-12-19 00:24:30.109756: Epoch 789\n",
      "2025-12-19 00:24:30.125685: Current learning rate: 0.00247\n",
      "2025-12-19 00:26:48.225134: train_loss -0.861\n",
      "2025-12-19 00:26:48.226875: val_loss -0.8764\n",
      "2025-12-19 00:26:48.232882: Pseudo dice [0.9265, 0.9565, 0.938]\n",
      "2025-12-19 00:26:48.236886: Epoch time: 138.12 s\n",
      "2025-12-19 00:26:48.984855: \n",
      "2025-12-19 00:26:48.984855: Epoch 790\n",
      "2025-12-19 00:26:49.000733: Current learning rate: 0.00245\n",
      "2025-12-19 00:29:07.036000: train_loss -0.8585\n",
      "2025-12-19 00:29:07.038002: val_loss -0.8846\n",
      "2025-12-19 00:29:07.045748: Pseudo dice [0.9318, 0.9615, 0.9438]\n",
      "2025-12-19 00:29:07.049752: Epoch time: 138.05 s\n",
      "2025-12-19 00:29:07.715431: \n",
      "2025-12-19 00:29:07.715431: Epoch 791\n",
      "2025-12-19 00:29:07.715431: Current learning rate: 0.00244\n",
      "2025-12-19 00:31:25.787392: train_loss -0.8572\n",
      "2025-12-19 00:31:25.789394: val_loss -0.8698\n",
      "2025-12-19 00:31:25.795138: Pseudo dice [0.9255, 0.955, 0.9377]\n",
      "2025-12-19 00:31:25.797140: Epoch time: 138.07 s\n",
      "2025-12-19 00:31:26.458252: \n",
      "2025-12-19 00:31:26.458252: Epoch 792\n",
      "2025-12-19 00:31:26.474179: Current learning rate: 0.00243\n",
      "2025-12-19 00:33:44.651338: train_loss -0.8541\n",
      "2025-12-19 00:33:44.653340: val_loss -0.8728\n",
      "2025-12-19 00:33:44.661087: Pseudo dice [0.9309, 0.9578, 0.9284]\n",
      "2025-12-19 00:33:44.668834: Epoch time: 138.19 s\n",
      "2025-12-19 00:33:45.446995: \n",
      "2025-12-19 00:33:45.446995: Epoch 793\n",
      "2025-12-19 00:33:45.462775: Current learning rate: 0.00242\n",
      "2025-12-19 00:36:03.711941: train_loss -0.8482\n",
      "2025-12-19 00:36:03.711941: val_loss -0.8774\n",
      "2025-12-19 00:36:03.727738: Pseudo dice [0.9307, 0.9631, 0.9374]\n",
      "2025-12-19 00:36:03.727738: Epoch time: 138.26 s\n",
      "2025-12-19 00:36:04.552678: \n",
      "2025-12-19 00:36:04.552678: Epoch 794\n",
      "2025-12-19 00:36:04.552678: Current learning rate: 0.00241\n",
      "2025-12-19 00:38:22.769963: train_loss -0.8591\n",
      "2025-12-19 00:38:22.769963: val_loss -0.8709\n",
      "2025-12-19 00:38:22.769963: Pseudo dice [0.9231, 0.9548, 0.943]\n",
      "2025-12-19 00:38:22.769963: Epoch time: 138.22 s\n",
      "2025-12-19 00:38:23.435684: \n",
      "2025-12-19 00:38:23.435684: Epoch 795\n",
      "2025-12-19 00:38:23.435684: Current learning rate: 0.0024\n",
      "2025-12-19 00:40:41.555727: train_loss -0.8609\n",
      "2025-12-19 00:40:41.557729: val_loss -0.8852\n",
      "2025-12-19 00:40:41.562317: Pseudo dice [0.9326, 0.9624, 0.9464]\n",
      "2025-12-19 00:40:41.562317: Epoch time: 138.12 s\n",
      "2025-12-19 00:40:42.320145: \n",
      "2025-12-19 00:40:42.320145: Epoch 796\n",
      "2025-12-19 00:40:42.336244: Current learning rate: 0.00239\n",
      "2025-12-19 00:43:00.494617: train_loss -0.8574\n",
      "2025-12-19 00:43:00.494617: val_loss -0.8789\n",
      "2025-12-19 00:43:00.502364: Pseudo dice [0.9271, 0.9574, 0.9431]\n",
      "2025-12-19 00:43:00.508369: Epoch time: 138.17 s\n",
      "2025-12-19 00:43:00.516116: Yayy! New best EMA pseudo Dice: 0.9419\n",
      "2025-12-19 00:43:01.445702: \n",
      "2025-12-19 00:43:01.445702: Epoch 797\n",
      "2025-12-19 00:43:01.445702: Current learning rate: 0.00238\n",
      "2025-12-19 00:45:19.769773: train_loss -0.8617\n",
      "2025-12-19 00:45:19.771775: val_loss -0.8743\n",
      "2025-12-19 00:45:19.771775: Pseudo dice [0.9256, 0.9557, 0.946]\n",
      "2025-12-19 00:45:19.778324: Epoch time: 138.33 s\n",
      "2025-12-19 00:45:19.778324: Yayy! New best EMA pseudo Dice: 0.942\n",
      "2025-12-19 00:45:20.729810: \n",
      "2025-12-19 00:45:20.731813: Epoch 798\n",
      "2025-12-19 00:45:20.731813: Current learning rate: 0.00237\n",
      "2025-12-19 00:47:38.952540: train_loss -0.859\n",
      "2025-12-19 00:47:38.954041: val_loss -0.8738\n",
      "2025-12-19 00:47:38.960303: Pseudo dice [0.9287, 0.9532, 0.9397]\n",
      "2025-12-19 00:47:38.964306: Epoch time: 138.22 s\n",
      "2025-12-19 00:47:39.842990: \n",
      "2025-12-19 00:47:39.844992: Epoch 799\n",
      "2025-12-19 00:47:39.844992: Current learning rate: 0.00236\n",
      "2025-12-19 00:49:59.100657: train_loss -0.8504\n",
      "2025-12-19 00:49:59.104160: val_loss -0.8652\n",
      "2025-12-19 00:49:59.104160: Pseudo dice [0.923, 0.9532, 0.9316]\n",
      "2025-12-19 00:49:59.104160: Epoch time: 139.26 s\n",
      "2025-12-19 00:50:00.036371: \n",
      "2025-12-19 00:50:00.038373: Epoch 800\n",
      "2025-12-19 00:50:00.044230: Current learning rate: 0.00235\n",
      "2025-12-19 00:52:18.821812: train_loss -0.8619\n",
      "2025-12-19 00:52:18.821812: val_loss -0.895\n",
      "2025-12-19 00:52:18.827818: Pseudo dice [0.941, 0.9636, 0.9475]\n",
      "2025-12-19 00:52:18.833645: Epoch time: 138.79 s\n",
      "2025-12-19 00:52:18.839651: Yayy! New best EMA pseudo Dice: 0.9422\n",
      "2025-12-19 00:52:19.779027: \n",
      "2025-12-19 00:52:19.779027: Epoch 801\n",
      "2025-12-19 00:52:19.779027: Current learning rate: 0.00234\n",
      "2025-12-19 00:54:38.514163: train_loss -0.8561\n",
      "2025-12-19 00:54:38.515171: val_loss -0.8773\n",
      "2025-12-19 00:54:38.518036: Pseudo dice [0.9258, 0.96, 0.9473]\n",
      "2025-12-19 00:54:38.518036: Epoch time: 138.74 s\n",
      "2025-12-19 00:54:38.518036: Yayy! New best EMA pseudo Dice: 0.9424\n",
      "2025-12-19 00:54:39.472028: \n",
      "2025-12-19 00:54:39.472028: Epoch 802\n",
      "2025-12-19 00:54:39.472028: Current learning rate: 0.00233\n",
      "2025-12-19 00:56:58.267587: train_loss -0.8556\n",
      "2025-12-19 00:56:58.267587: val_loss -0.8702\n",
      "2025-12-19 00:56:58.275333: Pseudo dice [0.9205, 0.9512, 0.9455]\n",
      "2025-12-19 00:56:58.279337: Epoch time: 138.8 s\n",
      "2025-12-19 00:56:59.046315: \n",
      "2025-12-19 00:56:59.046315: Epoch 803\n",
      "2025-12-19 00:56:59.046315: Current learning rate: 0.00232\n",
      "2025-12-19 00:59:17.580174: train_loss -0.8562\n",
      "2025-12-19 00:59:17.580174: val_loss -0.8803\n",
      "2025-12-19 00:59:17.589192: Pseudo dice [0.9319, 0.9599, 0.9362]\n",
      "2025-12-19 00:59:17.597205: Epoch time: 138.55 s\n",
      "2025-12-19 00:59:18.272507: \n",
      "2025-12-19 00:59:18.272507: Epoch 804\n",
      "2025-12-19 00:59:18.272507: Current learning rate: 0.00231\n",
      "2025-12-19 01:01:36.592858: train_loss -0.8562\n",
      "2025-12-19 01:01:36.592858: val_loss -0.8753\n",
      "2025-12-19 01:01:36.592858: Pseudo dice [0.9272, 0.9558, 0.9431]\n",
      "2025-12-19 01:01:36.610546: Epoch time: 138.32 s\n",
      "2025-12-19 01:01:37.443228: \n",
      "2025-12-19 01:01:37.443228: Epoch 805\n",
      "2025-12-19 01:01:37.449206: Current learning rate: 0.0023\n",
      "2025-12-19 01:03:55.703164: train_loss -0.8548\n",
      "2025-12-19 01:03:55.703164: val_loss -0.8712\n",
      "2025-12-19 01:03:55.708908: Pseudo dice [0.9216, 0.9531, 0.9441]\n",
      "2025-12-19 01:03:55.708908: Epoch time: 138.26 s\n",
      "2025-12-19 01:03:56.527824: \n",
      "2025-12-19 01:03:56.527824: Epoch 806\n",
      "2025-12-19 01:03:56.529515: Current learning rate: 0.00229\n",
      "2025-12-19 01:06:14.713365: train_loss -0.8592\n",
      "2025-12-19 01:06:14.713365: val_loss -0.8767\n",
      "2025-12-19 01:06:14.729281: Pseudo dice [0.9282, 0.9575, 0.944]\n",
      "2025-12-19 01:06:14.729281: Epoch time: 138.2 s\n",
      "2025-12-19 01:06:15.395682: \n",
      "2025-12-19 01:06:15.395682: Epoch 807\n",
      "2025-12-19 01:06:15.395682: Current learning rate: 0.00228\n",
      "2025-12-19 01:08:33.554083: train_loss -0.8588\n",
      "2025-12-19 01:08:33.554083: val_loss -0.8737\n",
      "2025-12-19 01:08:33.561325: Pseudo dice [0.9259, 0.9547, 0.9373]\n",
      "2025-12-19 01:08:33.567331: Epoch time: 138.16 s\n",
      "2025-12-19 01:08:34.251654: \n",
      "2025-12-19 01:08:34.251654: Epoch 808\n",
      "2025-12-19 01:08:34.251654: Current learning rate: 0.00226\n",
      "2025-12-19 01:10:52.857283: train_loss -0.8574\n",
      "2025-12-19 01:10:52.859285: val_loss -0.879\n",
      "2025-12-19 01:10:52.865597: Pseudo dice [0.9277, 0.9562, 0.9419]\n",
      "2025-12-19 01:10:52.869602: Epoch time: 138.61 s\n",
      "2025-12-19 01:10:53.603991: \n",
      "2025-12-19 01:10:53.603991: Epoch 809\n",
      "2025-12-19 01:10:53.611584: Current learning rate: 0.00225\n",
      "2025-12-19 01:13:11.714190: train_loss -0.8557\n",
      "2025-12-19 01:13:11.716009: val_loss -0.8821\n",
      "2025-12-19 01:13:11.722015: Pseudo dice [0.9292, 0.9597, 0.9439]\n",
      "2025-12-19 01:13:11.726020: Epoch time: 138.11 s\n",
      "2025-12-19 01:13:12.548405: \n",
      "2025-12-19 01:13:12.548405: Epoch 810\n",
      "2025-12-19 01:13:12.548405: Current learning rate: 0.00224\n",
      "2025-12-19 01:15:30.652410: train_loss -0.8581\n",
      "2025-12-19 01:15:30.652410: val_loss -0.8759\n",
      "2025-12-19 01:15:30.658416: Pseudo dice [0.9269, 0.9535, 0.9479]\n",
      "2025-12-19 01:15:30.664421: Epoch time: 138.1 s\n",
      "2025-12-19 01:15:31.336008: \n",
      "2025-12-19 01:15:31.336008: Epoch 811\n",
      "2025-12-19 01:15:31.341560: Current learning rate: 0.00223\n",
      "2025-12-19 01:17:49.532230: train_loss -0.8562\n",
      "2025-12-19 01:17:49.532230: val_loss -0.8797\n",
      "2025-12-19 01:17:49.538236: Pseudo dice [0.9295, 0.9554, 0.9488]\n",
      "2025-12-19 01:17:49.539976: Epoch time: 138.2 s\n",
      "2025-12-19 01:17:50.222353: \n",
      "2025-12-19 01:17:50.222353: Epoch 812\n",
      "2025-12-19 01:17:50.228523: Current learning rate: 0.00222\n",
      "2025-12-19 01:20:08.370044: train_loss -0.856\n",
      "2025-12-19 01:20:08.370044: val_loss -0.8761\n",
      "2025-12-19 01:20:08.379798: Pseudo dice [0.9238, 0.9556, 0.9483]\n",
      "2025-12-19 01:20:08.383802: Epoch time: 138.15 s\n",
      "2025-12-19 01:20:09.126480: \n",
      "2025-12-19 01:20:09.128220: Epoch 813\n",
      "2025-12-19 01:20:09.129224: Current learning rate: 0.00221\n",
      "2025-12-19 01:22:27.345412: train_loss -0.8563\n",
      "2025-12-19 01:22:27.345412: val_loss -0.8784\n",
      "2025-12-19 01:22:27.349245: Pseudo dice [0.93, 0.9559, 0.9374]\n",
      "2025-12-19 01:22:27.349245: Epoch time: 138.22 s\n",
      "2025-12-19 01:22:28.027162: \n",
      "2025-12-19 01:22:28.027162: Epoch 814\n",
      "2025-12-19 01:22:28.027162: Current learning rate: 0.0022\n",
      "2025-12-19 01:24:46.338979: train_loss -0.8576\n",
      "2025-12-19 01:24:46.338979: val_loss -0.8796\n",
      "2025-12-19 01:24:46.345319: Pseudo dice [0.9305, 0.957, 0.935]\n",
      "2025-12-19 01:24:46.347322: Epoch time: 138.31 s\n",
      "2025-12-19 01:24:47.106721: \n",
      "2025-12-19 01:24:47.106721: Epoch 815\n",
      "2025-12-19 01:24:47.106721: Current learning rate: 0.00219\n",
      "2025-12-19 01:27:05.355545: train_loss -0.8602\n",
      "2025-12-19 01:27:05.355545: val_loss -0.8789\n",
      "2025-12-19 01:27:05.355545: Pseudo dice [0.928, 0.9582, 0.9388]\n",
      "2025-12-19 01:27:05.355545: Epoch time: 138.25 s\n",
      "2025-12-19 01:27:06.178825: \n",
      "2025-12-19 01:27:06.178825: Epoch 816\n",
      "2025-12-19 01:27:06.194591: Current learning rate: 0.00218\n",
      "2025-12-19 01:29:24.465570: train_loss -0.8611\n",
      "2025-12-19 01:29:24.465570: val_loss -0.8663\n",
      "2025-12-19 01:29:24.465570: Pseudo dice [0.9233, 0.9472, 0.9333]\n",
      "2025-12-19 01:29:24.465570: Epoch time: 138.29 s\n",
      "2025-12-19 01:29:25.141377: \n",
      "2025-12-19 01:29:25.141377: Epoch 817\n",
      "2025-12-19 01:29:25.147768: Current learning rate: 0.00217\n",
      "2025-12-19 01:31:43.360452: train_loss -0.8635\n",
      "2025-12-19 01:31:43.362456: val_loss -0.8752\n",
      "2025-12-19 01:31:43.370465: Pseudo dice [0.9298, 0.9589, 0.9312]\n",
      "2025-12-19 01:31:43.376210: Epoch time: 138.22 s\n",
      "2025-12-19 01:31:44.037658: \n",
      "2025-12-19 01:31:44.037658: Epoch 818\n",
      "2025-12-19 01:31:44.053481: Current learning rate: 0.00216\n",
      "2025-12-19 01:34:02.277924: train_loss -0.8599\n",
      "2025-12-19 01:34:02.277924: val_loss -0.8653\n",
      "2025-12-19 01:34:02.285732: Pseudo dice [0.9187, 0.9573, 0.9363]\n",
      "2025-12-19 01:34:02.289736: Epoch time: 138.24 s\n",
      "2025-12-19 01:34:02.957198: \n",
      "2025-12-19 01:34:02.957198: Epoch 819\n",
      "2025-12-19 01:34:02.957198: Current learning rate: 0.00215\n",
      "2025-12-19 01:36:21.077157: train_loss -0.8594\n",
      "2025-12-19 01:36:21.077157: val_loss -0.8791\n",
      "2025-12-19 01:36:21.095093: Pseudo dice [0.9301, 0.959, 0.941]\n",
      "2025-12-19 01:36:21.099097: Epoch time: 138.12 s\n",
      "2025-12-19 01:36:21.728064: \n",
      "2025-12-19 01:36:21.728064: Epoch 820\n",
      "2025-12-19 01:36:21.743914: Current learning rate: 0.00214\n",
      "2025-12-19 01:38:39.862907: train_loss -0.8538\n",
      "2025-12-19 01:38:39.864910: val_loss -0.8694\n",
      "2025-12-19 01:38:39.870917: Pseudo dice [0.9234, 0.9558, 0.9374]\n",
      "2025-12-19 01:38:39.872919: Epoch time: 138.13 s\n",
      "2025-12-19 01:38:40.530190: \n",
      "2025-12-19 01:38:40.532192: Epoch 821\n",
      "2025-12-19 01:38:40.532192: Current learning rate: 0.00213\n",
      "2025-12-19 01:40:58.578146: train_loss -0.8608\n",
      "2025-12-19 01:40:58.580149: val_loss -0.8722\n",
      "2025-12-19 01:40:58.586157: Pseudo dice [0.9237, 0.9565, 0.9394]\n",
      "2025-12-19 01:40:58.591447: Epoch time: 138.05 s\n",
      "2025-12-19 01:40:59.404001: \n",
      "2025-12-19 01:40:59.404001: Epoch 822\n",
      "2025-12-19 01:40:59.404001: Current learning rate: 0.00212\n",
      "2025-12-19 01:43:17.434431: train_loss -0.8594\n",
      "2025-12-19 01:43:17.434431: val_loss -0.8714\n",
      "2025-12-19 01:43:17.445935: Pseudo dice [0.9245, 0.9513, 0.9385]\n",
      "2025-12-19 01:43:17.449939: Epoch time: 138.03 s\n",
      "2025-12-19 01:43:18.098128: \n",
      "2025-12-19 01:43:18.098128: Epoch 823\n",
      "2025-12-19 01:43:18.098128: Current learning rate: 0.0021\n",
      "2025-12-19 01:45:36.246262: train_loss -0.8636\n",
      "2025-12-19 01:45:36.246262: val_loss -0.8797\n",
      "2025-12-19 01:45:36.246262: Pseudo dice [0.9283, 0.9574, 0.9446]\n",
      "2025-12-19 01:45:36.262289: Epoch time: 138.15 s\n",
      "2025-12-19 01:45:36.976378: \n",
      "2025-12-19 01:45:36.976378: Epoch 824\n",
      "2025-12-19 01:45:36.976378: Current learning rate: 0.00209\n",
      "2025-12-19 01:47:55.230092: train_loss -0.8592\n",
      "2025-12-19 01:47:55.230092: val_loss -0.883\n",
      "2025-12-19 01:47:55.236098: Pseudo dice [0.9315, 0.9582, 0.9468]\n",
      "2025-12-19 01:47:55.242104: Epoch time: 138.26 s\n",
      "2025-12-19 01:47:55.889227: \n",
      "2025-12-19 01:47:55.889227: Epoch 825\n",
      "2025-12-19 01:47:55.889227: Current learning rate: 0.00208\n",
      "2025-12-19 01:50:13.961223: train_loss -0.8573\n",
      "2025-12-19 01:50:13.961223: val_loss -0.8858\n",
      "2025-12-19 01:50:13.961223: Pseudo dice [0.9336, 0.9597, 0.9428]\n",
      "2025-12-19 01:50:13.976980: Epoch time: 138.07 s\n",
      "2025-12-19 01:50:14.640506: \n",
      "2025-12-19 01:50:14.640506: Epoch 826\n",
      "2025-12-19 01:50:14.640506: Current learning rate: 0.00207\n",
      "2025-12-19 01:52:32.911507: train_loss -0.8602\n",
      "2025-12-19 01:52:32.911507: val_loss -0.8807\n",
      "2025-12-19 01:52:32.918514: Pseudo dice [0.929, 0.9567, 0.9482]\n",
      "2025-12-19 01:52:32.920727: Epoch time: 138.27 s\n",
      "2025-12-19 01:52:33.658146: \n",
      "2025-12-19 01:52:33.658146: Epoch 827\n",
      "2025-12-19 01:52:33.658146: Current learning rate: 0.00206\n",
      "2025-12-19 01:54:51.942712: train_loss -0.8607\n",
      "2025-12-19 01:54:51.942712: val_loss -0.8789\n",
      "2025-12-19 01:54:51.948718: Pseudo dice [0.9289, 0.9582, 0.9417]\n",
      "2025-12-19 01:54:51.954724: Epoch time: 138.28 s\n",
      "2025-12-19 01:54:52.579853: \n",
      "2025-12-19 01:54:52.579853: Epoch 828\n",
      "2025-12-19 01:54:52.595902: Current learning rate: 0.00205\n",
      "2025-12-19 01:57:10.839687: train_loss -0.8652\n",
      "2025-12-19 01:57:10.839687: val_loss -0.8899\n",
      "2025-12-19 01:57:10.855340: Pseudo dice [0.9358, 0.9629, 0.9398]\n",
      "2025-12-19 01:57:10.855340: Epoch time: 138.26 s\n",
      "2025-12-19 01:57:10.855340: Yayy! New best EMA pseudo Dice: 0.9425\n",
      "2025-12-19 01:57:11.997550: \n",
      "2025-12-19 01:57:11.997550: Epoch 829\n",
      "2025-12-19 01:57:11.997550: Current learning rate: 0.00204\n",
      "2025-12-19 01:59:30.152996: train_loss -0.8638\n",
      "2025-12-19 01:59:30.154819: val_loss -0.8837\n",
      "2025-12-19 01:59:30.160825: Pseudo dice [0.9315, 0.9574, 0.9453]\n",
      "2025-12-19 01:59:30.164829: Epoch time: 138.16 s\n",
      "2025-12-19 01:59:30.170835: Yayy! New best EMA pseudo Dice: 0.9427\n",
      "2025-12-19 01:59:31.097703: \n",
      "2025-12-19 01:59:31.097703: Epoch 830\n",
      "2025-12-19 01:59:31.097703: Current learning rate: 0.00203\n",
      "2025-12-19 02:01:49.278834: train_loss -0.8628\n",
      "2025-12-19 02:01:49.278834: val_loss -0.8766\n",
      "2025-12-19 02:01:49.284839: Pseudo dice [0.9291, 0.9571, 0.9327]\n",
      "2025-12-19 02:01:49.290584: Epoch time: 138.18 s\n",
      "2025-12-19 02:01:50.011681: \n",
      "2025-12-19 02:01:50.013422: Epoch 831\n",
      "2025-12-19 02:01:50.016190: Current learning rate: 0.00202\n",
      "2025-12-19 02:04:08.238538: train_loss -0.8593\n",
      "2025-12-19 02:04:08.238538: val_loss -0.8763\n",
      "2025-12-19 02:04:08.248051: Pseudo dice [0.927, 0.9583, 0.943]\n",
      "2025-12-19 02:04:08.254093: Epoch time: 138.23 s\n",
      "2025-12-19 02:04:08.897456: \n",
      "2025-12-19 02:04:08.897456: Epoch 832\n",
      "2025-12-19 02:04:08.900169: Current learning rate: 0.00201\n",
      "2025-12-19 02:06:26.997820: train_loss -0.8607\n",
      "2025-12-19 02:06:26.997820: val_loss -0.8826\n",
      "2025-12-19 02:06:27.003407: Pseudo dice [0.9294, 0.9596, 0.9408]\n",
      "2025-12-19 02:06:27.009413: Epoch time: 138.1 s\n",
      "2025-12-19 02:06:27.669174: \n",
      "2025-12-19 02:06:27.671177: Epoch 833\n",
      "2025-12-19 02:06:27.672920: Current learning rate: 0.002\n",
      "2025-12-19 02:08:45.896767: train_loss -0.8626\n",
      "2025-12-19 02:08:45.896767: val_loss -0.8756\n",
      "2025-12-19 02:08:45.896767: Pseudo dice [0.9267, 0.9533, 0.9488]\n",
      "2025-12-19 02:08:45.896767: Epoch time: 138.23 s\n",
      "2025-12-19 02:08:46.549844: \n",
      "2025-12-19 02:08:46.549844: Epoch 834\n",
      "2025-12-19 02:08:46.553963: Current learning rate: 0.00199\n",
      "2025-12-19 02:11:04.904953: train_loss -0.8601\n",
      "2025-12-19 02:11:04.904953: val_loss -0.879\n",
      "2025-12-19 02:11:04.914701: Pseudo dice [0.93, 0.956, 0.9386]\n",
      "2025-12-19 02:11:04.920707: Epoch time: 138.36 s\n",
      "2025-12-19 02:11:05.560389: \n",
      "2025-12-19 02:11:05.560389: Epoch 835\n",
      "2025-12-19 02:11:05.560389: Current learning rate: 0.00198\n",
      "2025-12-19 02:13:23.669363: train_loss -0.8587\n",
      "2025-12-19 02:13:23.669363: val_loss -0.8743\n",
      "2025-12-19 02:13:23.671365: Pseudo dice [0.9244, 0.9555, 0.9397]\n",
      "2025-12-19 02:13:23.684613: Epoch time: 138.11 s\n",
      "2025-12-19 02:13:24.383518: \n",
      "2025-12-19 02:13:24.383518: Epoch 836\n",
      "2025-12-19 02:13:24.399359: Current learning rate: 0.00196\n",
      "2025-12-19 02:15:42.534425: train_loss -0.8599\n",
      "2025-12-19 02:15:42.534425: val_loss -0.8816\n",
      "2025-12-19 02:15:42.550237: Pseudo dice [0.9331, 0.962, 0.9402]\n",
      "2025-12-19 02:15:42.550237: Epoch time: 138.15 s\n",
      "2025-12-19 02:15:43.184424: \n",
      "2025-12-19 02:15:43.184424: Epoch 837\n",
      "2025-12-19 02:15:43.184424: Current learning rate: 0.00195\n",
      "2025-12-19 02:18:01.414402: train_loss -0.8641\n",
      "2025-12-19 02:18:01.414402: val_loss -0.8762\n",
      "2025-12-19 02:18:01.414402: Pseudo dice [0.9237, 0.954, 0.941]\n",
      "2025-12-19 02:18:01.430304: Epoch time: 138.23 s\n",
      "2025-12-19 02:18:02.047481: \n",
      "2025-12-19 02:18:02.047481: Epoch 838\n",
      "2025-12-19 02:18:02.063249: Current learning rate: 0.00194\n",
      "2025-12-19 02:20:20.184736: train_loss -0.8648\n",
      "2025-12-19 02:20:20.184736: val_loss -0.8868\n",
      "2025-12-19 02:20:20.184736: Pseudo dice [0.9351, 0.9609, 0.9378]\n",
      "2025-12-19 02:20:20.184736: Epoch time: 138.14 s\n",
      "2025-12-19 02:20:20.821925: \n",
      "2025-12-19 02:20:20.821925: Epoch 839\n",
      "2025-12-19 02:20:20.831938: Current learning rate: 0.00193\n",
      "2025-12-19 02:22:38.935257: train_loss -0.8629\n",
      "2025-12-19 02:22:38.935257: val_loss -0.8849\n",
      "2025-12-19 02:22:38.935257: Pseudo dice [0.9339, 0.9595, 0.9392]\n",
      "2025-12-19 02:22:38.935257: Epoch time: 138.11 s\n",
      "2025-12-19 02:22:39.573977: \n",
      "2025-12-19 02:22:39.575979: Epoch 840\n",
      "2025-12-19 02:22:39.577721: Current learning rate: 0.00192\n",
      "2025-12-19 02:24:57.723215: train_loss -0.8651\n",
      "2025-12-19 02:24:57.723215: val_loss -0.8856\n",
      "2025-12-19 02:24:57.731002: Pseudo dice [0.9346, 0.9599, 0.9384]\n",
      "2025-12-19 02:24:57.737008: Epoch time: 138.15 s\n",
      "2025-12-19 02:24:57.739010: Yayy! New best EMA pseudo Dice: 0.9428\n",
      "2025-12-19 02:24:58.818444: \n",
      "2025-12-19 02:24:58.818444: Epoch 841\n",
      "2025-12-19 02:24:58.834486: Current learning rate: 0.00191\n",
      "2025-12-19 02:27:16.946901: train_loss -0.8626\n",
      "2025-12-19 02:27:16.948904: val_loss -0.8779\n",
      "2025-12-19 02:27:16.948904: Pseudo dice [0.925, 0.9545, 0.9425]\n",
      "2025-12-19 02:27:16.955315: Epoch time: 138.13 s\n",
      "2025-12-19 02:27:17.614735: \n",
      "2025-12-19 02:27:17.614735: Epoch 842\n",
      "2025-12-19 02:27:17.620754: Current learning rate: 0.0019\n",
      "2025-12-19 02:29:35.527150: train_loss -0.8669\n",
      "2025-12-19 02:29:35.527150: val_loss -0.8848\n",
      "2025-12-19 02:29:35.546907: Pseudo dice [0.9294, 0.9621, 0.945]\n",
      "2025-12-19 02:29:35.552913: Epoch time: 137.93 s\n",
      "2025-12-19 02:29:35.556916: Yayy! New best EMA pseudo Dice: 0.9429\n",
      "2025-12-19 02:29:36.479404: \n",
      "2025-12-19 02:29:36.479404: Epoch 843\n",
      "2025-12-19 02:29:36.493273: Current learning rate: 0.00189\n",
      "2025-12-19 02:31:54.695480: train_loss -0.8626\n",
      "2025-12-19 02:31:54.695480: val_loss -0.8791\n",
      "2025-12-19 02:31:54.703488: Pseudo dice [0.93, 0.9568, 0.9359]\n",
      "2025-12-19 02:31:54.707492: Epoch time: 138.22 s\n",
      "2025-12-19 02:31:55.346330: \n",
      "2025-12-19 02:31:55.346330: Epoch 844\n",
      "2025-12-19 02:31:55.353445: Current learning rate: 0.00188\n",
      "2025-12-19 02:34:13.527336: train_loss -0.8567\n",
      "2025-12-19 02:34:13.527336: val_loss -0.8821\n",
      "2025-12-19 02:34:13.527336: Pseudo dice [0.9317, 0.9572, 0.9476]\n",
      "2025-12-19 02:34:13.539997: Epoch time: 138.18 s\n",
      "2025-12-19 02:34:13.543295: Yayy! New best EMA pseudo Dice: 0.9429\n",
      "2025-12-19 02:34:14.509162: \n",
      "2025-12-19 02:34:14.509162: Epoch 845\n",
      "2025-12-19 02:34:14.509162: Current learning rate: 0.00187\n",
      "2025-12-19 02:36:32.710570: train_loss -0.8617\n",
      "2025-12-19 02:36:32.710570: val_loss -0.8755\n",
      "2025-12-19 02:36:32.719765: Pseudo dice [0.9279, 0.9561, 0.944]\n",
      "2025-12-19 02:36:32.726511: Epoch time: 138.2 s\n",
      "2025-12-19 02:36:33.557430: \n",
      "2025-12-19 02:36:33.557430: Epoch 846\n",
      "2025-12-19 02:36:33.573215: Current learning rate: 0.00186\n",
      "2025-12-19 02:38:51.741091: train_loss -0.8601\n",
      "2025-12-19 02:38:51.741091: val_loss -0.879\n",
      "2025-12-19 02:38:51.753095: Pseudo dice [0.9311, 0.9563, 0.9411]\n",
      "2025-12-19 02:38:51.758841: Epoch time: 138.18 s\n",
      "2025-12-19 02:38:52.388669: \n",
      "2025-12-19 02:38:52.388669: Epoch 847\n",
      "2025-12-19 02:38:52.404294: Current learning rate: 0.00185\n",
      "2025-12-19 02:41:10.675260: train_loss -0.8589\n",
      "2025-12-19 02:41:10.675260: val_loss -0.8795\n",
      "2025-12-19 02:41:10.675260: Pseudo dice [0.9281, 0.9569, 0.9457]\n",
      "2025-12-19 02:41:10.675260: Epoch time: 138.29 s\n",
      "2025-12-19 02:41:10.692944: Yayy! New best EMA pseudo Dice: 0.943\n",
      "2025-12-19 02:41:11.661896: \n",
      "2025-12-19 02:41:11.661896: Epoch 848\n",
      "2025-12-19 02:41:11.668558: Current learning rate: 0.00184\n",
      "2025-12-19 02:43:29.653173: train_loss -0.8635\n",
      "2025-12-19 02:43:29.653173: val_loss -0.8785\n",
      "2025-12-19 02:43:29.653173: Pseudo dice [0.9293, 0.9564, 0.9396]\n",
      "2025-12-19 02:43:29.663594: Epoch time: 137.99 s\n",
      "2025-12-19 02:43:30.291294: \n",
      "2025-12-19 02:43:30.291294: Epoch 849\n",
      "2025-12-19 02:43:30.291294: Current learning rate: 0.00182\n",
      "2025-12-19 02:45:48.533319: train_loss -0.8608\n",
      "2025-12-19 02:45:48.533319: val_loss -0.8707\n",
      "2025-12-19 02:45:48.535935: Pseudo dice [0.9201, 0.955, 0.9458]\n",
      "2025-12-19 02:45:48.535935: Epoch time: 138.24 s\n",
      "2025-12-19 02:45:49.438667: \n",
      "2025-12-19 02:45:49.438667: Epoch 850\n",
      "2025-12-19 02:45:49.438667: Current learning rate: 0.00181\n",
      "2025-12-19 02:48:07.718066: train_loss -0.8625\n",
      "2025-12-19 02:48:07.718066: val_loss -0.8768\n",
      "2025-12-19 02:48:07.729577: Pseudo dice [0.9286, 0.9554, 0.9341]\n",
      "2025-12-19 02:48:07.733581: Epoch time: 138.28 s\n",
      "2025-12-19 02:48:08.362493: \n",
      "2025-12-19 02:48:08.362493: Epoch 851\n",
      "2025-12-19 02:48:08.376302: Current learning rate: 0.0018\n",
      "2025-12-19 02:50:26.573678: train_loss -0.8649\n",
      "2025-12-19 02:50:26.573678: val_loss -0.8865\n",
      "2025-12-19 02:50:26.594398: Pseudo dice [0.9354, 0.9617, 0.9443]\n",
      "2025-12-19 02:50:26.600404: Epoch time: 138.21 s\n",
      "2025-12-19 02:50:27.484426: \n",
      "2025-12-19 02:50:27.484426: Epoch 852\n",
      "2025-12-19 02:50:27.490974: Current learning rate: 0.00179\n",
      "2025-12-19 02:52:45.527710: train_loss -0.8607\n",
      "2025-12-19 02:52:45.527710: val_loss -0.8909\n",
      "2025-12-19 02:52:45.543807: Pseudo dice [0.9371, 0.9597, 0.9481]\n",
      "2025-12-19 02:52:45.543807: Epoch time: 138.05 s\n",
      "2025-12-19 02:52:45.543807: Yayy! New best EMA pseudo Dice: 0.9433\n",
      "2025-12-19 02:52:46.462801: \n",
      "2025-12-19 02:52:46.462801: Epoch 853\n",
      "2025-12-19 02:52:46.478559: Current learning rate: 0.00178\n",
      "2025-12-19 02:55:04.943769: train_loss -0.862\n",
      "2025-12-19 02:55:04.943769: val_loss -0.8756\n",
      "2025-12-19 02:55:04.953017: Pseudo dice [0.9256, 0.9568, 0.9442]\n",
      "2025-12-19 02:55:04.959023: Epoch time: 138.48 s\n",
      "2025-12-19 02:55:05.575692: \n",
      "2025-12-19 02:55:05.575692: Epoch 854\n",
      "2025-12-19 02:55:05.591493: Current learning rate: 0.00177\n",
      "2025-12-19 02:57:23.657552: train_loss -0.8618\n",
      "2025-12-19 02:57:23.657552: val_loss -0.8815\n",
      "2025-12-19 02:57:23.665769: Pseudo dice [0.9337, 0.9588, 0.939]\n",
      "2025-12-19 02:57:23.673777: Epoch time: 138.08 s\n",
      "2025-12-19 02:57:24.299095: \n",
      "2025-12-19 02:57:24.299095: Epoch 855\n",
      "2025-12-19 02:57:24.299095: Current learning rate: 0.00176\n",
      "2025-12-19 02:59:42.417429: train_loss -0.859\n",
      "2025-12-19 02:59:42.417429: val_loss -0.8754\n",
      "2025-12-19 02:59:42.428935: Pseudo dice [0.9264, 0.9526, 0.9413]\n",
      "2025-12-19 02:59:42.432939: Epoch time: 138.12 s\n",
      "2025-12-19 02:59:43.050288: \n",
      "2025-12-19 02:59:43.050288: Epoch 856\n",
      "2025-12-19 02:59:43.066223: Current learning rate: 0.00175\n",
      "2025-12-19 03:02:01.217775: train_loss -0.862\n",
      "2025-12-19 03:02:01.217775: val_loss -0.8769\n",
      "2025-12-19 03:02:01.225522: Pseudo dice [0.9279, 0.9599, 0.941]\n",
      "2025-12-19 03:02:01.231528: Epoch time: 138.17 s\n",
      "2025-12-19 03:02:01.938953: \n",
      "2025-12-19 03:02:01.938953: Epoch 857\n",
      "2025-12-19 03:02:01.938953: Current learning rate: 0.00174\n",
      "2025-12-19 03:04:19.908613: train_loss -0.8684\n",
      "2025-12-19 03:04:19.908613: val_loss -0.8782\n",
      "2025-12-19 03:04:19.918360: Pseudo dice [0.9298, 0.9567, 0.9344]\n",
      "2025-12-19 03:04:19.924368: Epoch time: 137.97 s\n",
      "2025-12-19 03:04:20.546207: \n",
      "2025-12-19 03:04:20.546207: Epoch 858\n",
      "2025-12-19 03:04:20.546207: Current learning rate: 0.00173\n",
      "2025-12-19 03:06:38.671880: train_loss -0.8642\n",
      "2025-12-19 03:06:38.671880: val_loss -0.8807\n",
      "2025-12-19 03:06:38.679889: Pseudo dice [0.9244, 0.958, 0.9504]\n",
      "2025-12-19 03:06:38.687638: Epoch time: 138.13 s\n",
      "2025-12-19 03:06:39.497529: \n",
      "2025-12-19 03:06:39.497529: Epoch 859\n",
      "2025-12-19 03:06:39.497529: Current learning rate: 0.00172\n",
      "2025-12-19 03:08:57.661282: train_loss -0.8627\n",
      "2025-12-19 03:08:57.661282: val_loss -0.8787\n",
      "2025-12-19 03:08:57.667495: Pseudo dice [0.9278, 0.9568, 0.938]\n",
      "2025-12-19 03:08:57.667495: Epoch time: 138.16 s\n",
      "2025-12-19 03:08:58.304308: \n",
      "2025-12-19 03:08:58.304308: Epoch 860\n",
      "2025-12-19 03:08:58.310525: Current learning rate: 0.0017\n",
      "2025-12-19 03:11:16.581661: train_loss -0.8589\n",
      "2025-12-19 03:11:16.583664: val_loss -0.8812\n",
      "2025-12-19 03:11:16.583664: Pseudo dice [0.93, 0.9544, 0.9461]\n",
      "2025-12-19 03:11:16.592126: Epoch time: 138.28 s\n",
      "2025-12-19 03:11:17.218315: \n",
      "2025-12-19 03:11:17.218315: Epoch 861\n",
      "2025-12-19 03:11:17.225582: Current learning rate: 0.00169\n",
      "2025-12-19 03:13:35.413634: train_loss -0.8591\n",
      "2025-12-19 03:13:35.413634: val_loss -0.8708\n",
      "2025-12-19 03:13:35.429644: Pseudo dice [0.9227, 0.9534, 0.9368]\n",
      "2025-12-19 03:13:35.429644: Epoch time: 138.2 s\n",
      "2025-12-19 03:13:36.050171: \n",
      "2025-12-19 03:13:36.050171: Epoch 862\n",
      "2025-12-19 03:13:36.050171: Current learning rate: 0.00168\n",
      "2025-12-19 03:15:54.059896: train_loss -0.862\n",
      "2025-12-19 03:15:54.061898: val_loss -0.876\n",
      "2025-12-19 03:15:54.061898: Pseudo dice [0.9252, 0.957, 0.9445]\n",
      "2025-12-19 03:15:54.070140: Epoch time: 138.01 s\n",
      "2025-12-19 03:15:54.688085: \n",
      "2025-12-19 03:15:54.688085: Epoch 863\n",
      "2025-12-19 03:15:54.704083: Current learning rate: 0.00167\n",
      "2025-12-19 03:18:12.691835: train_loss -0.8638\n",
      "2025-12-19 03:18:12.693838: val_loss -0.8769\n",
      "2025-12-19 03:18:12.701849: Pseudo dice [0.9258, 0.9542, 0.9448]\n",
      "2025-12-19 03:18:12.709596: Epoch time: 138.0 s\n",
      "2025-12-19 03:18:13.342695: \n",
      "2025-12-19 03:18:13.342695: Epoch 864\n",
      "2025-12-19 03:18:13.347673: Current learning rate: 0.00166\n",
      "2025-12-19 03:20:31.438920: train_loss -0.8558\n",
      "2025-12-19 03:20:31.438920: val_loss -0.8779\n",
      "2025-12-19 03:20:31.446928: Pseudo dice [0.9268, 0.9566, 0.9424]\n",
      "2025-12-19 03:20:31.452935: Epoch time: 138.11 s\n",
      "2025-12-19 03:20:32.235466: \n",
      "2025-12-19 03:20:32.235466: Epoch 865\n",
      "2025-12-19 03:20:32.255150: Current learning rate: 0.00165\n",
      "2025-12-19 03:22:50.378555: train_loss -0.8604\n",
      "2025-12-19 03:22:50.380558: val_loss -0.889\n",
      "2025-12-19 03:22:50.386544: Pseudo dice [0.9345, 0.9608, 0.9439]\n",
      "2025-12-19 03:22:50.392675: Epoch time: 138.14 s\n",
      "2025-12-19 03:22:51.074853: \n",
      "2025-12-19 03:22:51.074853: Epoch 866\n",
      "2025-12-19 03:22:51.094642: Current learning rate: 0.00164\n",
      "2025-12-19 03:25:09.147047: train_loss -0.8647\n",
      "2025-12-19 03:25:09.148788: val_loss -0.8779\n",
      "2025-12-19 03:25:09.155059: Pseudo dice [0.9285, 0.9571, 0.9411]\n",
      "2025-12-19 03:25:09.159658: Epoch time: 138.07 s\n",
      "2025-12-19 03:25:09.782438: \n",
      "2025-12-19 03:25:09.782438: Epoch 867\n",
      "2025-12-19 03:25:09.798263: Current learning rate: 0.00163\n",
      "2025-12-19 03:27:27.787060: train_loss -0.8662\n",
      "2025-12-19 03:27:27.787060: val_loss -0.8786\n",
      "2025-12-19 03:27:27.793066: Pseudo dice [0.9271, 0.9582, 0.9452]\n",
      "2025-12-19 03:27:27.798878: Epoch time: 138.0 s\n",
      "2025-12-19 03:27:28.431392: \n",
      "2025-12-19 03:27:28.431392: Epoch 868\n",
      "2025-12-19 03:27:28.431392: Current learning rate: 0.00162\n",
      "2025-12-19 03:29:46.393107: train_loss -0.8622\n",
      "2025-12-19 03:29:46.393107: val_loss -0.888\n",
      "2025-12-19 03:29:46.408851: Pseudo dice [0.9352, 0.9612, 0.9465]\n",
      "2025-12-19 03:29:46.418362: Epoch time: 137.96 s\n",
      "2025-12-19 03:29:47.102689: \n",
      "2025-12-19 03:29:47.102689: Epoch 869\n",
      "2025-12-19 03:29:47.122911: Current learning rate: 0.00161\n",
      "2025-12-19 03:32:05.460600: train_loss -0.856\n",
      "2025-12-19 03:32:05.460600: val_loss -0.8821\n",
      "2025-12-19 03:32:05.476456: Pseudo dice [0.9288, 0.9523, 0.9488]\n",
      "2025-12-19 03:32:05.485965: Epoch time: 138.36 s\n",
      "2025-12-19 03:32:06.111192: \n",
      "2025-12-19 03:32:06.111192: Epoch 870\n",
      "2025-12-19 03:32:06.124981: Current learning rate: 0.00159\n",
      "2025-12-19 03:34:24.242687: train_loss -0.8621\n",
      "2025-12-19 03:34:24.258317: val_loss -0.8771\n",
      "2025-12-19 03:34:24.258317: Pseudo dice [0.9283, 0.9563, 0.9363]\n",
      "2025-12-19 03:34:24.270730: Epoch time: 138.13 s\n",
      "2025-12-19 03:34:25.113000: \n",
      "2025-12-19 03:34:25.113000: Epoch 871\n",
      "2025-12-19 03:34:25.122360: Current learning rate: 0.00158\n",
      "2025-12-19 03:36:43.264248: train_loss -0.8668\n",
      "2025-12-19 03:36:43.264248: val_loss -0.8798\n",
      "2025-12-19 03:36:43.280259: Pseudo dice [0.9288, 0.9599, 0.9414]\n",
      "2025-12-19 03:36:43.285582: Epoch time: 138.15 s\n",
      "2025-12-19 03:36:43.993959: \n",
      "2025-12-19 03:36:43.993959: Epoch 872\n",
      "2025-12-19 03:36:44.013330: Current learning rate: 0.00157\n",
      "2025-12-19 03:39:02.094033: train_loss -0.8609\n",
      "2025-12-19 03:39:02.094033: val_loss -0.8813\n",
      "2025-12-19 03:39:02.096036: Pseudo dice [0.9307, 0.96, 0.9394]\n",
      "2025-12-19 03:39:02.108035: Epoch time: 138.1 s\n",
      "2025-12-19 03:39:02.786263: \n",
      "2025-12-19 03:39:02.786263: Epoch 873\n",
      "2025-12-19 03:39:02.802339: Current learning rate: 0.00156\n",
      "2025-12-19 03:41:21.146148: train_loss -0.8647\n",
      "2025-12-19 03:41:21.146148: val_loss -0.8783\n",
      "2025-12-19 03:41:21.146148: Pseudo dice [0.9301, 0.9552, 0.9391]\n",
      "2025-12-19 03:41:21.158392: Epoch time: 138.36 s\n",
      "2025-12-19 03:41:21.826808: \n",
      "2025-12-19 03:41:21.826808: Epoch 874\n",
      "2025-12-19 03:41:21.842560: Current learning rate: 0.00155\n",
      "2025-12-19 03:43:40.112751: train_loss -0.8588\n",
      "2025-12-19 03:43:40.112751: val_loss -0.8749\n",
      "2025-12-19 03:43:40.128566: Pseudo dice [0.9246, 0.9574, 0.9463]\n",
      "2025-12-19 03:43:40.128566: Epoch time: 138.29 s\n",
      "2025-12-19 03:43:40.778148: \n",
      "2025-12-19 03:43:40.778148: Epoch 875\n",
      "2025-12-19 03:43:40.778148: Current learning rate: 0.00154\n",
      "2025-12-19 03:45:58.890755: train_loss -0.86\n",
      "2025-12-19 03:45:58.890755: val_loss -0.8738\n",
      "2025-12-19 03:45:58.890755: Pseudo dice [0.9258, 0.9548, 0.9372]\n",
      "2025-12-19 03:45:58.890755: Epoch time: 138.11 s\n",
      "2025-12-19 03:45:59.587633: \n",
      "2025-12-19 03:45:59.587633: Epoch 876\n",
      "2025-12-19 03:45:59.587633: Current learning rate: 0.00153\n",
      "2025-12-19 03:48:17.811875: train_loss -0.862\n",
      "2025-12-19 03:48:17.811875: val_loss -0.8752\n",
      "2025-12-19 03:48:17.821634: Pseudo dice [0.924, 0.9554, 0.9422]\n",
      "2025-12-19 03:48:17.828855: Epoch time: 138.22 s\n",
      "2025-12-19 03:48:18.513210: \n",
      "2025-12-19 03:48:18.513210: Epoch 877\n",
      "2025-12-19 03:48:18.533029: Current learning rate: 0.00152\n",
      "2025-12-19 03:50:36.699311: train_loss -0.8639\n",
      "2025-12-19 03:50:36.699311: val_loss -0.8777\n",
      "2025-12-19 03:50:36.710276: Pseudo dice [0.93, 0.9587, 0.9352]\n",
      "2025-12-19 03:50:36.714300: Epoch time: 138.19 s\n",
      "2025-12-19 03:50:37.585573: \n",
      "2025-12-19 03:50:37.585573: Epoch 878\n",
      "2025-12-19 03:50:37.590910: Current learning rate: 0.00151\n",
      "2025-12-19 03:52:55.728189: train_loss -0.8622\n",
      "2025-12-19 03:52:55.728189: val_loss -0.8775\n",
      "2025-12-19 03:52:55.736199: Pseudo dice [0.928, 0.9583, 0.9415]\n",
      "2025-12-19 03:52:55.743947: Epoch time: 138.16 s\n",
      "2025-12-19 03:52:56.436050: \n",
      "2025-12-19 03:52:56.438053: Epoch 879\n",
      "2025-12-19 03:52:56.441798: Current learning rate: 0.00149\n",
      "2025-12-19 03:55:14.692155: train_loss -0.8593\n",
      "2025-12-19 03:55:14.692155: val_loss -0.8708\n",
      "2025-12-19 03:55:14.692155: Pseudo dice [0.9249, 0.9568, 0.9407]\n",
      "2025-12-19 03:55:14.708158: Epoch time: 138.26 s\n",
      "2025-12-19 03:55:15.378248: \n",
      "2025-12-19 03:55:15.378248: Epoch 880\n",
      "2025-12-19 03:55:15.384986: Current learning rate: 0.00148\n",
      "2025-12-19 03:57:33.700719: train_loss -0.8601\n",
      "2025-12-19 03:57:33.700719: val_loss -0.879\n",
      "2025-12-19 03:57:33.708528: Pseudo dice [0.9302, 0.9578, 0.9377]\n",
      "2025-12-19 03:57:33.708528: Epoch time: 138.32 s\n",
      "2025-12-19 03:57:34.365685: \n",
      "2025-12-19 03:57:34.365685: Epoch 881\n",
      "2025-12-19 03:57:34.385602: Current learning rate: 0.00147\n",
      "2025-12-19 03:59:52.397183: train_loss -0.8639\n",
      "2025-12-19 03:59:52.397183: val_loss -0.8767\n",
      "2025-12-19 03:59:52.413291: Pseudo dice [0.9305, 0.9556, 0.9374]\n",
      "2025-12-19 03:59:52.413291: Epoch time: 138.03 s\n",
      "2025-12-19 03:59:53.033294: \n",
      "2025-12-19 03:59:53.033294: Epoch 882\n",
      "2025-12-19 03:59:53.047156: Current learning rate: 0.00146\n",
      "2025-12-19 04:02:11.226870: train_loss -0.8678\n",
      "2025-12-19 04:02:11.226870: val_loss -0.8822\n",
      "2025-12-19 04:02:11.235299: Pseudo dice [0.9317, 0.9584, 0.9417]\n",
      "2025-12-19 04:02:11.235299: Epoch time: 138.19 s\n",
      "2025-12-19 04:02:11.860486: \n",
      "2025-12-19 04:02:11.860486: Epoch 883\n",
      "2025-12-19 04:02:11.876561: Current learning rate: 0.00145\n",
      "2025-12-19 04:04:30.264688: train_loss -0.8628\n",
      "2025-12-19 04:04:30.266692: val_loss -0.8731\n",
      "2025-12-19 04:04:30.270855: Pseudo dice [0.9212, 0.9554, 0.9489]\n",
      "2025-12-19 04:04:30.270855: Epoch time: 138.4 s\n",
      "2025-12-19 04:04:30.948082: \n",
      "2025-12-19 04:04:30.948082: Epoch 884\n",
      "2025-12-19 04:04:30.948082: Current learning rate: 0.00144\n",
      "2025-12-19 04:06:48.970800: train_loss -0.8635\n",
      "2025-12-19 04:06:48.970800: val_loss -0.8883\n",
      "2025-12-19 04:06:48.986707: Pseudo dice [0.9343, 0.9611, 0.9469]\n",
      "2025-12-19 04:06:48.986707: Epoch time: 138.02 s\n",
      "2025-12-19 04:06:49.858785: \n",
      "2025-12-19 04:06:49.858785: Epoch 885\n",
      "2025-12-19 04:06:49.858785: Current learning rate: 0.00143\n",
      "2025-12-19 04:09:08.257237: train_loss -0.8641\n",
      "2025-12-19 04:09:08.257237: val_loss -0.8755\n",
      "2025-12-19 04:09:08.275308: Pseudo dice [0.9224, 0.9553, 0.9482]\n",
      "2025-12-19 04:09:08.277311: Epoch time: 138.4 s\n",
      "2025-12-19 04:09:08.908025: \n",
      "2025-12-19 04:09:08.908025: Epoch 886\n",
      "2025-12-19 04:09:08.923784: Current learning rate: 0.00142\n",
      "2025-12-19 04:11:27.186862: train_loss -0.8632\n",
      "2025-12-19 04:11:27.188604: val_loss -0.8835\n",
      "2025-12-19 04:11:27.196615: Pseudo dice [0.9305, 0.9625, 0.9415]\n",
      "2025-12-19 04:11:27.204363: Epoch time: 138.28 s\n",
      "2025-12-19 04:11:27.838708: \n",
      "2025-12-19 04:11:27.838708: Epoch 887\n",
      "2025-12-19 04:11:27.852850: Current learning rate: 0.00141\n",
      "2025-12-19 04:13:45.948037: train_loss -0.8612\n",
      "2025-12-19 04:13:45.950038: val_loss -0.8804\n",
      "2025-12-19 04:13:45.951040: Pseudo dice [0.9306, 0.9573, 0.9427]\n",
      "2025-12-19 04:13:45.951040: Epoch time: 138.11 s\n",
      "2025-12-19 04:13:46.632890: \n",
      "2025-12-19 04:13:46.632890: Epoch 888\n",
      "2025-12-19 04:13:46.648670: Current learning rate: 0.00139\n",
      "2025-12-19 04:16:04.859926: train_loss -0.8625\n",
      "2025-12-19 04:16:04.859926: val_loss -0.8837\n",
      "2025-12-19 04:16:04.865933: Pseudo dice [0.9309, 0.9557, 0.9405]\n",
      "2025-12-19 04:16:04.875942: Epoch time: 138.23 s\n",
      "2025-12-19 04:16:05.564994: \n",
      "2025-12-19 04:16:05.564994: Epoch 889\n",
      "2025-12-19 04:16:05.580690: Current learning rate: 0.00138\n",
      "2025-12-19 04:18:23.717078: train_loss -0.8644\n",
      "2025-12-19 04:18:23.717078: val_loss -0.8782\n",
      "2025-12-19 04:18:23.732841: Pseudo dice [0.9295, 0.959, 0.9366]\n",
      "2025-12-19 04:18:23.732841: Epoch time: 138.15 s\n",
      "2025-12-19 04:18:24.397108: \n",
      "2025-12-19 04:18:24.397108: Epoch 890\n",
      "2025-12-19 04:18:24.397108: Current learning rate: 0.00137\n",
      "2025-12-19 04:20:42.421927: train_loss -0.8634\n",
      "2025-12-19 04:20:42.421927: val_loss -0.8817\n",
      "2025-12-19 04:20:42.427933: Pseudo dice [0.9298, 0.96, 0.9453]\n",
      "2025-12-19 04:20:42.433677: Epoch time: 138.02 s\n",
      "2025-12-19 04:20:43.232967: \n",
      "2025-12-19 04:20:43.232967: Epoch 891\n",
      "2025-12-19 04:20:43.240481: Current learning rate: 0.00136\n",
      "2025-12-19 04:23:01.301424: train_loss -0.8606\n",
      "2025-12-19 04:23:01.301424: val_loss -0.8746\n",
      "2025-12-19 04:23:01.316571: Pseudo dice [0.9252, 0.9592, 0.9398]\n",
      "2025-12-19 04:23:01.324809: Epoch time: 138.07 s\n",
      "2025-12-19 04:23:01.949632: \n",
      "2025-12-19 04:23:01.949632: Epoch 892\n",
      "2025-12-19 04:23:01.949632: Current learning rate: 0.00135\n",
      "2025-12-19 04:25:19.964744: train_loss -0.8637\n",
      "2025-12-19 04:25:19.964744: val_loss -0.8858\n",
      "2025-12-19 04:25:19.984855: Pseudo dice [0.9325, 0.9601, 0.9461]\n",
      "2025-12-19 04:25:19.988859: Epoch time: 138.03 s\n",
      "2025-12-19 04:25:20.711083: \n",
      "2025-12-19 04:25:20.711083: Epoch 893\n",
      "2025-12-19 04:25:20.711083: Current learning rate: 0.00134\n",
      "2025-12-19 04:27:38.962700: train_loss -0.8643\n",
      "2025-12-19 04:27:38.962700: val_loss -0.8819\n",
      "2025-12-19 04:27:38.962700: Pseudo dice [0.9305, 0.9571, 0.9466]\n",
      "2025-12-19 04:27:38.962700: Epoch time: 138.25 s\n",
      "2025-12-19 04:27:39.581138: \n",
      "2025-12-19 04:27:39.581138: Epoch 894\n",
      "2025-12-19 04:27:39.596884: Current learning rate: 0.00133\n",
      "2025-12-19 04:29:57.577326: train_loss -0.865\n",
      "2025-12-19 04:29:57.577326: val_loss -0.8887\n",
      "2025-12-19 04:29:57.584448: Pseudo dice [0.9378, 0.9646, 0.9346]\n",
      "2025-12-19 04:29:57.587045: Epoch time: 138.0 s\n",
      "2025-12-19 04:29:57.587045: Yayy! New best EMA pseudo Dice: 0.9435\n",
      "2025-12-19 04:29:58.483844: \n",
      "2025-12-19 04:29:58.483844: Epoch 895\n",
      "2025-12-19 04:29:58.486126: Current learning rate: 0.00132\n",
      "2025-12-19 04:32:16.516346: train_loss -0.8641\n",
      "2025-12-19 04:32:16.516346: val_loss -0.8707\n",
      "2025-12-19 04:32:16.531980: Pseudo dice [0.9244, 0.9558, 0.9323]\n",
      "2025-12-19 04:32:16.531980: Epoch time: 138.03 s\n",
      "2025-12-19 04:32:17.306920: \n",
      "2025-12-19 04:32:17.306920: Epoch 896\n",
      "2025-12-19 04:32:17.322881: Current learning rate: 0.0013\n",
      "2025-12-19 04:34:35.463950: train_loss -0.8651\n",
      "2025-12-19 04:34:35.465451: val_loss -0.8807\n",
      "2025-12-19 04:34:35.471457: Pseudo dice [0.9311, 0.9564, 0.9431]\n",
      "2025-12-19 04:34:35.477463: Epoch time: 138.16 s\n",
      "2025-12-19 04:34:36.105571: \n",
      "2025-12-19 04:34:36.105571: Epoch 897\n",
      "2025-12-19 04:34:36.111472: Current learning rate: 0.00129\n",
      "2025-12-19 04:36:54.151491: train_loss -0.8629\n",
      "2025-12-19 04:36:54.151491: val_loss -0.8851\n",
      "2025-12-19 04:36:54.163243: Pseudo dice [0.9306, 0.9589, 0.9468]\n",
      "2025-12-19 04:36:54.167247: Epoch time: 138.05 s\n",
      "2025-12-19 04:36:55.061971: \n",
      "2025-12-19 04:36:55.061971: Epoch 898\n",
      "2025-12-19 04:36:55.061971: Current learning rate: 0.00128\n",
      "2025-12-19 04:39:13.121730: train_loss -0.8694\n",
      "2025-12-19 04:39:13.122731: val_loss -0.8846\n",
      "2025-12-19 04:39:13.128738: Pseudo dice [0.9311, 0.959, 0.9441]\n",
      "2025-12-19 04:39:13.134744: Epoch time: 138.06 s\n",
      "2025-12-19 04:39:13.921369: \n",
      "2025-12-19 04:39:13.921369: Epoch 899\n",
      "2025-12-19 04:39:13.927115: Current learning rate: 0.00127\n",
      "2025-12-19 04:41:31.994016: train_loss -0.8621\n",
      "2025-12-19 04:41:31.994016: val_loss -0.8774\n",
      "2025-12-19 04:41:31.994016: Pseudo dice [0.9286, 0.9563, 0.9432]\n",
      "2025-12-19 04:41:31.994016: Epoch time: 138.07 s\n",
      "2025-12-19 04:41:32.884217: \n",
      "2025-12-19 04:41:32.884217: Epoch 900\n",
      "2025-12-19 04:41:32.897363: Current learning rate: 0.00126\n",
      "2025-12-19 04:43:51.048600: train_loss -0.8629\n",
      "2025-12-19 04:43:51.048600: val_loss -0.8875\n",
      "2025-12-19 04:43:51.060207: Pseudo dice [0.9334, 0.9601, 0.9455]\n",
      "2025-12-19 04:43:51.067714: Epoch time: 138.16 s\n",
      "2025-12-19 04:43:51.071718: Yayy! New best EMA pseudo Dice: 0.9436\n",
      "2025-12-19 04:43:52.022224: \n",
      "2025-12-19 04:43:52.022224: Epoch 901\n",
      "2025-12-19 04:43:52.022224: Current learning rate: 0.00125\n",
      "2025-12-19 04:46:10.153082: train_loss -0.8601\n",
      "2025-12-19 04:46:10.153082: val_loss -0.877\n",
      "2025-12-19 04:46:10.164754: Pseudo dice [0.9262, 0.9581, 0.939]\n",
      "2025-12-19 04:46:10.170763: Epoch time: 138.13 s\n",
      "2025-12-19 04:46:10.910823: \n",
      "2025-12-19 04:46:10.910823: Epoch 902\n",
      "2025-12-19 04:46:10.926849: Current learning rate: 0.00124\n",
      "2025-12-19 04:48:29.024918: train_loss -0.8652\n",
      "2025-12-19 04:48:29.024918: val_loss -0.8668\n",
      "2025-12-19 04:48:29.032665: Pseudo dice [0.9213, 0.9507, 0.9425]\n",
      "2025-12-19 04:48:29.038169: Epoch time: 138.11 s\n",
      "2025-12-19 04:48:29.665444: \n",
      "2025-12-19 04:48:29.665444: Epoch 903\n",
      "2025-12-19 04:48:29.665444: Current learning rate: 0.00122\n",
      "2025-12-19 04:50:47.682156: train_loss -0.8625\n",
      "2025-12-19 04:50:47.682156: val_loss -0.8819\n",
      "2025-12-19 04:50:47.694172: Pseudo dice [0.9295, 0.9605, 0.9394]\n",
      "2025-12-19 04:50:47.703929: Epoch time: 138.02 s\n",
      "2025-12-19 04:50:48.491788: \n",
      "2025-12-19 04:50:48.491788: Epoch 904\n",
      "2025-12-19 04:50:48.491788: Current learning rate: 0.00121\n",
      "2025-12-19 04:53:06.470064: train_loss -0.8607\n",
      "2025-12-19 04:53:06.470064: val_loss -0.8924\n",
      "2025-12-19 04:53:06.477573: Pseudo dice [0.9358, 0.9636, 0.9455]\n",
      "2025-12-19 04:53:06.483417: Epoch time: 137.98 s\n",
      "2025-12-19 04:53:07.263455: \n",
      "2025-12-19 04:53:07.263455: Epoch 905\n",
      "2025-12-19 04:53:07.270410: Current learning rate: 0.0012\n",
      "2025-12-19 04:55:25.542592: train_loss -0.8652\n",
      "2025-12-19 04:55:25.544593: val_loss -0.8811\n",
      "2025-12-19 04:55:25.552340: Pseudo dice [0.9292, 0.9599, 0.9403]\n",
      "2025-12-19 04:55:25.556344: Epoch time: 138.3 s\n",
      "2025-12-19 04:55:26.179461: \n",
      "2025-12-19 04:55:26.179461: Epoch 906\n",
      "2025-12-19 04:55:26.195234: Current learning rate: 0.00119\n",
      "2025-12-19 04:57:44.341047: train_loss -0.865\n",
      "2025-12-19 04:57:44.341047: val_loss -0.8898\n",
      "2025-12-19 04:57:44.348906: Pseudo dice [0.9338, 0.9602, 0.9477]\n",
      "2025-12-19 04:57:44.354912: Epoch time: 138.16 s\n",
      "2025-12-19 04:57:44.360656: Yayy! New best EMA pseudo Dice: 0.9438\n",
      "2025-12-19 04:57:45.247082: \n",
      "2025-12-19 04:57:45.247082: Epoch 907\n",
      "2025-12-19 04:57:45.262826: Current learning rate: 0.00118\n",
      "2025-12-19 05:00:03.310493: train_loss -0.8618\n",
      "2025-12-19 05:00:03.310493: val_loss -0.8883\n",
      "2025-12-19 05:00:03.318505: Pseudo dice [0.932, 0.9622, 0.946]\n",
      "2025-12-19 05:00:03.321509: Epoch time: 138.06 s\n",
      "2025-12-19 05:00:03.330298: Yayy! New best EMA pseudo Dice: 0.9441\n",
      "2025-12-19 05:00:04.444468: \n",
      "2025-12-19 05:00:04.444468: Epoch 908\n",
      "2025-12-19 05:00:04.451969: Current learning rate: 0.00117\n",
      "2025-12-19 05:02:22.646405: train_loss -0.8624\n",
      "2025-12-19 05:02:22.646405: val_loss -0.8808\n",
      "2025-12-19 05:02:22.646405: Pseudo dice [0.9293, 0.955, 0.936]\n",
      "2025-12-19 05:02:22.658785: Epoch time: 138.2 s\n",
      "2025-12-19 05:02:23.283262: \n",
      "2025-12-19 05:02:23.283262: Epoch 909\n",
      "2025-12-19 05:02:23.287202: Current learning rate: 0.00116\n",
      "2025-12-19 05:04:41.576255: train_loss -0.8623\n",
      "2025-12-19 05:04:41.576255: val_loss -0.8858\n",
      "2025-12-19 05:04:41.576255: Pseudo dice [0.9332, 0.9585, 0.9438]\n",
      "2025-12-19 05:04:41.592037: Epoch time: 138.29 s\n",
      "2025-12-19 05:04:42.376357: \n",
      "2025-12-19 05:04:42.376357: Epoch 910\n",
      "2025-12-19 05:04:42.376357: Current learning rate: 0.00115\n",
      "2025-12-19 05:07:00.321654: train_loss -0.863\n",
      "2025-12-19 05:07:00.321654: val_loss -0.8832\n",
      "2025-12-19 05:07:00.328162: Pseudo dice [0.929, 0.9564, 0.9495]\n",
      "2025-12-19 05:07:00.333667: Epoch time: 137.95 s\n",
      "2025-12-19 05:07:01.062442: \n",
      "2025-12-19 05:07:01.062442: Epoch 911\n",
      "2025-12-19 05:07:01.070453: Current learning rate: 0.00113\n",
      "2025-12-19 05:09:19.404330: train_loss -0.8596\n",
      "2025-12-19 05:09:19.406332: val_loss -0.8828\n",
      "2025-12-19 05:09:19.412338: Pseudo dice [0.9356, 0.9559, 0.9406]\n",
      "2025-12-19 05:09:19.418082: Epoch time: 138.34 s\n",
      "2025-12-19 05:09:20.091559: \n",
      "2025-12-19 05:09:20.091559: Epoch 912\n",
      "2025-12-19 05:09:20.094414: Current learning rate: 0.00112\n",
      "2025-12-19 05:11:38.511692: train_loss -0.8568\n",
      "2025-12-19 05:11:38.511692: val_loss -0.8786\n",
      "2025-12-19 05:11:38.521666: Pseudo dice [0.9285, 0.9563, 0.9434]\n",
      "2025-12-19 05:11:38.529412: Epoch time: 138.42 s\n",
      "2025-12-19 05:11:39.189377: \n",
      "2025-12-19 05:11:39.189377: Epoch 913\n",
      "2025-12-19 05:11:39.189377: Current learning rate: 0.00111\n",
      "2025-12-19 05:13:57.274463: train_loss -0.8655\n",
      "2025-12-19 05:13:57.274463: val_loss -0.8727\n",
      "2025-12-19 05:13:57.281965: Pseudo dice [0.9232, 0.9531, 0.9418]\n",
      "2025-12-19 05:13:57.287973: Epoch time: 138.09 s\n",
      "2025-12-19 05:13:58.004317: \n",
      "2025-12-19 05:13:58.004317: Epoch 914\n",
      "2025-12-19 05:13:58.004317: Current learning rate: 0.0011\n",
      "2025-12-19 05:16:16.041046: train_loss -0.867\n",
      "2025-12-19 05:16:16.041046: val_loss -0.8868\n",
      "2025-12-19 05:16:16.041046: Pseudo dice [0.933, 0.9621, 0.9421]\n",
      "2025-12-19 05:16:16.057074: Epoch time: 138.04 s\n",
      "2025-12-19 05:16:16.706038: \n",
      "2025-12-19 05:16:16.706038: Epoch 915\n",
      "2025-12-19 05:16:16.706038: Current learning rate: 0.00109\n",
      "2025-12-19 05:18:34.698854: train_loss -0.8674\n",
      "2025-12-19 05:18:34.698854: val_loss -0.8852\n",
      "2025-12-19 05:18:34.706862: Pseudo dice [0.931, 0.9584, 0.9502]\n",
      "2025-12-19 05:18:34.712868: Epoch time: 137.99 s\n",
      "2025-12-19 05:18:35.501711: \n",
      "2025-12-19 05:18:35.501711: Epoch 916\n",
      "2025-12-19 05:18:35.517392: Current learning rate: 0.00108\n",
      "2025-12-19 05:20:53.920088: train_loss -0.8656\n",
      "2025-12-19 05:20:53.920088: val_loss -0.8834\n",
      "2025-12-19 05:20:53.928105: Pseudo dice [0.9324, 0.9593, 0.944]\n",
      "2025-12-19 05:20:53.932109: Epoch time: 138.42 s\n",
      "2025-12-19 05:20:54.663550: \n",
      "2025-12-19 05:20:54.663550: Epoch 917\n",
      "2025-12-19 05:20:54.663550: Current learning rate: 0.00106\n",
      "2025-12-19 05:23:12.721630: train_loss -0.8636\n",
      "2025-12-19 05:23:12.721630: val_loss -0.8805\n",
      "2025-12-19 05:23:12.721630: Pseudo dice [0.9309, 0.9579, 0.9369]\n",
      "2025-12-19 05:23:12.731263: Epoch time: 138.06 s\n",
      "2025-12-19 05:23:13.345296: \n",
      "2025-12-19 05:23:13.345296: Epoch 918\n",
      "2025-12-19 05:23:13.361070: Current learning rate: 0.00105\n",
      "2025-12-19 05:25:31.486924: train_loss -0.8627\n",
      "2025-12-19 05:25:31.486924: val_loss -0.8884\n",
      "2025-12-19 05:25:31.492932: Pseudo dice [0.9356, 0.9606, 0.9395]\n",
      "2025-12-19 05:25:31.498938: Epoch time: 138.14 s\n",
      "2025-12-19 05:25:32.124090: \n",
      "2025-12-19 05:25:32.124090: Epoch 919\n",
      "2025-12-19 05:25:32.128475: Current learning rate: 0.00104\n",
      "2025-12-19 05:27:50.197673: train_loss -0.8613\n",
      "2025-12-19 05:27:50.199679: val_loss -0.8885\n",
      "2025-12-19 05:27:50.205687: Pseudo dice [0.9387, 0.9608, 0.9344]\n",
      "2025-12-19 05:27:50.211431: Epoch time: 138.07 s\n",
      "2025-12-19 05:27:50.922110: \n",
      "2025-12-19 05:27:50.922110: Epoch 920\n",
      "2025-12-19 05:27:50.922110: Current learning rate: 0.00103\n",
      "2025-12-19 05:30:09.048125: train_loss -0.8661\n",
      "2025-12-19 05:30:09.048125: val_loss -0.8739\n",
      "2025-12-19 05:30:09.053869: Pseudo dice [0.9235, 0.9558, 0.9441]\n",
      "2025-12-19 05:30:09.061122: Epoch time: 138.13 s\n",
      "2025-12-19 05:30:09.696774: \n",
      "2025-12-19 05:30:09.696774: Epoch 921\n",
      "2025-12-19 05:30:09.696774: Current learning rate: 0.00102\n",
      "2025-12-19 05:32:27.856635: train_loss -0.8596\n",
      "2025-12-19 05:32:27.856635: val_loss -0.8787\n",
      "2025-12-19 05:32:27.862641: Pseudo dice [0.9276, 0.9604, 0.9391]\n",
      "2025-12-19 05:32:27.870224: Epoch time: 138.16 s\n",
      "2025-12-19 05:32:28.494462: \n",
      "2025-12-19 05:32:28.494462: Epoch 922\n",
      "2025-12-19 05:32:28.510544: Current learning rate: 0.00101\n",
      "2025-12-19 05:34:46.796740: train_loss -0.8591\n",
      "2025-12-19 05:34:46.796740: val_loss -0.8838\n",
      "2025-12-19 05:34:46.810255: Pseudo dice [0.933, 0.9589, 0.9393]\n",
      "2025-12-19 05:34:46.816262: Epoch time: 138.3 s\n",
      "2025-12-19 05:34:47.699792: \n",
      "2025-12-19 05:34:47.699792: Epoch 923\n",
      "2025-12-19 05:34:47.699792: Current learning rate: 0.001\n",
      "2025-12-19 05:37:05.915381: train_loss -0.8642\n",
      "2025-12-19 05:37:05.915381: val_loss -0.8803\n",
      "2025-12-19 05:37:05.921629: Pseudo dice [0.9318, 0.9575, 0.9389]\n",
      "2025-12-19 05:37:05.927634: Epoch time: 138.22 s\n",
      "2025-12-19 05:37:06.559080: \n",
      "2025-12-19 05:37:06.559080: Epoch 924\n",
      "2025-12-19 05:37:06.560822: Current learning rate: 0.00098\n",
      "2025-12-19 05:39:24.485443: train_loss -0.8683\n",
      "2025-12-19 05:39:24.485443: val_loss -0.8854\n",
      "2025-12-19 05:39:24.501274: Pseudo dice [0.9325, 0.9618, 0.9462]\n",
      "2025-12-19 05:39:24.501274: Epoch time: 137.93 s\n",
      "2025-12-19 05:39:25.119738: \n",
      "2025-12-19 05:39:25.119738: Epoch 925\n",
      "2025-12-19 05:39:25.135376: Current learning rate: 0.00097\n",
      "2025-12-19 05:41:43.259551: train_loss -0.8659\n",
      "2025-12-19 05:41:43.259551: val_loss -0.8898\n",
      "2025-12-19 05:41:43.279475: Pseudo dice [0.9345, 0.9611, 0.9463]\n",
      "2025-12-19 05:41:43.285482: Epoch time: 138.14 s\n",
      "2025-12-19 05:41:43.291228: Yayy! New best EMA pseudo Dice: 0.9442\n",
      "2025-12-19 05:41:44.270873: \n",
      "2025-12-19 05:41:44.270873: Epoch 926\n",
      "2025-12-19 05:41:44.279799: Current learning rate: 0.00096\n",
      "2025-12-19 05:44:02.424746: train_loss -0.8616\n",
      "2025-12-19 05:44:02.424746: val_loss -0.8849\n",
      "2025-12-19 05:44:02.432247: Pseudo dice [0.9334, 0.9584, 0.9435]\n",
      "2025-12-19 05:44:02.438253: Epoch time: 138.15 s\n",
      "2025-12-19 05:44:02.443997: Yayy! New best EMA pseudo Dice: 0.9443\n",
      "2025-12-19 05:44:03.347096: \n",
      "2025-12-19 05:44:03.347096: Epoch 927\n",
      "2025-12-19 05:44:03.363012: Current learning rate: 0.00095\n",
      "2025-12-19 05:46:21.653628: train_loss -0.8616\n",
      "2025-12-19 05:46:21.653628: val_loss -0.8914\n",
      "2025-12-19 05:46:21.659634: Pseudo dice [0.9374, 0.9647, 0.9416]\n",
      "2025-12-19 05:46:21.667643: Epoch time: 138.31 s\n",
      "2025-12-19 05:46:21.672148: Yayy! New best EMA pseudo Dice: 0.9446\n",
      "2025-12-19 05:46:22.591284: \n",
      "2025-12-19 05:46:22.591284: Epoch 928\n",
      "2025-12-19 05:46:22.591284: Current learning rate: 0.00094\n",
      "2025-12-19 05:48:40.765960: train_loss -0.8616\n",
      "2025-12-19 05:48:40.765960: val_loss -0.8784\n",
      "2025-12-19 05:48:40.773710: Pseudo dice [0.9299, 0.9559, 0.9394]\n",
      "2025-12-19 05:48:40.773710: Epoch time: 138.17 s\n",
      "2025-12-19 05:48:41.572497: \n",
      "2025-12-19 05:48:41.572497: Epoch 929\n",
      "2025-12-19 05:48:41.572497: Current learning rate: 0.00092\n",
      "2025-12-19 05:50:59.656133: train_loss -0.8661\n",
      "2025-12-19 05:50:59.656133: val_loss -0.8855\n",
      "2025-12-19 05:50:59.656133: Pseudo dice [0.9332, 0.9593, 0.941]\n",
      "2025-12-19 05:50:59.672110: Epoch time: 138.1 s\n",
      "2025-12-19 05:51:00.300669: \n",
      "2025-12-19 05:51:00.300669: Epoch 930\n",
      "2025-12-19 05:51:00.307710: Current learning rate: 0.00091\n",
      "2025-12-19 05:53:18.433895: train_loss -0.865\n",
      "2025-12-19 05:53:18.433895: val_loss -0.8705\n",
      "2025-12-19 05:53:18.444406: Pseudo dice [0.9199, 0.9513, 0.9497]\n",
      "2025-12-19 05:53:18.449913: Epoch time: 138.14 s\n",
      "2025-12-19 05:53:19.144812: \n",
      "2025-12-19 05:53:19.144812: Epoch 931\n",
      "2025-12-19 05:53:19.155796: Current learning rate: 0.0009\n",
      "2025-12-19 05:55:37.387280: train_loss -0.8675\n",
      "2025-12-19 05:55:37.387280: val_loss -0.8833\n",
      "2025-12-19 05:55:37.397218: Pseudo dice [0.9315, 0.9582, 0.942]\n",
      "2025-12-19 05:55:37.403224: Epoch time: 138.24 s\n",
      "2025-12-19 05:55:38.067588: \n",
      "2025-12-19 05:55:38.067588: Epoch 932\n",
      "2025-12-19 05:55:38.067588: Current learning rate: 0.00089\n",
      "2025-12-19 05:57:56.169238: train_loss -0.8661\n",
      "2025-12-19 05:57:56.170979: val_loss -0.8843\n",
      "2025-12-19 05:57:56.177700: Pseudo dice [0.9333, 0.9588, 0.9318]\n",
      "2025-12-19 05:57:56.181704: Epoch time: 138.1 s\n",
      "2025-12-19 05:57:56.803693: \n",
      "2025-12-19 05:57:56.803693: Epoch 933\n",
      "2025-12-19 05:57:56.819437: Current learning rate: 0.00088\n",
      "2025-12-19 06:00:14.993037: train_loss -0.868\n",
      "2025-12-19 06:00:14.993037: val_loss -0.8808\n",
      "2025-12-19 06:00:15.009083: Pseudo dice [0.9276, 0.9559, 0.944]\n",
      "2025-12-19 06:00:15.013087: Epoch time: 138.19 s\n",
      "2025-12-19 06:00:15.673620: \n",
      "2025-12-19 06:00:15.673620: Epoch 934\n",
      "2025-12-19 06:00:15.694865: Current learning rate: 0.00087\n",
      "2025-12-19 06:02:33.793569: train_loss -0.8611\n",
      "2025-12-19 06:02:33.796754: val_loss -0.869\n",
      "2025-12-19 06:02:33.802762: Pseudo dice [0.9243, 0.9536, 0.9373]\n",
      "2025-12-19 06:02:33.810772: Epoch time: 138.12 s\n",
      "2025-12-19 06:02:34.662982: \n",
      "2025-12-19 06:02:34.662982: Epoch 935\n",
      "2025-12-19 06:02:34.662982: Current learning rate: 0.00085\n",
      "2025-12-19 06:04:52.767054: train_loss -0.8648\n",
      "2025-12-19 06:04:52.767054: val_loss -0.878\n",
      "2025-12-19 06:04:52.773061: Pseudo dice [0.9283, 0.955, 0.9398]\n",
      "2025-12-19 06:04:52.780183: Epoch time: 138.1 s\n",
      "2025-12-19 06:04:53.405394: \n",
      "2025-12-19 06:04:53.405394: Epoch 936\n",
      "2025-12-19 06:04:53.405394: Current learning rate: 0.00084\n",
      "2025-12-19 06:07:11.660577: train_loss -0.8603\n",
      "2025-12-19 06:07:11.660577: val_loss -0.8855\n",
      "2025-12-19 06:07:11.660577: Pseudo dice [0.9342, 0.9601, 0.9408]\n",
      "2025-12-19 06:07:11.660577: Epoch time: 138.26 s\n",
      "2025-12-19 06:07:12.290960: \n",
      "2025-12-19 06:07:12.290960: Epoch 937\n",
      "2025-12-19 06:07:12.306696: Current learning rate: 0.00083\n",
      "2025-12-19 06:09:30.690598: train_loss -0.8674\n",
      "2025-12-19 06:09:30.690598: val_loss -0.8844\n",
      "2025-12-19 06:09:30.700348: Pseudo dice [0.9328, 0.958, 0.9455]\n",
      "2025-12-19 06:09:30.710100: Epoch time: 138.4 s\n",
      "2025-12-19 06:09:31.414872: \n",
      "2025-12-19 06:09:31.414872: Epoch 938\n",
      "2025-12-19 06:09:31.414872: Current learning rate: 0.00082\n",
      "2025-12-19 06:11:49.783672: train_loss -0.8653\n",
      "2025-12-19 06:11:49.783672: val_loss -0.875\n",
      "2025-12-19 06:11:49.797180: Pseudo dice [0.9269, 0.9561, 0.9398]\n",
      "2025-12-19 06:11:49.804731: Epoch time: 138.37 s\n",
      "2025-12-19 06:11:50.438452: \n",
      "2025-12-19 06:11:50.438452: Epoch 939\n",
      "2025-12-19 06:11:50.441200: Current learning rate: 0.00081\n",
      "2025-12-19 06:14:08.703309: train_loss -0.864\n",
      "2025-12-19 06:14:08.705312: val_loss -0.8818\n",
      "2025-12-19 06:14:08.713060: Pseudo dice [0.9315, 0.9595, 0.941]\n",
      "2025-12-19 06:14:08.721069: Epoch time: 138.27 s\n",
      "2025-12-19 06:14:09.345860: \n",
      "2025-12-19 06:14:09.345860: Epoch 940\n",
      "2025-12-19 06:14:09.345860: Current learning rate: 0.00079\n",
      "2025-12-19 06:16:27.398955: train_loss -0.8671\n",
      "2025-12-19 06:16:27.398955: val_loss -0.877\n",
      "2025-12-19 06:16:27.406963: Pseudo dice [0.9257, 0.9542, 0.9428]\n",
      "2025-12-19 06:16:27.411023: Epoch time: 138.05 s\n",
      "2025-12-19 06:16:28.037168: \n",
      "2025-12-19 06:16:28.040677: Epoch 941\n",
      "2025-12-19 06:16:28.040677: Current learning rate: 0.00078\n",
      "2025-12-19 06:18:46.212415: train_loss -0.8695\n",
      "2025-12-19 06:18:46.212415: val_loss -0.8756\n",
      "2025-12-19 06:18:46.222921: Pseudo dice [0.9247, 0.9546, 0.947]\n",
      "2025-12-19 06:18:46.228427: Epoch time: 138.18 s\n",
      "2025-12-19 06:18:47.034250: \n",
      "2025-12-19 06:18:47.034250: Epoch 942\n",
      "2025-12-19 06:18:47.034250: Current learning rate: 0.00077\n",
      "2025-12-19 06:21:05.002068: train_loss -0.8648\n",
      "2025-12-19 06:21:05.002068: val_loss -0.8824\n",
      "2025-12-19 06:21:05.002068: Pseudo dice [0.9307, 0.9579, 0.9431]\n",
      "2025-12-19 06:21:05.017798: Epoch time: 137.98 s\n",
      "2025-12-19 06:21:05.635331: \n",
      "2025-12-19 06:21:05.635331: Epoch 943\n",
      "2025-12-19 06:21:05.635331: Current learning rate: 0.00076\n",
      "2025-12-19 06:23:23.761329: train_loss -0.8687\n",
      "2025-12-19 06:23:23.763332: val_loss -0.8754\n",
      "2025-12-19 06:23:23.773080: Pseudo dice [0.9251, 0.9561, 0.9444]\n",
      "2025-12-19 06:23:23.781088: Epoch time: 138.13 s\n",
      "2025-12-19 06:23:24.429775: \n",
      "2025-12-19 06:23:24.429775: Epoch 944\n",
      "2025-12-19 06:23:24.435930: Current learning rate: 0.00075\n",
      "2025-12-19 06:25:42.449858: train_loss -0.8688\n",
      "2025-12-19 06:25:42.449858: val_loss -0.8843\n",
      "2025-12-19 06:25:42.453862: Pseudo dice [0.9306, 0.9576, 0.9531]\n",
      "2025-12-19 06:25:42.461941: Epoch time: 138.02 s\n",
      "2025-12-19 06:25:43.086443: \n",
      "2025-12-19 06:25:43.086443: Epoch 945\n",
      "2025-12-19 06:25:43.086443: Current learning rate: 0.00074\n",
      "2025-12-19 06:28:01.185405: train_loss -0.8645\n",
      "2025-12-19 06:28:01.185405: val_loss -0.8789\n",
      "2025-12-19 06:28:01.191136: Pseudo dice [0.9286, 0.9581, 0.9438]\n",
      "2025-12-19 06:28:01.197142: Epoch time: 138.1 s\n",
      "2025-12-19 06:28:01.823019: \n",
      "2025-12-19 06:28:01.823019: Epoch 946\n",
      "2025-12-19 06:28:01.823019: Current learning rate: 0.00072\n",
      "2025-12-19 06:30:20.007633: train_loss -0.8682\n",
      "2025-12-19 06:30:20.007633: val_loss -0.8787\n",
      "2025-12-19 06:30:20.017391: Pseudo dice [0.93, 0.9582, 0.9366]\n",
      "2025-12-19 06:30:20.027164: Epoch time: 138.18 s\n",
      "2025-12-19 06:30:20.697646: \n",
      "2025-12-19 06:30:20.697646: Epoch 947\n",
      "2025-12-19 06:30:20.697646: Current learning rate: 0.00071\n",
      "2025-12-19 06:32:38.837785: train_loss -0.8633\n",
      "2025-12-19 06:32:38.837785: val_loss -0.8844\n",
      "2025-12-19 06:32:38.843530: Pseudo dice [0.9307, 0.9576, 0.9478]\n",
      "2025-12-19 06:32:38.849535: Epoch time: 138.14 s\n",
      "2025-12-19 06:32:39.487550: \n",
      "2025-12-19 06:32:39.487550: Epoch 948\n",
      "2025-12-19 06:32:39.487550: Current learning rate: 0.0007\n",
      "2025-12-19 06:34:57.748695: train_loss -0.8648\n",
      "2025-12-19 06:34:57.762645: val_loss -0.8847\n",
      "2025-12-19 06:34:57.762645: Pseudo dice [0.9309, 0.9605, 0.9448]\n",
      "2025-12-19 06:34:57.762645: Epoch time: 138.26 s\n",
      "2025-12-19 06:34:58.586730: \n",
      "2025-12-19 06:34:58.586730: Epoch 949\n",
      "2025-12-19 06:34:58.586730: Current learning rate: 0.00069\n",
      "2025-12-19 06:37:16.653780: train_loss -0.8698\n",
      "2025-12-19 06:37:16.653780: val_loss -0.8802\n",
      "2025-12-19 06:37:16.669567: Pseudo dice [0.9287, 0.954, 0.9451]\n",
      "2025-12-19 06:37:16.669567: Epoch time: 138.07 s\n",
      "2025-12-19 06:37:17.619200: \n",
      "2025-12-19 06:37:17.619200: Epoch 950\n",
      "2025-12-19 06:37:17.619200: Current learning rate: 0.00067\n",
      "2025-12-19 06:39:35.628320: train_loss -0.867\n",
      "2025-12-19 06:39:35.628320: val_loss -0.8854\n",
      "2025-12-19 06:39:35.632325: Pseudo dice [0.9337, 0.9608, 0.938]\n",
      "2025-12-19 06:39:35.632325: Epoch time: 138.01 s\n",
      "2025-12-19 06:39:36.267363: \n",
      "2025-12-19 06:39:36.267363: Epoch 951\n",
      "2025-12-19 06:39:36.267363: Current learning rate: 0.00066\n",
      "2025-12-19 06:41:54.568153: train_loss -0.8641\n",
      "2025-12-19 06:41:54.570156: val_loss -0.8894\n",
      "2025-12-19 06:41:54.575902: Pseudo dice [0.9353, 0.9615, 0.9423]\n",
      "2025-12-19 06:41:54.583647: Epoch time: 138.3 s\n",
      "2025-12-19 06:41:55.207321: \n",
      "2025-12-19 06:41:55.207321: Epoch 952\n",
      "2025-12-19 06:41:55.207321: Current learning rate: 0.00065\n",
      "2025-12-19 06:44:13.419553: train_loss -0.8647\n",
      "2025-12-19 06:44:13.421556: val_loss -0.8838\n",
      "2025-12-19 06:44:13.429928: Pseudo dice [0.9288, 0.9594, 0.9519]\n",
      "2025-12-19 06:44:13.437936: Epoch time: 138.21 s\n",
      "2025-12-19 06:44:14.151040: \n",
      "2025-12-19 06:44:14.151040: Epoch 953\n",
      "2025-12-19 06:44:14.166978: Current learning rate: 0.00064\n",
      "2025-12-19 06:46:32.257999: train_loss -0.8666\n",
      "2025-12-19 06:46:32.257999: val_loss -0.8875\n",
      "2025-12-19 06:46:32.266246: Pseudo dice [0.9343, 0.9619, 0.94]\n",
      "2025-12-19 06:46:32.266246: Epoch time: 138.11 s\n",
      "2025-12-19 06:46:32.904303: \n",
      "2025-12-19 06:46:32.904303: Epoch 954\n",
      "2025-12-19 06:46:32.904303: Current learning rate: 0.00063\n",
      "2025-12-19 06:48:51.041558: train_loss -0.866\n",
      "2025-12-19 06:48:51.043561: val_loss -0.8875\n",
      "2025-12-19 06:48:51.049568: Pseudo dice [0.9338, 0.9582, 0.9402]\n",
      "2025-12-19 06:48:51.055312: Epoch time: 138.15 s\n",
      "2025-12-19 06:48:51.848262: \n",
      "2025-12-19 06:48:51.848262: Epoch 955\n",
      "2025-12-19 06:48:51.848262: Current learning rate: 0.00061\n",
      "2025-12-19 06:51:09.860536: train_loss -0.8685\n",
      "2025-12-19 06:51:09.860536: val_loss -0.8833\n",
      "2025-12-19 06:51:09.868545: Pseudo dice [0.9303, 0.9549, 0.9454]\n",
      "2025-12-19 06:51:09.874288: Epoch time: 138.01 s\n",
      "2025-12-19 06:51:10.634307: \n",
      "2025-12-19 06:51:10.634307: Epoch 956\n",
      "2025-12-19 06:51:10.650241: Current learning rate: 0.0006\n",
      "2025-12-19 06:53:28.857978: train_loss -0.8677\n",
      "2025-12-19 06:53:28.859981: val_loss -0.8863\n",
      "2025-12-19 06:53:28.867221: Pseudo dice [0.9301, 0.9566, 0.9518]\n",
      "2025-12-19 06:53:28.867221: Epoch time: 138.22 s\n",
      "2025-12-19 06:53:29.564720: \n",
      "2025-12-19 06:53:29.564720: Epoch 957\n",
      "2025-12-19 06:53:29.564720: Current learning rate: 0.00059\n",
      "2025-12-19 06:55:47.699803: train_loss -0.8727\n",
      "2025-12-19 06:55:47.699803: val_loss -0.8825\n",
      "2025-12-19 06:55:47.707548: Pseudo dice [0.9309, 0.9601, 0.9449]\n",
      "2025-12-19 06:55:47.713554: Epoch time: 138.14 s\n",
      "2025-12-19 06:55:48.403736: \n",
      "2025-12-19 06:55:48.403736: Epoch 958\n",
      "2025-12-19 06:55:48.419646: Current learning rate: 0.00058\n",
      "2025-12-19 06:58:06.567202: train_loss -0.8634\n",
      "2025-12-19 06:58:06.567202: val_loss -0.8743\n",
      "2025-12-19 06:58:06.575212: Pseudo dice [0.9249, 0.9534, 0.9451]\n",
      "2025-12-19 06:58:06.578955: Epoch time: 138.16 s\n",
      "2025-12-19 06:58:07.335862: \n",
      "2025-12-19 06:58:07.335862: Epoch 959\n",
      "2025-12-19 06:58:07.351774: Current learning rate: 0.00056\n",
      "2025-12-19 07:00:25.373215: train_loss -0.8673\n",
      "2025-12-19 07:00:25.375217: val_loss -0.891\n",
      "2025-12-19 07:00:25.380960: Pseudo dice [0.9357, 0.959, 0.9501]\n",
      "2025-12-19 07:00:25.384964: Epoch time: 138.04 s\n",
      "2025-12-19 07:00:26.018128: \n",
      "2025-12-19 07:00:26.018128: Epoch 960\n",
      "2025-12-19 07:00:26.018128: Current learning rate: 0.00055\n",
      "2025-12-19 07:02:44.157502: train_loss -0.8649\n",
      "2025-12-19 07:02:44.157502: val_loss -0.8862\n",
      "2025-12-19 07:02:44.173222: Pseudo dice [0.9306, 0.9591, 0.9434]\n",
      "2025-12-19 07:02:44.173222: Epoch time: 138.14 s\n",
      "2025-12-19 07:02:44.853733: \n",
      "2025-12-19 07:02:44.853733: Epoch 961\n",
      "2025-12-19 07:02:44.869495: Current learning rate: 0.00054\n",
      "2025-12-19 07:05:03.209992: train_loss -0.8659\n",
      "2025-12-19 07:05:03.209992: val_loss -0.8919\n",
      "2025-12-19 07:05:03.213734: Pseudo dice [0.9386, 0.9616, 0.9421]\n",
      "2025-12-19 07:05:03.223243: Epoch time: 138.35 s\n",
      "2025-12-19 07:05:03.229250: Yayy! New best EMA pseudo Dice: 0.9448\n",
      "2025-12-19 07:05:04.447283: \n",
      "2025-12-19 07:05:04.447283: Epoch 962\n",
      "2025-12-19 07:05:04.447283: Current learning rate: 0.00053\n",
      "2025-12-19 07:07:22.532253: train_loss -0.8641\n",
      "2025-12-19 07:07:22.534255: val_loss -0.8773\n",
      "2025-12-19 07:07:22.540000: Pseudo dice [0.9266, 0.96, 0.9419]\n",
      "2025-12-19 07:07:22.546007: Epoch time: 138.08 s\n",
      "2025-12-19 07:07:23.275955: \n",
      "2025-12-19 07:07:23.275955: Epoch 963\n",
      "2025-12-19 07:07:23.293728: Current learning rate: 0.00051\n",
      "2025-12-19 07:09:41.844095: train_loss -0.8641\n",
      "2025-12-19 07:09:41.844095: val_loss -0.8815\n",
      "2025-12-19 07:09:41.851846: Pseudo dice [0.9281, 0.9573, 0.9423]\n",
      "2025-12-19 07:09:41.864857: Epoch time: 138.57 s\n",
      "2025-12-19 07:09:42.548236: \n",
      "2025-12-19 07:09:42.548236: Epoch 964\n",
      "2025-12-19 07:09:42.548236: Current learning rate: 0.0005\n",
      "2025-12-19 07:12:00.772732: train_loss -0.8697\n",
      "2025-12-19 07:12:00.772732: val_loss -0.8814\n",
      "2025-12-19 07:12:00.778738: Pseudo dice [0.9271, 0.9573, 0.9466]\n",
      "2025-12-19 07:12:00.784482: Epoch time: 138.22 s\n",
      "2025-12-19 07:12:01.510458: \n",
      "2025-12-19 07:12:01.510458: Epoch 965\n",
      "2025-12-19 07:12:01.510458: Current learning rate: 0.00049\n",
      "2025-12-19 07:14:19.722011: train_loss -0.8681\n",
      "2025-12-19 07:14:19.722011: val_loss -0.8816\n",
      "2025-12-19 07:14:19.733562: Pseudo dice [0.9288, 0.9592, 0.9459]\n",
      "2025-12-19 07:14:19.739643: Epoch time: 138.21 s\n",
      "2025-12-19 07:14:20.402038: \n",
      "2025-12-19 07:14:20.402038: Epoch 966\n",
      "2025-12-19 07:14:20.417829: Current learning rate: 0.00048\n",
      "2025-12-19 07:16:38.344476: train_loss -0.8693\n",
      "2025-12-19 07:16:38.346477: val_loss -0.8847\n",
      "2025-12-19 07:16:38.353264: Pseudo dice [0.9331, 0.9583, 0.9445]\n",
      "2025-12-19 07:16:38.353264: Epoch time: 137.94 s\n",
      "2025-12-19 07:16:38.983158: \n",
      "2025-12-19 07:16:38.983158: Epoch 967\n",
      "2025-12-19 07:16:38.998918: Current learning rate: 0.00046\n",
      "2025-12-19 07:18:57.185287: train_loss -0.8662\n",
      "2025-12-19 07:18:57.185287: val_loss -0.8808\n",
      "2025-12-19 07:18:57.193295: Pseudo dice [0.928, 0.957, 0.9441]\n",
      "2025-12-19 07:18:57.199589: Epoch time: 138.2 s\n",
      "2025-12-19 07:18:58.034077: \n",
      "2025-12-19 07:18:58.034077: Epoch 968\n",
      "2025-12-19 07:18:58.034077: Current learning rate: 0.00045\n",
      "2025-12-19 07:21:16.213401: train_loss -0.8686\n",
      "2025-12-19 07:21:16.215404: val_loss -0.8774\n",
      "2025-12-19 07:21:16.223151: Pseudo dice [0.9299, 0.9575, 0.9319]\n",
      "2025-12-19 07:21:16.232294: Epoch time: 138.18 s\n",
      "2025-12-19 07:21:16.872577: \n",
      "2025-12-19 07:21:16.872577: Epoch 969\n",
      "2025-12-19 07:21:16.886596: Current learning rate: 0.00044\n",
      "2025-12-19 07:23:35.137047: train_loss -0.8655\n",
      "2025-12-19 07:23:35.138788: val_loss -0.8883\n",
      "2025-12-19 07:23:35.144793: Pseudo dice [0.9363, 0.9636, 0.9384]\n",
      "2025-12-19 07:23:35.150799: Epoch time: 138.26 s\n",
      "2025-12-19 07:23:35.788311: \n",
      "2025-12-19 07:23:35.788311: Epoch 970\n",
      "2025-12-19 07:23:35.788311: Current learning rate: 0.00043\n",
      "2025-12-19 07:25:53.909625: train_loss -0.8661\n",
      "2025-12-19 07:25:53.909625: val_loss -0.8811\n",
      "2025-12-19 07:25:53.921382: Pseudo dice [0.9261, 0.9591, 0.9466]\n",
      "2025-12-19 07:25:53.929131: Epoch time: 138.12 s\n",
      "2025-12-19 07:25:54.586886: \n",
      "2025-12-19 07:25:54.586886: Epoch 971\n",
      "2025-12-19 07:25:54.586886: Current learning rate: 0.00041\n",
      "2025-12-19 07:28:12.680146: train_loss -0.8667\n",
      "2025-12-19 07:28:12.680146: val_loss -0.877\n",
      "2025-12-19 07:28:12.688538: Pseudo dice [0.9283, 0.9563, 0.9387]\n",
      "2025-12-19 07:28:12.696045: Epoch time: 138.1 s\n",
      "2025-12-19 07:28:13.328250: \n",
      "2025-12-19 07:28:13.328250: Epoch 972\n",
      "2025-12-19 07:28:13.328250: Current learning rate: 0.0004\n",
      "2025-12-19 07:30:31.504641: train_loss -0.8678\n",
      "2025-12-19 07:30:31.504641: val_loss -0.8839\n",
      "2025-12-19 07:30:31.504641: Pseudo dice [0.9309, 0.9577, 0.9518]\n",
      "2025-12-19 07:30:31.504641: Epoch time: 138.18 s\n",
      "2025-12-19 07:30:32.138703: \n",
      "2025-12-19 07:30:32.138703: Epoch 973\n",
      "2025-12-19 07:30:32.138703: Current learning rate: 0.00039\n",
      "2025-12-19 07:32:50.401731: train_loss -0.8655\n",
      "2025-12-19 07:32:50.401731: val_loss -0.8745\n",
      "2025-12-19 07:32:50.411482: Pseudo dice [0.9226, 0.9555, 0.9468]\n",
      "2025-12-19 07:32:50.419490: Epoch time: 138.26 s\n",
      "2025-12-19 07:32:51.441308: \n",
      "2025-12-19 07:32:51.441308: Epoch 974\n",
      "2025-12-19 07:32:51.441308: Current learning rate: 0.00037\n",
      "2025-12-19 07:35:09.560045: train_loss -0.8689\n",
      "2025-12-19 07:35:09.560045: val_loss -0.8796\n",
      "2025-12-19 07:35:09.575963: Pseudo dice [0.927, 0.956, 0.9484]\n",
      "2025-12-19 07:35:09.575963: Epoch time: 138.12 s\n",
      "2025-12-19 07:35:10.209042: \n",
      "2025-12-19 07:35:10.209042: Epoch 975\n",
      "2025-12-19 07:35:10.209042: Current learning rate: 0.00036\n",
      "2025-12-19 07:37:28.431905: train_loss -0.8667\n",
      "2025-12-19 07:37:28.431905: val_loss -0.875\n",
      "2025-12-19 07:37:28.431905: Pseudo dice [0.9224, 0.9515, 0.9445]\n",
      "2025-12-19 07:37:28.447652: Epoch time: 138.22 s\n",
      "2025-12-19 07:37:29.081496: \n",
      "2025-12-19 07:37:29.081496: Epoch 976\n",
      "2025-12-19 07:37:29.081496: Current learning rate: 0.00035\n",
      "2025-12-19 07:39:47.293460: train_loss -0.8647\n",
      "2025-12-19 07:39:47.293460: val_loss -0.8851\n",
      "2025-12-19 07:39:47.295462: Pseudo dice [0.9325, 0.9596, 0.9403]\n",
      "2025-12-19 07:39:47.307099: Epoch time: 138.21 s\n",
      "2025-12-19 07:39:48.093551: \n",
      "2025-12-19 07:39:48.093551: Epoch 977\n",
      "2025-12-19 07:39:48.097941: Current learning rate: 0.00034\n",
      "2025-12-19 07:42:06.426389: train_loss -0.8675\n",
      "2025-12-19 07:42:06.426389: val_loss -0.8847\n",
      "2025-12-19 07:42:06.440151: Pseudo dice [0.9338, 0.9611, 0.9407]\n",
      "2025-12-19 07:42:06.447898: Epoch time: 138.33 s\n",
      "2025-12-19 07:42:07.087984: \n",
      "2025-12-19 07:42:07.087984: Epoch 978\n",
      "2025-12-19 07:42:07.087984: Current learning rate: 0.00032\n",
      "2025-12-19 07:44:25.199426: train_loss -0.8716\n",
      "2025-12-19 07:44:25.199426: val_loss -0.8866\n",
      "2025-12-19 07:44:25.215063: Pseudo dice [0.9341, 0.9613, 0.936]\n",
      "2025-12-19 07:44:25.215063: Epoch time: 138.13 s\n",
      "2025-12-19 07:44:25.847386: \n",
      "2025-12-19 07:44:25.847386: Epoch 979\n",
      "2025-12-19 07:44:25.863102: Current learning rate: 0.00031\n",
      "2025-12-19 07:46:44.034494: train_loss -0.8644\n",
      "2025-12-19 07:46:44.034494: val_loss -0.8894\n",
      "2025-12-19 07:46:44.050299: Pseudo dice [0.9305, 0.9628, 0.9486]\n",
      "2025-12-19 07:46:44.050299: Epoch time: 138.19 s\n",
      "2025-12-19 07:46:45.008024: \n",
      "2025-12-19 07:46:45.010026: Epoch 980\n",
      "2025-12-19 07:46:45.010026: Current learning rate: 0.0003\n",
      "2025-12-19 07:49:03.092555: train_loss -0.8708\n",
      "2025-12-19 07:49:03.092555: val_loss -0.8884\n",
      "2025-12-19 07:49:03.104311: Pseudo dice [0.9309, 0.962, 0.9519]\n",
      "2025-12-19 07:49:03.110055: Epoch time: 138.08 s\n",
      "2025-12-19 07:49:03.745475: \n",
      "2025-12-19 07:49:03.745475: Epoch 981\n",
      "2025-12-19 07:49:03.759702: Current learning rate: 0.00028\n",
      "2025-12-19 07:51:21.939102: train_loss -0.8647\n",
      "2025-12-19 07:51:21.940844: val_loss -0.8846\n",
      "2025-12-19 07:51:21.948853: Pseudo dice [0.9307, 0.962, 0.9432]\n",
      "2025-12-19 07:51:21.958603: Epoch time: 138.19 s\n",
      "2025-12-19 07:51:22.592422: \n",
      "2025-12-19 07:51:22.592422: Epoch 982\n",
      "2025-12-19 07:51:22.592422: Current learning rate: 0.00027\n",
      "2025-12-19 07:53:40.818166: train_loss -0.8672\n",
      "2025-12-19 07:53:40.820169: val_loss -0.8732\n",
      "2025-12-19 07:53:40.829920: Pseudo dice [0.9218, 0.9533, 0.9495]\n",
      "2025-12-19 07:53:40.837667: Epoch time: 138.23 s\n",
      "2025-12-19 07:53:41.599513: \n",
      "2025-12-19 07:53:41.599513: Epoch 983\n",
      "2025-12-19 07:53:41.599513: Current learning rate: 0.00026\n",
      "2025-12-19 07:55:59.658710: train_loss -0.8706\n",
      "2025-12-19 07:55:59.660712: val_loss -0.8785\n",
      "2025-12-19 07:55:59.666718: Pseudo dice [0.9288, 0.9546, 0.9477]\n",
      "2025-12-19 07:55:59.672461: Epoch time: 138.06 s\n",
      "2025-12-19 07:56:00.307935: \n",
      "2025-12-19 07:56:00.307935: Epoch 984\n",
      "2025-12-19 07:56:00.324044: Current learning rate: 0.00024\n",
      "2025-12-19 07:58:18.315069: train_loss -0.868\n",
      "2025-12-19 07:58:18.315069: val_loss -0.8902\n",
      "2025-12-19 07:58:18.317072: Pseudo dice [0.9344, 0.9623, 0.9417]\n",
      "2025-12-19 07:58:18.317072: Epoch time: 138.01 s\n",
      "2025-12-19 07:58:19.042184: \n",
      "2025-12-19 07:58:19.042184: Epoch 985\n",
      "2025-12-19 07:58:19.057948: Current learning rate: 0.00023\n",
      "2025-12-19 08:00:37.356374: train_loss -0.8682\n",
      "2025-12-19 08:00:37.356374: val_loss -0.8784\n",
      "2025-12-19 08:00:37.362380: Pseudo dice [0.9241, 0.9574, 0.9483]\n",
      "2025-12-19 08:00:37.368386: Epoch time: 138.31 s\n",
      "2025-12-19 08:00:38.378362: \n",
      "2025-12-19 08:00:38.378362: Epoch 986\n",
      "2025-12-19 08:00:38.378362: Current learning rate: 0.00021\n",
      "2025-12-19 08:02:56.483520: train_loss -0.8689\n",
      "2025-12-19 08:02:56.483520: val_loss -0.8764\n",
      "2025-12-19 08:02:56.491735: Pseudo dice [0.9255, 0.9551, 0.9534]\n",
      "2025-12-19 08:02:56.496975: Epoch time: 138.11 s\n",
      "2025-12-19 08:02:57.218793: \n",
      "2025-12-19 08:02:57.218793: Epoch 987\n",
      "2025-12-19 08:02:57.232224: Current learning rate: 0.0002\n",
      "2025-12-19 08:05:15.426075: train_loss -0.8664\n",
      "2025-12-19 08:05:15.427901: val_loss -0.8787\n",
      "2025-12-19 08:05:15.431905: Pseudo dice [0.9269, 0.9583, 0.9434]\n",
      "2025-12-19 08:05:15.431905: Epoch time: 138.21 s\n",
      "2025-12-19 08:05:16.138127: \n",
      "2025-12-19 08:05:16.138127: Epoch 988\n",
      "2025-12-19 08:05:16.153881: Current learning rate: 0.00019\n",
      "2025-12-19 08:07:34.199299: train_loss -0.8702\n",
      "2025-12-19 08:07:34.199299: val_loss -0.8823\n",
      "2025-12-19 08:07:34.209047: Pseudo dice [0.9259, 0.957, 0.9505]\n",
      "2025-12-19 08:07:34.213051: Epoch time: 138.06 s\n",
      "2025-12-19 08:07:34.935933: \n",
      "2025-12-19 08:07:34.935933: Epoch 989\n",
      "2025-12-19 08:07:34.949940: Current learning rate: 0.00017\n",
      "2025-12-19 08:09:54.179106: train_loss -0.8644\n",
      "2025-12-19 08:09:54.179106: val_loss -0.8804\n",
      "2025-12-19 08:09:54.179106: Pseudo dice [0.9303, 0.9543, 0.9432]\n",
      "2025-12-19 08:09:54.179106: Epoch time: 139.24 s\n",
      "2025-12-19 08:09:54.900539: \n",
      "2025-12-19 08:09:54.900539: Epoch 990\n",
      "2025-12-19 08:09:54.916224: Current learning rate: 0.00016\n",
      "2025-12-19 08:12:13.169602: train_loss -0.868\n",
      "2025-12-19 08:12:13.169602: val_loss -0.8823\n",
      "2025-12-19 08:12:13.172606: Pseudo dice [0.9275, 0.9588, 0.9516]\n",
      "2025-12-19 08:12:13.172606: Epoch time: 138.27 s\n",
      "2025-12-19 08:12:13.812536: \n",
      "2025-12-19 08:12:13.812536: Epoch 991\n",
      "2025-12-19 08:12:13.828441: Current learning rate: 0.00014\n",
      "2025-12-19 08:14:32.096805: train_loss -0.8673\n",
      "2025-12-19 08:14:32.098545: val_loss -0.8829\n",
      "2025-12-19 08:14:32.108293: Pseudo dice [0.9338, 0.958, 0.942]\n",
      "2025-12-19 08:14:32.116300: Epoch time: 138.28 s\n",
      "2025-12-19 08:14:32.852508: \n",
      "2025-12-19 08:14:32.852508: Epoch 992\n",
      "2025-12-19 08:14:32.868563: Current learning rate: 0.00013\n",
      "2025-12-19 08:16:51.053445: train_loss -0.8679\n",
      "2025-12-19 08:16:51.053445: val_loss -0.8863\n",
      "2025-12-19 08:16:51.053445: Pseudo dice [0.9336, 0.9573, 0.9465]\n",
      "2025-12-19 08:16:51.071323: Epoch time: 138.2 s\n",
      "2025-12-19 08:16:51.868578: \n",
      "2025-12-19 08:16:51.868578: Epoch 993\n",
      "2025-12-19 08:16:51.884475: Current learning rate: 0.00011\n",
      "2025-12-19 08:19:10.013692: train_loss -0.872\n",
      "2025-12-19 08:19:10.013692: val_loss -0.8898\n",
      "2025-12-19 08:19:10.023701: Pseudo dice [0.9358, 0.9629, 0.9415]\n",
      "2025-12-19 08:19:10.031460: Epoch time: 138.15 s\n",
      "2025-12-19 08:19:10.668734: \n",
      "2025-12-19 08:19:10.668734: Epoch 994\n",
      "2025-12-19 08:19:10.668734: Current learning rate: 0.0001\n",
      "2025-12-19 08:21:28.938999: train_loss -0.864\n",
      "2025-12-19 08:21:28.938999: val_loss -0.8799\n",
      "2025-12-19 08:21:28.944744: Pseudo dice [0.9258, 0.9567, 0.9488]\n",
      "2025-12-19 08:21:28.951869: Epoch time: 138.27 s\n",
      "2025-12-19 08:21:29.750031: \n",
      "2025-12-19 08:21:29.750031: Epoch 995\n",
      "2025-12-19 08:21:29.750031: Current learning rate: 8e-05\n",
      "2025-12-19 08:23:47.924504: train_loss -0.8702\n",
      "2025-12-19 08:23:47.924504: val_loss -0.8818\n",
      "2025-12-19 08:23:47.943885: Pseudo dice [0.9274, 0.9592, 0.9428]\n",
      "2025-12-19 08:23:47.943885: Epoch time: 138.17 s\n",
      "2025-12-19 08:23:48.588402: \n",
      "2025-12-19 08:23:48.588402: Epoch 996\n",
      "2025-12-19 08:23:48.588402: Current learning rate: 7e-05\n",
      "2025-12-19 08:26:06.824187: train_loss -0.867\n",
      "2025-12-19 08:26:06.824187: val_loss -0.8866\n",
      "2025-12-19 08:26:06.839938: Pseudo dice [0.9319, 0.9572, 0.9483]\n",
      "2025-12-19 08:26:06.839938: Epoch time: 138.24 s\n",
      "2025-12-19 08:26:07.473414: \n",
      "2025-12-19 08:26:07.473414: Epoch 997\n",
      "2025-12-19 08:26:07.489185: Current learning rate: 5e-05\n",
      "2025-12-19 08:28:25.583501: train_loss -0.8674\n",
      "2025-12-19 08:28:25.585503: val_loss -0.8886\n",
      "2025-12-19 08:28:25.591247: Pseudo dice [0.934, 0.9631, 0.9472]\n",
      "2025-12-19 08:28:25.594711: Epoch time: 138.11 s\n",
      "2025-12-19 08:28:25.600649: Yayy! New best EMA pseudo Dice: 0.9449\n",
      "2025-12-19 08:28:26.527335: \n",
      "2025-12-19 08:28:26.527335: Epoch 998\n",
      "2025-12-19 08:28:26.541259: Current learning rate: 4e-05\n",
      "2025-12-19 08:30:44.639422: train_loss -0.8685\n",
      "2025-12-19 08:30:44.639422: val_loss -0.8862\n",
      "2025-12-19 08:30:44.643426: Pseudo dice [0.9299, 0.9619, 0.9513]\n",
      "2025-12-19 08:30:44.643426: Epoch time: 138.11 s\n",
      "2025-12-19 08:30:44.659058: Yayy! New best EMA pseudo Dice: 0.9452\n",
      "2025-12-19 08:30:45.788004: \n",
      "2025-12-19 08:30:45.788004: Epoch 999\n",
      "2025-12-19 08:30:45.790007: Current learning rate: 2e-05\n",
      "2025-12-19 08:33:04.043484: train_loss -0.8649\n",
      "2025-12-19 08:33:04.045486: val_loss -0.8807\n",
      "2025-12-19 08:33:04.055235: Pseudo dice [0.9325, 0.9585, 0.9351]\n",
      "2025-12-19 08:33:04.063244: Epoch time: 138.26 s\n",
      "2025-12-19 08:33:05.090122: Training done.\n",
      "2025-12-19 08:33:05.163525: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-19 08:33:05.163525: The split file contains 5 splits.\n",
      "2025-12-19 08:33:05.179626: Desired fold for training: 2\n",
      "2025-12-19 08:33:05.197919: This split has 400 training and 100 validation cases.\n",
      "2025-12-19 08:33:05.211366: predicting OAS30014_MR_d0196_7\n",
      "2025-12-19 08:33:05.479931: OAS30014_MR_d0196_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:33:25.455577: predicting OAS30014_MR_d0196_8\n",
      "2025-12-19 08:33:25.455577: OAS30014_MR_d0196_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:33:42.204875: predicting OAS30017_MR_d0054_3\n",
      "2025-12-19 08:33:42.222891: OAS30017_MR_d0054_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:33:58.958171: predicting OAS30017_MR_d0054_4\n",
      "2025-12-19 08:33:58.973857: OAS30017_MR_d0054_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:34:15.719516: predicting OAS30017_MR_d0054_5\n",
      "2025-12-19 08:34:15.719516: OAS30017_MR_d0054_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:34:32.462546: predicting OAS30025_MR_d0210_10\n",
      "2025-12-19 08:34:32.475166: OAS30025_MR_d0210_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:34:49.227192: predicting OAS30036_MR_d0059_9\n",
      "2025-12-19 08:34:49.240566: OAS30036_MR_d0059_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:35:05.963352: predicting OAS30039_MR_d1203_2\n",
      "2025-12-19 08:35:05.986114: OAS30039_MR_d1203_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:35:22.732595: predicting OAS30039_MR_d1203_4\n",
      "2025-12-19 08:35:22.743263: OAS30039_MR_d1203_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:35:39.490876: predicting OAS30039_MR_d1203_5\n",
      "2025-12-19 08:35:39.508557: OAS30039_MR_d1203_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:35:56.273760: predicting OAS30039_MR_d1203_7\n",
      "2025-12-19 08:35:56.295452: OAS30039_MR_d1203_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:36:13.043492: predicting OAS30039_MR_d1203_9\n",
      "2025-12-19 08:36:13.055324: OAS30039_MR_d1203_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:36:29.815817: predicting OAS30052_MR_d0693_3\n",
      "2025-12-19 08:36:29.824446: OAS30052_MR_d0693_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:36:46.554788: predicting OAS30052_MR_d0693_7\n",
      "2025-12-19 08:36:46.577798: OAS30052_MR_d0693_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:37:03.301434: predicting OAS30083_MR_d0465_10\n",
      "2025-12-19 08:37:03.315829: OAS30083_MR_d0465_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:37:20.052633: predicting OAS30087_MR_d0260_9\n",
      "2025-12-19 08:37:20.068430: OAS30087_MR_d0260_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:37:36.799148: predicting OAS30099_MR_d0032_5\n",
      "2025-12-19 08:37:36.806815: OAS30099_MR_d0032_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:37:53.535245: predicting OAS30102_MR_d0024_4\n",
      "2025-12-19 08:37:53.557173: OAS30102_MR_d0024_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:38:10.289899: predicting OAS30104_MR_d0328_4\n",
      "2025-12-19 08:38:10.314343: OAS30104_MR_d0328_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:38:27.055836: predicting OAS30104_MR_d0328_7\n",
      "2025-12-19 08:38:27.073947: OAS30104_MR_d0328_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:38:43.807440: predicting OAS30107_MR_d0387_1\n",
      "2025-12-19 08:38:43.815455: OAS30107_MR_d0387_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:39:00.541993: predicting OAS30107_MR_d0387_2\n",
      "2025-12-19 08:39:00.557802: OAS30107_MR_d0387_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:39:17.329958: predicting OAS30107_MR_d0387_7\n",
      "2025-12-19 08:39:17.340092: OAS30107_MR_d0387_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:39:34.076544: predicting OAS30125_MR_d0201_6\n",
      "2025-12-19 08:39:34.086637: OAS30125_MR_d0201_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:39:50.814892: predicting OAS30127_MR_d0098_1\n",
      "2025-12-19 08:39:50.826841: OAS30127_MR_d0098_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:40:07.546544: predicting OAS30127_MR_d0098_10\n",
      "2025-12-19 08:40:07.564497: OAS30127_MR_d0098_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:40:24.333075: predicting OAS30127_MR_d0098_5\n",
      "2025-12-19 08:40:24.343697: OAS30127_MR_d0098_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:40:41.086072: predicting OAS30127_MR_d0098_7\n",
      "2025-12-19 08:40:41.110837: OAS30127_MR_d0098_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:40:57.889864: predicting OAS30127_MR_d0098_9\n",
      "2025-12-19 08:40:57.899881: OAS30127_MR_d0098_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:41:14.647813: predicting OAS30134_MR_d0080_2\n",
      "2025-12-19 08:41:14.657616: OAS30134_MR_d0080_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:41:31.391556: predicting OAS30140_MR_d0172_5\n",
      "2025-12-19 08:41:31.401313: OAS30140_MR_d0172_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:41:48.122048: predicting OAS30147_MR_d0048_1\n",
      "2025-12-19 08:41:48.141811: OAS30147_MR_d0048_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:42:04.872485: predicting OAS30147_MR_d0048_3\n",
      "2025-12-19 08:42:04.884445: OAS30147_MR_d0048_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:42:21.658775: predicting OAS30147_MR_d0048_4\n",
      "2025-12-19 08:42:21.658775: OAS30147_MR_d0048_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:42:38.401988: predicting OAS30147_MR_d0048_7\n",
      "2025-12-19 08:42:38.410982: OAS30147_MR_d0048_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:42:55.145530: predicting OAS30147_MR_d0048_9\n",
      "2025-12-19 08:42:55.164698: OAS30147_MR_d0048_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:43:11.912992: predicting OAS30167_MR_d0111_5\n",
      "2025-12-19 08:43:11.921374: OAS30167_MR_d0111_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:43:28.649956: predicting OAS30176_MR_d0000_2\n",
      "2025-12-19 08:43:28.662444: OAS30176_MR_d0000_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:43:45.373481: predicting OAS30176_MR_d0000_9\n",
      "2025-12-19 08:43:45.396047: OAS30176_MR_d0000_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:44:02.151570: predicting OAS30195_MR_d1596_7\n",
      "2025-12-19 08:44:02.164924: OAS30195_MR_d1596_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:44:18.956748: predicting OAS30226_MR_d0183_2\n",
      "2025-12-19 08:44:18.967911: OAS30226_MR_d0183_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:44:35.695157: predicting OAS30226_MR_d0183_9\n",
      "2025-12-19 08:44:35.705988: OAS30226_MR_d0183_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:44:52.472626: predicting OAS30234_MR_d2098_9\n",
      "2025-12-19 08:44:52.492356: OAS30234_MR_d2098_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:45:09.222697: predicting OAS30238_MR_d0037_3\n",
      "2025-12-19 08:45:09.230901: OAS30238_MR_d0037_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:45:26.004443: predicting OAS30238_MR_d0037_9\n",
      "2025-12-19 08:45:26.017911: OAS30238_MR_d0037_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:45:42.743142: predicting OAS30250_MR_d0389_3\n",
      "2025-12-19 08:45:42.752913: OAS30250_MR_d0389_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:45:59.476436: predicting OAS30250_MR_d0389_4\n",
      "2025-12-19 08:45:59.486454: OAS30250_MR_d0389_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:46:16.216479: predicting OAS30250_MR_d0389_6\n",
      "2025-12-19 08:46:16.229818: OAS30250_MR_d0389_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:46:32.952113: predicting OAS30262_MR_d0037_5\n",
      "2025-12-19 08:46:32.974146: OAS30262_MR_d0037_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:46:49.710545: predicting OAS30262_MR_d0037_6\n",
      "2025-12-19 08:46:49.718268: OAS30262_MR_d0037_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:47:06.464904: predicting OAS30274_MR_d3332_1\n",
      "2025-12-19 08:47:06.486142: OAS30274_MR_d3332_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:47:23.247147: predicting OAS30274_MR_d3332_3\n",
      "2025-12-19 08:47:23.264893: OAS30274_MR_d3332_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:47:39.995805: predicting OAS30274_MR_d3332_5\n",
      "2025-12-19 08:47:40.006696: OAS30274_MR_d3332_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:47:56.734012: predicting OAS30292_MR_d0165_2\n",
      "2025-12-19 08:47:56.757675: OAS30292_MR_d0165_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:48:13.483404: predicting OAS30292_MR_d0165_6\n",
      "2025-12-19 08:48:13.499048: OAS30292_MR_d0165_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:48:30.220617: predicting OAS30300_MR_d0100_6\n",
      "2025-12-19 08:48:30.236640: OAS30300_MR_d0100_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:48:46.967800: predicting OAS30300_MR_d0100_8\n",
      "2025-12-19 08:48:46.979009: OAS30300_MR_d0100_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:49:03.709458: predicting OAS30302_MR_d0262_10\n",
      "2025-12-19 08:49:03.727299: OAS30302_MR_d0262_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:49:20.447858: predicting OAS30302_MR_d0262_7\n",
      "2025-12-19 08:49:20.467681: OAS30302_MR_d0262_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:49:37.179617: predicting OAS30306_MR_d0028_1\n",
      "2025-12-19 08:49:37.204069: OAS30306_MR_d0028_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:49:53.964109: predicting OAS30306_MR_d0028_8\n",
      "2025-12-19 08:49:53.979865: OAS30306_MR_d0028_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:50:10.746253: predicting OAS30321_MR_d3003_1\n",
      "2025-12-19 08:50:10.764360: OAS30321_MR_d3003_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:50:27.490380: predicting OAS30321_MR_d3003_4\n",
      "2025-12-19 08:50:27.506009: OAS30321_MR_d3003_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:50:44.241111: predicting OAS30321_MR_d3003_6\n",
      "2025-12-19 08:50:44.251182: OAS30321_MR_d3003_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:51:00.997309: predicting OAS30321_MR_d3003_8\n",
      "2025-12-19 08:51:01.009213: OAS30321_MR_d3003_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:51:17.730018: predicting OAS30321_MR_d3003_9\n",
      "2025-12-19 08:51:17.752109: OAS30321_MR_d3003_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:51:34.482194: predicting OAS30325_MR_d0032_8\n",
      "2025-12-19 08:51:34.495774: OAS30325_MR_d0032_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:51:51.273049: predicting OAS30343_MR_d4178_4\n",
      "2025-12-19 08:51:51.285925: OAS30343_MR_d4178_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:52:08.043189: predicting OAS30343_MR_d4178_9\n",
      "2025-12-19 08:52:08.051059: OAS30343_MR_d4178_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:52:24.822605: predicting OAS30349_MR_d0699_1\n",
      "2025-12-19 08:52:24.832517: OAS30349_MR_d0699_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:52:41.562523: predicting OAS30349_MR_d0699_4\n",
      "2025-12-19 08:52:41.570538: OAS30349_MR_d0699_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:52:58.327792: predicting OAS30349_MR_d0699_8\n",
      "2025-12-19 08:52:58.341914: OAS30349_MR_d0699_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:53:15.082111: predicting OAS30350_MR_d0018_5\n",
      "2025-12-19 08:53:15.097251: OAS30350_MR_d0018_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:53:31.835168: predicting OAS30352_MR_d0099_4\n",
      "2025-12-19 08:53:31.844034: OAS30352_MR_d0099_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:53:48.611955: predicting OAS30352_MR_d0099_6\n",
      "2025-12-19 08:53:48.619922: OAS30352_MR_d0099_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:54:05.358206: predicting OAS30354_MR_d0056_1\n",
      "2025-12-19 08:54:05.380130: OAS30354_MR_d0056_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:54:22.117763: predicting OAS30354_MR_d0056_2\n",
      "2025-12-19 08:54:22.127949: OAS30354_MR_d0056_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:54:38.862920: predicting OAS30354_MR_d0056_9\n",
      "2025-12-19 08:54:38.872002: OAS30354_MR_d0056_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:54:55.597474: predicting OAS30355_MR_d0048_3\n",
      "2025-12-19 08:54:55.606545: OAS30355_MR_d0048_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:55:12.358996: predicting OAS30355_MR_d0048_6\n",
      "2025-12-19 08:55:12.371765: OAS30355_MR_d0048_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:55:29.106082: predicting OAS30361_MR_d1457_3\n",
      "2025-12-19 08:55:29.119380: OAS30361_MR_d1457_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:55:45.872919: predicting OAS30361_MR_d1457_7\n",
      "2025-12-19 08:55:45.890583: OAS30361_MR_d1457_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:56:02.619496: predicting OAS30367_MR_d1540_2\n",
      "2025-12-19 08:56:02.643171: OAS30367_MR_d1540_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:56:19.386618: predicting OAS30367_MR_d1540_6\n",
      "2025-12-19 08:56:19.397636: OAS30367_MR_d1540_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:56:36.142198: predicting OAS30367_MR_d1540_8\n",
      "2025-12-19 08:56:36.151541: OAS30367_MR_d1540_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:56:52.894622: predicting OAS30369_MR_d4058_10\n",
      "2025-12-19 08:56:52.904459: OAS30369_MR_d4058_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:57:09.697634: predicting OAS30369_MR_d4058_2\n",
      "2025-12-19 08:57:09.708275: OAS30369_MR_d4058_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:57:26.452096: predicting OAS30371_MR_d0338_3\n",
      "2025-12-19 08:57:26.452096: OAS30371_MR_d0338_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:57:43.190436: predicting OAS30371_MR_d0338_5\n",
      "2025-12-19 08:57:43.208295: OAS30371_MR_d0338_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:57:59.936004: predicting OAS30371_MR_d0338_7\n",
      "2025-12-19 08:57:59.952044: OAS30371_MR_d0338_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:58:16.718182: predicting OAS30373_MR_d1211_8\n",
      "2025-12-19 08:58:16.737833: OAS30373_MR_d1211_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:58:33.501601: predicting OAS30379_MR_d2106_2\n",
      "2025-12-19 08:58:33.518740: OAS30379_MR_d2106_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:58:50.312766: predicting OAS30379_MR_d2106_3\n",
      "2025-12-19 08:58:50.322093: OAS30379_MR_d2106_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:59:07.093355: predicting OAS30379_MR_d2106_7\n",
      "2025-12-19 08:59:07.105470: OAS30379_MR_d2106_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:59:23.892780: predicting OAS30379_MR_d2106_8\n",
      "2025-12-19 08:59:23.903430: OAS30379_MR_d2106_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:59:40.676071: predicting OAS30380_MR_d3446_2\n",
      "2025-12-19 08:59:40.688038: OAS30380_MR_d3446_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 08:59:57.433002: predicting OAS30380_MR_d3446_6\n",
      "2025-12-19 08:59:57.450710: OAS30380_MR_d3446_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 09:00:14.165278: predicting OAS30383_MR_d0134_6\n",
      "2025-12-19 09:00:14.186544: OAS30383_MR_d0134_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 09:00:30.954693: predicting OAS30383_MR_d0134_9\n",
      "2025-12-19 09:00:30.964202: OAS30383_MR_d0134_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 09:00:47.701690: predicting OAS30388_MR_d0073_8\n",
      "2025-12-19 09:00:47.724864: OAS30388_MR_d0073_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-19 09:01:29.042474: Validation complete\n",
      "2025-12-19 09:01:29.042474: Mean Validation Dice:  0.932712131954295\n"
     ]
    }
   ],
   "source": [
    "#Train nnU-Net on fold 2\n",
    "!nnUNetv2_train 500 3d_lowres 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d71be7d-8e64-451b-b899-31a55869303b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-12-19 09:01:52.609130: do_dummy_2d_data_aug: False\n",
      "2025-12-19 09:01:52.624773: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-19 09:01:52.624773: The split file contains 5 splits.\n",
      "2025-12-19 09:01:52.624773: Desired fold for training: 3\n",
      "2025-12-19 09:01:52.624773: This split has 400 training and 100 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2025-12-19 09:02:24.625197: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_lowres\n",
      " {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [202, 202, 202], 'spacing': [1.2667700813876164, 1.2667700813876164, 1.2667700813876164], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset500_MRI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [256, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.422696590423584, 'median': 0.4194243550300598, 'min': 0.0027002037968486547, 'percentile_00_5': 0.05628390982747078, 'percentile_99_5': 0.8565635681152344, 'std': 0.19347868859767914}}} \n",
      "\n",
      "2025-12-19 09:02:24.625197: unpacking dataset...\n",
      "2025-12-19 09:02:25.181536: unpacking done...\n",
      "2025-12-19 09:02:25.181536: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-12-19 09:02:25.211472: \n",
      "2025-12-19 09:02:25.211472: Epoch 0\n",
      "2025-12-19 09:02:25.211472: Current learning rate: 0.01\n",
      "2025-12-19 09:04:52.880780: train_loss 0.2065\n",
      "2025-12-19 09:04:52.880780: val_loss 0.0083\n",
      "2025-12-19 09:04:52.890371: Pseudo dice [0.5296, 0.5208, 0.0]\n",
      "2025-12-19 09:04:52.890371: Epoch time: 147.67 s\n",
      "2025-12-19 09:04:52.890371: Yayy! New best EMA pseudo Dice: 0.3501\n",
      "2025-12-19 09:04:53.798219: \n",
      "2025-12-19 09:04:53.798219: Epoch 1\n",
      "2025-12-19 09:04:53.798219: Current learning rate: 0.00999\n",
      "2025-12-19 09:07:11.796130: train_loss -0.0036\n",
      "2025-12-19 09:07:11.796130: val_loss -0.1236\n",
      "2025-12-19 09:07:11.796130: Pseudo dice [0.54, 0.5745, 0.3402]\n",
      "2025-12-19 09:07:11.796130: Epoch time: 138.01 s\n",
      "2025-12-19 09:07:11.796130: Yayy! New best EMA pseudo Dice: 0.3636\n",
      "2025-12-19 09:07:12.784961: \n",
      "2025-12-19 09:07:12.784961: Epoch 2\n",
      "2025-12-19 09:07:12.784961: Current learning rate: 0.00998\n",
      "2025-12-19 09:09:30.947353: train_loss -0.1169\n",
      "2025-12-19 09:09:30.947353: val_loss -0.1913\n",
      "2025-12-19 09:09:30.949355: Pseudo dice [0.553, 0.6002, 0.4822]\n",
      "2025-12-19 09:09:30.951357: Epoch time: 138.16 s\n",
      "2025-12-19 09:09:30.951357: Yayy! New best EMA pseudo Dice: 0.3818\n",
      "2025-12-19 09:09:31.896147: \n",
      "2025-12-19 09:09:31.896147: Epoch 3\n",
      "2025-12-19 09:09:31.896147: Current learning rate: 0.00997\n",
      "2025-12-19 09:11:50.014784: train_loss -0.1758\n",
      "2025-12-19 09:11:50.014784: val_loss -0.2259\n",
      "2025-12-19 09:11:50.016786: Pseudo dice [0.5889, 0.6248, 0.5161]\n",
      "2025-12-19 09:11:50.016786: Epoch time: 138.12 s\n",
      "2025-12-19 09:11:50.016786: Yayy! New best EMA pseudo Dice: 0.4012\n",
      "2025-12-19 09:11:50.904916: \n",
      "2025-12-19 09:11:50.904916: Epoch 4\n",
      "2025-12-19 09:11:50.904916: Current learning rate: 0.00996\n",
      "2025-12-19 09:14:08.882660: train_loss -0.2129\n",
      "2025-12-19 09:14:08.882660: val_loss -0.2822\n",
      "2025-12-19 09:14:08.882660: Pseudo dice [0.6151, 0.684, 0.5247]\n",
      "2025-12-19 09:14:08.882660: Epoch time: 137.98 s\n",
      "2025-12-19 09:14:08.882660: Yayy! New best EMA pseudo Dice: 0.4219\n",
      "2025-12-19 09:14:09.802205: \n",
      "2025-12-19 09:14:09.802205: Epoch 5\n",
      "2025-12-19 09:14:09.802205: Current learning rate: 0.00995\n",
      "2025-12-19 09:16:27.612292: train_loss -0.2932\n",
      "2025-12-19 09:16:27.612292: val_loss -0.3562\n",
      "2025-12-19 09:16:27.614155: Pseudo dice [0.6739, 0.7219, 0.5806]\n",
      "2025-12-19 09:16:27.614155: Epoch time: 137.81 s\n",
      "2025-12-19 09:16:27.614155: Yayy! New best EMA pseudo Dice: 0.4456\n",
      "2025-12-19 09:16:28.497406: \n",
      "2025-12-19 09:16:28.497406: Epoch 6\n",
      "2025-12-19 09:16:28.497406: Current learning rate: 0.00995\n",
      "2025-12-19 09:18:46.440196: train_loss -0.3294\n",
      "2025-12-19 09:18:46.440196: val_loss -0.377\n",
      "2025-12-19 09:18:46.442198: Pseudo dice [0.6775, 0.7441, 0.6]\n",
      "2025-12-19 09:18:46.442198: Epoch time: 137.94 s\n",
      "2025-12-19 09:18:46.442198: Yayy! New best EMA pseudo Dice: 0.4684\n",
      "2025-12-19 09:18:47.497951: \n",
      "2025-12-19 09:18:47.499954: Epoch 7\n",
      "2025-12-19 09:18:47.499954: Current learning rate: 0.00994\n",
      "2025-12-19 09:21:05.308061: train_loss -0.3582\n",
      "2025-12-19 09:21:05.308061: val_loss -0.4294\n",
      "2025-12-19 09:21:05.310063: Pseudo dice [0.703, 0.7866, 0.6448]\n",
      "2025-12-19 09:21:05.310063: Epoch time: 137.81 s\n",
      "2025-12-19 09:21:05.312064: Yayy! New best EMA pseudo Dice: 0.4927\n",
      "2025-12-19 09:21:06.231192: \n",
      "2025-12-19 09:21:06.231192: Epoch 8\n",
      "2025-12-19 09:21:06.231192: Current learning rate: 0.00993\n",
      "2025-12-19 09:23:24.189329: train_loss -0.4096\n",
      "2025-12-19 09:23:24.189329: val_loss -0.4643\n",
      "2025-12-19 09:23:24.193125: Pseudo dice [0.7475, 0.7887, 0.6701]\n",
      "2025-12-19 09:23:24.193125: Epoch time: 137.96 s\n",
      "2025-12-19 09:23:24.193125: Yayy! New best EMA pseudo Dice: 0.517\n",
      "2025-12-19 09:23:25.118304: \n",
      "2025-12-19 09:23:25.118304: Epoch 9\n",
      "2025-12-19 09:23:25.118304: Current learning rate: 0.00992\n",
      "2025-12-19 09:25:43.128682: train_loss -0.4403\n",
      "2025-12-19 09:25:43.128682: val_loss -0.4917\n",
      "2025-12-19 09:25:43.128682: Pseudo dice [0.7533, 0.8049, 0.667]\n",
      "2025-12-19 09:25:43.128682: Epoch time: 138.01 s\n",
      "2025-12-19 09:25:43.128682: Yayy! New best EMA pseudo Dice: 0.5395\n",
      "2025-12-19 09:25:44.033222: \n",
      "2025-12-19 09:25:44.033222: Epoch 10\n",
      "2025-12-19 09:25:44.033222: Current learning rate: 0.00991\n",
      "2025-12-19 09:28:01.881882: train_loss -0.4873\n",
      "2025-12-19 09:28:01.881882: val_loss -0.5194\n",
      "2025-12-19 09:28:01.881882: Pseudo dice [0.7777, 0.8399, 0.6871]\n",
      "2025-12-19 09:28:01.881882: Epoch time: 137.85 s\n",
      "2025-12-19 09:28:01.881882: Yayy! New best EMA pseudo Dice: 0.5623\n",
      "2025-12-19 09:28:02.743676: \n",
      "2025-12-19 09:28:02.743676: Epoch 11\n",
      "2025-12-19 09:28:02.759603: Current learning rate: 0.0099\n",
      "2025-12-19 09:30:20.728602: train_loss -0.5113\n",
      "2025-12-19 09:30:20.728602: val_loss -0.5613\n",
      "2025-12-19 09:30:20.730604: Pseudo dice [0.7841, 0.8342, 0.7392]\n",
      "2025-12-19 09:30:20.732607: Epoch time: 137.98 s\n",
      "2025-12-19 09:30:20.732607: Yayy! New best EMA pseudo Dice: 0.5847\n",
      "2025-12-19 09:30:21.619642: \n",
      "2025-12-19 09:30:21.619642: Epoch 12\n",
      "2025-12-19 09:30:21.619642: Current learning rate: 0.00989\n",
      "2025-12-19 09:32:39.387157: train_loss -0.527\n",
      "2025-12-19 09:32:39.387157: val_loss -0.5567\n",
      "2025-12-19 09:32:39.391981: Pseudo dice [0.789, 0.85, 0.7492]\n",
      "2025-12-19 09:32:39.391981: Epoch time: 137.77 s\n",
      "2025-12-19 09:32:39.393983: Yayy! New best EMA pseudo Dice: 0.6058\n",
      "2025-12-19 09:32:40.449818: \n",
      "2025-12-19 09:32:40.449818: Epoch 13\n",
      "2025-12-19 09:32:40.449818: Current learning rate: 0.00988\n",
      "2025-12-19 09:34:58.620230: train_loss -0.5503\n",
      "2025-12-19 09:34:58.622233: val_loss -0.5981\n",
      "2025-12-19 09:34:58.622233: Pseudo dice [0.8063, 0.8682, 0.7695]\n",
      "2025-12-19 09:34:58.622233: Epoch time: 138.17 s\n",
      "2025-12-19 09:34:58.622233: Yayy! New best EMA pseudo Dice: 0.6267\n",
      "2025-12-19 09:34:59.498681: \n",
      "2025-12-19 09:34:59.498681: Epoch 14\n",
      "2025-12-19 09:34:59.498681: Current learning rate: 0.00987\n",
      "2025-12-19 09:37:17.376241: train_loss -0.5782\n",
      "2025-12-19 09:37:17.376241: val_loss -0.6181\n",
      "2025-12-19 09:37:17.376241: Pseudo dice [0.8211, 0.8787, 0.7618]\n",
      "2025-12-19 09:37:17.376241: Epoch time: 137.88 s\n",
      "2025-12-19 09:37:17.380239: Yayy! New best EMA pseudo Dice: 0.6461\n",
      "2025-12-19 09:37:18.301929: \n",
      "2025-12-19 09:37:18.301929: Epoch 15\n",
      "2025-12-19 09:37:18.301929: Current learning rate: 0.00986\n",
      "2025-12-19 09:39:36.199530: train_loss -0.5911\n",
      "2025-12-19 09:39:36.199530: val_loss -0.6198\n",
      "2025-12-19 09:39:36.199530: Pseudo dice [0.8253, 0.8756, 0.755]\n",
      "2025-12-19 09:39:36.199530: Epoch time: 137.9 s\n",
      "2025-12-19 09:39:36.199530: Yayy! New best EMA pseudo Dice: 0.6634\n",
      "2025-12-19 09:39:37.080150: \n",
      "2025-12-19 09:39:37.080150: Epoch 16\n",
      "2025-12-19 09:39:37.080150: Current learning rate: 0.00986\n",
      "2025-12-19 09:41:55.023408: train_loss -0.6034\n",
      "2025-12-19 09:41:55.023408: val_loss -0.6181\n",
      "2025-12-19 09:41:55.023408: Pseudo dice [0.8136, 0.8674, 0.7884]\n",
      "2025-12-19 09:41:55.023408: Epoch time: 137.94 s\n",
      "2025-12-19 09:41:55.023408: Yayy! New best EMA pseudo Dice: 0.6793\n",
      "2025-12-19 09:41:55.903478: \n",
      "2025-12-19 09:41:55.903478: Epoch 17\n",
      "2025-12-19 09:41:55.919366: Current learning rate: 0.00985\n",
      "2025-12-19 09:44:14.126452: train_loss -0.5997\n",
      "2025-12-19 09:44:14.126452: val_loss -0.6426\n",
      "2025-12-19 09:44:14.126452: Pseudo dice [0.8313, 0.8783, 0.794]\n",
      "2025-12-19 09:44:14.126452: Epoch time: 138.22 s\n",
      "2025-12-19 09:44:14.126452: Yayy! New best EMA pseudo Dice: 0.6949\n",
      "2025-12-19 09:44:15.030125: \n",
      "2025-12-19 09:44:15.030125: Epoch 18\n",
      "2025-12-19 09:44:15.030125: Current learning rate: 0.00984\n",
      "2025-12-19 09:46:33.210879: train_loss -0.6017\n",
      "2025-12-19 09:46:33.210879: val_loss -0.6506\n",
      "2025-12-19 09:46:33.212619: Pseudo dice [0.8294, 0.8867, 0.8108]\n",
      "2025-12-19 09:46:33.212619: Epoch time: 138.18 s\n",
      "2025-12-19 09:46:33.212619: Yayy! New best EMA pseudo Dice: 0.7096\n",
      "2025-12-19 09:46:34.274539: \n",
      "2025-12-19 09:46:34.274539: Epoch 19\n",
      "2025-12-19 09:46:34.274539: Current learning rate: 0.00983\n",
      "2025-12-19 09:48:52.094475: train_loss -0.597\n",
      "2025-12-19 09:48:52.096477: val_loss -0.6352\n",
      "2025-12-19 09:48:52.096477: Pseudo dice [0.8232, 0.8747, 0.7967]\n",
      "2025-12-19 09:48:52.098479: Epoch time: 137.82 s\n",
      "2025-12-19 09:48:52.098479: Yayy! New best EMA pseudo Dice: 0.7218\n",
      "2025-12-19 09:48:52.988665: \n",
      "2025-12-19 09:48:52.988665: Epoch 20\n",
      "2025-12-19 09:48:52.988665: Current learning rate: 0.00982\n",
      "2025-12-19 09:51:11.035809: train_loss -0.611\n",
      "2025-12-19 09:51:11.035809: val_loss -0.6466\n",
      "2025-12-19 09:51:11.050262: Pseudo dice [0.8259, 0.8813, 0.8108]\n",
      "2025-12-19 09:51:11.051782: Epoch time: 138.05 s\n",
      "2025-12-19 09:51:11.051782: Yayy! New best EMA pseudo Dice: 0.7335\n",
      "2025-12-19 09:51:11.957175: \n",
      "2025-12-19 09:51:11.957175: Epoch 21\n",
      "2025-12-19 09:51:11.957175: Current learning rate: 0.00981\n",
      "2025-12-19 09:53:29.913188: train_loss -0.6366\n",
      "2025-12-19 09:53:29.913188: val_loss -0.6589\n",
      "2025-12-19 09:53:29.915190: Pseudo dice [0.8269, 0.9, 0.8232]\n",
      "2025-12-19 09:53:29.915190: Epoch time: 137.96 s\n",
      "2025-12-19 09:53:29.917192: Yayy! New best EMA pseudo Dice: 0.7452\n",
      "2025-12-19 09:53:30.788404: \n",
      "2025-12-19 09:53:30.788404: Epoch 22\n",
      "2025-12-19 09:53:30.788404: Current learning rate: 0.0098\n",
      "2025-12-19 09:55:48.792325: train_loss -0.6286\n",
      "2025-12-19 09:55:48.792325: val_loss -0.6655\n",
      "2025-12-19 09:55:48.808094: Pseudo dice [0.8442, 0.8843, 0.8218]\n",
      "2025-12-19 09:55:48.809974: Epoch time: 138.0 s\n",
      "2025-12-19 09:55:48.809974: Yayy! New best EMA pseudo Dice: 0.7557\n",
      "2025-12-19 09:55:49.681410: \n",
      "2025-12-19 09:55:49.681410: Epoch 23\n",
      "2025-12-19 09:55:49.681410: Current learning rate: 0.00979\n",
      "2025-12-19 09:58:07.611593: train_loss -0.6469\n",
      "2025-12-19 09:58:07.611593: val_loss -0.6881\n",
      "2025-12-19 09:58:07.611593: Pseudo dice [0.8527, 0.907, 0.8239]\n",
      "2025-12-19 09:58:07.611593: Epoch time: 137.93 s\n",
      "2025-12-19 09:58:07.611593: Yayy! New best EMA pseudo Dice: 0.7662\n",
      "2025-12-19 09:58:08.466876: \n",
      "2025-12-19 09:58:08.466876: Epoch 24\n",
      "2025-12-19 09:58:08.466876: Current learning rate: 0.00978\n",
      "2025-12-19 10:00:26.381218: train_loss -0.6572\n",
      "2025-12-19 10:00:26.383221: val_loss -0.6963\n",
      "2025-12-19 10:00:26.383221: Pseudo dice [0.8524, 0.8991, 0.8454]\n",
      "2025-12-19 10:00:26.385224: Epoch time: 137.91 s\n",
      "2025-12-19 10:00:26.385224: Yayy! New best EMA pseudo Dice: 0.7762\n",
      "2025-12-19 10:00:27.438064: \n",
      "2025-12-19 10:00:27.438064: Epoch 25\n",
      "2025-12-19 10:00:27.447445: Current learning rate: 0.00977\n",
      "2025-12-19 10:02:45.551216: train_loss -0.6773\n",
      "2025-12-19 10:02:45.551216: val_loss -0.6993\n",
      "2025-12-19 10:02:45.551216: Pseudo dice [0.847, 0.8989, 0.8525]\n",
      "2025-12-19 10:02:45.567149: Epoch time: 138.11 s\n",
      "2025-12-19 10:02:45.567149: Yayy! New best EMA pseudo Dice: 0.7852\n",
      "2025-12-19 10:02:46.453839: \n",
      "2025-12-19 10:02:46.455579: Epoch 26\n",
      "2025-12-19 10:02:46.455579: Current learning rate: 0.00977\n",
      "2025-12-19 10:05:04.531039: train_loss -0.68\n",
      "2025-12-19 10:05:04.531039: val_loss -0.7164\n",
      "2025-12-19 10:05:04.533041: Pseudo dice [0.8606, 0.9124, 0.854]\n",
      "2025-12-19 10:05:04.535004: Epoch time: 138.08 s\n",
      "2025-12-19 10:05:04.535004: Yayy! New best EMA pseudo Dice: 0.7942\n",
      "2025-12-19 10:05:05.417505: \n",
      "2025-12-19 10:05:05.417505: Epoch 27\n",
      "2025-12-19 10:05:05.417505: Current learning rate: 0.00976\n",
      "2025-12-19 10:07:23.353731: train_loss -0.6896\n",
      "2025-12-19 10:07:23.353731: val_loss -0.7073\n",
      "2025-12-19 10:07:23.357478: Pseudo dice [0.8613, 0.9104, 0.8412]\n",
      "2025-12-19 10:07:23.359482: Epoch time: 137.94 s\n",
      "2025-12-19 10:07:23.359482: Yayy! New best EMA pseudo Dice: 0.8019\n",
      "2025-12-19 10:07:24.243982: \n",
      "2025-12-19 10:07:24.243982: Epoch 28\n",
      "2025-12-19 10:07:24.243982: Current learning rate: 0.00975\n",
      "2025-12-19 10:09:42.599908: train_loss -0.6845\n",
      "2025-12-19 10:09:42.601911: val_loss -0.7103\n",
      "2025-12-19 10:09:42.601911: Pseudo dice [0.8558, 0.9032, 0.8546]\n",
      "2025-12-19 10:09:42.601911: Epoch time: 138.36 s\n",
      "2025-12-19 10:09:42.601911: Yayy! New best EMA pseudo Dice: 0.8088\n",
      "2025-12-19 10:09:43.500958: \n",
      "2025-12-19 10:09:43.502960: Epoch 29\n",
      "2025-12-19 10:09:43.502960: Current learning rate: 0.00974\n",
      "2025-12-19 10:12:01.423368: train_loss -0.6908\n",
      "2025-12-19 10:12:01.423368: val_loss -0.7257\n",
      "2025-12-19 10:12:01.423368: Pseudo dice [0.8649, 0.911, 0.8634]\n",
      "2025-12-19 10:12:01.423368: Epoch time: 137.92 s\n",
      "2025-12-19 10:12:01.423368: Yayy! New best EMA pseudo Dice: 0.8159\n",
      "2025-12-19 10:12:02.308418: \n",
      "2025-12-19 10:12:02.308418: Epoch 30\n",
      "2025-12-19 10:12:02.308418: Current learning rate: 0.00973\n",
      "2025-12-19 10:14:20.439262: train_loss -0.6962\n",
      "2025-12-19 10:14:20.439262: val_loss -0.7278\n",
      "2025-12-19 10:14:20.439262: Pseudo dice [0.8637, 0.9052, 0.8682]\n",
      "2025-12-19 10:14:20.439262: Epoch time: 138.13 s\n",
      "2025-12-19 10:14:20.439262: Yayy! New best EMA pseudo Dice: 0.8222\n",
      "2025-12-19 10:14:21.314640: \n",
      "2025-12-19 10:14:21.314640: Epoch 31\n",
      "2025-12-19 10:14:21.314640: Current learning rate: 0.00972\n",
      "2025-12-19 10:16:39.183911: train_loss -0.6994\n",
      "2025-12-19 10:16:39.183911: val_loss -0.7261\n",
      "2025-12-19 10:16:39.183911: Pseudo dice [0.8624, 0.9128, 0.8649]\n",
      "2025-12-19 10:16:39.199696: Epoch time: 137.87 s\n",
      "2025-12-19 10:16:39.199696: Yayy! New best EMA pseudo Dice: 0.828\n",
      "2025-12-19 10:16:40.091276: \n",
      "2025-12-19 10:16:40.091276: Epoch 32\n",
      "2025-12-19 10:16:40.091276: Current learning rate: 0.00971\n",
      "2025-12-19 10:18:57.978890: train_loss -0.7116\n",
      "2025-12-19 10:18:57.978890: val_loss -0.7331\n",
      "2025-12-19 10:18:57.978890: Pseudo dice [0.866, 0.9107, 0.8644]\n",
      "2025-12-19 10:18:57.978890: Epoch time: 137.89 s\n",
      "2025-12-19 10:18:57.989044: Yayy! New best EMA pseudo Dice: 0.8333\n",
      "2025-12-19 10:18:58.882818: \n",
      "2025-12-19 10:18:58.884820: Epoch 33\n",
      "2025-12-19 10:18:58.884820: Current learning rate: 0.0097\n",
      "2025-12-19 10:21:16.882917: train_loss -0.7111\n",
      "2025-12-19 10:21:16.882917: val_loss -0.7461\n",
      "2025-12-19 10:21:16.886418: Pseudo dice [0.8724, 0.9266, 0.8723]\n",
      "2025-12-19 10:21:16.886418: Epoch time: 138.0 s\n",
      "2025-12-19 10:21:16.886418: Yayy! New best EMA pseudo Dice: 0.839\n",
      "2025-12-19 10:21:17.778454: \n",
      "2025-12-19 10:21:17.778454: Epoch 34\n",
      "2025-12-19 10:21:17.778454: Current learning rate: 0.00969\n",
      "2025-12-19 10:23:35.783435: train_loss -0.7113\n",
      "2025-12-19 10:23:35.785438: val_loss -0.7527\n",
      "2025-12-19 10:23:35.785438: Pseudo dice [0.8804, 0.9236, 0.8739]\n",
      "2025-12-19 10:23:35.785438: Epoch time: 138.01 s\n",
      "2025-12-19 10:23:35.785438: Yayy! New best EMA pseudo Dice: 0.8443\n",
      "2025-12-19 10:23:36.686108: \n",
      "2025-12-19 10:23:36.686108: Epoch 35\n",
      "2025-12-19 10:23:36.686108: Current learning rate: 0.00968\n",
      "2025-12-19 10:25:54.846648: train_loss -0.7217\n",
      "2025-12-19 10:25:54.846648: val_loss -0.7461\n",
      "2025-12-19 10:25:54.848650: Pseudo dice [0.8746, 0.9151, 0.8804]\n",
      "2025-12-19 10:25:54.850652: Epoch time: 138.16 s\n",
      "2025-12-19 10:25:54.850652: Yayy! New best EMA pseudo Dice: 0.8489\n",
      "2025-12-19 10:25:55.933208: \n",
      "2025-12-19 10:25:55.935211: Epoch 36\n",
      "2025-12-19 10:25:55.935211: Current learning rate: 0.00968\n",
      "2025-12-19 10:28:13.939637: train_loss -0.732\n",
      "2025-12-19 10:28:13.941640: val_loss -0.7562\n",
      "2025-12-19 10:28:13.941640: Pseudo dice [0.8772, 0.9267, 0.8685]\n",
      "2025-12-19 10:28:13.944004: Epoch time: 138.01 s\n",
      "2025-12-19 10:28:13.944004: Yayy! New best EMA pseudo Dice: 0.8531\n",
      "2025-12-19 10:28:14.847597: \n",
      "2025-12-19 10:28:14.847597: Epoch 37\n",
      "2025-12-19 10:28:14.847597: Current learning rate: 0.00967\n",
      "2025-12-19 10:30:32.739246: train_loss -0.7227\n",
      "2025-12-19 10:30:32.739246: val_loss -0.7445\n",
      "2025-12-19 10:30:32.739246: Pseudo dice [0.8747, 0.9123, 0.8626]\n",
      "2025-12-19 10:30:32.739246: Epoch time: 137.89 s\n",
      "2025-12-19 10:30:32.739246: Yayy! New best EMA pseudo Dice: 0.8561\n",
      "2025-12-19 10:30:33.677404: \n",
      "2025-12-19 10:30:33.677404: Epoch 38\n",
      "2025-12-19 10:30:33.677404: Current learning rate: 0.00966\n",
      "2025-12-19 10:32:51.559753: train_loss -0.7369\n",
      "2025-12-19 10:32:51.561756: val_loss -0.7641\n",
      "2025-12-19 10:32:51.561756: Pseudo dice [0.8829, 0.9242, 0.8709]\n",
      "2025-12-19 10:32:51.563758: Epoch time: 137.88 s\n",
      "2025-12-19 10:32:51.563758: Yayy! New best EMA pseudo Dice: 0.8598\n",
      "2025-12-19 10:32:52.478564: \n",
      "2025-12-19 10:32:52.480566: Epoch 39\n",
      "2025-12-19 10:32:52.480566: Current learning rate: 0.00965\n",
      "2025-12-19 10:35:10.601338: train_loss -0.7296\n",
      "2025-12-19 10:35:10.601338: val_loss -0.7532\n",
      "2025-12-19 10:35:10.601338: Pseudo dice [0.8723, 0.917, 0.8793]\n",
      "2025-12-19 10:35:10.604848: Epoch time: 138.12 s\n",
      "2025-12-19 10:35:10.604848: Yayy! New best EMA pseudo Dice: 0.8627\n",
      "2025-12-19 10:35:11.523290: \n",
      "2025-12-19 10:35:11.523290: Epoch 40\n",
      "2025-12-19 10:35:11.523290: Current learning rate: 0.00964\n",
      "2025-12-19 10:37:29.659312: train_loss -0.7344\n",
      "2025-12-19 10:37:29.659312: val_loss -0.7656\n",
      "2025-12-19 10:37:29.659312: Pseudo dice [0.8843, 0.93, 0.8794]\n",
      "2025-12-19 10:37:29.659312: Epoch time: 138.14 s\n",
      "2025-12-19 10:37:29.659312: Yayy! New best EMA pseudo Dice: 0.8663\n",
      "2025-12-19 10:37:30.581228: \n",
      "2025-12-19 10:37:30.581228: Epoch 41\n",
      "2025-12-19 10:37:30.581228: Current learning rate: 0.00963\n",
      "2025-12-19 10:39:48.532269: train_loss -0.7335\n",
      "2025-12-19 10:39:48.532269: val_loss -0.7659\n",
      "2025-12-19 10:39:48.534271: Pseudo dice [0.8826, 0.9255, 0.8909]\n",
      "2025-12-19 10:39:48.534271: Epoch time: 137.95 s\n",
      "2025-12-19 10:39:48.536272: Yayy! New best EMA pseudo Dice: 0.8696\n",
      "2025-12-19 10:39:49.680493: \n",
      "2025-12-19 10:39:49.680493: Epoch 42\n",
      "2025-12-19 10:39:49.680493: Current learning rate: 0.00962\n",
      "2025-12-19 10:42:07.778653: train_loss -0.7337\n",
      "2025-12-19 10:42:07.793392: val_loss -0.7785\n",
      "2025-12-19 10:42:07.794395: Pseudo dice [0.8888, 0.9352, 0.8898]\n",
      "2025-12-19 10:42:07.794395: Epoch time: 138.11 s\n",
      "2025-12-19 10:42:07.794395: Yayy! New best EMA pseudo Dice: 0.8731\n",
      "2025-12-19 10:42:08.684943: \n",
      "2025-12-19 10:42:08.684943: Epoch 43\n",
      "2025-12-19 10:42:08.684943: Current learning rate: 0.00961\n",
      "2025-12-19 10:44:26.601080: train_loss -0.7432\n",
      "2025-12-19 10:44:26.601080: val_loss -0.7711\n",
      "2025-12-19 10:44:26.603083: Pseudo dice [0.8826, 0.9294, 0.885]\n",
      "2025-12-19 10:44:26.605085: Epoch time: 137.92 s\n",
      "2025-12-19 10:44:26.605085: Yayy! New best EMA pseudo Dice: 0.8757\n",
      "2025-12-19 10:44:27.485603: \n",
      "2025-12-19 10:44:27.485603: Epoch 44\n",
      "2025-12-19 10:44:27.485603: Current learning rate: 0.0096\n",
      "2025-12-19 10:46:45.529883: train_loss -0.7449\n",
      "2025-12-19 10:46:45.531888: val_loss -0.7708\n",
      "2025-12-19 10:46:45.533890: Pseudo dice [0.8815, 0.9307, 0.8805]\n",
      "2025-12-19 10:46:45.533890: Epoch time: 138.05 s\n",
      "2025-12-19 10:46:45.535893: Yayy! New best EMA pseudo Dice: 0.8779\n",
      "2025-12-19 10:46:46.585619: \n",
      "2025-12-19 10:46:46.585619: Epoch 45\n",
      "2025-12-19 10:46:46.585619: Current learning rate: 0.00959\n",
      "2025-12-19 10:49:04.645311: train_loss -0.7412\n",
      "2025-12-19 10:49:04.645311: val_loss -0.7686\n",
      "2025-12-19 10:49:04.645311: Pseudo dice [0.8822, 0.9216, 0.8965]\n",
      "2025-12-19 10:49:04.645311: Epoch time: 138.06 s\n",
      "2025-12-19 10:49:04.645311: Yayy! New best EMA pseudo Dice: 0.8801\n",
      "2025-12-19 10:49:05.527970: \n",
      "2025-12-19 10:49:05.527970: Epoch 46\n",
      "2025-12-19 10:49:05.527970: Current learning rate: 0.00959\n",
      "2025-12-19 10:51:23.678865: train_loss -0.7304\n",
      "2025-12-19 10:51:23.678865: val_loss -0.7655\n",
      "2025-12-19 10:51:23.680868: Pseudo dice [0.8815, 0.93, 0.87]\n",
      "2025-12-19 10:51:23.682871: Epoch time: 138.15 s\n",
      "2025-12-19 10:51:23.682871: Yayy! New best EMA pseudo Dice: 0.8815\n",
      "2025-12-19 10:51:24.553235: \n",
      "2025-12-19 10:51:24.553235: Epoch 47\n",
      "2025-12-19 10:51:24.553235: Current learning rate: 0.00958\n",
      "2025-12-19 10:53:42.470570: train_loss -0.7453\n",
      "2025-12-19 10:53:42.470570: val_loss -0.78\n",
      "2025-12-19 10:53:42.470570: Pseudo dice [0.8856, 0.9298, 0.8973]\n",
      "2025-12-19 10:53:42.486673: Epoch time: 137.92 s\n",
      "2025-12-19 10:53:42.486673: Yayy! New best EMA pseudo Dice: 0.8837\n",
      "2025-12-19 10:53:43.664112: \n",
      "2025-12-19 10:53:43.664112: Epoch 48\n",
      "2025-12-19 10:53:43.664112: Current learning rate: 0.00957\n",
      "2025-12-19 10:56:01.518100: train_loss -0.7401\n",
      "2025-12-19 10:56:01.520101: val_loss -0.7928\n",
      "2025-12-19 10:56:01.520101: Pseudo dice [0.898, 0.9364, 0.8886]\n",
      "2025-12-19 10:56:01.520101: Epoch time: 137.86 s\n",
      "2025-12-19 10:56:01.523721: Yayy! New best EMA pseudo Dice: 0.8861\n",
      "2025-12-19 10:56:02.390407: \n",
      "2025-12-19 10:56:02.390407: Epoch 49\n",
      "2025-12-19 10:56:02.390407: Current learning rate: 0.00956\n",
      "2025-12-19 10:58:20.585692: train_loss -0.7293\n",
      "2025-12-19 10:58:20.585692: val_loss -0.7682\n",
      "2025-12-19 10:58:20.587695: Pseudo dice [0.8796, 0.9278, 0.8846]\n",
      "2025-12-19 10:58:20.589698: Epoch time: 138.2 s\n",
      "2025-12-19 10:58:20.826578: Yayy! New best EMA pseudo Dice: 0.8873\n",
      "2025-12-19 10:58:21.701673: \n",
      "2025-12-19 10:58:21.701673: Epoch 50\n",
      "2025-12-19 10:58:21.701673: Current learning rate: 0.00955\n",
      "2025-12-19 11:00:39.615581: train_loss -0.7305\n",
      "2025-12-19 11:00:39.615581: val_loss -0.7603\n",
      "2025-12-19 11:00:39.617584: Pseudo dice [0.8763, 0.9292, 0.8643]\n",
      "2025-12-19 11:00:39.619589: Epoch time: 137.91 s\n",
      "2025-12-19 11:00:39.623595: Yayy! New best EMA pseudo Dice: 0.8875\n",
      "2025-12-19 11:00:40.625065: \n",
      "2025-12-19 11:00:40.625065: Epoch 51\n",
      "2025-12-19 11:00:40.625065: Current learning rate: 0.00954\n",
      "2025-12-19 11:02:58.774713: train_loss -0.7364\n",
      "2025-12-19 11:02:58.776716: val_loss -0.7783\n",
      "2025-12-19 11:02:58.778719: Pseudo dice [0.8881, 0.9332, 0.8858]\n",
      "2025-12-19 11:02:58.780460: Epoch time: 138.15 s\n",
      "2025-12-19 11:02:58.782464: Yayy! New best EMA pseudo Dice: 0.889\n",
      "2025-12-19 11:02:59.664417: \n",
      "2025-12-19 11:02:59.666419: Epoch 52\n",
      "2025-12-19 11:02:59.666419: Current learning rate: 0.00953\n",
      "2025-12-19 11:05:17.781815: train_loss -0.7498\n",
      "2025-12-19 11:05:17.781815: val_loss -0.7861\n",
      "2025-12-19 11:05:17.783818: Pseudo dice [0.8887, 0.9291, 0.8911]\n",
      "2025-12-19 11:05:17.785821: Epoch time: 138.12 s\n",
      "2025-12-19 11:05:17.785821: Yayy! New best EMA pseudo Dice: 0.8904\n",
      "2025-12-19 11:05:18.688926: \n",
      "2025-12-19 11:05:18.688926: Epoch 53\n",
      "2025-12-19 11:05:18.688926: Current learning rate: 0.00952\n",
      "2025-12-19 11:07:36.639800: train_loss -0.7603\n",
      "2025-12-19 11:07:36.639800: val_loss -0.7897\n",
      "2025-12-19 11:07:36.641803: Pseudo dice [0.8923, 0.9358, 0.8788]\n",
      "2025-12-19 11:07:36.643806: Epoch time: 137.95 s\n",
      "2025-12-19 11:07:36.645808: Yayy! New best EMA pseudo Dice: 0.8916\n",
      "2025-12-19 11:07:37.893981: \n",
      "2025-12-19 11:07:37.893981: Epoch 54\n",
      "2025-12-19 11:07:37.895984: Current learning rate: 0.00951\n",
      "2025-12-19 11:09:56.191606: train_loss -0.7647\n",
      "2025-12-19 11:09:56.191606: val_loss -0.7902\n",
      "2025-12-19 11:09:56.195613: Pseudo dice [0.89, 0.935, 0.8941]\n",
      "2025-12-19 11:09:56.197617: Epoch time: 138.3 s\n",
      "2025-12-19 11:09:56.199620: Yayy! New best EMA pseudo Dice: 0.8931\n",
      "2025-12-19 11:09:57.072207: \n",
      "2025-12-19 11:09:57.072207: Epoch 55\n",
      "2025-12-19 11:09:57.072207: Current learning rate: 0.0095\n",
      "2025-12-19 11:12:15.277282: train_loss -0.7643\n",
      "2025-12-19 11:12:15.277282: val_loss -0.7888\n",
      "2025-12-19 11:12:15.279285: Pseudo dice [0.888, 0.9342, 0.8874]\n",
      "2025-12-19 11:12:15.281287: Epoch time: 138.21 s\n",
      "2025-12-19 11:12:15.281287: Yayy! New best EMA pseudo Dice: 0.8941\n",
      "2025-12-19 11:12:16.166343: \n",
      "2025-12-19 11:12:16.166343: Epoch 56\n",
      "2025-12-19 11:12:16.166343: Current learning rate: 0.00949\n",
      "2025-12-19 11:14:34.147776: train_loss -0.7603\n",
      "2025-12-19 11:14:34.147776: val_loss -0.783\n",
      "2025-12-19 11:14:34.149778: Pseudo dice [0.8807, 0.9256, 0.9027]\n",
      "2025-12-19 11:14:34.149778: Epoch time: 137.98 s\n",
      "2025-12-19 11:14:34.149778: Yayy! New best EMA pseudo Dice: 0.895\n",
      "2025-12-19 11:14:35.156048: \n",
      "2025-12-19 11:14:35.156048: Epoch 57\n",
      "2025-12-19 11:14:35.165558: Current learning rate: 0.00949\n",
      "2025-12-19 11:16:53.147048: train_loss -0.7686\n",
      "2025-12-19 11:16:53.147048: val_loss -0.791\n",
      "2025-12-19 11:16:53.149050: Pseudo dice [0.8869, 0.9343, 0.8885]\n",
      "2025-12-19 11:16:53.151052: Epoch time: 137.99 s\n",
      "2025-12-19 11:16:53.151052: Yayy! New best EMA pseudo Dice: 0.8958\n",
      "2025-12-19 11:16:54.039431: \n",
      "2025-12-19 11:16:54.039431: Epoch 58\n",
      "2025-12-19 11:16:54.039431: Current learning rate: 0.00948\n",
      "2025-12-19 11:19:12.010019: train_loss -0.7748\n",
      "2025-12-19 11:19:12.010019: val_loss -0.8098\n",
      "2025-12-19 11:19:12.012024: Pseudo dice [0.9001, 0.9365, 0.8997]\n",
      "2025-12-19 11:19:12.014027: Epoch time: 137.97 s\n",
      "2025-12-19 11:19:12.016029: Yayy! New best EMA pseudo Dice: 0.8974\n",
      "2025-12-19 11:19:12.906804: \n",
      "2025-12-19 11:19:12.906804: Epoch 59\n",
      "2025-12-19 11:19:12.906804: Current learning rate: 0.00947\n",
      "2025-12-19 11:21:30.828955: train_loss -0.7692\n",
      "2025-12-19 11:21:30.828955: val_loss -0.8037\n",
      "2025-12-19 11:21:30.830957: Pseudo dice [0.8955, 0.9328, 0.9076]\n",
      "2025-12-19 11:21:30.830957: Epoch time: 137.92 s\n",
      "2025-12-19 11:21:30.832959: Yayy! New best EMA pseudo Dice: 0.8989\n",
      "2025-12-19 11:21:32.004408: \n",
      "2025-12-19 11:21:32.004408: Epoch 60\n",
      "2025-12-19 11:21:32.004408: Current learning rate: 0.00946\n",
      "2025-12-19 11:23:49.998118: train_loss -0.7762\n",
      "2025-12-19 11:23:49.998118: val_loss -0.8175\n",
      "2025-12-19 11:23:49.998118: Pseudo dice [0.9031, 0.9401, 0.9048]\n",
      "2025-12-19 11:23:50.007108: Epoch time: 137.99 s\n",
      "2025-12-19 11:23:50.007108: Yayy! New best EMA pseudo Dice: 0.9006\n",
      "2025-12-19 11:23:50.872683: \n",
      "2025-12-19 11:23:50.872683: Epoch 61\n",
      "2025-12-19 11:23:50.888779: Current learning rate: 0.00945\n",
      "2025-12-19 11:26:09.011669: train_loss -0.7781\n",
      "2025-12-19 11:26:09.011669: val_loss -0.7986\n",
      "2025-12-19 11:26:09.011669: Pseudo dice [0.8936, 0.9325, 0.8985]\n",
      "2025-12-19 11:26:09.011669: Epoch time: 138.14 s\n",
      "2025-12-19 11:26:09.027514: Yayy! New best EMA pseudo Dice: 0.9014\n",
      "2025-12-19 11:26:09.898308: \n",
      "2025-12-19 11:26:09.898308: Epoch 62\n",
      "2025-12-19 11:26:09.898308: Current learning rate: 0.00944\n",
      "2025-12-19 11:28:27.976713: train_loss -0.7735\n",
      "2025-12-19 11:28:27.978715: val_loss -0.8037\n",
      "2025-12-19 11:28:27.980718: Pseudo dice [0.8968, 0.9369, 0.9002]\n",
      "2025-12-19 11:28:27.980718: Epoch time: 138.08 s\n",
      "2025-12-19 11:28:27.982722: Yayy! New best EMA pseudo Dice: 0.9023\n",
      "2025-12-19 11:28:28.945826: \n",
      "2025-12-19 11:28:28.945826: Epoch 63\n",
      "2025-12-19 11:28:28.945826: Current learning rate: 0.00943\n",
      "2025-12-19 11:30:46.972069: train_loss -0.7662\n",
      "2025-12-19 11:30:46.972069: val_loss -0.7971\n",
      "2025-12-19 11:30:46.972069: Pseudo dice [0.8923, 0.9352, 0.8952]\n",
      "2025-12-19 11:30:46.972069: Epoch time: 138.03 s\n",
      "2025-12-19 11:30:46.972069: Yayy! New best EMA pseudo Dice: 0.9029\n",
      "2025-12-19 11:30:47.860068: \n",
      "2025-12-19 11:30:47.860068: Epoch 64\n",
      "2025-12-19 11:30:47.860068: Current learning rate: 0.00942\n",
      "2025-12-19 11:33:05.782220: train_loss -0.7707\n",
      "2025-12-19 11:33:05.782220: val_loss -0.8113\n",
      "2025-12-19 11:33:05.782220: Pseudo dice [0.9017, 0.9364, 0.9104]\n",
      "2025-12-19 11:33:05.782220: Epoch time: 137.92 s\n",
      "2025-12-19 11:33:05.782220: Yayy! New best EMA pseudo Dice: 0.9042\n",
      "2025-12-19 11:33:06.845797: \n",
      "2025-12-19 11:33:06.845797: Epoch 65\n",
      "2025-12-19 11:33:06.861563: Current learning rate: 0.00941\n",
      "2025-12-19 11:35:24.898452: train_loss -0.7808\n",
      "2025-12-19 11:35:24.898452: val_loss -0.801\n",
      "2025-12-19 11:35:24.900193: Pseudo dice [0.895, 0.9335, 0.9043]\n",
      "2025-12-19 11:35:24.902195: Epoch time: 138.05 s\n",
      "2025-12-19 11:35:24.902195: Yayy! New best EMA pseudo Dice: 0.9049\n",
      "2025-12-19 11:35:25.849253: \n",
      "2025-12-19 11:35:25.849253: Epoch 66\n",
      "2025-12-19 11:35:25.849253: Current learning rate: 0.0094\n",
      "2025-12-19 11:37:43.807565: train_loss -0.7776\n",
      "2025-12-19 11:37:43.807565: val_loss -0.8117\n",
      "2025-12-19 11:37:43.807565: Pseudo dice [0.9012, 0.9374, 0.9022]\n",
      "2025-12-19 11:37:43.807565: Epoch time: 137.96 s\n",
      "2025-12-19 11:37:43.819585: Yayy! New best EMA pseudo Dice: 0.9057\n",
      "2025-12-19 11:37:44.695624: \n",
      "2025-12-19 11:37:44.711723: Epoch 67\n",
      "2025-12-19 11:37:44.711723: Current learning rate: 0.00939\n",
      "2025-12-19 11:40:02.710803: train_loss -0.7763\n",
      "2025-12-19 11:40:02.710803: val_loss -0.8039\n",
      "2025-12-19 11:40:02.710803: Pseudo dice [0.8929, 0.9332, 0.9015]\n",
      "2025-12-19 11:40:02.710803: Epoch time: 138.02 s\n",
      "2025-12-19 11:40:02.710803: Yayy! New best EMA pseudo Dice: 0.9061\n",
      "2025-12-19 11:40:03.614544: \n",
      "2025-12-19 11:40:03.614544: Epoch 68\n",
      "2025-12-19 11:40:03.614544: Current learning rate: 0.00939\n",
      "2025-12-19 11:42:21.536135: train_loss -0.7741\n",
      "2025-12-19 11:42:21.536135: val_loss -0.7871\n",
      "2025-12-19 11:42:21.536135: Pseudo dice [0.8846, 0.925, 0.8984]\n",
      "2025-12-19 11:42:21.536135: Epoch time: 137.92 s\n",
      "2025-12-19 11:42:22.215164: \n",
      "2025-12-19 11:42:22.215164: Epoch 69\n",
      "2025-12-19 11:42:22.215164: Current learning rate: 0.00938\n",
      "2025-12-19 11:44:40.221888: train_loss -0.7541\n",
      "2025-12-19 11:44:40.221888: val_loss -0.8069\n",
      "2025-12-19 11:44:40.221888: Pseudo dice [0.9015, 0.938, 0.8978]\n",
      "2025-12-19 11:44:40.221888: Epoch time: 138.02 s\n",
      "2025-12-19 11:44:40.221888: Yayy! New best EMA pseudo Dice: 0.9064\n",
      "2025-12-19 11:44:41.125735: \n",
      "2025-12-19 11:44:41.125735: Epoch 70\n",
      "2025-12-19 11:44:41.125735: Current learning rate: 0.00937\n",
      "2025-12-19 11:46:59.047588: train_loss -0.7826\n",
      "2025-12-19 11:46:59.047588: val_loss -0.81\n",
      "2025-12-19 11:46:59.047588: Pseudo dice [0.9018, 0.9347, 0.8977]\n",
      "2025-12-19 11:46:59.047588: Epoch time: 137.92 s\n",
      "2025-12-19 11:46:59.047588: Yayy! New best EMA pseudo Dice: 0.9069\n",
      "2025-12-19 11:47:00.127019: \n",
      "2025-12-19 11:47:00.127019: Epoch 71\n",
      "2025-12-19 11:47:00.127019: Current learning rate: 0.00936\n",
      "2025-12-19 11:49:18.052121: train_loss -0.7791\n",
      "2025-12-19 11:49:18.052121: val_loss -0.7969\n",
      "2025-12-19 11:49:18.052121: Pseudo dice [0.8874, 0.9307, 0.8974]\n",
      "2025-12-19 11:49:18.055642: Epoch time: 137.93 s\n",
      "2025-12-19 11:49:18.788287: \n",
      "2025-12-19 11:49:18.788287: Epoch 72\n",
      "2025-12-19 11:49:18.788287: Current learning rate: 0.00935\n",
      "2025-12-19 11:51:36.733748: train_loss -0.7822\n",
      "2025-12-19 11:51:36.733748: val_loss -0.8143\n",
      "2025-12-19 11:51:36.737753: Pseudo dice [0.8994, 0.9375, 0.9046]\n",
      "2025-12-19 11:51:36.739756: Epoch time: 137.95 s\n",
      "2025-12-19 11:51:36.739756: Yayy! New best EMA pseudo Dice: 0.9074\n",
      "2025-12-19 11:51:37.649996: \n",
      "2025-12-19 11:51:37.649996: Epoch 73\n",
      "2025-12-19 11:51:37.649996: Current learning rate: 0.00934\n",
      "2025-12-19 11:53:55.645607: train_loss -0.7819\n",
      "2025-12-19 11:53:55.645607: val_loss -0.8087\n",
      "2025-12-19 11:53:55.649615: Pseudo dice [0.8921, 0.9373, 0.9078]\n",
      "2025-12-19 11:53:55.651617: Epoch time: 138.0 s\n",
      "2025-12-19 11:53:55.651617: Yayy! New best EMA pseudo Dice: 0.9079\n",
      "2025-12-19 11:53:56.559239: \n",
      "2025-12-19 11:53:56.559239: Epoch 74\n",
      "2025-12-19 11:53:56.559239: Current learning rate: 0.00933\n",
      "2025-12-19 11:56:14.632806: train_loss -0.7814\n",
      "2025-12-19 11:56:14.634808: val_loss -0.8173\n",
      "2025-12-19 11:56:14.636810: Pseudo dice [0.9059, 0.9434, 0.9107]\n",
      "2025-12-19 11:56:14.638551: Epoch time: 138.07 s\n",
      "2025-12-19 11:56:14.640554: Yayy! New best EMA pseudo Dice: 0.9092\n",
      "2025-12-19 11:56:15.683320: \n",
      "2025-12-19 11:56:15.683320: Epoch 75\n",
      "2025-12-19 11:56:15.683320: Current learning rate: 0.00932\n",
      "2025-12-19 11:58:33.665887: train_loss -0.7839\n",
      "2025-12-19 11:58:33.665887: val_loss -0.8136\n",
      "2025-12-19 11:58:33.665887: Pseudo dice [0.8977, 0.9394, 0.9104]\n",
      "2025-12-19 11:58:33.681800: Epoch time: 137.98 s\n",
      "2025-12-19 11:58:33.681800: Yayy! New best EMA pseudo Dice: 0.9098\n",
      "2025-12-19 11:58:34.567950: \n",
      "2025-12-19 11:58:34.567950: Epoch 76\n",
      "2025-12-19 11:58:34.583694: Current learning rate: 0.00931\n",
      "2025-12-19 12:00:52.517625: train_loss -0.7862\n",
      "2025-12-19 12:00:52.517625: val_loss -0.8127\n",
      "2025-12-19 12:00:52.517625: Pseudo dice [0.8962, 0.9373, 0.9098]\n",
      "2025-12-19 12:00:52.523125: Epoch time: 137.95 s\n",
      "2025-12-19 12:00:52.525127: Yayy! New best EMA pseudo Dice: 0.9103\n",
      "2025-12-19 12:00:53.618481: \n",
      "2025-12-19 12:00:53.618481: Epoch 77\n",
      "2025-12-19 12:00:53.618481: Current learning rate: 0.0093\n",
      "2025-12-19 12:03:11.637908: train_loss -0.7915\n",
      "2025-12-19 12:03:11.637908: val_loss -0.8127\n",
      "2025-12-19 12:03:11.639911: Pseudo dice [0.8956, 0.9341, 0.9108]\n",
      "2025-12-19 12:03:11.641914: Epoch time: 138.02 s\n",
      "2025-12-19 12:03:11.641914: Yayy! New best EMA pseudo Dice: 0.9106\n",
      "2025-12-19 12:03:12.604124: \n",
      "2025-12-19 12:03:12.604124: Epoch 78\n",
      "2025-12-19 12:03:12.604124: Current learning rate: 0.0093\n",
      "2025-12-19 12:05:30.654915: train_loss -0.7936\n",
      "2025-12-19 12:05:30.654915: val_loss -0.8115\n",
      "2025-12-19 12:05:30.654915: Pseudo dice [0.8979, 0.9367, 0.9049]\n",
      "2025-12-19 12:05:30.662414: Epoch time: 138.05 s\n",
      "2025-12-19 12:05:30.662414: Yayy! New best EMA pseudo Dice: 0.9109\n",
      "2025-12-19 12:05:31.583124: \n",
      "2025-12-19 12:05:31.583124: Epoch 79\n",
      "2025-12-19 12:05:31.583124: Current learning rate: 0.00929\n",
      "2025-12-19 12:07:49.569417: train_loss -0.7987\n",
      "2025-12-19 12:07:49.571419: val_loss -0.8155\n",
      "2025-12-19 12:07:49.573421: Pseudo dice [0.9007, 0.9357, 0.912]\n",
      "2025-12-19 12:07:49.575423: Epoch time: 137.99 s\n",
      "2025-12-19 12:07:49.575423: Yayy! New best EMA pseudo Dice: 0.9114\n",
      "2025-12-19 12:07:50.490072: \n",
      "2025-12-19 12:07:50.490072: Epoch 80\n",
      "2025-12-19 12:07:50.490072: Current learning rate: 0.00928\n",
      "2025-12-19 12:10:09.031361: train_loss -0.7959\n",
      "2025-12-19 12:10:09.031361: val_loss -0.8301\n",
      "2025-12-19 12:10:09.032864: Pseudo dice [0.9114, 0.9507, 0.9055]\n",
      "2025-12-19 12:10:09.032864: Epoch time: 138.54 s\n",
      "2025-12-19 12:10:09.036426: Yayy! New best EMA pseudo Dice: 0.9125\n",
      "2025-12-19 12:10:09.953990: \n",
      "2025-12-19 12:10:09.953990: Epoch 81\n",
      "2025-12-19 12:10:09.953990: Current learning rate: 0.00927\n",
      "2025-12-19 12:12:27.876611: train_loss -0.7947\n",
      "2025-12-19 12:12:27.876611: val_loss -0.8351\n",
      "2025-12-19 12:12:27.878613: Pseudo dice [0.9121, 0.9452, 0.9147]\n",
      "2025-12-19 12:12:27.882622: Epoch time: 137.92 s\n",
      "2025-12-19 12:12:27.884625: Yayy! New best EMA pseudo Dice: 0.9137\n",
      "2025-12-19 12:12:28.792267: \n",
      "2025-12-19 12:12:28.792267: Epoch 82\n",
      "2025-12-19 12:12:28.792267: Current learning rate: 0.00926\n",
      "2025-12-19 12:14:46.937041: train_loss -0.7976\n",
      "2025-12-19 12:14:46.937041: val_loss -0.8243\n",
      "2025-12-19 12:14:46.941045: Pseudo dice [0.9091, 0.9453, 0.9061]\n",
      "2025-12-19 12:14:46.941045: Epoch time: 138.14 s\n",
      "2025-12-19 12:14:46.943047: Yayy! New best EMA pseudo Dice: 0.9143\n",
      "2025-12-19 12:14:47.986658: \n",
      "2025-12-19 12:14:47.986658: Epoch 83\n",
      "2025-12-19 12:14:47.986658: Current learning rate: 0.00925\n",
      "2025-12-19 12:17:05.939307: train_loss -0.7954\n",
      "2025-12-19 12:17:05.939307: val_loss -0.8237\n",
      "2025-12-19 12:17:05.943050: Pseudo dice [0.9022, 0.9425, 0.9123]\n",
      "2025-12-19 12:17:05.945052: Epoch time: 137.95 s\n",
      "2025-12-19 12:17:05.945052: Yayy! New best EMA pseudo Dice: 0.9148\n",
      "2025-12-19 12:17:06.829580: \n",
      "2025-12-19 12:17:06.829580: Epoch 84\n",
      "2025-12-19 12:17:06.829580: Current learning rate: 0.00924\n",
      "2025-12-19 12:19:24.710717: train_loss -0.7913\n",
      "2025-12-19 12:19:24.712719: val_loss -0.821\n",
      "2025-12-19 12:19:24.714721: Pseudo dice [0.8978, 0.9395, 0.9156]\n",
      "2025-12-19 12:19:24.714721: Epoch time: 137.88 s\n",
      "2025-12-19 12:19:24.716723: Yayy! New best EMA pseudo Dice: 0.9151\n",
      "2025-12-19 12:19:25.594388: \n",
      "2025-12-19 12:19:25.594388: Epoch 85\n",
      "2025-12-19 12:19:25.596394: Current learning rate: 0.00923\n",
      "2025-12-19 12:21:43.662708: train_loss -0.7906\n",
      "2025-12-19 12:21:43.662708: val_loss -0.8163\n",
      "2025-12-19 12:21:43.664711: Pseudo dice [0.8953, 0.9383, 0.9144]\n",
      "2025-12-19 12:21:43.664711: Epoch time: 138.07 s\n",
      "2025-12-19 12:21:43.666714: Yayy! New best EMA pseudo Dice: 0.9152\n",
      "2025-12-19 12:21:44.555436: \n",
      "2025-12-19 12:21:44.555436: Epoch 86\n",
      "2025-12-19 12:21:44.555436: Current learning rate: 0.00922\n",
      "2025-12-19 12:24:02.695847: train_loss -0.8009\n",
      "2025-12-19 12:24:02.695847: val_loss -0.8268\n",
      "2025-12-19 12:24:02.695847: Pseudo dice [0.9047, 0.9438, 0.9081]\n",
      "2025-12-19 12:24:02.695847: Epoch time: 138.14 s\n",
      "2025-12-19 12:24:02.695847: Yayy! New best EMA pseudo Dice: 0.9155\n",
      "2025-12-19 12:24:03.568253: \n",
      "2025-12-19 12:24:03.568253: Epoch 87\n",
      "2025-12-19 12:24:03.568253: Current learning rate: 0.00921\n",
      "2025-12-19 12:26:21.484599: train_loss -0.7907\n",
      "2025-12-19 12:26:21.484599: val_loss -0.826\n",
      "2025-12-19 12:26:21.484599: Pseudo dice [0.903, 0.9386, 0.9175]\n",
      "2025-12-19 12:26:21.489566: Epoch time: 137.92 s\n",
      "2025-12-19 12:26:21.491568: Yayy! New best EMA pseudo Dice: 0.9159\n",
      "2025-12-19 12:26:22.371326: \n",
      "2025-12-19 12:26:22.371326: Epoch 88\n",
      "2025-12-19 12:26:22.371326: Current learning rate: 0.0092\n",
      "2025-12-19 12:28:40.464561: train_loss -0.7962\n",
      "2025-12-19 12:28:40.464561: val_loss -0.8228\n",
      "2025-12-19 12:28:40.464561: Pseudo dice [0.9002, 0.9379, 0.9195]\n",
      "2025-12-19 12:28:40.464561: Epoch time: 138.09 s\n",
      "2025-12-19 12:28:40.464561: Yayy! New best EMA pseudo Dice: 0.9163\n",
      "2025-12-19 12:28:41.510679: \n",
      "2025-12-19 12:28:41.510679: Epoch 89\n",
      "2025-12-19 12:28:41.510679: Current learning rate: 0.0092\n",
      "2025-12-19 12:30:59.590899: train_loss -0.799\n",
      "2025-12-19 12:30:59.590899: val_loss -0.8323\n",
      "2025-12-19 12:30:59.590899: Pseudo dice [0.9071, 0.9402, 0.9166]\n",
      "2025-12-19 12:30:59.590899: Epoch time: 138.08 s\n",
      "2025-12-19 12:30:59.590899: Yayy! New best EMA pseudo Dice: 0.9168\n",
      "2025-12-19 12:31:00.478426: \n",
      "2025-12-19 12:31:00.478426: Epoch 90\n",
      "2025-12-19 12:31:00.494324: Current learning rate: 0.00919\n",
      "2025-12-19 12:33:18.447822: train_loss -0.7936\n",
      "2025-12-19 12:33:18.447822: val_loss -0.829\n",
      "2025-12-19 12:33:18.447822: Pseudo dice [0.9057, 0.948, 0.9104]\n",
      "2025-12-19 12:33:18.463514: Epoch time: 137.97 s\n",
      "2025-12-19 12:33:18.463514: Yayy! New best EMA pseudo Dice: 0.9172\n",
      "2025-12-19 12:33:19.318453: \n",
      "2025-12-19 12:33:19.318453: Epoch 91\n",
      "2025-12-19 12:33:19.334353: Current learning rate: 0.00918\n",
      "2025-12-19 12:35:37.310668: train_loss -0.7955\n",
      "2025-12-19 12:35:37.310668: val_loss -0.8239\n",
      "2025-12-19 12:35:37.326447: Pseudo dice [0.9068, 0.9413, 0.9066]\n",
      "2025-12-19 12:35:37.326447: Epoch time: 137.99 s\n",
      "2025-12-19 12:35:37.326447: Yayy! New best EMA pseudo Dice: 0.9173\n",
      "2025-12-19 12:35:38.184470: \n",
      "2025-12-19 12:35:38.184470: Epoch 92\n",
      "2025-12-19 12:35:38.184470: Current learning rate: 0.00917\n",
      "2025-12-19 12:37:56.242182: train_loss -0.7556\n",
      "2025-12-19 12:37:56.244184: val_loss -0.7973\n",
      "2025-12-19 12:37:56.246187: Pseudo dice [0.8952, 0.9316, 0.9003]\n",
      "2025-12-19 12:37:56.248188: Epoch time: 138.06 s\n",
      "2025-12-19 12:37:56.861599: \n",
      "2025-12-19 12:37:56.861599: Epoch 93\n",
      "2025-12-19 12:37:56.861599: Current learning rate: 0.00916\n",
      "2025-12-19 12:40:14.864883: train_loss -0.7675\n",
      "2025-12-19 12:40:14.866887: val_loss -0.8162\n",
      "2025-12-19 12:40:14.870895: Pseudo dice [0.8992, 0.9445, 0.9111]\n",
      "2025-12-19 12:40:14.872637: Epoch time: 138.0 s\n",
      "2025-12-19 12:40:15.488933: \n",
      "2025-12-19 12:40:15.488933: Epoch 94\n",
      "2025-12-19 12:40:15.488933: Current learning rate: 0.00915\n",
      "2025-12-19 12:42:33.521314: train_loss -0.7797\n",
      "2025-12-19 12:42:33.521314: val_loss -0.8255\n",
      "2025-12-19 12:42:33.523054: Pseudo dice [0.9077, 0.9411, 0.9101]\n",
      "2025-12-19 12:42:33.525766: Epoch time: 138.05 s\n",
      "2025-12-19 12:42:34.297764: \n",
      "2025-12-19 12:42:34.297764: Epoch 95\n",
      "2025-12-19 12:42:34.297764: Current learning rate: 0.00914\n",
      "2025-12-19 12:44:52.337843: train_loss -0.7873\n",
      "2025-12-19 12:44:52.337843: val_loss -0.8143\n",
      "2025-12-19 12:44:52.337843: Pseudo dice [0.8961, 0.9346, 0.9082]\n",
      "2025-12-19 12:44:52.337843: Epoch time: 138.04 s\n",
      "2025-12-19 12:44:52.986389: \n",
      "2025-12-19 12:44:52.986389: Epoch 96\n",
      "2025-12-19 12:44:53.002127: Current learning rate: 0.00913\n",
      "2025-12-19 12:47:10.908771: train_loss -0.7922\n",
      "2025-12-19 12:47:10.908771: val_loss -0.8251\n",
      "2025-12-19 12:47:10.908771: Pseudo dice [0.9071, 0.9436, 0.9116]\n",
      "2025-12-19 12:47:10.920504: Epoch time: 137.92 s\n",
      "2025-12-19 12:47:11.537499: \n",
      "2025-12-19 12:47:11.537499: Epoch 97\n",
      "2025-12-19 12:47:11.537499: Current learning rate: 0.00912\n",
      "2025-12-19 12:49:29.572873: train_loss -0.7879\n",
      "2025-12-19 12:49:29.572873: val_loss -0.8276\n",
      "2025-12-19 12:49:29.574878: Pseudo dice [0.9076, 0.9427, 0.9137]\n",
      "2025-12-19 12:49:29.576880: Epoch time: 138.04 s\n",
      "2025-12-19 12:49:29.578882: Yayy! New best EMA pseudo Dice: 0.9174\n",
      "2025-12-19 12:49:30.428632: \n",
      "2025-12-19 12:49:30.428632: Epoch 98\n",
      "2025-12-19 12:49:30.428632: Current learning rate: 0.00911\n",
      "2025-12-19 12:51:48.407753: train_loss -0.7953\n",
      "2025-12-19 12:51:48.407753: val_loss -0.8267\n",
      "2025-12-19 12:51:48.407753: Pseudo dice [0.9072, 0.9423, 0.9168]\n",
      "2025-12-19 12:51:48.407753: Epoch time: 137.98 s\n",
      "2025-12-19 12:51:48.407753: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2025-12-19 12:51:49.453039: \n",
      "2025-12-19 12:51:49.453039: Epoch 99\n",
      "2025-12-19 12:51:49.453039: Current learning rate: 0.0091\n",
      "2025-12-19 12:54:07.507227: train_loss -0.7965\n",
      "2025-12-19 12:54:07.507227: val_loss -0.8368\n",
      "2025-12-19 12:54:07.522969: Pseudo dice [0.9078, 0.9513, 0.9139]\n",
      "2025-12-19 12:54:07.522969: Epoch time: 138.05 s\n",
      "2025-12-19 12:54:07.756096: Yayy! New best EMA pseudo Dice: 0.9185\n",
      "2025-12-19 12:54:08.622706: \n",
      "2025-12-19 12:54:08.622706: Epoch 100\n",
      "2025-12-19 12:54:08.622706: Current learning rate: 0.0091\n",
      "2025-12-19 12:56:26.464505: train_loss -0.802\n",
      "2025-12-19 12:56:26.464505: val_loss -0.8325\n",
      "2025-12-19 12:56:26.466923: Pseudo dice [0.902, 0.94, 0.9187]\n",
      "2025-12-19 12:56:26.466923: Epoch time: 137.84 s\n",
      "2025-12-19 12:56:26.466923: Yayy! New best EMA pseudo Dice: 0.9187\n",
      "2025-12-19 12:56:27.515456: \n",
      "2025-12-19 12:56:27.515456: Epoch 101\n",
      "2025-12-19 12:56:27.515456: Current learning rate: 0.00909\n",
      "2025-12-19 12:58:45.425294: train_loss -0.8055\n",
      "2025-12-19 12:58:45.425294: val_loss -0.831\n",
      "2025-12-19 12:58:45.441340: Pseudo dice [0.9067, 0.9444, 0.9196]\n",
      "2025-12-19 12:58:45.441340: Epoch time: 137.91 s\n",
      "2025-12-19 12:58:45.441340: Yayy! New best EMA pseudo Dice: 0.9192\n",
      "2025-12-19 12:58:46.437643: \n",
      "2025-12-19 12:58:46.437643: Epoch 102\n",
      "2025-12-19 12:58:46.437643: Current learning rate: 0.00908\n",
      "2025-12-19 13:01:04.605343: train_loss -0.8088\n",
      "2025-12-19 13:01:04.605343: val_loss -0.8395\n",
      "2025-12-19 13:01:04.607800: Pseudo dice [0.9131, 0.9489, 0.9162]\n",
      "2025-12-19 13:01:04.607800: Epoch time: 138.17 s\n",
      "2025-12-19 13:01:04.607800: Yayy! New best EMA pseudo Dice: 0.9199\n",
      "2025-12-19 13:01:05.482641: \n",
      "2025-12-19 13:01:05.482641: Epoch 103\n",
      "2025-12-19 13:01:05.482641: Current learning rate: 0.00907\n",
      "2025-12-19 13:03:23.376317: train_loss -0.804\n",
      "2025-12-19 13:03:23.376317: val_loss -0.8365\n",
      "2025-12-19 13:03:23.378143: Pseudo dice [0.907, 0.9442, 0.9224]\n",
      "2025-12-19 13:03:23.380145: Epoch time: 137.9 s\n",
      "2025-12-19 13:03:23.382147: Yayy! New best EMA pseudo Dice: 0.9203\n",
      "2025-12-19 13:03:24.256947: \n",
      "2025-12-19 13:03:24.258949: Epoch 104\n",
      "2025-12-19 13:03:24.261050: Current learning rate: 0.00906\n",
      "2025-12-19 13:05:42.515838: train_loss -0.7974\n",
      "2025-12-19 13:05:42.517840: val_loss -0.8263\n",
      "2025-12-19 13:05:42.519580: Pseudo dice [0.9019, 0.9422, 0.9114]\n",
      "2025-12-19 13:05:42.519580: Epoch time: 138.26 s\n",
      "2025-12-19 13:05:43.247122: \n",
      "2025-12-19 13:05:43.247122: Epoch 105\n",
      "2025-12-19 13:05:43.263031: Current learning rate: 0.00905\n",
      "2025-12-19 13:08:01.410586: train_loss -0.8038\n",
      "2025-12-19 13:08:01.410586: val_loss -0.8274\n",
      "2025-12-19 13:08:01.412590: Pseudo dice [0.9014, 0.9395, 0.9162]\n",
      "2025-12-19 13:08:01.414593: Epoch time: 138.16 s\n",
      "2025-12-19 13:08:02.042958: \n",
      "2025-12-19 13:08:02.042958: Epoch 106\n",
      "2025-12-19 13:08:02.042958: Current learning rate: 0.00904\n",
      "2025-12-19 13:10:20.381718: train_loss -0.8087\n",
      "2025-12-19 13:10:20.381718: val_loss -0.8333\n",
      "2025-12-19 13:10:20.385724: Pseudo dice [0.9087, 0.94, 0.9178]\n",
      "2025-12-19 13:10:20.385724: Epoch time: 138.34 s\n",
      "2025-12-19 13:10:20.997411: \n",
      "2025-12-19 13:10:20.997411: Epoch 107\n",
      "2025-12-19 13:10:20.997411: Current learning rate: 0.00903\n",
      "2025-12-19 13:12:39.014054: train_loss -0.802\n",
      "2025-12-19 13:12:39.016056: val_loss -0.8224\n",
      "2025-12-19 13:12:39.018058: Pseudo dice [0.8976, 0.9356, 0.9233]\n",
      "2025-12-19 13:12:39.018058: Epoch time: 138.02 s\n",
      "2025-12-19 13:12:39.823996: \n",
      "2025-12-19 13:12:39.823996: Epoch 108\n",
      "2025-12-19 13:12:39.823996: Current learning rate: 0.00902\n",
      "2025-12-19 13:14:57.812978: train_loss -0.8048\n",
      "2025-12-19 13:14:57.812978: val_loss -0.8348\n",
      "2025-12-19 13:14:57.812978: Pseudo dice [0.9053, 0.9476, 0.9202]\n",
      "2025-12-19 13:14:57.812978: Epoch time: 137.99 s\n",
      "2025-12-19 13:14:57.812978: Yayy! New best EMA pseudo Dice: 0.9205\n",
      "2025-12-19 13:14:58.676285: \n",
      "2025-12-19 13:14:58.676285: Epoch 109\n",
      "2025-12-19 13:14:58.676285: Current learning rate: 0.00901\n",
      "2025-12-19 13:17:16.812282: train_loss -0.8048\n",
      "2025-12-19 13:17:16.812282: val_loss -0.8359\n",
      "2025-12-19 13:17:16.814717: Pseudo dice [0.9099, 0.9426, 0.9152]\n",
      "2025-12-19 13:17:16.814717: Epoch time: 138.14 s\n",
      "2025-12-19 13:17:16.814717: Yayy! New best EMA pseudo Dice: 0.9207\n",
      "2025-12-19 13:17:17.715801: \n",
      "2025-12-19 13:17:17.715801: Epoch 110\n",
      "2025-12-19 13:17:17.715801: Current learning rate: 0.009\n",
      "2025-12-19 13:19:35.555827: train_loss -0.8091\n",
      "2025-12-19 13:19:35.555827: val_loss -0.8454\n",
      "2025-12-19 13:19:35.555827: Pseudo dice [0.9121, 0.9475, 0.9268]\n",
      "2025-12-19 13:19:35.555827: Epoch time: 137.84 s\n",
      "2025-12-19 13:19:35.559841: Yayy! New best EMA pseudo Dice: 0.9215\n",
      "2025-12-19 13:19:36.588792: \n",
      "2025-12-19 13:19:36.588792: Epoch 111\n",
      "2025-12-19 13:19:36.588792: Current learning rate: 0.009\n",
      "2025-12-19 13:21:54.478394: train_loss -0.8059\n",
      "2025-12-19 13:21:54.478394: val_loss -0.8393\n",
      "2025-12-19 13:21:54.478394: Pseudo dice [0.909, 0.9475, 0.9175]\n",
      "2025-12-19 13:21:54.494059: Epoch time: 137.89 s\n",
      "2025-12-19 13:21:54.494059: Yayy! New best EMA pseudo Dice: 0.9219\n",
      "2025-12-19 13:21:55.541912: \n",
      "2025-12-19 13:21:55.541912: Epoch 112\n",
      "2025-12-19 13:21:55.544277: Current learning rate: 0.00899\n",
      "2025-12-19 13:24:13.659502: train_loss -0.809\n",
      "2025-12-19 13:24:13.659502: val_loss -0.8453\n",
      "2025-12-19 13:24:13.664743: Pseudo dice [0.9158, 0.9484, 0.9129]\n",
      "2025-12-19 13:24:13.666746: Epoch time: 138.12 s\n",
      "2025-12-19 13:24:13.668747: Yayy! New best EMA pseudo Dice: 0.9222\n",
      "2025-12-19 13:24:14.558671: \n",
      "2025-12-19 13:24:14.558671: Epoch 113\n",
      "2025-12-19 13:24:14.558671: Current learning rate: 0.00898\n",
      "2025-12-19 13:26:32.911451: train_loss -0.8056\n",
      "2025-12-19 13:26:32.911451: val_loss -0.8312\n",
      "2025-12-19 13:26:32.911451: Pseudo dice [0.9076, 0.9473, 0.9119]\n",
      "2025-12-19 13:26:32.911451: Epoch time: 138.35 s\n",
      "2025-12-19 13:26:32.911451: Yayy! New best EMA pseudo Dice: 0.9222\n",
      "2025-12-19 13:26:33.863852: \n",
      "2025-12-19 13:26:33.863852: Epoch 114\n",
      "2025-12-19 13:26:33.863852: Current learning rate: 0.00897\n",
      "2025-12-19 13:28:51.950580: train_loss -0.8062\n",
      "2025-12-19 13:28:51.950580: val_loss -0.8331\n",
      "2025-12-19 13:28:51.950580: Pseudo dice [0.9091, 0.9511, 0.9064]\n",
      "2025-12-19 13:28:51.950580: Epoch time: 138.09 s\n",
      "2025-12-19 13:28:52.591671: \n",
      "2025-12-19 13:28:52.591671: Epoch 115\n",
      "2025-12-19 13:28:52.593933: Current learning rate: 0.00896\n",
      "2025-12-19 13:31:10.446320: train_loss -0.8114\n",
      "2025-12-19 13:31:10.446320: val_loss -0.8416\n",
      "2025-12-19 13:31:10.452337: Pseudo dice [0.9123, 0.9483, 0.9228]\n",
      "2025-12-19 13:31:10.454343: Epoch time: 137.85 s\n",
      "2025-12-19 13:31:10.456347: Yayy! New best EMA pseudo Dice: 0.9228\n",
      "2025-12-19 13:31:11.355845: \n",
      "2025-12-19 13:31:11.355845: Epoch 116\n",
      "2025-12-19 13:31:11.357587: Current learning rate: 0.00895\n",
      "2025-12-19 13:33:29.341603: train_loss -0.8088\n",
      "2025-12-19 13:33:29.341603: val_loss -0.839\n",
      "2025-12-19 13:33:29.343605: Pseudo dice [0.9066, 0.9429, 0.9277]\n",
      "2025-12-19 13:33:29.345607: Epoch time: 137.99 s\n",
      "2025-12-19 13:33:29.347609: Yayy! New best EMA pseudo Dice: 0.9231\n",
      "2025-12-19 13:33:30.421168: \n",
      "2025-12-19 13:33:30.421168: Epoch 117\n",
      "2025-12-19 13:33:30.425174: Current learning rate: 0.00894\n",
      "2025-12-19 13:35:48.546821: train_loss -0.8082\n",
      "2025-12-19 13:35:48.546821: val_loss -0.8351\n",
      "2025-12-19 13:35:48.552120: Pseudo dice [0.9051, 0.9455, 0.9146]\n",
      "2025-12-19 13:35:48.554122: Epoch time: 138.13 s\n",
      "2025-12-19 13:35:49.207059: \n",
      "2025-12-19 13:35:49.207059: Epoch 118\n",
      "2025-12-19 13:35:49.207059: Current learning rate: 0.00893\n",
      "2025-12-19 13:38:07.172065: train_loss -0.8092\n",
      "2025-12-19 13:38:07.172065: val_loss -0.8289\n",
      "2025-12-19 13:38:07.172065: Pseudo dice [0.9017, 0.9434, 0.9229]\n",
      "2025-12-19 13:38:07.176081: Epoch time: 137.97 s\n",
      "2025-12-19 13:38:07.983996: \n",
      "2025-12-19 13:38:07.983996: Epoch 119\n",
      "2025-12-19 13:38:07.983996: Current learning rate: 0.00892\n",
      "2025-12-19 13:40:25.949046: train_loss -0.8083\n",
      "2025-12-19 13:40:25.949046: val_loss -0.8436\n",
      "2025-12-19 13:40:25.950787: Pseudo dice [0.9113, 0.9492, 0.9296]\n",
      "2025-12-19 13:40:25.954289: Epoch time: 137.97 s\n",
      "2025-12-19 13:40:25.954289: Yayy! New best EMA pseudo Dice: 0.9236\n",
      "2025-12-19 13:40:26.899266: \n",
      "2025-12-19 13:40:26.899266: Epoch 120\n",
      "2025-12-19 13:40:26.899266: Current learning rate: 0.00891\n",
      "2025-12-19 13:42:44.920522: train_loss -0.8078\n",
      "2025-12-19 13:42:44.920522: val_loss -0.8355\n",
      "2025-12-19 13:42:44.924527: Pseudo dice [0.9115, 0.944, 0.9176]\n",
      "2025-12-19 13:42:44.926267: Epoch time: 138.02 s\n",
      "2025-12-19 13:42:44.926267: Yayy! New best EMA pseudo Dice: 0.9237\n",
      "2025-12-19 13:42:45.809353: \n",
      "2025-12-19 13:42:45.809353: Epoch 121\n",
      "2025-12-19 13:42:45.809353: Current learning rate: 0.0089\n",
      "2025-12-19 13:45:03.929479: train_loss -0.8079\n",
      "2025-12-19 13:45:03.929479: val_loss -0.8378\n",
      "2025-12-19 13:45:03.931484: Pseudo dice [0.9095, 0.9448, 0.9146]\n",
      "2025-12-19 13:45:03.933486: Epoch time: 138.12 s\n",
      "2025-12-19 13:45:04.563431: \n",
      "2025-12-19 13:45:04.563431: Epoch 122\n",
      "2025-12-19 13:45:04.563431: Current learning rate: 0.00889\n",
      "2025-12-19 13:47:22.413298: train_loss -0.7759\n",
      "2025-12-19 13:47:22.413298: val_loss -0.7979\n",
      "2025-12-19 13:47:22.417041: Pseudo dice [0.8966, 0.9448, 0.8945]\n",
      "2025-12-19 13:47:22.419043: Epoch time: 137.85 s\n",
      "2025-12-19 13:47:23.221725: \n",
      "2025-12-19 13:47:23.221725: Epoch 123\n",
      "2025-12-19 13:47:23.237479: Current learning rate: 0.00889\n",
      "2025-12-19 13:49:41.276991: train_loss -0.7538\n",
      "2025-12-19 13:49:41.276991: val_loss -0.8142\n",
      "2025-12-19 13:49:41.291098: Pseudo dice [0.9012, 0.9421, 0.9051]\n",
      "2025-12-19 13:49:41.293980: Epoch time: 138.06 s\n",
      "2025-12-19 13:49:41.925590: \n",
      "2025-12-19 13:49:41.925590: Epoch 124\n",
      "2025-12-19 13:49:41.925590: Current learning rate: 0.00888\n",
      "2025-12-19 13:51:59.834448: train_loss -0.7835\n",
      "2025-12-19 13:51:59.834448: val_loss -0.8205\n",
      "2025-12-19 13:51:59.837175: Pseudo dice [0.9069, 0.9417, 0.905]\n",
      "2025-12-19 13:51:59.839180: Epoch time: 137.91 s\n",
      "2025-12-19 13:52:00.639831: \n",
      "2025-12-19 13:52:00.641834: Epoch 125\n",
      "2025-12-19 13:52:00.641834: Current learning rate: 0.00887\n",
      "2025-12-19 13:54:18.560203: train_loss -0.797\n",
      "2025-12-19 13:54:18.560203: val_loss -0.8312\n",
      "2025-12-19 13:54:18.561944: Pseudo dice [0.9069, 0.9463, 0.9164]\n",
      "2025-12-19 13:54:18.563810: Epoch time: 137.92 s\n",
      "2025-12-19 13:54:19.352222: \n",
      "2025-12-19 13:54:19.352222: Epoch 126\n",
      "2025-12-19 13:54:19.352222: Current learning rate: 0.00886\n",
      "2025-12-19 13:56:37.228467: train_loss -0.8053\n",
      "2025-12-19 13:56:37.228467: val_loss -0.8401\n",
      "2025-12-19 13:56:37.230469: Pseudo dice [0.9121, 0.9478, 0.9196]\n",
      "2025-12-19 13:56:37.230469: Epoch time: 137.89 s\n",
      "2025-12-19 13:56:37.887721: \n",
      "2025-12-19 13:56:37.887721: Epoch 127\n",
      "2025-12-19 13:56:37.887721: Current learning rate: 0.00885\n",
      "2025-12-19 13:58:55.742624: train_loss -0.8063\n",
      "2025-12-19 13:58:55.742624: val_loss -0.8304\n",
      "2025-12-19 13:58:55.758383: Pseudo dice [0.9048, 0.9451, 0.9046]\n",
      "2025-12-19 13:58:55.760241: Epoch time: 137.86 s\n",
      "2025-12-19 13:58:56.427045: \n",
      "2025-12-19 13:58:56.427045: Epoch 128\n",
      "2025-12-19 13:58:56.427045: Current learning rate: 0.00884\n",
      "2025-12-19 14:01:14.457136: train_loss -0.7908\n",
      "2025-12-19 14:01:14.457136: val_loss -0.8289\n",
      "2025-12-19 14:01:14.462563: Pseudo dice [0.9088, 0.9449, 0.9097]\n",
      "2025-12-19 14:01:14.464565: Epoch time: 138.03 s\n",
      "2025-12-19 14:01:15.264784: \n",
      "2025-12-19 14:01:15.264784: Epoch 129\n",
      "2025-12-19 14:01:15.264784: Current learning rate: 0.00883\n",
      "2025-12-19 14:03:33.199512: train_loss -0.7824\n",
      "2025-12-19 14:03:33.199512: val_loss -0.8228\n",
      "2025-12-19 14:03:33.199512: Pseudo dice [0.9086, 0.9439, 0.9108]\n",
      "2025-12-19 14:03:33.199512: Epoch time: 137.93 s\n",
      "2025-12-19 14:03:33.848496: \n",
      "2025-12-19 14:03:33.848496: Epoch 130\n",
      "2025-12-19 14:03:33.848496: Current learning rate: 0.00882\n",
      "2025-12-19 14:05:51.681030: train_loss -0.7997\n",
      "2025-12-19 14:05:51.682770: val_loss -0.8287\n",
      "2025-12-19 14:05:51.686774: Pseudo dice [0.9077, 0.9459, 0.9087]\n",
      "2025-12-19 14:05:51.686774: Epoch time: 137.83 s\n",
      "2025-12-19 14:05:52.491745: \n",
      "2025-12-19 14:05:52.491745: Epoch 131\n",
      "2025-12-19 14:05:52.491745: Current learning rate: 0.00881\n",
      "2025-12-19 14:08:10.400286: train_loss -0.7955\n",
      "2025-12-19 14:08:10.400286: val_loss -0.8348\n",
      "2025-12-19 14:08:10.400286: Pseudo dice [0.9045, 0.9412, 0.9254]\n",
      "2025-12-19 14:08:10.400286: Epoch time: 137.91 s\n",
      "2025-12-19 14:08:11.036225: \n",
      "2025-12-19 14:08:11.036225: Epoch 132\n",
      "2025-12-19 14:08:11.036225: Current learning rate: 0.0088\n",
      "2025-12-19 14:10:29.622644: train_loss -0.7955\n",
      "2025-12-19 14:10:29.622644: val_loss -0.8358\n",
      "2025-12-19 14:10:29.625648: Pseudo dice [0.907, 0.9497, 0.9211]\n",
      "2025-12-19 14:10:29.625648: Epoch time: 138.59 s\n",
      "2025-12-19 14:10:30.257218: \n",
      "2025-12-19 14:10:30.257218: Epoch 133\n",
      "2025-12-19 14:10:30.257218: Current learning rate: 0.00879\n",
      "2025-12-19 14:12:48.240630: train_loss -0.8065\n",
      "2025-12-19 14:12:48.240630: val_loss -0.8333\n",
      "2025-12-19 14:12:48.240630: Pseudo dice [0.9062, 0.9398, 0.926]\n",
      "2025-12-19 14:12:48.240630: Epoch time: 137.98 s\n",
      "2025-12-19 14:12:48.870052: \n",
      "2025-12-19 14:12:48.870052: Epoch 134\n",
      "2025-12-19 14:12:48.870052: Current learning rate: 0.00879\n",
      "2025-12-19 14:15:06.966439: train_loss -0.8099\n",
      "2025-12-19 14:15:06.968441: val_loss -0.8437\n",
      "2025-12-19 14:15:06.970443: Pseudo dice [0.9107, 0.9479, 0.9208]\n",
      "2025-12-19 14:15:06.972445: Epoch time: 138.1 s\n",
      "2025-12-19 14:15:07.607327: \n",
      "2025-12-19 14:15:07.607327: Epoch 135\n",
      "2025-12-19 14:15:07.607327: Current learning rate: 0.00878\n",
      "2025-12-19 14:17:25.555700: train_loss -0.8113\n",
      "2025-12-19 14:17:25.555700: val_loss -0.8373\n",
      "2025-12-19 14:17:25.559704: Pseudo dice [0.9096, 0.9476, 0.9144]\n",
      "2025-12-19 14:17:25.561444: Epoch time: 137.95 s\n",
      "2025-12-19 14:17:26.353680: \n",
      "2025-12-19 14:17:26.353680: Epoch 136\n",
      "2025-12-19 14:17:26.367699: Current learning rate: 0.00877\n",
      "2025-12-19 14:19:44.136742: train_loss -0.8128\n",
      "2025-12-19 14:19:44.136742: val_loss -0.8401\n",
      "2025-12-19 14:19:44.138743: Pseudo dice [0.9088, 0.9481, 0.9215]\n",
      "2025-12-19 14:19:44.141014: Epoch time: 137.78 s\n",
      "2025-12-19 14:19:44.778677: \n",
      "2025-12-19 14:19:44.778677: Epoch 137\n",
      "2025-12-19 14:19:44.778677: Current learning rate: 0.00876\n",
      "2025-12-19 14:22:03.012286: train_loss -0.8087\n",
      "2025-12-19 14:22:03.012286: val_loss -0.8439\n",
      "2025-12-19 14:22:03.014289: Pseudo dice [0.9185, 0.9476, 0.9201]\n",
      "2025-12-19 14:22:03.016291: Epoch time: 138.25 s\n",
      "2025-12-19 14:22:03.016291: Yayy! New best EMA pseudo Dice: 0.9237\n",
      "2025-12-19 14:22:03.902949: \n",
      "2025-12-19 14:22:03.902949: Epoch 138\n",
      "2025-12-19 14:22:03.902949: Current learning rate: 0.00875\n",
      "2025-12-19 14:24:21.962267: train_loss -0.8103\n",
      "2025-12-19 14:24:21.962267: val_loss -0.837\n",
      "2025-12-19 14:24:21.964269: Pseudo dice [0.9095, 0.9427, 0.9156]\n",
      "2025-12-19 14:24:21.966271: Epoch time: 138.06 s\n",
      "2025-12-19 14:24:22.619483: \n",
      "2025-12-19 14:24:22.621486: Epoch 139\n",
      "2025-12-19 14:24:22.621486: Current learning rate: 0.00874\n",
      "2025-12-19 14:26:40.523934: train_loss -0.8171\n",
      "2025-12-19 14:26:40.525937: val_loss -0.8396\n",
      "2025-12-19 14:26:40.525937: Pseudo dice [0.9099, 0.944, 0.9234]\n",
      "2025-12-19 14:26:40.529329: Epoch time: 137.9 s\n",
      "2025-12-19 14:26:40.531332: Yayy! New best EMA pseudo Dice: 0.9238\n",
      "2025-12-19 14:26:41.449200: \n",
      "2025-12-19 14:26:41.449200: Epoch 140\n",
      "2025-12-19 14:26:41.449200: Current learning rate: 0.00873\n",
      "2025-12-19 14:28:59.384461: train_loss -0.8181\n",
      "2025-12-19 14:28:59.386463: val_loss -0.8547\n",
      "2025-12-19 14:28:59.388465: Pseudo dice [0.9234, 0.9566, 0.9163]\n",
      "2025-12-19 14:28:59.388966: Epoch time: 137.94 s\n",
      "2025-12-19 14:28:59.390968: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2025-12-19 14:29:00.315346: \n",
      "2025-12-19 14:29:00.315346: Epoch 141\n",
      "2025-12-19 14:29:00.315346: Current learning rate: 0.00872\n",
      "2025-12-19 14:31:18.193768: train_loss -0.8172\n",
      "2025-12-19 14:31:18.193768: val_loss -0.8491\n",
      "2025-12-19 14:31:18.193768: Pseudo dice [0.9205, 0.9496, 0.9135]\n",
      "2025-12-19 14:31:18.205162: Epoch time: 137.88 s\n",
      "2025-12-19 14:31:18.207165: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2025-12-19 14:31:19.277271: \n",
      "2025-12-19 14:31:19.277271: Epoch 142\n",
      "2025-12-19 14:31:19.277271: Current learning rate: 0.00871\n",
      "2025-12-19 14:33:37.087610: train_loss -0.812\n",
      "2025-12-19 14:33:37.089613: val_loss -0.8465\n",
      "2025-12-19 14:33:37.093618: Pseudo dice [0.9137, 0.9498, 0.9209]\n",
      "2025-12-19 14:33:37.095621: Epoch time: 137.81 s\n",
      "2025-12-19 14:33:37.097361: Yayy! New best EMA pseudo Dice: 0.9253\n",
      "2025-12-19 14:33:38.007980: \n",
      "2025-12-19 14:33:38.007980: Epoch 143\n",
      "2025-12-19 14:33:38.007980: Current learning rate: 0.0087\n",
      "2025-12-19 14:35:55.952112: train_loss -0.8173\n",
      "2025-12-19 14:35:55.954114: val_loss -0.8501\n",
      "2025-12-19 14:35:55.956116: Pseudo dice [0.9165, 0.9492, 0.9308]\n",
      "2025-12-19 14:35:55.956116: Epoch time: 137.95 s\n",
      "2025-12-19 14:35:55.958118: Yayy! New best EMA pseudo Dice: 0.926\n",
      "2025-12-19 14:35:56.847980: \n",
      "2025-12-19 14:35:56.847980: Epoch 144\n",
      "2025-12-19 14:35:56.847980: Current learning rate: 0.00869\n",
      "2025-12-19 14:38:14.814718: train_loss -0.8173\n",
      "2025-12-19 14:38:14.814718: val_loss -0.8566\n",
      "2025-12-19 14:38:14.816720: Pseudo dice [0.9206, 0.9543, 0.9242]\n",
      "2025-12-19 14:38:14.818722: Epoch time: 137.97 s\n",
      "2025-12-19 14:38:14.820724: Yayy! New best EMA pseudo Dice: 0.9267\n",
      "2025-12-19 14:38:15.721376: \n",
      "2025-12-19 14:38:15.721376: Epoch 145\n",
      "2025-12-19 14:38:15.721376: Current learning rate: 0.00868\n",
      "2025-12-19 14:40:33.725716: train_loss -0.8155\n",
      "2025-12-19 14:40:33.725716: val_loss -0.8431\n",
      "2025-12-19 14:40:33.728166: Pseudo dice [0.9101, 0.9462, 0.9204]\n",
      "2025-12-19 14:40:33.728166: Epoch time: 138.01 s\n",
      "2025-12-19 14:40:34.380521: \n",
      "2025-12-19 14:40:34.380521: Epoch 146\n",
      "2025-12-19 14:40:34.383148: Current learning rate: 0.00868\n",
      "2025-12-19 14:42:52.318859: train_loss -0.8224\n",
      "2025-12-19 14:42:52.318859: val_loss -0.849\n",
      "2025-12-19 14:42:52.318859: Pseudo dice [0.9127, 0.9553, 0.9207]\n",
      "2025-12-19 14:42:52.318859: Epoch time: 137.94 s\n",
      "2025-12-19 14:42:52.318859: Yayy! New best EMA pseudo Dice: 0.9269\n",
      "2025-12-19 14:42:53.241485: \n",
      "2025-12-19 14:42:53.241485: Epoch 147\n",
      "2025-12-19 14:42:53.241485: Current learning rate: 0.00867\n",
      "2025-12-19 14:45:11.254609: train_loss -0.8263\n",
      "2025-12-19 14:45:11.254609: val_loss -0.8476\n",
      "2025-12-19 14:45:11.258615: Pseudo dice [0.9108, 0.9516, 0.9299]\n",
      "2025-12-19 14:45:11.260618: Epoch time: 138.02 s\n",
      "2025-12-19 14:45:11.260618: Yayy! New best EMA pseudo Dice: 0.9273\n",
      "2025-12-19 14:45:12.334789: \n",
      "2025-12-19 14:45:12.334789: Epoch 148\n",
      "2025-12-19 14:45:12.334789: Current learning rate: 0.00866\n",
      "2025-12-19 14:47:30.133010: train_loss -0.8238\n",
      "2025-12-19 14:47:30.133010: val_loss -0.8514\n",
      "2025-12-19 14:47:30.148772: Pseudo dice [0.9179, 0.9525, 0.9245]\n",
      "2025-12-19 14:47:30.148772: Epoch time: 137.8 s\n",
      "2025-12-19 14:47:30.152271: Yayy! New best EMA pseudo Dice: 0.9277\n",
      "2025-12-19 14:47:31.071116: \n",
      "2025-12-19 14:47:31.071116: Epoch 149\n",
      "2025-12-19 14:47:31.071116: Current learning rate: 0.00865\n",
      "2025-12-19 14:49:49.005927: train_loss -0.8172\n",
      "2025-12-19 14:49:49.007929: val_loss -0.8561\n",
      "2025-12-19 14:49:49.009932: Pseudo dice [0.9183, 0.9559, 0.9222]\n",
      "2025-12-19 14:49:49.009932: Epoch time: 137.94 s\n",
      "2025-12-19 14:49:49.253182: Yayy! New best EMA pseudo Dice: 0.9282\n",
      "2025-12-19 14:49:50.157584: \n",
      "2025-12-19 14:49:50.157584: Epoch 150\n",
      "2025-12-19 14:49:50.160186: Current learning rate: 0.00864\n",
      "2025-12-19 14:52:08.241251: train_loss -0.8192\n",
      "2025-12-19 14:52:08.241251: val_loss -0.8484\n",
      "2025-12-19 14:52:08.243253: Pseudo dice [0.918, 0.9546, 0.912]\n",
      "2025-12-19 14:52:08.245256: Epoch time: 138.09 s\n",
      "2025-12-19 14:52:08.247257: Yayy! New best EMA pseudo Dice: 0.9282\n",
      "2025-12-19 14:52:09.162727: \n",
      "2025-12-19 14:52:09.162727: Epoch 151\n",
      "2025-12-19 14:52:09.162727: Current learning rate: 0.00863\n",
      "2025-12-19 14:54:27.120557: train_loss -0.8191\n",
      "2025-12-19 14:54:27.120557: val_loss -0.846\n",
      "2025-12-19 14:54:27.120557: Pseudo dice [0.9113, 0.9488, 0.9248]\n",
      "2025-12-19 14:54:27.120557: Epoch time: 137.96 s\n",
      "2025-12-19 14:54:27.120557: Yayy! New best EMA pseudo Dice: 0.9282\n",
      "2025-12-19 14:54:28.072433: \n",
      "2025-12-19 14:54:28.072433: Epoch 152\n",
      "2025-12-19 14:54:28.072433: Current learning rate: 0.00862\n",
      "2025-12-19 14:56:45.890517: train_loss -0.8152\n",
      "2025-12-19 14:56:45.890517: val_loss -0.8452\n",
      "2025-12-19 14:56:45.906458: Pseudo dice [0.908, 0.9473, 0.9226]\n",
      "2025-12-19 14:56:45.906458: Epoch time: 137.82 s\n",
      "2025-12-19 14:56:46.549075: \n",
      "2025-12-19 14:56:46.551078: Epoch 153\n",
      "2025-12-19 14:56:46.551078: Current learning rate: 0.00861\n",
      "2025-12-19 14:59:04.371213: train_loss -0.8195\n",
      "2025-12-19 14:59:04.373217: val_loss -0.8487\n",
      "2025-12-19 14:59:04.375219: Pseudo dice [0.9136, 0.9507, 0.9232]\n",
      "2025-12-19 14:59:04.377222: Epoch time: 137.82 s\n",
      "2025-12-19 14:59:05.207849: \n",
      "2025-12-19 14:59:05.207849: Epoch 154\n",
      "2025-12-19 14:59:05.207849: Current learning rate: 0.0086\n",
      "2025-12-19 15:01:23.146241: train_loss -0.8174\n",
      "2025-12-19 15:01:23.148245: val_loss -0.8517\n",
      "2025-12-19 15:01:23.150248: Pseudo dice [0.9174, 0.9534, 0.922]\n",
      "2025-12-19 15:01:23.152251: Epoch time: 137.94 s\n",
      "2025-12-19 15:01:23.154253: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2025-12-19 15:01:24.060873: \n",
      "2025-12-19 15:01:24.060873: Epoch 155\n",
      "2025-12-19 15:01:24.060873: Current learning rate: 0.00859\n",
      "2025-12-19 15:03:42.021397: train_loss -0.823\n",
      "2025-12-19 15:03:42.021397: val_loss -0.8517\n",
      "2025-12-19 15:03:42.023400: Pseudo dice [0.917, 0.9504, 0.9214]\n",
      "2025-12-19 15:03:42.025403: Epoch time: 137.96 s\n",
      "2025-12-19 15:03:42.027405: Yayy! New best EMA pseudo Dice: 0.9285\n",
      "2025-12-19 15:03:42.936932: \n",
      "2025-12-19 15:03:42.936932: Epoch 156\n",
      "2025-12-19 15:03:42.936932: Current learning rate: 0.00858\n",
      "2025-12-19 15:06:00.934622: train_loss -0.8141\n",
      "2025-12-19 15:06:00.934622: val_loss -0.8521\n",
      "2025-12-19 15:06:00.938627: Pseudo dice [0.9156, 0.955, 0.9246]\n",
      "2025-12-19 15:06:00.940630: Epoch time: 138.0 s\n",
      "2025-12-19 15:06:00.944635: Yayy! New best EMA pseudo Dice: 0.9288\n",
      "2025-12-19 15:06:01.866977: \n",
      "2025-12-19 15:06:01.866977: Epoch 157\n",
      "2025-12-19 15:06:01.866977: Current learning rate: 0.00858\n",
      "2025-12-19 15:08:19.865929: train_loss -0.8232\n",
      "2025-12-19 15:08:19.865929: val_loss -0.8466\n",
      "2025-12-19 15:08:19.867931: Pseudo dice [0.9125, 0.9489, 0.9211]\n",
      "2025-12-19 15:08:19.869933: Epoch time: 138.0 s\n",
      "2025-12-19 15:08:20.566652: \n",
      "2025-12-19 15:08:20.566652: Epoch 158\n",
      "2025-12-19 15:08:20.566652: Current learning rate: 0.00857\n",
      "2025-12-19 15:10:38.990881: train_loss -0.8148\n",
      "2025-12-19 15:10:38.990881: val_loss -0.8434\n",
      "2025-12-19 15:10:38.990881: Pseudo dice [0.911, 0.9505, 0.9298]\n",
      "2025-12-19 15:10:39.006819: Epoch time: 138.42 s\n",
      "2025-12-19 15:10:39.006819: Yayy! New best EMA pseudo Dice: 0.9288\n",
      "2025-12-19 15:10:40.069041: \n",
      "2025-12-19 15:10:40.069041: Epoch 159\n",
      "2025-12-19 15:10:40.069041: Current learning rate: 0.00856\n",
      "2025-12-19 15:12:57.892669: train_loss -0.8135\n",
      "2025-12-19 15:12:57.892669: val_loss -0.8492\n",
      "2025-12-19 15:12:57.892669: Pseudo dice [0.9151, 0.9489, 0.925]\n",
      "2025-12-19 15:12:57.898843: Epoch time: 137.82 s\n",
      "2025-12-19 15:12:57.898843: Yayy! New best EMA pseudo Dice: 0.9289\n",
      "2025-12-19 15:12:58.818172: \n",
      "2025-12-19 15:12:58.818172: Epoch 160\n",
      "2025-12-19 15:12:58.818172: Current learning rate: 0.00855\n",
      "2025-12-19 15:15:16.810093: train_loss -0.8169\n",
      "2025-12-19 15:15:16.811834: val_loss -0.8488\n",
      "2025-12-19 15:15:16.815840: Pseudo dice [0.9136, 0.9512, 0.9277]\n",
      "2025-12-19 15:15:16.817845: Epoch time: 137.99 s\n",
      "2025-12-19 15:15:16.819850: Yayy! New best EMA pseudo Dice: 0.9291\n",
      "2025-12-19 15:15:17.748706: \n",
      "2025-12-19 15:15:17.750709: Epoch 161\n",
      "2025-12-19 15:15:17.750709: Current learning rate: 0.00854\n",
      "2025-12-19 15:17:35.844561: train_loss -0.8203\n",
      "2025-12-19 15:17:35.844561: val_loss -0.8483\n",
      "2025-12-19 15:17:35.844561: Pseudo dice [0.9135, 0.9452, 0.9273]\n",
      "2025-12-19 15:17:35.857156: Epoch time: 138.1 s\n",
      "2025-12-19 15:17:36.513157: \n",
      "2025-12-19 15:17:36.513157: Epoch 162\n",
      "2025-12-19 15:17:36.513157: Current learning rate: 0.00853\n",
      "2025-12-19 15:19:54.426478: train_loss -0.8168\n",
      "2025-12-19 15:19:54.426478: val_loss -0.8504\n",
      "2025-12-19 15:19:54.426478: Pseudo dice [0.916, 0.9526, 0.9263]\n",
      "2025-12-19 15:19:54.426478: Epoch time: 137.91 s\n",
      "2025-12-19 15:19:54.435000: Yayy! New best EMA pseudo Dice: 0.9293\n",
      "2025-12-19 15:19:55.340764: \n",
      "2025-12-19 15:19:55.340764: Epoch 163\n",
      "2025-12-19 15:19:55.340764: Current learning rate: 0.00852\n",
      "2025-12-19 15:22:13.369323: train_loss -0.8134\n",
      "2025-12-19 15:22:13.369323: val_loss -0.8422\n",
      "2025-12-19 15:22:13.369323: Pseudo dice [0.9065, 0.9521, 0.921]\n",
      "2025-12-19 15:22:13.380826: Epoch time: 138.03 s\n",
      "2025-12-19 15:22:14.033214: \n",
      "2025-12-19 15:22:14.033214: Epoch 164\n",
      "2025-12-19 15:22:14.033214: Current learning rate: 0.00851\n",
      "2025-12-19 15:24:31.991777: train_loss -0.7903\n",
      "2025-12-19 15:24:31.991777: val_loss -0.8228\n",
      "2025-12-19 15:24:31.991777: Pseudo dice [0.9044, 0.9399, 0.9154]\n",
      "2025-12-19 15:24:31.998075: Epoch time: 137.96 s\n",
      "2025-12-19 15:24:32.802408: \n",
      "2025-12-19 15:24:32.802408: Epoch 165\n",
      "2025-12-19 15:24:32.802408: Current learning rate: 0.0085\n",
      "2025-12-19 15:26:50.688813: train_loss -0.7842\n",
      "2025-12-19 15:26:50.688813: val_loss -0.8256\n",
      "2025-12-19 15:26:50.688813: Pseudo dice [0.9067, 0.946, 0.9188]\n",
      "2025-12-19 15:26:50.688813: Epoch time: 137.89 s\n",
      "2025-12-19 15:26:51.318792: \n",
      "2025-12-19 15:26:51.318792: Epoch 166\n",
      "2025-12-19 15:26:51.334673: Current learning rate: 0.00849\n",
      "2025-12-19 15:29:09.234582: train_loss -0.7997\n",
      "2025-12-19 15:29:09.234582: val_loss -0.839\n",
      "2025-12-19 15:29:09.236585: Pseudo dice [0.9089, 0.948, 0.9182]\n",
      "2025-12-19 15:29:09.240596: Epoch time: 137.92 s\n",
      "2025-12-19 15:29:09.956156: \n",
      "2025-12-19 15:29:09.956156: Epoch 167\n",
      "2025-12-19 15:29:09.956156: Current learning rate: 0.00848\n",
      "2025-12-19 15:31:27.885207: train_loss -0.8135\n",
      "2025-12-19 15:31:27.885207: val_loss -0.8369\n",
      "2025-12-19 15:31:27.885207: Pseudo dice [0.9037, 0.9459, 0.9231]\n",
      "2025-12-19 15:31:27.885207: Epoch time: 137.93 s\n",
      "2025-12-19 15:31:28.523290: \n",
      "2025-12-19 15:31:28.523290: Epoch 168\n",
      "2025-12-19 15:31:28.523290: Current learning rate: 0.00847\n",
      "2025-12-19 15:33:46.508228: train_loss -0.8133\n",
      "2025-12-19 15:33:46.510230: val_loss -0.844\n",
      "2025-12-19 15:33:46.510230: Pseudo dice [0.9134, 0.9459, 0.9314]\n",
      "2025-12-19 15:33:46.510230: Epoch time: 137.98 s\n",
      "2025-12-19 15:33:47.147072: \n",
      "2025-12-19 15:33:47.147072: Epoch 169\n",
      "2025-12-19 15:33:47.147072: Current learning rate: 0.00847\n",
      "2025-12-19 15:36:05.100850: train_loss -0.8144\n",
      "2025-12-19 15:36:05.100850: val_loss -0.8478\n",
      "2025-12-19 15:36:05.104345: Pseudo dice [0.9152, 0.9449, 0.9245]\n",
      "2025-12-19 15:36:05.106347: Epoch time: 137.95 s\n",
      "2025-12-19 15:36:05.938154: \n",
      "2025-12-19 15:36:05.938154: Epoch 170\n",
      "2025-12-19 15:36:05.938154: Current learning rate: 0.00846\n",
      "2025-12-19 15:38:23.776038: train_loss -0.8168\n",
      "2025-12-19 15:38:23.776038: val_loss -0.845\n",
      "2025-12-19 15:38:23.778040: Pseudo dice [0.9131, 0.9459, 0.9201]\n",
      "2025-12-19 15:38:23.780042: Epoch time: 137.84 s\n",
      "2025-12-19 15:38:24.592650: \n",
      "2025-12-19 15:38:24.592650: Epoch 171\n",
      "2025-12-19 15:38:24.592650: Current learning rate: 0.00845\n",
      "2025-12-19 15:40:42.380096: train_loss -0.8187\n",
      "2025-12-19 15:40:42.380096: val_loss -0.8345\n",
      "2025-12-19 15:40:42.395899: Pseudo dice [0.9009, 0.9399, 0.93]\n",
      "2025-12-19 15:40:42.395899: Epoch time: 137.79 s\n",
      "2025-12-19 15:40:43.031516: \n",
      "2025-12-19 15:40:43.031516: Epoch 172\n",
      "2025-12-19 15:40:43.047306: Current learning rate: 0.00844\n",
      "2025-12-19 15:43:01.030263: train_loss -0.8192\n",
      "2025-12-19 15:43:01.032265: val_loss -0.856\n",
      "2025-12-19 15:43:01.034267: Pseudo dice [0.9181, 0.9555, 0.9214]\n",
      "2025-12-19 15:43:01.036268: Epoch time: 138.0 s\n",
      "2025-12-19 15:43:01.787643: \n",
      "2025-12-19 15:43:01.787643: Epoch 173\n",
      "2025-12-19 15:43:01.787643: Current learning rate: 0.00843\n",
      "2025-12-19 15:45:19.736783: train_loss -0.818\n",
      "2025-12-19 15:45:19.745765: val_loss -0.854\n",
      "2025-12-19 15:45:19.747767: Pseudo dice [0.9181, 0.9567, 0.9162]\n",
      "2025-12-19 15:45:19.749769: Epoch time: 137.95 s\n",
      "2025-12-19 15:45:20.403060: \n",
      "2025-12-19 15:45:20.403060: Epoch 174\n",
      "2025-12-19 15:45:20.403060: Current learning rate: 0.00842\n",
      "2025-12-19 15:47:38.461472: train_loss -0.8245\n",
      "2025-12-19 15:47:38.463474: val_loss -0.849\n",
      "2025-12-19 15:47:38.465476: Pseudo dice [0.9114, 0.9466, 0.9341]\n",
      "2025-12-19 15:47:38.467478: Epoch time: 138.06 s\n",
      "2025-12-19 15:47:39.118027: \n",
      "2025-12-19 15:47:39.118027: Epoch 175\n",
      "2025-12-19 15:47:39.118027: Current learning rate: 0.00841\n",
      "2025-12-19 15:49:56.953804: train_loss -0.8206\n",
      "2025-12-19 15:49:56.953804: val_loss -0.8573\n",
      "2025-12-19 15:49:56.953804: Pseudo dice [0.9173, 0.9516, 0.9355]\n",
      "2025-12-19 15:49:56.953804: Epoch time: 137.84 s\n",
      "2025-12-19 15:49:57.675548: \n",
      "2025-12-19 15:49:57.675548: Epoch 176\n",
      "2025-12-19 15:49:57.675548: Current learning rate: 0.0084\n",
      "2025-12-19 15:52:15.605539: train_loss -0.828\n",
      "2025-12-19 15:52:15.605539: val_loss -0.853\n",
      "2025-12-19 15:52:15.609543: Pseudo dice [0.9165, 0.9499, 0.9264]\n",
      "2025-12-19 15:52:15.611546: Epoch time: 137.93 s\n",
      "2025-12-19 15:52:16.430953: \n",
      "2025-12-19 15:52:16.430953: Epoch 177\n",
      "2025-12-19 15:52:16.430953: Current learning rate: 0.00839\n",
      "2025-12-19 15:54:34.440028: train_loss -0.823\n",
      "2025-12-19 15:54:34.440028: val_loss -0.847\n",
      "2025-12-19 15:54:34.440028: Pseudo dice [0.9102, 0.9485, 0.9261]\n",
      "2025-12-19 15:54:34.440028: Epoch time: 138.01 s\n",
      "2025-12-19 15:54:35.085664: \n",
      "2025-12-19 15:54:35.085664: Epoch 178\n",
      "2025-12-19 15:54:35.101496: Current learning rate: 0.00838\n",
      "2025-12-19 15:56:52.982478: train_loss -0.8232\n",
      "2025-12-19 15:56:52.982478: val_loss -0.8491\n",
      "2025-12-19 15:56:52.984480: Pseudo dice [0.9125, 0.951, 0.9267]\n",
      "2025-12-19 15:56:52.986482: Epoch time: 137.89 s\n",
      "2025-12-19 15:56:53.635950: \n",
      "2025-12-19 15:56:53.635950: Epoch 179\n",
      "2025-12-19 15:56:53.635950: Current learning rate: 0.00837\n",
      "2025-12-19 15:59:11.538336: train_loss -0.8268\n",
      "2025-12-19 15:59:11.540076: val_loss -0.8558\n",
      "2025-12-19 15:59:11.542079: Pseudo dice [0.9215, 0.9517, 0.9238]\n",
      "2025-12-19 15:59:11.544080: Epoch time: 137.9 s\n",
      "2025-12-19 15:59:11.546082: Yayy! New best EMA pseudo Dice: 0.9293\n",
      "2025-12-19 15:59:12.433596: \n",
      "2025-12-19 15:59:12.433596: Epoch 180\n",
      "2025-12-19 15:59:12.436190: Current learning rate: 0.00836\n",
      "2025-12-19 16:01:30.322124: train_loss -0.8165\n",
      "2025-12-19 16:01:30.322124: val_loss -0.8505\n",
      "2025-12-19 16:01:30.326128: Pseudo dice [0.9173, 0.9469, 0.9298]\n",
      "2025-12-19 16:01:30.326128: Epoch time: 137.89 s\n",
      "2025-12-19 16:01:30.328130: Yayy! New best EMA pseudo Dice: 0.9295\n",
      "2025-12-19 16:01:31.232684: \n",
      "2025-12-19 16:01:31.232684: Epoch 181\n",
      "2025-12-19 16:01:31.232684: Current learning rate: 0.00836\n",
      "2025-12-19 16:03:49.414106: train_loss -0.8152\n",
      "2025-12-19 16:03:49.414106: val_loss -0.8475\n",
      "2025-12-19 16:03:49.414106: Pseudo dice [0.9162, 0.9486, 0.9245]\n",
      "2025-12-19 16:03:49.414106: Epoch time: 138.18 s\n",
      "2025-12-19 16:03:49.429829: Yayy! New best EMA pseudo Dice: 0.9296\n",
      "2025-12-19 16:03:50.507538: \n",
      "2025-12-19 16:03:50.507538: Epoch 182\n",
      "2025-12-19 16:03:50.507538: Current learning rate: 0.00835\n",
      "2025-12-19 16:06:10.098889: train_loss -0.8161\n",
      "2025-12-19 16:06:10.100892: val_loss -0.8422\n",
      "2025-12-19 16:06:10.104355: Pseudo dice [0.9138, 0.9432, 0.9226]\n",
      "2025-12-19 16:06:10.106357: Epoch time: 139.59 s\n",
      "2025-12-19 16:06:10.754877: \n",
      "2025-12-19 16:06:10.754877: Epoch 183\n",
      "2025-12-19 16:06:10.754877: Current learning rate: 0.00834\n",
      "2025-12-19 16:08:29.074709: train_loss -0.8271\n",
      "2025-12-19 16:08:29.074709: val_loss -0.8559\n",
      "2025-12-19 16:08:29.082315: Pseudo dice [0.9178, 0.9553, 0.9279]\n",
      "2025-12-19 16:08:29.084319: Epoch time: 138.32 s\n",
      "2025-12-19 16:08:29.086321: Yayy! New best EMA pseudo Dice: 0.9297\n",
      "2025-12-19 16:08:30.022545: \n",
      "2025-12-19 16:08:30.022545: Epoch 184\n",
      "2025-12-19 16:08:30.022545: Current learning rate: 0.00833\n",
      "2025-12-19 16:10:48.986593: train_loss -0.819\n",
      "2025-12-19 16:10:48.986593: val_loss -0.8483\n",
      "2025-12-19 16:10:48.988597: Pseudo dice [0.9122, 0.9501, 0.9246]\n",
      "2025-12-19 16:10:48.990600: Epoch time: 138.96 s\n",
      "2025-12-19 16:10:49.795310: \n",
      "2025-12-19 16:10:49.795310: Epoch 185\n",
      "2025-12-19 16:10:49.795310: Current learning rate: 0.00832\n",
      "2025-12-19 16:13:08.232722: train_loss -0.8163\n",
      "2025-12-19 16:13:08.232722: val_loss -0.8366\n",
      "2025-12-19 16:13:08.232722: Pseudo dice [0.912, 0.9465, 0.9084]\n",
      "2025-12-19 16:13:08.237901: Epoch time: 138.44 s\n",
      "2025-12-19 16:13:08.889150: \n",
      "2025-12-19 16:13:08.889150: Epoch 186\n",
      "2025-12-19 16:13:08.889150: Current learning rate: 0.00831\n",
      "2025-12-19 16:15:27.285690: train_loss -0.8115\n",
      "2025-12-19 16:15:27.285690: val_loss -0.8419\n",
      "2025-12-19 16:15:27.285690: Pseudo dice [0.915, 0.9502, 0.9051]\n",
      "2025-12-19 16:15:27.285690: Epoch time: 138.4 s\n",
      "2025-12-19 16:15:27.933364: \n",
      "2025-12-19 16:15:27.933364: Epoch 187\n",
      "2025-12-19 16:15:27.949817: Current learning rate: 0.0083\n",
      "2025-12-19 16:17:45.942407: train_loss -0.8203\n",
      "2025-12-19 16:17:45.942407: val_loss -0.852\n",
      "2025-12-19 16:17:45.942407: Pseudo dice [0.914, 0.9503, 0.9319]\n",
      "2025-12-19 16:17:45.942407: Epoch time: 138.01 s\n",
      "2025-12-19 16:17:46.924945: \n",
      "2025-12-19 16:17:46.924945: Epoch 188\n",
      "2025-12-19 16:17:46.924945: Current learning rate: 0.00829\n",
      "2025-12-19 16:20:05.003424: train_loss -0.8218\n",
      "2025-12-19 16:20:05.003424: val_loss -0.8554\n",
      "2025-12-19 16:20:05.003424: Pseudo dice [0.9177, 0.9556, 0.919]\n",
      "2025-12-19 16:20:05.019390: Epoch time: 138.08 s\n",
      "2025-12-19 16:20:05.669895: \n",
      "2025-12-19 16:20:05.669895: Epoch 189\n",
      "2025-12-19 16:20:05.669895: Current learning rate: 0.00828\n",
      "2025-12-19 16:22:23.894647: train_loss -0.8192\n",
      "2025-12-19 16:22:23.894647: val_loss -0.8452\n",
      "2025-12-19 16:22:23.897317: Pseudo dice [0.9094, 0.9469, 0.9277]\n",
      "2025-12-19 16:22:23.899319: Epoch time: 138.22 s\n",
      "2025-12-19 16:22:24.554504: \n",
      "2025-12-19 16:22:24.554504: Epoch 190\n",
      "2025-12-19 16:22:24.554504: Current learning rate: 0.00827\n",
      "2025-12-19 16:24:42.720167: train_loss -0.8245\n",
      "2025-12-19 16:24:42.720167: val_loss -0.8589\n",
      "2025-12-19 16:24:42.722170: Pseudo dice [0.9195, 0.9548, 0.9276]\n",
      "2025-12-19 16:24:42.722170: Epoch time: 138.17 s\n",
      "2025-12-19 16:24:43.525136: \n",
      "2025-12-19 16:24:43.525136: Epoch 191\n",
      "2025-12-19 16:24:43.525136: Current learning rate: 0.00826\n",
      "2025-12-19 16:27:01.529278: train_loss -0.8288\n",
      "2025-12-19 16:27:01.529278: val_loss -0.8547\n",
      "2025-12-19 16:27:01.533283: Pseudo dice [0.9144, 0.9518, 0.9275]\n",
      "2025-12-19 16:27:01.535285: Epoch time: 138.0 s\n",
      "2025-12-19 16:27:02.179132: \n",
      "2025-12-19 16:27:02.179132: Epoch 192\n",
      "2025-12-19 16:27:02.195170: Current learning rate: 0.00825\n",
      "2025-12-19 16:29:20.257875: train_loss -0.8262\n",
      "2025-12-19 16:29:20.259878: val_loss -0.8523\n",
      "2025-12-19 16:29:20.259878: Pseudo dice [0.9131, 0.9419, 0.9329]\n",
      "2025-12-19 16:29:20.264535: Epoch time: 138.08 s\n",
      "2025-12-19 16:29:20.923414: \n",
      "2025-12-19 16:29:20.923414: Epoch 193\n",
      "2025-12-19 16:29:20.923414: Current learning rate: 0.00824\n",
      "2025-12-19 16:31:39.096084: train_loss -0.8295\n",
      "2025-12-19 16:31:39.096084: val_loss -0.8653\n",
      "2025-12-19 16:31:39.098087: Pseudo dice [0.9229, 0.9561, 0.9338]\n",
      "2025-12-19 16:31:39.101091: Epoch time: 138.17 s\n",
      "2025-12-19 16:31:39.101091: Yayy! New best EMA pseudo Dice: 0.9303\n",
      "2025-12-19 16:31:40.369216: \n",
      "2025-12-19 16:31:40.369216: Epoch 194\n",
      "2025-12-19 16:31:40.369216: Current learning rate: 0.00824\n",
      "2025-12-19 16:33:58.336182: train_loss -0.8287\n",
      "2025-12-19 16:33:58.336182: val_loss -0.8572\n",
      "2025-12-19 16:33:58.340411: Pseudo dice [0.9179, 0.9523, 0.925]\n",
      "2025-12-19 16:33:58.340411: Epoch time: 137.97 s\n",
      "2025-12-19 16:33:58.340411: Yayy! New best EMA pseudo Dice: 0.9305\n",
      "2025-12-19 16:33:59.251964: \n",
      "2025-12-19 16:33:59.251964: Epoch 195\n",
      "2025-12-19 16:33:59.254281: Current learning rate: 0.00823\n",
      "2025-12-19 16:36:17.257328: train_loss -0.8065\n",
      "2025-12-19 16:36:17.257328: val_loss -0.8386\n",
      "2025-12-19 16:36:17.273395: Pseudo dice [0.9105, 0.9448, 0.9231]\n",
      "2025-12-19 16:36:17.276957: Epoch time: 138.01 s\n",
      "2025-12-19 16:36:17.924072: \n",
      "2025-12-19 16:36:17.924072: Epoch 196\n",
      "2025-12-19 16:36:17.924072: Current learning rate: 0.00822\n",
      "2025-12-19 16:38:36.181590: train_loss -0.8012\n",
      "2025-12-19 16:38:36.183592: val_loss -0.8446\n",
      "2025-12-19 16:38:36.185594: Pseudo dice [0.914, 0.9432, 0.9275]\n",
      "2025-12-19 16:38:36.187596: Epoch time: 138.26 s\n",
      "2025-12-19 16:38:37.019638: \n",
      "2025-12-19 16:38:37.019638: Epoch 197\n",
      "2025-12-19 16:38:37.037574: Current learning rate: 0.00821\n",
      "2025-12-19 16:40:55.052642: train_loss -0.8198\n",
      "2025-12-19 16:40:55.052642: val_loss -0.8465\n",
      "2025-12-19 16:40:55.052642: Pseudo dice [0.912, 0.947, 0.9235]\n",
      "2025-12-19 16:40:55.052642: Epoch time: 138.03 s\n",
      "2025-12-19 16:40:55.703803: \n",
      "2025-12-19 16:40:55.703803: Epoch 198\n",
      "2025-12-19 16:40:55.703803: Current learning rate: 0.0082\n",
      "2025-12-19 16:43:13.672326: train_loss -0.8261\n",
      "2025-12-19 16:43:13.672326: val_loss -0.8544\n",
      "2025-12-19 16:43:13.688333: Pseudo dice [0.9186, 0.953, 0.925]\n",
      "2025-12-19 16:43:13.688333: Epoch time: 137.97 s\n",
      "2025-12-19 16:43:14.405409: \n",
      "2025-12-19 16:43:14.405409: Epoch 199\n",
      "2025-12-19 16:43:14.405409: Current learning rate: 0.00819\n",
      "2025-12-19 16:45:32.502128: train_loss -0.826\n",
      "2025-12-19 16:45:32.504132: val_loss -0.8548\n",
      "2025-12-19 16:45:32.507137: Pseudo dice [0.9142, 0.9555, 0.9309]\n",
      "2025-12-19 16:45:32.509139: Epoch time: 138.1 s\n",
      "2025-12-19 16:45:33.769323: \n",
      "2025-12-19 16:45:33.769323: Epoch 200\n",
      "2025-12-19 16:45:33.783623: Current learning rate: 0.00818\n",
      "2025-12-19 16:47:51.728756: train_loss -0.8165\n",
      "2025-12-19 16:47:51.728756: val_loss -0.8542\n",
      "2025-12-19 16:47:51.730759: Pseudo dice [0.9183, 0.9573, 0.922]\n",
      "2025-12-19 16:47:51.732761: Epoch time: 137.96 s\n",
      "2025-12-19 16:47:51.732761: Yayy! New best EMA pseudo Dice: 0.9305\n",
      "2025-12-19 16:47:52.635860: \n",
      "2025-12-19 16:47:52.635860: Epoch 201\n",
      "2025-12-19 16:47:52.635860: Current learning rate: 0.00817\n",
      "2025-12-19 16:50:10.801798: train_loss -0.8152\n",
      "2025-12-19 16:50:10.801798: val_loss -0.8459\n",
      "2025-12-19 16:50:10.803800: Pseudo dice [0.9051, 0.9481, 0.9372]\n",
      "2025-12-19 16:50:10.807806: Epoch time: 138.17 s\n",
      "2025-12-19 16:50:11.463720: \n",
      "2025-12-19 16:50:11.463720: Epoch 202\n",
      "2025-12-19 16:50:11.468109: Current learning rate: 0.00816\n",
      "2025-12-19 16:52:29.493796: train_loss -0.8178\n",
      "2025-12-19 16:52:29.493796: val_loss -0.8492\n",
      "2025-12-19 16:52:29.493796: Pseudo dice [0.9165, 0.9467, 0.9264]\n",
      "2025-12-19 16:52:29.493796: Epoch time: 138.03 s\n",
      "2025-12-19 16:52:30.332244: \n",
      "2025-12-19 16:52:30.332244: Epoch 203\n",
      "2025-12-19 16:52:30.332244: Current learning rate: 0.00815\n",
      "2025-12-19 16:54:48.428845: train_loss -0.827\n",
      "2025-12-19 16:54:48.428845: val_loss -0.8518\n",
      "2025-12-19 16:54:48.432850: Pseudo dice [0.9153, 0.9467, 0.9293]\n",
      "2025-12-19 16:54:48.432850: Epoch time: 138.1 s\n",
      "2025-12-19 16:54:49.092544: \n",
      "2025-12-19 16:54:49.092544: Epoch 204\n",
      "2025-12-19 16:54:49.092544: Current learning rate: 0.00814\n",
      "2025-12-19 16:57:07.092704: train_loss -0.8225\n",
      "2025-12-19 16:57:07.092704: val_loss -0.8565\n",
      "2025-12-19 16:57:07.108612: Pseudo dice [0.9175, 0.9498, 0.9286]\n",
      "2025-12-19 16:57:07.108612: Epoch time: 138.0 s\n",
      "2025-12-19 16:57:07.108612: Yayy! New best EMA pseudo Dice: 0.9305\n",
      "2025-12-19 16:57:07.996404: \n",
      "2025-12-19 16:57:07.996404: Epoch 205\n",
      "2025-12-19 16:57:07.996404: Current learning rate: 0.00813\n",
      "2025-12-19 16:59:25.837280: train_loss -0.8307\n",
      "2025-12-19 16:59:25.837280: val_loss -0.853\n",
      "2025-12-19 16:59:25.841024: Pseudo dice [0.9115, 0.9457, 0.937]\n",
      "2025-12-19 16:59:25.843027: Epoch time: 137.84 s\n",
      "2025-12-19 16:59:25.843027: Yayy! New best EMA pseudo Dice: 0.9306\n",
      "2025-12-19 16:59:26.917942: \n",
      "2025-12-19 16:59:26.917942: Epoch 206\n",
      "2025-12-19 16:59:26.917942: Current learning rate: 0.00813\n",
      "2025-12-19 17:01:45.039069: train_loss -0.8267\n",
      "2025-12-19 17:01:45.039069: val_loss -0.861\n",
      "2025-12-19 17:01:45.043843: Pseudo dice [0.9235, 0.9547, 0.9261]\n",
      "2025-12-19 17:01:45.043843: Epoch time: 138.12 s\n",
      "2025-12-19 17:01:45.043843: Yayy! New best EMA pseudo Dice: 0.931\n",
      "2025-12-19 17:01:45.927237: \n",
      "2025-12-19 17:01:45.927237: Epoch 207\n",
      "2025-12-19 17:01:45.932297: Current learning rate: 0.00812\n",
      "2025-12-19 17:04:04.030123: train_loss -0.8254\n",
      "2025-12-19 17:04:04.030123: val_loss -0.8692\n",
      "2025-12-19 17:04:04.048028: Pseudo dice [0.9285, 0.9582, 0.9273]\n",
      "2025-12-19 17:04:04.048028: Epoch time: 138.1 s\n",
      "2025-12-19 17:04:04.048028: Yayy! New best EMA pseudo Dice: 0.9317\n",
      "2025-12-19 17:04:04.934134: \n",
      "2025-12-19 17:04:04.934134: Epoch 208\n",
      "2025-12-19 17:04:04.938863: Current learning rate: 0.00811\n",
      "2025-12-19 17:06:22.799629: train_loss -0.8325\n",
      "2025-12-19 17:06:22.799629: val_loss -0.8644\n",
      "2025-12-19 17:06:22.799629: Pseudo dice [0.922, 0.9538, 0.9352]\n",
      "2025-12-19 17:06:22.799629: Epoch time: 137.87 s\n",
      "2025-12-19 17:06:22.799629: Yayy! New best EMA pseudo Dice: 0.9323\n",
      "2025-12-19 17:06:23.846592: \n",
      "2025-12-19 17:06:23.846592: Epoch 209\n",
      "2025-12-19 17:06:23.861272: Current learning rate: 0.0081\n",
      "2025-12-19 17:08:41.988153: train_loss -0.8279\n",
      "2025-12-19 17:08:41.988153: val_loss -0.861\n",
      "2025-12-19 17:08:42.003861: Pseudo dice [0.9185, 0.9598, 0.9276]\n",
      "2025-12-19 17:08:42.003861: Epoch time: 138.14 s\n",
      "2025-12-19 17:08:42.003861: Yayy! New best EMA pseudo Dice: 0.9326\n",
      "2025-12-19 17:08:42.904132: \n",
      "2025-12-19 17:08:42.904132: Epoch 210\n",
      "2025-12-19 17:08:42.904132: Current learning rate: 0.00809\n",
      "2025-12-19 17:11:01.365209: train_loss -0.826\n",
      "2025-12-19 17:11:01.365209: val_loss -0.8593\n",
      "2025-12-19 17:11:01.369101: Pseudo dice [0.9191, 0.9562, 0.9281]\n",
      "2025-12-19 17:11:01.372101: Epoch time: 138.46 s\n",
      "2025-12-19 17:11:01.372101: Yayy! New best EMA pseudo Dice: 0.9328\n",
      "2025-12-19 17:11:02.419890: \n",
      "2025-12-19 17:11:02.419890: Epoch 211\n",
      "2025-12-19 17:11:02.427556: Current learning rate: 0.00808\n",
      "2025-12-19 17:13:20.295690: train_loss -0.8292\n",
      "2025-12-19 17:13:20.295690: val_loss -0.8527\n",
      "2025-12-19 17:13:20.297692: Pseudo dice [0.9162, 0.9461, 0.9278]\n",
      "2025-12-19 17:13:20.299695: Epoch time: 137.88 s\n",
      "2025-12-19 17:13:21.041066: \n",
      "2025-12-19 17:13:21.041066: Epoch 212\n",
      "2025-12-19 17:13:21.041066: Current learning rate: 0.00807\n",
      "2025-12-19 17:15:39.396466: train_loss -0.8269\n",
      "2025-12-19 17:15:39.396466: val_loss -0.8645\n",
      "2025-12-19 17:15:39.398207: Pseudo dice [0.9221, 0.9507, 0.9377]\n",
      "2025-12-19 17:15:39.401808: Epoch time: 138.36 s\n",
      "2025-12-19 17:15:39.401808: Yayy! New best EMA pseudo Dice: 0.9329\n",
      "2025-12-19 17:15:40.305030: \n",
      "2025-12-19 17:15:40.305030: Epoch 213\n",
      "2025-12-19 17:15:40.305030: Current learning rate: 0.00806\n",
      "2025-12-19 17:17:58.299259: train_loss -0.8238\n",
      "2025-12-19 17:17:58.299259: val_loss -0.8559\n",
      "2025-12-19 17:17:58.314992: Pseudo dice [0.9191, 0.9531, 0.9258]\n",
      "2025-12-19 17:17:58.317024: Epoch time: 138.0 s\n",
      "2025-12-19 17:17:58.954165: \n",
      "2025-12-19 17:17:58.954165: Epoch 214\n",
      "2025-12-19 17:17:58.954165: Current learning rate: 0.00805\n",
      "2025-12-19 17:20:16.767851: train_loss -0.8343\n",
      "2025-12-19 17:20:16.767851: val_loss -0.8594\n",
      "2025-12-19 17:20:16.767851: Pseudo dice [0.916, 0.9539, 0.9373]\n",
      "2025-12-19 17:20:16.767851: Epoch time: 137.82 s\n",
      "2025-12-19 17:20:16.767851: Yayy! New best EMA pseudo Dice: 0.9332\n",
      "2025-12-19 17:20:17.786107: \n",
      "2025-12-19 17:20:17.788109: Epoch 215\n",
      "2025-12-19 17:20:17.790113: Current learning rate: 0.00804\n",
      "2025-12-19 17:22:35.857266: train_loss -0.83\n",
      "2025-12-19 17:22:35.857266: val_loss -0.8533\n",
      "2025-12-19 17:22:35.857266: Pseudo dice [0.9148, 0.9499, 0.9194]\n",
      "2025-12-19 17:22:35.857266: Epoch time: 138.07 s\n",
      "2025-12-19 17:22:36.490369: \n",
      "2025-12-19 17:22:36.490369: Epoch 216\n",
      "2025-12-19 17:22:36.490369: Current learning rate: 0.00803\n",
      "2025-12-19 17:24:54.505877: train_loss -0.8331\n",
      "2025-12-19 17:24:54.505877: val_loss -0.8611\n",
      "2025-12-19 17:24:54.505877: Pseudo dice [0.9195, 0.948, 0.9387]\n",
      "2025-12-19 17:24:54.521798: Epoch time: 138.03 s\n",
      "2025-12-19 17:24:55.312420: \n",
      "2025-12-19 17:24:55.312420: Epoch 217\n",
      "2025-12-19 17:24:55.312420: Current learning rate: 0.00802\n",
      "2025-12-19 17:27:13.155398: train_loss -0.8295\n",
      "2025-12-19 17:27:13.155398: val_loss -0.8467\n",
      "2025-12-19 17:27:13.173233: Pseudo dice [0.9108, 0.9442, 0.9281]\n",
      "2025-12-19 17:27:13.173233: Epoch time: 137.84 s\n",
      "2025-12-19 17:27:13.944199: \n",
      "2025-12-19 17:27:13.944199: Epoch 218\n",
      "2025-12-19 17:27:13.960111: Current learning rate: 0.00801\n",
      "2025-12-19 17:29:31.919211: train_loss -0.8349\n",
      "2025-12-19 17:29:31.919211: val_loss -0.8666\n",
      "2025-12-19 17:29:31.923216: Pseudo dice [0.9215, 0.9607, 0.9322]\n",
      "2025-12-19 17:29:31.926960: Epoch time: 137.98 s\n",
      "2025-12-19 17:29:32.557358: \n",
      "2025-12-19 17:29:32.557358: Epoch 219\n",
      "2025-12-19 17:29:32.557358: Current learning rate: 0.00801\n",
      "2025-12-19 17:31:50.639069: train_loss -0.8304\n",
      "2025-12-19 17:31:50.639069: val_loss -0.8593\n",
      "2025-12-19 17:31:50.657195: Pseudo dice [0.921, 0.9508, 0.9286]\n",
      "2025-12-19 17:31:50.659197: Epoch time: 138.08 s\n",
      "2025-12-19 17:31:51.288307: \n",
      "2025-12-19 17:31:51.288307: Epoch 220\n",
      "2025-12-19 17:31:51.304317: Current learning rate: 0.008\n",
      "2025-12-19 17:34:09.229843: train_loss -0.8276\n",
      "2025-12-19 17:34:09.229843: val_loss -0.8626\n",
      "2025-12-19 17:34:09.231845: Pseudo dice [0.9172, 0.957, 0.9368]\n",
      "2025-12-19 17:34:09.231845: Epoch time: 137.94 s\n",
      "2025-12-19 17:34:09.231845: Yayy! New best EMA pseudo Dice: 0.9334\n",
      "2025-12-19 17:34:10.229154: \n",
      "2025-12-19 17:34:10.229154: Epoch 221\n",
      "2025-12-19 17:34:10.229154: Current learning rate: 0.00799\n",
      "2025-12-19 17:36:28.198294: train_loss -0.8267\n",
      "2025-12-19 17:36:28.198294: val_loss -0.8568\n",
      "2025-12-19 17:36:28.202298: Pseudo dice [0.9207, 0.9506, 0.9242]\n",
      "2025-12-19 17:36:28.204301: Epoch time: 137.97 s\n",
      "2025-12-19 17:36:28.828590: \n",
      "2025-12-19 17:36:28.828590: Epoch 222\n",
      "2025-12-19 17:36:28.840334: Current learning rate: 0.00798\n",
      "2025-12-19 17:38:46.910584: train_loss -0.831\n",
      "2025-12-19 17:38:46.910584: val_loss -0.8484\n",
      "2025-12-19 17:38:46.914588: Pseudo dice [0.9106, 0.9521, 0.925]\n",
      "2025-12-19 17:38:46.916394: Epoch time: 138.08 s\n",
      "2025-12-19 17:38:47.547615: \n",
      "2025-12-19 17:38:47.547615: Epoch 223\n",
      "2025-12-19 17:38:47.550839: Current learning rate: 0.00797\n",
      "2025-12-19 17:41:05.565569: train_loss -0.8231\n",
      "2025-12-19 17:41:05.567571: val_loss -0.8551\n",
      "2025-12-19 17:41:05.569574: Pseudo dice [0.9201, 0.9514, 0.9293]\n",
      "2025-12-19 17:41:05.569574: Epoch time: 138.02 s\n",
      "2025-12-19 17:41:06.494728: \n",
      "2025-12-19 17:41:06.494728: Epoch 224\n",
      "2025-12-19 17:41:06.494728: Current learning rate: 0.00796\n",
      "2025-12-19 17:43:24.501350: train_loss -0.8265\n",
      "2025-12-19 17:43:24.501350: val_loss -0.8599\n",
      "2025-12-19 17:43:24.517215: Pseudo dice [0.9186, 0.9534, 0.9257]\n",
      "2025-12-19 17:43:24.517215: Epoch time: 138.01 s\n",
      "2025-12-19 17:43:25.135322: \n",
      "2025-12-19 17:43:25.135322: Epoch 225\n",
      "2025-12-19 17:43:25.135322: Current learning rate: 0.00795\n",
      "2025-12-19 17:45:43.294989: train_loss -0.8262\n",
      "2025-12-19 17:45:43.294989: val_loss -0.8661\n",
      "2025-12-19 17:45:43.299564: Pseudo dice [0.9241, 0.9572, 0.9323]\n",
      "2025-12-19 17:45:43.301566: Epoch time: 138.16 s\n",
      "2025-12-19 17:45:43.916716: \n",
      "2025-12-19 17:45:43.916716: Epoch 226\n",
      "2025-12-19 17:45:43.916716: Current learning rate: 0.00794\n",
      "2025-12-19 17:48:01.856030: train_loss -0.8282\n",
      "2025-12-19 17:48:01.856030: val_loss -0.8612\n",
      "2025-12-19 17:48:01.858034: Pseudo dice [0.9199, 0.9543, 0.9358]\n",
      "2025-12-19 17:48:01.858034: Epoch time: 137.94 s\n",
      "2025-12-19 17:48:01.858034: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-19 17:48:02.839959: \n",
      "2025-12-19 17:48:02.839959: Epoch 227\n",
      "2025-12-19 17:48:02.855859: Current learning rate: 0.00793\n",
      "2025-12-19 17:50:20.796589: train_loss -0.8256\n",
      "2025-12-19 17:50:20.796589: val_loss -0.8713\n",
      "2025-12-19 17:50:20.796589: Pseudo dice [0.9285, 0.9604, 0.9258]\n",
      "2025-12-19 17:50:20.812675: Epoch time: 137.96 s\n",
      "2025-12-19 17:50:20.812675: Yayy! New best EMA pseudo Dice: 0.9342\n",
      "2025-12-19 17:50:21.684425: \n",
      "2025-12-19 17:50:21.684425: Epoch 228\n",
      "2025-12-19 17:50:21.700301: Current learning rate: 0.00792\n",
      "2025-12-19 17:52:39.854069: train_loss -0.8316\n",
      "2025-12-19 17:52:39.854069: val_loss -0.8605\n",
      "2025-12-19 17:52:39.856071: Pseudo dice [0.9158, 0.9567, 0.9332]\n",
      "2025-12-19 17:52:39.856071: Epoch time: 138.17 s\n",
      "2025-12-19 17:52:39.856071: Yayy! New best EMA pseudo Dice: 0.9343\n",
      "2025-12-19 17:52:40.737114: \n",
      "2025-12-19 17:52:40.737114: Epoch 229\n",
      "2025-12-19 17:52:40.737114: Current learning rate: 0.00791\n",
      "2025-12-19 17:54:58.713956: train_loss -0.8334\n",
      "2025-12-19 17:54:58.713956: val_loss -0.8562\n",
      "2025-12-19 17:54:58.715958: Pseudo dice [0.9148, 0.9492, 0.9306]\n",
      "2025-12-19 17:54:58.719963: Epoch time: 137.98 s\n",
      "2025-12-19 17:54:59.614104: \n",
      "2025-12-19 17:54:59.614104: Epoch 230\n",
      "2025-12-19 17:54:59.614104: Current learning rate: 0.0079\n",
      "2025-12-19 17:57:17.750009: train_loss -0.8267\n",
      "2025-12-19 17:57:17.750009: val_loss -0.8478\n",
      "2025-12-19 17:57:17.766009: Pseudo dice [0.9101, 0.9495, 0.9305]\n",
      "2025-12-19 17:57:17.766009: Epoch time: 138.14 s\n",
      "2025-12-19 17:57:18.398950: \n",
      "2025-12-19 17:57:18.398950: Epoch 231\n",
      "2025-12-19 17:57:18.398950: Current learning rate: 0.00789\n",
      "2025-12-19 17:59:36.339523: train_loss -0.8375\n",
      "2025-12-19 17:59:36.339523: val_loss -0.8491\n",
      "2025-12-19 17:59:36.343527: Pseudo dice [0.9114, 0.9493, 0.9207]\n",
      "2025-12-19 17:59:36.345529: Epoch time: 137.96 s\n",
      "2025-12-19 17:59:36.949239: \n",
      "2025-12-19 17:59:36.965225: Epoch 232\n",
      "2025-12-19 17:59:36.965225: Current learning rate: 0.00789\n",
      "2025-12-19 18:01:54.992895: train_loss -0.8246\n",
      "2025-12-19 18:01:54.992895: val_loss -0.8587\n",
      "2025-12-19 18:01:54.992895: Pseudo dice [0.9201, 0.9489, 0.9329]\n",
      "2025-12-19 18:01:54.992895: Epoch time: 138.04 s\n",
      "2025-12-19 18:01:55.609680: \n",
      "2025-12-19 18:01:55.609680: Epoch 233\n",
      "2025-12-19 18:01:55.609680: Current learning rate: 0.00788\n",
      "2025-12-19 18:04:13.741588: train_loss -0.8288\n",
      "2025-12-19 18:04:13.741588: val_loss -0.849\n",
      "2025-12-19 18:04:13.741588: Pseudo dice [0.9095, 0.948, 0.9292]\n",
      "2025-12-19 18:04:13.757601: Epoch time: 138.13 s\n",
      "2025-12-19 18:04:14.374156: \n",
      "2025-12-19 18:04:14.374156: Epoch 234\n",
      "2025-12-19 18:04:14.374156: Current learning rate: 0.00787\n",
      "2025-12-19 18:06:32.451109: train_loss -0.8296\n",
      "2025-12-19 18:06:32.451109: val_loss -0.863\n",
      "2025-12-19 18:06:32.453111: Pseudo dice [0.921, 0.9511, 0.9341]\n",
      "2025-12-19 18:06:32.455113: Epoch time: 138.08 s\n",
      "2025-12-19 18:06:33.078897: \n",
      "2025-12-19 18:06:33.078897: Epoch 235\n",
      "2025-12-19 18:06:33.078897: Current learning rate: 0.00786\n",
      "2025-12-19 18:08:51.291587: train_loss -0.8305\n",
      "2025-12-19 18:08:51.291587: val_loss -0.8492\n",
      "2025-12-19 18:08:51.307219: Pseudo dice [0.9144, 0.9485, 0.9259]\n",
      "2025-12-19 18:08:51.309222: Epoch time: 138.21 s\n",
      "2025-12-19 18:08:52.086623: \n",
      "2025-12-19 18:08:52.086623: Epoch 236\n",
      "2025-12-19 18:08:52.100632: Current learning rate: 0.00785\n",
      "2025-12-19 18:11:10.290100: train_loss -0.837\n",
      "2025-12-19 18:11:10.290100: val_loss -0.8523\n",
      "2025-12-19 18:11:10.303740: Pseudo dice [0.9143, 0.9449, 0.9393]\n",
      "2025-12-19 18:11:10.303740: Epoch time: 138.2 s\n",
      "2025-12-19 18:11:10.919315: \n",
      "2025-12-19 18:11:10.919315: Epoch 237\n",
      "2025-12-19 18:11:10.919315: Current learning rate: 0.00784\n",
      "2025-12-19 18:13:28.990128: train_loss -0.8336\n",
      "2025-12-19 18:13:28.990128: val_loss -0.8625\n",
      "2025-12-19 18:13:28.994132: Pseudo dice [0.9197, 0.9517, 0.9304]\n",
      "2025-12-19 18:13:28.996536: Epoch time: 138.07 s\n",
      "2025-12-19 18:13:29.604064: \n",
      "2025-12-19 18:13:29.604064: Epoch 238\n",
      "2025-12-19 18:13:29.619851: Current learning rate: 0.00783\n",
      "2025-12-19 18:15:47.670114: train_loss -0.8319\n",
      "2025-12-19 18:15:47.670114: val_loss -0.853\n",
      "2025-12-19 18:15:47.670114: Pseudo dice [0.913, 0.9491, 0.9272]\n",
      "2025-12-19 18:15:47.685827: Epoch time: 138.07 s\n",
      "2025-12-19 18:15:48.301817: \n",
      "2025-12-19 18:15:48.301817: Epoch 239\n",
      "2025-12-19 18:15:48.301817: Current learning rate: 0.00782\n",
      "2025-12-19 18:18:06.347186: train_loss -0.8324\n",
      "2025-12-19 18:18:06.349188: val_loss -0.8612\n",
      "2025-12-19 18:18:06.351190: Pseudo dice [0.9156, 0.9542, 0.9309]\n",
      "2025-12-19 18:18:06.353192: Epoch time: 138.05 s\n",
      "2025-12-19 18:18:06.975809: \n",
      "2025-12-19 18:18:06.975809: Epoch 240\n",
      "2025-12-19 18:18:06.985624: Current learning rate: 0.00781\n",
      "2025-12-19 18:20:25.038430: train_loss -0.8273\n",
      "2025-12-19 18:20:25.040433: val_loss -0.8514\n",
      "2025-12-19 18:20:25.042434: Pseudo dice [0.9092, 0.9514, 0.935]\n",
      "2025-12-19 18:20:25.044436: Epoch time: 138.06 s\n",
      "2025-12-19 18:20:25.668819: \n",
      "2025-12-19 18:20:25.668819: Epoch 241\n",
      "2025-12-19 18:20:25.680331: Current learning rate: 0.0078\n",
      "2025-12-19 18:22:43.738045: train_loss -0.834\n",
      "2025-12-19 18:22:43.740047: val_loss -0.8521\n",
      "2025-12-19 18:22:43.742049: Pseudo dice [0.9161, 0.9502, 0.9198]\n",
      "2025-12-19 18:22:43.742049: Epoch time: 138.07 s\n",
      "2025-12-19 18:22:44.540273: \n",
      "2025-12-19 18:22:44.540273: Epoch 242\n",
      "2025-12-19 18:22:44.547785: Current learning rate: 0.00779\n",
      "2025-12-19 18:25:02.596495: train_loss -0.8304\n",
      "2025-12-19 18:25:02.598498: val_loss -0.867\n",
      "2025-12-19 18:25:02.604508: Pseudo dice [0.9241, 0.957, 0.932]\n",
      "2025-12-19 18:25:02.606514: Epoch time: 138.06 s\n",
      "2025-12-19 18:25:03.229740: \n",
      "2025-12-19 18:25:03.229740: Epoch 243\n",
      "2025-12-19 18:25:03.239190: Current learning rate: 0.00778\n",
      "2025-12-19 18:27:21.349432: train_loss -0.8277\n",
      "2025-12-19 18:27:21.349432: val_loss -0.8616\n",
      "2025-12-19 18:27:21.365465: Pseudo dice [0.922, 0.9534, 0.9277]\n",
      "2025-12-19 18:27:21.365465: Epoch time: 138.12 s\n",
      "2025-12-19 18:27:22.079118: \n",
      "2025-12-19 18:27:22.079118: Epoch 244\n",
      "2025-12-19 18:27:22.079118: Current learning rate: 0.00777\n",
      "2025-12-19 18:29:39.971787: train_loss -0.8353\n",
      "2025-12-19 18:29:39.973789: val_loss -0.8574\n",
      "2025-12-19 18:29:39.973789: Pseudo dice [0.9194, 0.9548, 0.9274]\n",
      "2025-12-19 18:29:39.980126: Epoch time: 137.89 s\n",
      "2025-12-19 18:29:40.606032: \n",
      "2025-12-19 18:29:40.606032: Epoch 245\n",
      "2025-12-19 18:29:40.614505: Current learning rate: 0.00777\n",
      "2025-12-19 18:31:58.716394: train_loss -0.8374\n",
      "2025-12-19 18:31:58.716394: val_loss -0.8538\n",
      "2025-12-19 18:31:58.718396: Pseudo dice [0.9154, 0.9486, 0.9299]\n",
      "2025-12-19 18:31:58.718396: Epoch time: 138.11 s\n",
      "2025-12-19 18:31:59.345984: \n",
      "2025-12-19 18:31:59.345984: Epoch 246\n",
      "2025-12-19 18:31:59.354928: Current learning rate: 0.00776\n",
      "2025-12-19 18:34:17.268648: train_loss -0.8321\n",
      "2025-12-19 18:34:17.280949: val_loss -0.8577\n",
      "2025-12-19 18:34:17.282953: Pseudo dice [0.916, 0.951, 0.9332]\n",
      "2025-12-19 18:34:17.284695: Epoch time: 137.92 s\n",
      "2025-12-19 18:34:18.044674: \n",
      "2025-12-19 18:34:18.044674: Epoch 247\n",
      "2025-12-19 18:34:18.060402: Current learning rate: 0.00775\n",
      "2025-12-19 18:36:36.077018: train_loss -0.8292\n",
      "2025-12-19 18:36:36.077018: val_loss -0.8437\n",
      "2025-12-19 18:36:36.081022: Pseudo dice [0.9034, 0.9472, 0.9335]\n",
      "2025-12-19 18:36:36.081022: Epoch time: 138.03 s\n",
      "2025-12-19 18:36:36.735765: \n",
      "2025-12-19 18:36:36.735765: Epoch 248\n",
      "2025-12-19 18:36:36.739897: Current learning rate: 0.00774\n",
      "2025-12-19 18:38:54.769694: train_loss -0.8258\n",
      "2025-12-19 18:38:54.769694: val_loss -0.8561\n",
      "2025-12-19 18:38:54.773698: Pseudo dice [0.9139, 0.9496, 0.9298]\n",
      "2025-12-19 18:38:54.775699: Epoch time: 138.03 s\n",
      "2025-12-19 18:38:55.572070: \n",
      "2025-12-19 18:38:55.572070: Epoch 249\n",
      "2025-12-19 18:38:55.580104: Current learning rate: 0.00773\n",
      "2025-12-19 18:41:13.599767: train_loss -0.8317\n",
      "2025-12-19 18:41:13.599767: val_loss -0.866\n",
      "2025-12-19 18:41:13.605774: Pseudo dice [0.9236, 0.9531, 0.9349]\n",
      "2025-12-19 18:41:13.607777: Epoch time: 138.03 s\n",
      "2025-12-19 18:41:14.642234: \n",
      "2025-12-19 18:41:14.642234: Epoch 250\n",
      "2025-12-19 18:41:14.642234: Current learning rate: 0.00772\n",
      "2025-12-19 18:43:32.738438: train_loss -0.8357\n",
      "2025-12-19 18:43:32.738438: val_loss -0.8584\n",
      "2025-12-19 18:43:32.742442: Pseudo dice [0.9173, 0.9505, 0.9293]\n",
      "2025-12-19 18:43:32.746185: Epoch time: 138.1 s\n",
      "2025-12-19 18:43:33.379091: \n",
      "2025-12-19 18:43:33.379091: Epoch 251\n",
      "2025-12-19 18:43:33.379091: Current learning rate: 0.00771\n",
      "2025-12-19 18:45:51.393994: train_loss -0.8338\n",
      "2025-12-19 18:45:51.393994: val_loss -0.8551\n",
      "2025-12-19 18:45:51.409767: Pseudo dice [0.9135, 0.9504, 0.9318]\n",
      "2025-12-19 18:45:51.411615: Epoch time: 138.01 s\n",
      "2025-12-19 18:45:52.027041: \n",
      "2025-12-19 18:45:52.041059: Epoch 252\n",
      "2025-12-19 18:45:52.041059: Current learning rate: 0.0077\n",
      "2025-12-19 18:48:10.015120: train_loss -0.8323\n",
      "2025-12-19 18:48:10.015120: val_loss -0.8572\n",
      "2025-12-19 18:48:10.017122: Pseudo dice [0.919, 0.9543, 0.9269]\n",
      "2025-12-19 18:48:10.019123: Epoch time: 137.99 s\n",
      "2025-12-19 18:48:10.733779: \n",
      "2025-12-19 18:48:10.733779: Epoch 253\n",
      "2025-12-19 18:48:10.743978: Current learning rate: 0.00769\n",
      "2025-12-19 18:50:28.654953: train_loss -0.8217\n",
      "2025-12-19 18:50:28.654953: val_loss -0.8356\n",
      "2025-12-19 18:50:28.654953: Pseudo dice [0.9076, 0.9454, 0.915]\n",
      "2025-12-19 18:50:28.654953: Epoch time: 137.92 s\n",
      "2025-12-19 18:50:29.272654: \n",
      "2025-12-19 18:50:29.272654: Epoch 254\n",
      "2025-12-19 18:50:29.272654: Current learning rate: 0.00768\n",
      "2025-12-19 18:52:47.270049: train_loss -0.8081\n",
      "2025-12-19 18:52:47.270049: val_loss -0.8539\n",
      "2025-12-19 18:52:47.274055: Pseudo dice [0.9207, 0.9498, 0.9239]\n",
      "2025-12-19 18:52:47.274055: Epoch time: 138.0 s\n",
      "2025-12-19 18:52:48.071404: \n",
      "2025-12-19 18:52:48.071404: Epoch 255\n",
      "2025-12-19 18:52:48.071404: Current learning rate: 0.00767\n",
      "2025-12-19 18:55:06.067448: train_loss -0.8177\n",
      "2025-12-19 18:55:06.067448: val_loss -0.85\n",
      "2025-12-19 18:55:06.067448: Pseudo dice [0.9094, 0.9467, 0.9257]\n",
      "2025-12-19 18:55:06.067448: Epoch time: 138.0 s\n",
      "2025-12-19 18:55:06.719472: \n",
      "2025-12-19 18:55:06.719472: Epoch 256\n",
      "2025-12-19 18:55:06.735117: Current learning rate: 0.00766\n",
      "2025-12-19 18:57:24.655275: train_loss -0.8304\n",
      "2025-12-19 18:57:24.655275: val_loss -0.8666\n",
      "2025-12-19 18:57:24.655275: Pseudo dice [0.9225, 0.956, 0.936]\n",
      "2025-12-19 18:57:24.655275: Epoch time: 137.94 s\n",
      "2025-12-19 18:57:25.289770: \n",
      "2025-12-19 18:57:25.289770: Epoch 257\n",
      "2025-12-19 18:57:25.289770: Current learning rate: 0.00765\n",
      "2025-12-19 18:59:43.164595: train_loss -0.8354\n",
      "2025-12-19 18:59:43.166598: val_loss -0.8623\n",
      "2025-12-19 18:59:43.170344: Pseudo dice [0.9214, 0.9542, 0.9274]\n",
      "2025-12-19 18:59:43.174350: Epoch time: 137.87 s\n",
      "2025-12-19 18:59:43.817447: \n",
      "2025-12-19 18:59:43.817447: Epoch 258\n",
      "2025-12-19 18:59:43.817447: Current learning rate: 0.00764\n",
      "2025-12-19 19:02:01.740254: train_loss -0.8389\n",
      "2025-12-19 19:02:01.742256: val_loss -0.8576\n",
      "2025-12-19 19:02:01.745999: Pseudo dice [0.9142, 0.9478, 0.9343]\n",
      "2025-12-19 19:02:01.748001: Epoch time: 137.92 s\n",
      "2025-12-19 19:02:02.360708: \n",
      "2025-12-19 19:02:02.360708: Epoch 259\n",
      "2025-12-19 19:02:02.376650: Current learning rate: 0.00764\n",
      "2025-12-19 19:04:20.513083: train_loss -0.835\n",
      "2025-12-19 19:04:20.513083: val_loss -0.8622\n",
      "2025-12-19 19:04:20.513083: Pseudo dice [0.9189, 0.9532, 0.932]\n",
      "2025-12-19 19:04:20.519809: Epoch time: 138.15 s\n",
      "2025-12-19 19:04:21.145256: \n",
      "2025-12-19 19:04:21.145256: Epoch 260\n",
      "2025-12-19 19:04:21.145256: Current learning rate: 0.00763\n",
      "2025-12-19 19:06:39.109445: train_loss -0.8096\n",
      "2025-12-19 19:06:39.109445: val_loss -0.8336\n",
      "2025-12-19 19:06:39.125190: Pseudo dice [0.906, 0.943, 0.9258]\n",
      "2025-12-19 19:06:39.125190: Epoch time: 137.96 s\n",
      "2025-12-19 19:06:39.742639: \n",
      "2025-12-19 19:06:39.742639: Epoch 261\n",
      "2025-12-19 19:06:39.742639: Current learning rate: 0.00762\n",
      "2025-12-19 19:08:57.839722: train_loss -0.8038\n",
      "2025-12-19 19:08:57.839722: val_loss -0.8525\n",
      "2025-12-19 19:08:57.843311: Pseudo dice [0.9182, 0.9549, 0.9222]\n",
      "2025-12-19 19:08:57.843311: Epoch time: 138.1 s\n",
      "2025-12-19 19:08:58.653538: \n",
      "2025-12-19 19:08:58.653538: Epoch 262\n",
      "2025-12-19 19:08:58.653538: Current learning rate: 0.00761\n",
      "2025-12-19 19:11:16.820896: train_loss -0.8267\n",
      "2025-12-19 19:11:16.820896: val_loss -0.8618\n",
      "2025-12-19 19:11:16.820896: Pseudo dice [0.9208, 0.9573, 0.9253]\n",
      "2025-12-19 19:11:16.836618: Epoch time: 138.17 s\n",
      "2025-12-19 19:11:17.457408: \n",
      "2025-12-19 19:11:17.457408: Epoch 263\n",
      "2025-12-19 19:11:17.457408: Current learning rate: 0.0076\n",
      "2025-12-19 19:13:35.523947: train_loss -0.8247\n",
      "2025-12-19 19:13:35.523947: val_loss -0.8657\n",
      "2025-12-19 19:13:35.530014: Pseudo dice [0.9241, 0.9614, 0.9232]\n",
      "2025-12-19 19:13:35.530014: Epoch time: 138.07 s\n",
      "2025-12-19 19:13:36.206351: \n",
      "2025-12-19 19:13:36.206351: Epoch 264\n",
      "2025-12-19 19:13:36.206351: Current learning rate: 0.00759\n",
      "2025-12-19 19:15:54.190833: train_loss -0.8269\n",
      "2025-12-19 19:15:54.190833: val_loss -0.8582\n",
      "2025-12-19 19:15:54.190833: Pseudo dice [0.9186, 0.9564, 0.9238]\n",
      "2025-12-19 19:15:54.190833: Epoch time: 137.98 s\n",
      "2025-12-19 19:15:54.826040: \n",
      "2025-12-19 19:15:54.826040: Epoch 265\n",
      "2025-12-19 19:15:54.826040: Current learning rate: 0.00758\n",
      "2025-12-19 19:18:12.782794: train_loss -0.8293\n",
      "2025-12-19 19:18:12.782794: val_loss -0.8605\n",
      "2025-12-19 19:18:12.786798: Pseudo dice [0.9167, 0.9551, 0.9316]\n",
      "2025-12-19 19:18:12.788800: Epoch time: 137.96 s\n",
      "2025-12-19 19:18:13.434528: \n",
      "2025-12-19 19:18:13.434528: Epoch 266\n",
      "2025-12-19 19:18:13.434528: Current learning rate: 0.00757\n",
      "2025-12-19 19:20:31.500974: train_loss -0.8298\n",
      "2025-12-19 19:20:31.502978: val_loss -0.8598\n",
      "2025-12-19 19:20:31.506728: Pseudo dice [0.9173, 0.9527, 0.9277]\n",
      "2025-12-19 19:20:31.508731: Epoch time: 138.07 s\n",
      "2025-12-19 19:20:32.285103: \n",
      "2025-12-19 19:20:32.285103: Epoch 267\n",
      "2025-12-19 19:20:32.287106: Current learning rate: 0.00756\n",
      "2025-12-19 19:22:50.245954: train_loss -0.8353\n",
      "2025-12-19 19:22:50.247956: val_loss -0.8665\n",
      "2025-12-19 19:22:50.249959: Pseudo dice [0.9233, 0.9533, 0.9345]\n",
      "2025-12-19 19:22:50.253883: Epoch time: 137.96 s\n",
      "2025-12-19 19:22:51.054948: \n",
      "2025-12-19 19:22:51.054948: Epoch 268\n",
      "2025-12-19 19:22:51.054948: Current learning rate: 0.00755\n",
      "2025-12-19 19:25:09.158180: train_loss -0.8342\n",
      "2025-12-19 19:25:09.158180: val_loss -0.8605\n",
      "2025-12-19 19:25:09.167915: Pseudo dice [0.9194, 0.9555, 0.9294]\n",
      "2025-12-19 19:25:09.167915: Epoch time: 138.1 s\n",
      "2025-12-19 19:25:09.807608: \n",
      "2025-12-19 19:25:09.807608: Epoch 269\n",
      "2025-12-19 19:25:09.807608: Current learning rate: 0.00754\n",
      "2025-12-19 19:27:27.906651: train_loss -0.8304\n",
      "2025-12-19 19:27:27.908503: val_loss -0.8504\n",
      "2025-12-19 19:27:27.908503: Pseudo dice [0.9101, 0.9443, 0.9297]\n",
      "2025-12-19 19:27:27.914224: Epoch time: 138.1 s\n",
      "2025-12-19 19:27:28.649782: \n",
      "2025-12-19 19:27:28.649782: Epoch 270\n",
      "2025-12-19 19:27:28.649782: Current learning rate: 0.00753\n",
      "2025-12-19 19:29:46.561436: train_loss -0.8341\n",
      "2025-12-19 19:29:46.563443: val_loss -0.8625\n",
      "2025-12-19 19:29:46.567452: Pseudo dice [0.9189, 0.9497, 0.9338]\n",
      "2025-12-19 19:29:46.569455: Epoch time: 137.91 s\n",
      "2025-12-19 19:29:47.192115: \n",
      "2025-12-19 19:29:47.192115: Epoch 271\n",
      "2025-12-19 19:29:47.207943: Current learning rate: 0.00752\n",
      "2025-12-19 19:32:05.321444: train_loss -0.8323\n",
      "2025-12-19 19:32:05.321444: val_loss -0.867\n",
      "2025-12-19 19:32:05.321444: Pseudo dice [0.9215, 0.9557, 0.9345]\n",
      "2025-12-19 19:32:05.337132: Epoch time: 138.13 s\n",
      "2025-12-19 19:32:05.955332: \n",
      "2025-12-19 19:32:05.955332: Epoch 272\n",
      "2025-12-19 19:32:05.955332: Current learning rate: 0.00751\n",
      "2025-12-19 19:34:24.063269: train_loss -0.8232\n",
      "2025-12-19 19:34:24.063269: val_loss -0.86\n",
      "2025-12-19 19:34:24.079009: Pseudo dice [0.9179, 0.9522, 0.9297]\n",
      "2025-12-19 19:34:24.079009: Epoch time: 138.11 s\n",
      "2025-12-19 19:34:24.824803: \n",
      "2025-12-19 19:34:24.824803: Epoch 273\n",
      "2025-12-19 19:34:24.824803: Current learning rate: 0.00751\n",
      "2025-12-19 19:36:42.815227: train_loss -0.8331\n",
      "2025-12-19 19:36:42.815227: val_loss -0.8697\n",
      "2025-12-19 19:36:42.817229: Pseudo dice [0.9267, 0.9583, 0.9287]\n",
      "2025-12-19 19:36:42.821233: Epoch time: 137.99 s\n",
      "2025-12-19 19:36:43.628378: \n",
      "2025-12-19 19:36:43.628378: Epoch 274\n",
      "2025-12-19 19:36:43.628378: Current learning rate: 0.0075\n",
      "2025-12-19 19:39:01.846585: train_loss -0.8338\n",
      "2025-12-19 19:39:01.846585: val_loss -0.8484\n",
      "2025-12-19 19:39:01.848588: Pseudo dice [0.9099, 0.9438, 0.9296]\n",
      "2025-12-19 19:39:01.848588: Epoch time: 138.22 s\n",
      "2025-12-19 19:39:02.481115: \n",
      "2025-12-19 19:39:02.481115: Epoch 275\n",
      "2025-12-19 19:39:02.481115: Current learning rate: 0.00749\n",
      "2025-12-19 19:41:20.529641: train_loss -0.8385\n",
      "2025-12-19 19:41:20.529641: val_loss -0.858\n",
      "2025-12-19 19:41:20.532626: Pseudo dice [0.9137, 0.951, 0.9323]\n",
      "2025-12-19 19:41:20.532626: Epoch time: 138.05 s\n",
      "2025-12-19 19:41:21.257986: \n",
      "2025-12-19 19:41:21.257986: Epoch 276\n",
      "2025-12-19 19:41:21.257986: Current learning rate: 0.00748\n",
      "2025-12-19 19:43:39.276978: train_loss -0.8344\n",
      "2025-12-19 19:43:39.276978: val_loss -0.856\n",
      "2025-12-19 19:43:39.280720: Pseudo dice [0.916, 0.9523, 0.9318]\n",
      "2025-12-19 19:43:39.282722: Epoch time: 138.03 s\n",
      "2025-12-19 19:43:39.919278: \n",
      "2025-12-19 19:43:39.919278: Epoch 277\n",
      "2025-12-19 19:43:39.919278: Current learning rate: 0.00747\n",
      "2025-12-19 19:45:58.150461: train_loss -0.8304\n",
      "2025-12-19 19:45:58.150461: val_loss -0.8527\n",
      "2025-12-19 19:45:58.150461: Pseudo dice [0.9138, 0.9481, 0.9297]\n",
      "2025-12-19 19:45:58.150461: Epoch time: 138.23 s\n",
      "2025-12-19 19:45:58.785350: \n",
      "2025-12-19 19:45:58.785350: Epoch 278\n",
      "2025-12-19 19:45:58.785350: Current learning rate: 0.00746\n",
      "2025-12-19 19:48:16.884788: train_loss -0.8339\n",
      "2025-12-19 19:48:16.884788: val_loss -0.8607\n",
      "2025-12-19 19:48:16.888531: Pseudo dice [0.92, 0.9511, 0.9367]\n",
      "2025-12-19 19:48:16.893728: Epoch time: 138.1 s\n",
      "2025-12-19 19:48:17.629459: \n",
      "2025-12-19 19:48:17.629459: Epoch 279\n",
      "2025-12-19 19:48:17.629459: Current learning rate: 0.00745\n",
      "2025-12-19 19:50:35.648664: train_loss -0.838\n",
      "2025-12-19 19:50:35.648664: val_loss -0.8618\n",
      "2025-12-19 19:50:35.648664: Pseudo dice [0.9199, 0.949, 0.933]\n",
      "2025-12-19 19:50:35.664341: Epoch time: 138.02 s\n",
      "2025-12-19 19:50:36.455797: \n",
      "2025-12-19 19:50:36.455797: Epoch 280\n",
      "2025-12-19 19:50:36.471747: Current learning rate: 0.00744\n",
      "2025-12-19 19:52:54.498842: train_loss -0.8424\n",
      "2025-12-19 19:52:54.498842: val_loss -0.8648\n",
      "2025-12-19 19:52:54.502846: Pseudo dice [0.924, 0.9511, 0.9302]\n",
      "2025-12-19 19:52:54.506589: Epoch time: 138.04 s\n",
      "2025-12-19 19:52:55.136596: \n",
      "2025-12-19 19:52:55.136596: Epoch 281\n",
      "2025-12-19 19:52:55.136596: Current learning rate: 0.00743\n",
      "2025-12-19 19:55:13.156449: train_loss -0.8318\n",
      "2025-12-19 19:55:13.158451: val_loss -0.8599\n",
      "2025-12-19 19:55:13.162194: Pseudo dice [0.9201, 0.9532, 0.9262]\n",
      "2025-12-19 19:55:13.164196: Epoch time: 138.04 s\n",
      "2025-12-19 19:55:13.777373: \n",
      "2025-12-19 19:55:13.777373: Epoch 282\n",
      "2025-12-19 19:55:13.793150: Current learning rate: 0.00742\n",
      "2025-12-19 19:57:31.796888: train_loss -0.8341\n",
      "2025-12-19 19:57:31.798890: val_loss -0.8557\n",
      "2025-12-19 19:57:31.802633: Pseudo dice [0.9124, 0.9495, 0.9355]\n",
      "2025-12-19 19:57:31.804635: Epoch time: 138.02 s\n",
      "2025-12-19 19:57:32.434991: \n",
      "2025-12-19 19:57:32.434991: Epoch 283\n",
      "2025-12-19 19:57:32.434991: Current learning rate: 0.00741\n",
      "2025-12-19 19:59:50.503342: train_loss -0.8301\n",
      "2025-12-19 19:59:50.503342: val_loss -0.8615\n",
      "2025-12-19 19:59:50.505344: Pseudo dice [0.9228, 0.9557, 0.9343]\n",
      "2025-12-19 19:59:50.509354: Epoch time: 138.07 s\n",
      "2025-12-19 19:59:51.128229: \n",
      "2025-12-19 19:59:51.128229: Epoch 284\n",
      "2025-12-19 19:59:51.144691: Current learning rate: 0.0074\n",
      "2025-12-19 20:02:09.082078: train_loss -0.824\n",
      "2025-12-19 20:02:09.082078: val_loss -0.8573\n",
      "2025-12-19 20:02:09.089829: Pseudo dice [0.9177, 0.9542, 0.9282]\n",
      "2025-12-19 20:02:09.093842: Epoch time: 137.95 s\n",
      "2025-12-19 20:02:09.734900: \n",
      "2025-12-19 20:02:09.734900: Epoch 285\n",
      "2025-12-19 20:02:09.737313: Current learning rate: 0.00739\n",
      "2025-12-19 20:04:27.902777: train_loss -0.8352\n",
      "2025-12-19 20:04:27.902777: val_loss -0.8587\n",
      "2025-12-19 20:04:27.905771: Pseudo dice [0.9153, 0.954, 0.9341]\n",
      "2025-12-19 20:04:27.905771: Epoch time: 138.17 s\n",
      "2025-12-19 20:04:28.696710: \n",
      "2025-12-19 20:04:28.696710: Epoch 286\n",
      "2025-12-19 20:04:28.704312: Current learning rate: 0.00738\n",
      "2025-12-19 20:06:46.595681: train_loss -0.8373\n",
      "2025-12-19 20:06:46.595681: val_loss -0.8646\n",
      "2025-12-19 20:06:46.605934: Pseudo dice [0.924, 0.9527, 0.9241]\n",
      "2025-12-19 20:06:46.605934: Epoch time: 137.9 s\n",
      "2025-12-19 20:06:47.244622: \n",
      "2025-12-19 20:06:47.244622: Epoch 287\n",
      "2025-12-19 20:06:47.257806: Current learning rate: 0.00738\n",
      "2025-12-19 20:09:05.425222: train_loss -0.8358\n",
      "2025-12-19 20:09:05.425222: val_loss -0.854\n",
      "2025-12-19 20:09:05.425222: Pseudo dice [0.9167, 0.9475, 0.9241]\n",
      "2025-12-19 20:09:05.425222: Epoch time: 138.18 s\n",
      "2025-12-19 20:09:06.076804: \n",
      "2025-12-19 20:09:06.076804: Epoch 288\n",
      "2025-12-19 20:09:06.083468: Current learning rate: 0.00737\n",
      "2025-12-19 20:11:24.252155: train_loss -0.8374\n",
      "2025-12-19 20:11:24.252155: val_loss -0.8683\n",
      "2025-12-19 20:11:24.252155: Pseudo dice [0.9231, 0.957, 0.9357]\n",
      "2025-12-19 20:11:24.252155: Epoch time: 138.18 s\n",
      "2025-12-19 20:11:24.901130: \n",
      "2025-12-19 20:11:24.901130: Epoch 289\n",
      "2025-12-19 20:11:24.907611: Current learning rate: 0.00736\n",
      "2025-12-19 20:13:42.781069: train_loss -0.8298\n",
      "2025-12-19 20:13:42.783072: val_loss -0.8634\n",
      "2025-12-19 20:13:42.787077: Pseudo dice [0.9228, 0.9547, 0.9229]\n",
      "2025-12-19 20:13:42.789081: Epoch time: 137.88 s\n",
      "2025-12-19 20:13:43.470990: \n",
      "2025-12-19 20:13:43.486763: Epoch 290\n",
      "2025-12-19 20:13:43.486763: Current learning rate: 0.00735\n",
      "2025-12-19 20:16:01.636735: train_loss -0.8312\n",
      "2025-12-19 20:16:01.638738: val_loss -0.867\n",
      "2025-12-19 20:16:01.644748: Pseudo dice [0.9229, 0.9543, 0.944]\n",
      "2025-12-19 20:16:01.648498: Epoch time: 138.17 s\n",
      "2025-12-19 20:16:01.652512: Yayy! New best EMA pseudo Dice: 0.9345\n",
      "2025-12-19 20:16:02.530504: \n",
      "2025-12-19 20:16:02.530504: Epoch 291\n",
      "2025-12-19 20:16:02.530504: Current learning rate: 0.00734\n",
      "2025-12-19 20:18:20.521836: train_loss -0.8376\n",
      "2025-12-19 20:18:20.521836: val_loss -0.8673\n",
      "2025-12-19 20:18:20.525840: Pseudo dice [0.9239, 0.9539, 0.9333]\n",
      "2025-12-19 20:18:20.527842: Epoch time: 137.99 s\n",
      "2025-12-19 20:18:20.529844: Yayy! New best EMA pseudo Dice: 0.9347\n",
      "2025-12-19 20:18:21.614962: \n",
      "2025-12-19 20:18:21.614962: Epoch 292\n",
      "2025-12-19 20:18:21.614962: Current learning rate: 0.00733\n",
      "2025-12-19 20:20:39.618747: train_loss -0.8367\n",
      "2025-12-19 20:20:39.618747: val_loss -0.8636\n",
      "2025-12-19 20:20:39.624754: Pseudo dice [0.9214, 0.9536, 0.9288]\n",
      "2025-12-19 20:20:39.626757: Epoch time: 138.01 s\n",
      "2025-12-19 20:20:40.283343: \n",
      "2025-12-19 20:20:40.283343: Epoch 293\n",
      "2025-12-19 20:20:40.283343: Current learning rate: 0.00732\n",
      "2025-12-19 20:22:58.393216: train_loss -0.8379\n",
      "2025-12-19 20:22:58.393216: val_loss -0.8631\n",
      "2025-12-19 20:22:58.397220: Pseudo dice [0.9229, 0.9534, 0.9198]\n",
      "2025-12-19 20:22:58.399221: Epoch time: 138.11 s\n",
      "2025-12-19 20:22:59.036112: \n",
      "2025-12-19 20:22:59.036112: Epoch 294\n",
      "2025-12-19 20:22:59.036112: Current learning rate: 0.00731\n",
      "2025-12-19 20:25:17.112763: train_loss -0.8402\n",
      "2025-12-19 20:25:17.114766: val_loss -0.8628\n",
      "2025-12-19 20:25:17.114766: Pseudo dice [0.9175, 0.9528, 0.9368]\n",
      "2025-12-19 20:25:17.114766: Epoch time: 138.08 s\n",
      "2025-12-19 20:25:17.738727: \n",
      "2025-12-19 20:25:17.738727: Epoch 295\n",
      "2025-12-19 20:25:17.738727: Current learning rate: 0.0073\n",
      "2025-12-19 20:27:36.011945: train_loss -0.8381\n",
      "2025-12-19 20:27:36.011945: val_loss -0.8618\n",
      "2025-12-19 20:27:36.015949: Pseudo dice [0.9193, 0.9504, 0.9375]\n",
      "2025-12-19 20:27:36.019953: Epoch time: 138.27 s\n",
      "2025-12-19 20:27:36.670698: \n",
      "2025-12-19 20:27:36.670698: Epoch 296\n",
      "2025-12-19 20:27:36.670698: Current learning rate: 0.00729\n",
      "2025-12-19 20:29:54.753304: train_loss -0.8417\n",
      "2025-12-19 20:29:54.753304: val_loss -0.8642\n",
      "2025-12-19 20:29:54.753304: Pseudo dice [0.9179, 0.9544, 0.939]\n",
      "2025-12-19 20:29:54.769120: Epoch time: 138.1 s\n",
      "2025-12-19 20:29:54.769120: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2025-12-19 20:29:55.626197: \n",
      "2025-12-19 20:29:55.626197: Epoch 297\n",
      "2025-12-19 20:29:55.626197: Current learning rate: 0.00728\n",
      "2025-12-19 20:32:13.556207: train_loss -0.8336\n",
      "2025-12-19 20:32:13.556207: val_loss -0.8695\n",
      "2025-12-19 20:32:13.562908: Pseudo dice [0.9263, 0.9565, 0.9342]\n",
      "2025-12-19 20:32:13.564910: Epoch time: 137.93 s\n",
      "2025-12-19 20:32:13.568914: Yayy! New best EMA pseudo Dice: 0.9353\n",
      "2025-12-19 20:32:14.665315: \n",
      "2025-12-19 20:32:14.665315: Epoch 298\n",
      "2025-12-19 20:32:14.667056: Current learning rate: 0.00727\n",
      "2025-12-19 20:34:32.625662: train_loss -0.8424\n",
      "2025-12-19 20:34:32.625662: val_loss -0.8689\n",
      "2025-12-19 20:34:32.631670: Pseudo dice [0.9256, 0.9564, 0.9317]\n",
      "2025-12-19 20:34:32.635676: Epoch time: 137.96 s\n",
      "2025-12-19 20:34:32.639422: Yayy! New best EMA pseudo Dice: 0.9356\n",
      "2025-12-19 20:34:33.574425: \n",
      "2025-12-19 20:34:33.574425: Epoch 299\n",
      "2025-12-19 20:34:33.590166: Current learning rate: 0.00726\n",
      "2025-12-19 20:36:51.731910: train_loss -0.8313\n",
      "2025-12-19 20:36:51.731910: val_loss -0.8636\n",
      "2025-12-19 20:36:51.747562: Pseudo dice [0.9222, 0.9569, 0.9217]\n",
      "2025-12-19 20:36:51.753068: Epoch time: 138.16 s\n",
      "2025-12-19 20:36:52.634546: \n",
      "2025-12-19 20:36:52.634546: Epoch 300\n",
      "2025-12-19 20:36:52.634546: Current learning rate: 0.00725\n",
      "2025-12-19 20:39:10.783603: train_loss -0.8231\n",
      "2025-12-19 20:39:10.783603: val_loss -0.8688\n",
      "2025-12-19 20:39:10.783603: Pseudo dice [0.926, 0.9575, 0.9357]\n",
      "2025-12-19 20:39:10.783603: Epoch time: 138.15 s\n",
      "2025-12-19 20:39:10.783603: Yayy! New best EMA pseudo Dice: 0.9358\n",
      "2025-12-19 20:39:11.682689: \n",
      "2025-12-19 20:39:11.682689: Epoch 301\n",
      "2025-12-19 20:39:11.682689: Current learning rate: 0.00724\n",
      "2025-12-19 20:41:29.841250: train_loss -0.8287\n",
      "2025-12-19 20:41:29.841250: val_loss -0.867\n",
      "2025-12-19 20:41:29.841250: Pseudo dice [0.9256, 0.9567, 0.93]\n",
      "2025-12-19 20:41:29.841250: Epoch time: 138.16 s\n",
      "2025-12-19 20:41:29.841250: Yayy! New best EMA pseudo Dice: 0.936\n",
      "2025-12-19 20:41:30.768947: \n",
      "2025-12-19 20:41:30.768947: Epoch 302\n",
      "2025-12-19 20:41:30.768947: Current learning rate: 0.00724\n",
      "2025-12-19 20:43:48.860379: train_loss -0.841\n",
      "2025-12-19 20:43:48.864683: val_loss -0.8651\n",
      "2025-12-19 20:43:48.870429: Pseudo dice [0.923, 0.9597, 0.9256]\n",
      "2025-12-19 20:43:48.874436: Epoch time: 138.09 s\n",
      "2025-12-19 20:43:48.878443: Yayy! New best EMA pseudo Dice: 0.936\n",
      "2025-12-19 20:43:49.787377: \n",
      "2025-12-19 20:43:49.787377: Epoch 303\n",
      "2025-12-19 20:43:49.787377: Current learning rate: 0.00723\n",
      "2025-12-19 20:46:08.066914: train_loss -0.8356\n",
      "2025-12-19 20:46:08.066914: val_loss -0.8605\n",
      "2025-12-19 20:46:08.071388: Pseudo dice [0.915, 0.9509, 0.9382]\n",
      "2025-12-19 20:46:08.073258: Epoch time: 138.28 s\n",
      "2025-12-19 20:46:08.886969: \n",
      "2025-12-19 20:46:08.886969: Epoch 304\n",
      "2025-12-19 20:46:08.886969: Current learning rate: 0.00722\n",
      "2025-12-19 20:48:26.880410: train_loss -0.8358\n",
      "2025-12-19 20:48:26.882413: val_loss -0.8683\n",
      "2025-12-19 20:48:26.884548: Pseudo dice [0.9244, 0.9581, 0.9289]\n",
      "2025-12-19 20:48:26.884548: Epoch time: 137.99 s\n",
      "2025-12-19 20:48:27.513691: \n",
      "2025-12-19 20:48:27.513691: Epoch 305\n",
      "2025-12-19 20:48:27.517204: Current learning rate: 0.00721\n",
      "2025-12-19 20:50:45.512209: train_loss -0.835\n",
      "2025-12-19 20:50:45.512209: val_loss -0.8715\n",
      "2025-12-19 20:50:45.525455: Pseudo dice [0.929, 0.9564, 0.933]\n",
      "2025-12-19 20:50:45.527457: Epoch time: 138.0 s\n",
      "2025-12-19 20:50:45.527457: Yayy! New best EMA pseudo Dice: 0.9363\n",
      "2025-12-19 20:50:46.457733: \n",
      "2025-12-19 20:50:46.457733: Epoch 306\n",
      "2025-12-19 20:50:46.473825: Current learning rate: 0.0072\n",
      "2025-12-19 20:53:04.672740: train_loss -0.8326\n",
      "2025-12-19 20:53:04.674742: val_loss -0.8717\n",
      "2025-12-19 20:53:04.676746: Pseudo dice [0.9246, 0.96, 0.9366]\n",
      "2025-12-19 20:53:04.680750: Epoch time: 138.22 s\n",
      "2025-12-19 20:53:04.682751: Yayy! New best EMA pseudo Dice: 0.9368\n",
      "2025-12-19 20:53:05.569955: \n",
      "2025-12-19 20:53:05.569955: Epoch 307\n",
      "2025-12-19 20:53:05.569955: Current learning rate: 0.00719\n",
      "2025-12-19 20:55:23.633708: train_loss -0.8348\n",
      "2025-12-19 20:55:23.635710: val_loss -0.8633\n",
      "2025-12-19 20:55:23.639454: Pseudo dice [0.9193, 0.9543, 0.9282]\n",
      "2025-12-19 20:55:23.642275: Epoch time: 138.06 s\n",
      "2025-12-19 20:55:24.274384: \n",
      "2025-12-19 20:55:24.274384: Epoch 308\n",
      "2025-12-19 20:55:24.274384: Current learning rate: 0.00718\n",
      "2025-12-19 20:57:42.256546: train_loss -0.8416\n",
      "2025-12-19 20:57:42.256546: val_loss -0.8602\n",
      "2025-12-19 20:57:42.256546: Pseudo dice [0.9163, 0.9513, 0.9371]\n",
      "2025-12-19 20:57:42.271796: Epoch time: 137.98 s\n",
      "2025-12-19 20:57:42.905248: \n",
      "2025-12-19 20:57:42.905248: Epoch 309\n",
      "2025-12-19 20:57:42.905248: Current learning rate: 0.00717\n",
      "2025-12-19 21:00:00.914076: train_loss -0.8374\n",
      "2025-12-19 21:00:00.914076: val_loss -0.8663\n",
      "2025-12-19 21:00:00.914076: Pseudo dice [0.9219, 0.9576, 0.9307]\n",
      "2025-12-19 21:00:00.927385: Epoch time: 138.01 s\n",
      "2025-12-19 21:00:01.736272: \n",
      "2025-12-19 21:00:01.736272: Epoch 310\n",
      "2025-12-19 21:00:01.736272: Current learning rate: 0.00716\n",
      "2025-12-19 21:02:19.623730: train_loss -0.8411\n",
      "2025-12-19 21:02:19.625733: val_loss -0.8678\n",
      "2025-12-19 21:02:19.625733: Pseudo dice [0.9215, 0.9544, 0.9406]\n",
      "2025-12-19 21:02:19.631443: Epoch time: 137.9 s\n",
      "2025-12-19 21:02:20.300774: \n",
      "2025-12-19 21:02:20.300774: Epoch 311\n",
      "2025-12-19 21:02:20.300774: Current learning rate: 0.00715\n",
      "2025-12-19 21:04:38.407779: train_loss -0.838\n",
      "2025-12-19 21:04:38.407779: val_loss -0.8609\n",
      "2025-12-19 21:04:38.415256: Pseudo dice [0.9158, 0.9516, 0.9389]\n",
      "2025-12-19 21:04:38.420112: Epoch time: 138.11 s\n",
      "2025-12-19 21:04:39.061792: \n",
      "2025-12-19 21:04:39.061792: Epoch 312\n",
      "2025-12-19 21:04:39.066119: Current learning rate: 0.00714\n",
      "2025-12-19 21:06:57.010696: train_loss -0.841\n",
      "2025-12-19 21:06:57.010696: val_loss -0.8709\n",
      "2025-12-19 21:06:57.014700: Pseudo dice [0.9234, 0.9554, 0.9336]\n",
      "2025-12-19 21:06:57.016702: Epoch time: 137.95 s\n",
      "2025-12-19 21:06:57.665474: \n",
      "2025-12-19 21:06:57.665474: Epoch 313\n",
      "2025-12-19 21:06:57.665474: Current learning rate: 0.00713\n",
      "2025-12-19 21:09:15.807988: train_loss -0.8438\n",
      "2025-12-19 21:09:15.807988: val_loss -0.8594\n",
      "2025-12-19 21:09:15.811598: Pseudo dice [0.9155, 0.9517, 0.9354]\n",
      "2025-12-19 21:09:15.815389: Epoch time: 138.14 s\n",
      "2025-12-19 21:09:16.493145: \n",
      "2025-12-19 21:09:16.493145: Epoch 314\n",
      "2025-12-19 21:09:16.493145: Current learning rate: 0.00712\n",
      "2025-12-19 21:11:34.754318: train_loss -0.8374\n",
      "2025-12-19 21:11:34.754318: val_loss -0.8637\n",
      "2025-12-19 21:11:34.757890: Pseudo dice [0.9191, 0.9588, 0.9346]\n",
      "2025-12-19 21:11:34.761894: Epoch time: 138.26 s\n",
      "2025-12-19 21:11:35.420868: \n",
      "2025-12-19 21:11:35.420868: Epoch 315\n",
      "2025-12-19 21:11:35.420868: Current learning rate: 0.00711\n",
      "2025-12-19 21:13:53.483600: train_loss -0.8365\n",
      "2025-12-19 21:13:53.483600: val_loss -0.8718\n",
      "2025-12-19 21:13:53.499522: Pseudo dice [0.9228, 0.9549, 0.9402]\n",
      "2025-12-19 21:13:53.499522: Epoch time: 138.06 s\n",
      "2025-12-19 21:13:54.121056: \n",
      "2025-12-19 21:13:54.121056: Epoch 316\n",
      "2025-12-19 21:13:54.137134: Current learning rate: 0.0071\n",
      "2025-12-19 21:16:12.130033: train_loss -0.8387\n",
      "2025-12-19 21:16:12.130033: val_loss -0.8742\n",
      "2025-12-19 21:16:12.134037: Pseudo dice [0.9292, 0.9591, 0.9332]\n",
      "2025-12-19 21:16:12.136039: Epoch time: 138.01 s\n",
      "2025-12-19 21:16:12.140044: Yayy! New best EMA pseudo Dice: 0.9371\n",
      "2025-12-19 21:16:13.289953: \n",
      "2025-12-19 21:16:13.289953: Epoch 317\n",
      "2025-12-19 21:16:13.289953: Current learning rate: 0.0071\n",
      "2025-12-19 21:18:31.359018: train_loss -0.8398\n",
      "2025-12-19 21:18:31.361022: val_loss -0.8772\n",
      "2025-12-19 21:18:31.368541: Pseudo dice [0.9361, 0.9597, 0.9344]\n",
      "2025-12-19 21:18:31.372553: Epoch time: 138.07 s\n",
      "2025-12-19 21:18:31.374556: Yayy! New best EMA pseudo Dice: 0.9377\n",
      "2025-12-19 21:18:32.282108: \n",
      "2025-12-19 21:18:32.282108: Epoch 318\n",
      "2025-12-19 21:18:32.282108: Current learning rate: 0.00709\n",
      "2025-12-19 21:20:50.320129: train_loss -0.8429\n",
      "2025-12-19 21:20:50.320129: val_loss -0.8656\n",
      "2025-12-19 21:20:50.320129: Pseudo dice [0.9189, 0.9525, 0.9342]\n",
      "2025-12-19 21:20:50.327025: Epoch time: 138.05 s\n",
      "2025-12-19 21:20:50.986997: \n",
      "2025-12-19 21:20:50.986997: Epoch 319\n",
      "2025-12-19 21:20:50.986997: Current learning rate: 0.00708\n",
      "2025-12-19 21:23:09.074582: train_loss -0.84\n",
      "2025-12-19 21:23:09.074582: val_loss -0.8682\n",
      "2025-12-19 21:23:09.078587: Pseudo dice [0.9233, 0.9558, 0.9281]\n",
      "2025-12-19 21:23:09.084333: Epoch time: 138.09 s\n",
      "2025-12-19 21:23:09.842655: \n",
      "2025-12-19 21:23:09.842655: Epoch 320\n",
      "2025-12-19 21:23:09.842655: Current learning rate: 0.00707\n",
      "2025-12-19 21:25:27.935385: train_loss -0.8383\n",
      "2025-12-19 21:25:27.935385: val_loss -0.8633\n",
      "2025-12-19 21:25:27.951150: Pseudo dice [0.9186, 0.9537, 0.9318]\n",
      "2025-12-19 21:25:27.955957: Epoch time: 138.09 s\n",
      "2025-12-19 21:25:28.586017: \n",
      "2025-12-19 21:25:28.601839: Epoch 321\n",
      "2025-12-19 21:25:28.601839: Current learning rate: 0.00706\n",
      "2025-12-19 21:27:46.359950: train_loss -0.8421\n",
      "2025-12-19 21:27:46.361954: val_loss -0.8641\n",
      "2025-12-19 21:27:46.365960: Pseudo dice [0.9176, 0.9515, 0.9377]\n",
      "2025-12-19 21:27:46.368518: Epoch time: 137.77 s\n",
      "2025-12-19 21:27:47.162663: \n",
      "2025-12-19 21:27:47.162663: Epoch 322\n",
      "2025-12-19 21:27:47.171468: Current learning rate: 0.00705\n",
      "2025-12-19 21:30:05.203121: train_loss -0.8259\n",
      "2025-12-19 21:30:05.203121: val_loss -0.8482\n",
      "2025-12-19 21:30:05.207126: Pseudo dice [0.9157, 0.9511, 0.9321]\n",
      "2025-12-19 21:30:05.211132: Epoch time: 138.04 s\n",
      "2025-12-19 21:30:05.974046: \n",
      "2025-12-19 21:30:05.974046: Epoch 323\n",
      "2025-12-19 21:30:05.974046: Current learning rate: 0.00704\n",
      "2025-12-19 21:32:23.977143: train_loss -0.8141\n",
      "2025-12-19 21:32:23.977143: val_loss -0.8489\n",
      "2025-12-19 21:32:23.981148: Pseudo dice [0.9144, 0.9499, 0.928]\n",
      "2025-12-19 21:32:23.984642: Epoch time: 138.0 s\n",
      "2025-12-19 21:32:24.636912: \n",
      "2025-12-19 21:32:24.636912: Epoch 324\n",
      "2025-12-19 21:32:24.636912: Current learning rate: 0.00703\n",
      "2025-12-19 21:34:42.632961: train_loss -0.8231\n",
      "2025-12-19 21:34:42.632961: val_loss -0.8623\n",
      "2025-12-19 21:34:42.632961: Pseudo dice [0.9223, 0.9565, 0.9221]\n",
      "2025-12-19 21:34:42.639526: Epoch time: 138.01 s\n",
      "2025-12-19 21:34:43.267116: \n",
      "2025-12-19 21:34:43.267116: Epoch 325\n",
      "2025-12-19 21:34:43.267116: Current learning rate: 0.00702\n",
      "2025-12-19 21:37:01.318248: train_loss -0.8206\n",
      "2025-12-19 21:37:01.318248: val_loss -0.8484\n",
      "2025-12-19 21:37:01.318248: Pseudo dice [0.9108, 0.9514, 0.9295]\n",
      "2025-12-19 21:37:01.326966: Epoch time: 138.05 s\n",
      "2025-12-19 21:37:02.062827: \n",
      "2025-12-19 21:37:02.062827: Epoch 326\n",
      "2025-12-19 21:37:02.074042: Current learning rate: 0.00701\n",
      "2025-12-19 21:39:19.973580: train_loss -0.8271\n",
      "2025-12-19 21:39:19.973580: val_loss -0.8538\n",
      "2025-12-19 21:39:19.988436: Pseudo dice [0.9195, 0.9536, 0.9197]\n",
      "2025-12-19 21:39:19.989439: Epoch time: 137.91 s\n",
      "2025-12-19 21:39:20.623686: \n",
      "2025-12-19 21:39:20.623686: Epoch 327\n",
      "2025-12-19 21:39:20.638430: Current learning rate: 0.007\n",
      "2025-12-19 21:41:38.699546: train_loss -0.8075\n",
      "2025-12-19 21:41:38.699546: val_loss -0.8301\n",
      "2025-12-19 21:41:38.699546: Pseudo dice [0.9012, 0.9416, 0.9295]\n",
      "2025-12-19 21:41:38.699546: Epoch time: 138.08 s\n",
      "2025-12-19 21:41:39.348595: \n",
      "2025-12-19 21:41:39.348595: Epoch 328\n",
      "2025-12-19 21:41:39.348595: Current learning rate: 0.00699\n",
      "2025-12-19 21:43:57.396701: train_loss -0.8155\n",
      "2025-12-19 21:43:57.396701: val_loss -0.8609\n",
      "2025-12-19 21:43:57.400707: Pseudo dice [0.9243, 0.9561, 0.9283]\n",
      "2025-12-19 21:43:57.404711: Epoch time: 138.05 s\n",
      "2025-12-19 21:43:58.361509: \n",
      "2025-12-19 21:43:58.361509: Epoch 329\n",
      "2025-12-19 21:43:58.361509: Current learning rate: 0.00698\n",
      "2025-12-19 21:46:16.270543: train_loss -0.8298\n",
      "2025-12-19 21:46:16.270543: val_loss -0.8668\n",
      "2025-12-19 21:46:16.283813: Pseudo dice [0.9249, 0.9519, 0.9324]\n",
      "2025-12-19 21:46:16.286439: Epoch time: 137.91 s\n",
      "2025-12-19 21:46:16.921606: \n",
      "2025-12-19 21:46:16.921606: Epoch 330\n",
      "2025-12-19 21:46:16.921606: Current learning rate: 0.00697\n",
      "2025-12-19 21:48:35.003967: train_loss -0.8348\n",
      "2025-12-19 21:48:35.003967: val_loss -0.8599\n",
      "2025-12-19 21:48:35.009049: Pseudo dice [0.9176, 0.9525, 0.9335]\n",
      "2025-12-19 21:48:35.012355: Epoch time: 138.08 s\n",
      "2025-12-19 21:48:35.639206: \n",
      "2025-12-19 21:48:35.639206: Epoch 331\n",
      "2025-12-19 21:48:35.643765: Current learning rate: 0.00696\n",
      "2025-12-19 21:50:53.750119: train_loss -0.8294\n",
      "2025-12-19 21:50:53.750119: val_loss -0.8705\n",
      "2025-12-19 21:50:53.755580: Pseudo dice [0.9294, 0.9575, 0.9356]\n",
      "2025-12-19 21:50:53.759122: Epoch time: 138.11 s\n",
      "2025-12-19 21:50:54.445886: \n",
      "2025-12-19 21:50:54.445886: Epoch 332\n",
      "2025-12-19 21:50:54.445886: Current learning rate: 0.00696\n",
      "2025-12-19 21:53:12.574882: train_loss -0.8345\n",
      "2025-12-19 21:53:12.574882: val_loss -0.8696\n",
      "2025-12-19 21:53:12.574882: Pseudo dice [0.9218, 0.9567, 0.9425]\n",
      "2025-12-19 21:53:12.590632: Epoch time: 138.13 s\n",
      "2025-12-19 21:53:13.207630: \n",
      "2025-12-19 21:53:13.207630: Epoch 333\n",
      "2025-12-19 21:53:13.223369: Current learning rate: 0.00695\n",
      "2025-12-19 21:55:31.152077: train_loss -0.8316\n",
      "2025-12-19 21:55:31.152077: val_loss -0.8641\n",
      "2025-12-19 21:55:31.167857: Pseudo dice [0.92, 0.9502, 0.9368]\n",
      "2025-12-19 21:55:31.170362: Epoch time: 137.94 s\n",
      "2025-12-19 21:55:31.816369: \n",
      "2025-12-19 21:55:31.816369: Epoch 334\n",
      "2025-12-19 21:55:31.816369: Current learning rate: 0.00694\n",
      "2025-12-19 21:57:49.804764: train_loss -0.8351\n",
      "2025-12-19 21:57:49.804764: val_loss -0.8726\n",
      "2025-12-19 21:57:49.820400: Pseudo dice [0.9255, 0.9555, 0.9405]\n",
      "2025-12-19 21:57:49.820400: Epoch time: 137.99 s\n",
      "2025-12-19 21:57:50.628080: \n",
      "2025-12-19 21:57:50.628080: Epoch 335\n",
      "2025-12-19 21:57:50.628080: Current learning rate: 0.00693\n",
      "2025-12-19 22:00:08.624587: train_loss -0.8336\n",
      "2025-12-19 22:00:08.626328: val_loss -0.8699\n",
      "2025-12-19 22:00:08.626328: Pseudo dice [0.9224, 0.9524, 0.9396]\n",
      "2025-12-19 22:00:08.631979: Epoch time: 138.0 s\n",
      "2025-12-19 22:00:09.273234: \n",
      "2025-12-19 22:00:09.273234: Epoch 336\n",
      "2025-12-19 22:00:09.273234: Current learning rate: 0.00692\n",
      "2025-12-19 22:02:27.172841: train_loss -0.8366\n",
      "2025-12-19 22:02:27.172841: val_loss -0.8607\n",
      "2025-12-19 22:02:27.176674: Pseudo dice [0.9176, 0.9545, 0.9282]\n",
      "2025-12-19 22:02:27.176674: Epoch time: 137.9 s\n",
      "2025-12-19 22:02:27.822613: \n",
      "2025-12-19 22:02:27.822613: Epoch 337\n",
      "2025-12-19 22:02:27.822613: Current learning rate: 0.00691\n",
      "2025-12-19 22:04:45.979273: train_loss -0.8369\n",
      "2025-12-19 22:04:45.985020: val_loss -0.8642\n",
      "2025-12-19 22:04:45.985020: Pseudo dice [0.9207, 0.9516, 0.9333]\n",
      "2025-12-19 22:04:45.990512: Epoch time: 138.16 s\n",
      "2025-12-19 22:04:46.629088: \n",
      "2025-12-19 22:04:46.629088: Epoch 338\n",
      "2025-12-19 22:04:46.629088: Current learning rate: 0.0069\n",
      "2025-12-19 22:07:04.665983: train_loss -0.8428\n",
      "2025-12-19 22:07:04.665983: val_loss -0.8725\n",
      "2025-12-19 22:07:04.676765: Pseudo dice [0.9283, 0.9606, 0.9308]\n",
      "2025-12-19 22:07:04.679881: Epoch time: 138.04 s\n",
      "2025-12-19 22:07:05.299613: \n",
      "2025-12-19 22:07:05.299613: Epoch 339\n",
      "2025-12-19 22:07:05.315640: Current learning rate: 0.00689\n",
      "2025-12-19 22:09:23.515485: train_loss -0.8371\n",
      "2025-12-19 22:09:23.515485: val_loss -0.865\n",
      "2025-12-19 22:09:23.515485: Pseudo dice [0.9188, 0.9488, 0.9375]\n",
      "2025-12-19 22:09:23.522660: Epoch time: 138.22 s\n",
      "2025-12-19 22:09:24.336667: \n",
      "2025-12-19 22:09:24.336667: Epoch 340\n",
      "2025-12-19 22:09:24.349591: Current learning rate: 0.00688\n",
      "2025-12-19 22:11:42.621188: train_loss -0.8389\n",
      "2025-12-19 22:11:42.621188: val_loss -0.8722\n",
      "2025-12-19 22:11:42.621188: Pseudo dice [0.9256, 0.9554, 0.936]\n",
      "2025-12-19 22:11:42.621188: Epoch time: 138.28 s\n",
      "2025-12-19 22:11:43.268181: \n",
      "2025-12-19 22:11:43.268181: Epoch 341\n",
      "2025-12-19 22:11:43.284039: Current learning rate: 0.00687\n",
      "2025-12-19 22:14:01.345684: train_loss -0.8418\n",
      "2025-12-19 22:14:01.345684: val_loss -0.8727\n",
      "2025-12-19 22:14:01.361424: Pseudo dice [0.9249, 0.9579, 0.9399]\n",
      "2025-12-19 22:14:01.361424: Epoch time: 138.08 s\n",
      "2025-12-19 22:14:02.027312: \n",
      "2025-12-19 22:14:02.027312: Epoch 342\n",
      "2025-12-19 22:14:02.042995: Current learning rate: 0.00686\n",
      "2025-12-19 22:16:20.276899: train_loss -0.8386\n",
      "2025-12-19 22:16:20.276899: val_loss -0.876\n",
      "2025-12-19 22:16:20.280905: Pseudo dice [0.9302, 0.9566, 0.9359]\n",
      "2025-12-19 22:16:20.284025: Epoch time: 138.25 s\n",
      "2025-12-19 22:16:20.937018: \n",
      "2025-12-19 22:16:20.937018: Epoch 343\n",
      "2025-12-19 22:16:20.937018: Current learning rate: 0.00685\n",
      "2025-12-19 22:18:39.053977: train_loss -0.8428\n",
      "2025-12-19 22:18:39.053977: val_loss -0.8706\n",
      "2025-12-19 22:18:39.057719: Pseudo dice [0.9226, 0.9591, 0.936]\n",
      "2025-12-19 22:18:39.061723: Epoch time: 138.13 s\n",
      "2025-12-19 22:18:39.722863: \n",
      "2025-12-19 22:18:39.722863: Epoch 344\n",
      "2025-12-19 22:18:39.722863: Current learning rate: 0.00684\n",
      "2025-12-19 22:20:57.865889: train_loss -0.8409\n",
      "2025-12-19 22:20:57.865889: val_loss -0.8668\n",
      "2025-12-19 22:20:57.865889: Pseudo dice [0.9213, 0.9578, 0.9332]\n",
      "2025-12-19 22:20:57.865889: Epoch time: 138.16 s\n",
      "2025-12-19 22:20:58.562295: \n",
      "2025-12-19 22:20:58.562295: Epoch 345\n",
      "2025-12-19 22:20:58.562295: Current learning rate: 0.00683\n",
      "2025-12-19 22:23:17.165154: train_loss -0.838\n",
      "2025-12-19 22:23:17.165154: val_loss -0.8691\n",
      "2025-12-19 22:23:17.167156: Pseudo dice [0.9197, 0.9521, 0.9415]\n",
      "2025-12-19 22:23:17.171452: Epoch time: 138.6 s\n",
      "2025-12-19 22:23:18.007717: \n",
      "2025-12-19 22:23:18.007717: Epoch 346\n",
      "2025-12-19 22:23:18.007717: Current learning rate: 0.00682\n",
      "2025-12-19 22:25:37.013307: train_loss -0.8416\n",
      "2025-12-19 22:25:37.013307: val_loss -0.8702\n",
      "2025-12-19 22:25:37.017313: Pseudo dice [0.9249, 0.9543, 0.9345]\n",
      "2025-12-19 22:25:37.021317: Epoch time: 139.01 s\n",
      "2025-12-19 22:25:37.659103: \n",
      "2025-12-19 22:25:37.659103: Epoch 347\n",
      "2025-12-19 22:25:37.659103: Current learning rate: 0.00681\n",
      "2025-12-19 22:27:56.247918: train_loss -0.8384\n",
      "2025-12-19 22:27:56.249920: val_loss -0.871\n",
      "2025-12-19 22:27:56.255668: Pseudo dice [0.9227, 0.9536, 0.9396]\n",
      "2025-12-19 22:27:56.257669: Epoch time: 138.59 s\n",
      "2025-12-19 22:27:56.896294: \n",
      "2025-12-19 22:27:56.896294: Epoch 348\n",
      "2025-12-19 22:27:56.910436: Current learning rate: 0.0068\n",
      "2025-12-19 22:30:15.490221: train_loss -0.8396\n",
      "2025-12-19 22:30:15.492223: val_loss -0.8748\n",
      "2025-12-19 22:30:15.494993: Pseudo dice [0.9285, 0.9561, 0.9426]\n",
      "2025-12-19 22:30:15.499593: Epoch time: 138.59 s\n",
      "2025-12-19 22:30:15.502570: Yayy! New best EMA pseudo Dice: 0.9381\n",
      "2025-12-19 22:30:16.396778: \n",
      "2025-12-19 22:30:16.396778: Epoch 349\n",
      "2025-12-19 22:30:16.412667: Current learning rate: 0.0068\n",
      "2025-12-19 22:32:34.945780: train_loss -0.8429\n",
      "2025-12-19 22:32:34.945780: val_loss -0.8722\n",
      "2025-12-19 22:32:34.949785: Pseudo dice [0.9284, 0.9555, 0.9312]\n",
      "2025-12-19 22:32:34.953648: Epoch time: 138.55 s\n",
      "2025-12-19 22:32:35.220980: Yayy! New best EMA pseudo Dice: 0.9381\n",
      "2025-12-19 22:32:36.290843: \n",
      "2025-12-19 22:32:36.290843: Epoch 350\n",
      "2025-12-19 22:32:36.306569: Current learning rate: 0.00679\n",
      "2025-12-19 22:34:54.448636: train_loss -0.843\n",
      "2025-12-19 22:34:54.448636: val_loss -0.8679\n",
      "2025-12-19 22:34:54.464328: Pseudo dice [0.9202, 0.9584, 0.935]\n",
      "2025-12-19 22:34:54.464328: Epoch time: 138.16 s\n",
      "2025-12-19 22:34:55.113793: \n",
      "2025-12-19 22:34:55.113793: Epoch 351\n",
      "2025-12-19 22:34:55.113793: Current learning rate: 0.00678\n",
      "2025-12-19 22:37:13.147365: train_loss -0.8424\n",
      "2025-12-19 22:37:13.147365: val_loss -0.8722\n",
      "2025-12-19 22:37:13.156833: Pseudo dice [0.9217, 0.9527, 0.9409]\n",
      "2025-12-19 22:37:13.156833: Epoch time: 138.05 s\n",
      "2025-12-19 22:37:13.156833: Yayy! New best EMA pseudo Dice: 0.9381\n",
      "2025-12-19 22:37:14.303622: \n",
      "2025-12-19 22:37:14.303622: Epoch 352\n",
      "2025-12-19 22:37:14.303622: Current learning rate: 0.00677\n",
      "2025-12-19 22:39:32.207982: train_loss -0.8397\n",
      "2025-12-19 22:39:32.207982: val_loss -0.8677\n",
      "2025-12-19 22:39:32.207982: Pseudo dice [0.9231, 0.9566, 0.929]\n",
      "2025-12-19 22:39:32.207982: Epoch time: 137.92 s\n",
      "2025-12-19 22:39:32.970092: \n",
      "2025-12-19 22:39:32.970092: Epoch 353\n",
      "2025-12-19 22:39:32.970092: Current learning rate: 0.00676\n",
      "2025-12-19 22:41:50.978278: train_loss -0.8422\n",
      "2025-12-19 22:41:50.980281: val_loss -0.8681\n",
      "2025-12-19 22:41:50.984285: Pseudo dice [0.9239, 0.9589, 0.9308]\n",
      "2025-12-19 22:41:50.987288: Epoch time: 138.01 s\n",
      "2025-12-19 22:41:51.618627: \n",
      "2025-12-19 22:41:51.618627: Epoch 354\n",
      "2025-12-19 22:41:51.634462: Current learning rate: 0.00675\n",
      "2025-12-19 22:44:09.878267: train_loss -0.8383\n",
      "2025-12-19 22:44:09.878267: val_loss -0.8754\n",
      "2025-12-19 22:44:09.882272: Pseudo dice [0.9256, 0.9561, 0.9389]\n",
      "2025-12-19 22:44:09.886277: Epoch time: 138.26 s\n",
      "2025-12-19 22:44:09.888133: Yayy! New best EMA pseudo Dice: 0.9382\n",
      "2025-12-19 22:44:10.780052: \n",
      "2025-12-19 22:44:10.780052: Epoch 355\n",
      "2025-12-19 22:44:10.780052: Current learning rate: 0.00674\n",
      "2025-12-19 22:46:28.691348: train_loss -0.8387\n",
      "2025-12-19 22:46:28.693351: val_loss -0.866\n",
      "2025-12-19 22:46:28.695353: Pseudo dice [0.9216, 0.951, 0.9341]\n",
      "2025-12-19 22:46:28.695353: Epoch time: 137.93 s\n",
      "2025-12-19 22:46:29.488580: \n",
      "2025-12-19 22:46:29.490582: Epoch 356\n",
      "2025-12-19 22:46:29.492585: Current learning rate: 0.00673\n",
      "2025-12-19 22:48:47.426624: train_loss -0.8394\n",
      "2025-12-19 22:48:47.426624: val_loss -0.864\n",
      "2025-12-19 22:48:47.430629: Pseudo dice [0.9209, 0.9555, 0.936]\n",
      "2025-12-19 22:48:47.436642: Epoch time: 137.94 s\n",
      "2025-12-19 22:48:48.074594: \n",
      "2025-12-19 22:48:48.074594: Epoch 357\n",
      "2025-12-19 22:48:48.074594: Current learning rate: 0.00672\n",
      "2025-12-19 22:51:06.139699: train_loss -0.8378\n",
      "2025-12-19 22:51:06.139699: val_loss -0.8771\n",
      "2025-12-19 22:51:06.139699: Pseudo dice [0.9294, 0.9598, 0.9403]\n",
      "2025-12-19 22:51:06.155772: Epoch time: 138.07 s\n",
      "2025-12-19 22:51:06.155772: Yayy! New best EMA pseudo Dice: 0.9384\n",
      "2025-12-19 22:51:07.230706: \n",
      "2025-12-19 22:51:07.230706: Epoch 358\n",
      "2025-12-19 22:51:07.230706: Current learning rate: 0.00671\n",
      "2025-12-19 22:53:25.193327: train_loss -0.8382\n",
      "2025-12-19 22:53:25.193327: val_loss -0.8665\n",
      "2025-12-19 22:53:25.197331: Pseudo dice [0.9217, 0.9545, 0.9371]\n",
      "2025-12-19 22:53:25.201335: Epoch time: 137.97 s\n",
      "2025-12-19 22:53:26.045942: \n",
      "2025-12-19 22:53:26.047945: Epoch 359\n",
      "2025-12-19 22:53:26.050341: Current learning rate: 0.0067\n",
      "2025-12-19 22:55:44.018260: train_loss -0.8414\n",
      "2025-12-19 22:55:44.020000: val_loss -0.8717\n",
      "2025-12-19 22:55:44.024305: Pseudo dice [0.9237, 0.9572, 0.936]\n",
      "2025-12-19 22:55:44.026308: Epoch time: 137.97 s\n",
      "2025-12-19 22:55:44.677927: \n",
      "2025-12-19 22:55:44.677927: Epoch 360\n",
      "2025-12-19 22:55:44.677927: Current learning rate: 0.00669\n",
      "2025-12-19 22:58:02.685958: train_loss -0.8394\n",
      "2025-12-19 22:58:02.685958: val_loss -0.8652\n",
      "2025-12-19 22:58:02.689964: Pseudo dice [0.9215, 0.956, 0.9258]\n",
      "2025-12-19 22:58:02.693968: Epoch time: 138.01 s\n",
      "2025-12-19 22:58:03.343677: \n",
      "2025-12-19 22:58:03.343677: Epoch 361\n",
      "2025-12-19 22:58:03.343677: Current learning rate: 0.00668\n",
      "2025-12-19 23:00:21.414802: train_loss -0.8361\n",
      "2025-12-19 23:00:21.414802: val_loss -0.8631\n",
      "2025-12-19 23:00:21.418807: Pseudo dice [0.9172, 0.9544, 0.9389]\n",
      "2025-12-19 23:00:21.422814: Epoch time: 138.07 s\n",
      "2025-12-19 23:00:22.060395: \n",
      "2025-12-19 23:00:22.076496: Epoch 362\n",
      "2025-12-19 23:00:22.079974: Current learning rate: 0.00667\n",
      "2025-12-19 23:02:40.111654: train_loss -0.8308\n",
      "2025-12-19 23:02:40.113657: val_loss -0.876\n",
      "2025-12-19 23:02:40.116517: Pseudo dice [0.932, 0.9592, 0.9342]\n",
      "2025-12-19 23:02:40.116517: Epoch time: 138.05 s\n",
      "2025-12-19 23:02:40.766691: \n",
      "2025-12-19 23:02:40.766691: Epoch 363\n",
      "2025-12-19 23:02:40.766691: Current learning rate: 0.00666\n",
      "2025-12-19 23:04:58.731554: train_loss -0.8287\n",
      "2025-12-19 23:04:58.731554: val_loss -0.8716\n",
      "2025-12-19 23:04:58.747252: Pseudo dice [0.9288, 0.9569, 0.9293]\n",
      "2025-12-19 23:04:58.747252: Epoch time: 137.96 s\n",
      "2025-12-19 23:04:59.380111: \n",
      "2025-12-19 23:04:59.380111: Epoch 364\n",
      "2025-12-19 23:04:59.380111: Current learning rate: 0.00665\n",
      "2025-12-19 23:07:17.362919: train_loss -0.8391\n",
      "2025-12-19 23:07:17.362919: val_loss -0.8653\n",
      "2025-12-19 23:07:17.379011: Pseudo dice [0.9213, 0.9533, 0.9342]\n",
      "2025-12-19 23:07:17.379011: Epoch time: 137.98 s\n",
      "2025-12-19 23:07:18.028779: \n",
      "2025-12-19 23:07:18.028779: Epoch 365\n",
      "2025-12-19 23:07:18.028779: Current learning rate: 0.00665\n",
      "2025-12-19 23:09:36.270874: train_loss -0.8442\n",
      "2025-12-19 23:09:36.270874: val_loss -0.8628\n",
      "2025-12-19 23:09:36.286655: Pseudo dice [0.9155, 0.9504, 0.9356]\n",
      "2025-12-19 23:09:36.289805: Epoch time: 138.24 s\n",
      "2025-12-19 23:09:36.953068: \n",
      "2025-12-19 23:09:36.953068: Epoch 366\n",
      "2025-12-19 23:09:36.953068: Current learning rate: 0.00664\n",
      "2025-12-19 23:11:54.998345: train_loss -0.8411\n",
      "2025-12-19 23:11:54.998345: val_loss -0.8657\n",
      "2025-12-19 23:11:55.004864: Pseudo dice [0.9219, 0.9535, 0.9308]\n",
      "2025-12-19 23:11:55.007484: Epoch time: 138.06 s\n",
      "2025-12-19 23:11:55.682831: \n",
      "2025-12-19 23:11:55.682831: Epoch 367\n",
      "2025-12-19 23:11:55.682831: Current learning rate: 0.00663\n",
      "2025-12-19 23:14:13.621062: train_loss -0.8416\n",
      "2025-12-19 23:14:13.621062: val_loss -0.8626\n",
      "2025-12-19 23:14:13.627078: Pseudo dice [0.9149, 0.9533, 0.9404]\n",
      "2025-12-19 23:14:13.630822: Epoch time: 137.94 s\n",
      "2025-12-19 23:14:14.279010: \n",
      "2025-12-19 23:14:14.279010: Epoch 368\n",
      "2025-12-19 23:14:14.279010: Current learning rate: 0.00662\n",
      "2025-12-19 23:16:32.307186: train_loss -0.8443\n",
      "2025-12-19 23:16:32.307186: val_loss -0.8646\n",
      "2025-12-19 23:16:32.314857: Pseudo dice [0.9186, 0.952, 0.9396]\n",
      "2025-12-19 23:16:32.316823: Epoch time: 138.03 s\n",
      "2025-12-19 23:16:33.115377: \n",
      "2025-12-19 23:16:33.115377: Epoch 369\n",
      "2025-12-19 23:16:33.127520: Current learning rate: 0.00661\n",
      "2025-12-19 23:18:51.189755: train_loss -0.8427\n",
      "2025-12-19 23:18:51.189755: val_loss -0.8698\n",
      "2025-12-19 23:18:51.195501: Pseudo dice [0.9227, 0.9528, 0.9368]\n",
      "2025-12-19 23:18:51.199506: Epoch time: 138.07 s\n",
      "2025-12-19 23:18:51.856676: \n",
      "2025-12-19 23:18:51.856676: Epoch 370\n",
      "2025-12-19 23:18:51.856676: Current learning rate: 0.0066\n",
      "2025-12-19 23:21:09.827373: train_loss -0.8463\n",
      "2025-12-19 23:21:09.827373: val_loss -0.8738\n",
      "2025-12-19 23:21:09.832735: Pseudo dice [0.9235, 0.959, 0.9395]\n",
      "2025-12-19 23:21:09.836589: Epoch time: 137.97 s\n",
      "2025-12-19 23:21:10.491475: \n",
      "2025-12-19 23:21:10.491475: Epoch 371\n",
      "2025-12-19 23:21:10.491475: Current learning rate: 0.00659\n",
      "2025-12-19 23:23:28.599669: train_loss -0.844\n",
      "2025-12-19 23:23:28.599669: val_loss -0.8643\n",
      "2025-12-19 23:23:28.607842: Pseudo dice [0.921, 0.9534, 0.9292]\n",
      "2025-12-19 23:23:28.610710: Epoch time: 138.11 s\n",
      "2025-12-19 23:23:29.267226: \n",
      "2025-12-19 23:23:29.267226: Epoch 372\n",
      "2025-12-19 23:23:29.267226: Current learning rate: 0.00658\n",
      "2025-12-19 23:25:47.509793: train_loss -0.8396\n",
      "2025-12-19 23:25:47.509793: val_loss -0.8628\n",
      "2025-12-19 23:25:47.520957: Pseudo dice [0.9196, 0.9468, 0.9345]\n",
      "2025-12-19 23:25:47.520957: Epoch time: 138.26 s\n",
      "2025-12-19 23:25:48.240904: \n",
      "2025-12-19 23:25:48.240904: Epoch 373\n",
      "2025-12-19 23:25:48.249496: Current learning rate: 0.00657\n",
      "2025-12-19 23:28:06.258200: train_loss -0.8237\n",
      "2025-12-19 23:28:06.258200: val_loss -0.8435\n",
      "2025-12-19 23:28:06.258200: Pseudo dice [0.9077, 0.943, 0.9321]\n",
      "2025-12-19 23:28:06.274111: Epoch time: 138.02 s\n",
      "2025-12-19 23:28:06.924062: \n",
      "2025-12-19 23:28:06.924062: Epoch 374\n",
      "2025-12-19 23:28:06.924062: Current learning rate: 0.00656\n",
      "2025-12-19 23:30:24.808759: train_loss -0.8306\n",
      "2025-12-19 23:30:24.808759: val_loss -0.8611\n",
      "2025-12-19 23:30:24.808759: Pseudo dice [0.9214, 0.9534, 0.9268]\n",
      "2025-12-19 23:30:24.808759: Epoch time: 137.88 s\n",
      "2025-12-19 23:30:25.649976: \n",
      "2025-12-19 23:30:25.649976: Epoch 375\n",
      "2025-12-19 23:30:25.649976: Current learning rate: 0.00655\n",
      "2025-12-19 23:32:43.991877: train_loss -0.8361\n",
      "2025-12-19 23:32:43.991877: val_loss -0.8656\n",
      "2025-12-19 23:32:44.007609: Pseudo dice [0.9224, 0.957, 0.9325]\n",
      "2025-12-19 23:32:44.007609: Epoch time: 138.34 s\n",
      "2025-12-19 23:32:44.799478: \n",
      "2025-12-19 23:32:44.807498: Epoch 376\n",
      "2025-12-19 23:32:44.807498: Current learning rate: 0.00654\n",
      "2025-12-19 23:35:02.947086: train_loss -0.8414\n",
      "2025-12-19 23:35:02.948826: val_loss -0.8699\n",
      "2025-12-19 23:35:02.952319: Pseudo dice [0.9232, 0.9551, 0.9376]\n",
      "2025-12-19 23:35:02.952319: Epoch time: 138.15 s\n",
      "2025-12-19 23:35:03.600477: \n",
      "2025-12-19 23:35:03.600477: Epoch 377\n",
      "2025-12-19 23:35:03.600477: Current learning rate: 0.00653\n",
      "2025-12-19 23:37:21.634027: train_loss -0.8332\n",
      "2025-12-19 23:37:21.634027: val_loss -0.8564\n",
      "2025-12-19 23:37:21.638033: Pseudo dice [0.9151, 0.9536, 0.9259]\n",
      "2025-12-19 23:37:21.642039: Epoch time: 138.03 s\n",
      "2025-12-19 23:37:22.297619: \n",
      "2025-12-19 23:37:22.297619: Epoch 378\n",
      "2025-12-19 23:37:22.305516: Current learning rate: 0.00652\n",
      "2025-12-19 23:39:40.206611: train_loss -0.8396\n",
      "2025-12-19 23:39:40.206611: val_loss -0.8673\n",
      "2025-12-19 23:39:40.210616: Pseudo dice [0.9181, 0.9557, 0.9453]\n",
      "2025-12-19 23:39:40.214621: Epoch time: 137.91 s\n",
      "2025-12-19 23:39:40.940618: \n",
      "2025-12-19 23:39:40.940618: Epoch 379\n",
      "2025-12-19 23:39:40.940618: Current learning rate: 0.00651\n",
      "2025-12-19 23:41:58.946625: train_loss -0.8403\n",
      "2025-12-19 23:41:58.946625: val_loss -0.8619\n",
      "2025-12-19 23:41:58.960468: Pseudo dice [0.9153, 0.9505, 0.9365]\n",
      "2025-12-19 23:41:58.964473: Epoch time: 138.01 s\n",
      "2025-12-19 23:41:59.593845: \n",
      "2025-12-19 23:41:59.609580: Epoch 380\n",
      "2025-12-19 23:41:59.609580: Current learning rate: 0.0065\n",
      "2025-12-19 23:44:17.651780: train_loss -0.8435\n",
      "2025-12-19 23:44:17.651780: val_loss -0.8666\n",
      "2025-12-19 23:44:17.651780: Pseudo dice [0.9202, 0.9506, 0.9378]\n",
      "2025-12-19 23:44:17.651780: Epoch time: 138.06 s\n",
      "2025-12-19 23:44:18.462108: \n",
      "2025-12-19 23:44:18.462108: Epoch 381\n",
      "2025-12-19 23:44:18.462108: Current learning rate: 0.00649\n",
      "2025-12-19 23:46:36.343838: train_loss -0.8385\n",
      "2025-12-19 23:46:36.343838: val_loss -0.8667\n",
      "2025-12-19 23:46:36.357881: Pseudo dice [0.9202, 0.952, 0.9369]\n",
      "2025-12-19 23:46:36.359884: Epoch time: 137.88 s\n",
      "2025-12-19 23:46:37.133597: \n",
      "2025-12-19 23:46:37.133597: Epoch 382\n",
      "2025-12-19 23:46:37.133597: Current learning rate: 0.00648\n",
      "2025-12-19 23:48:55.082322: train_loss -0.8396\n",
      "2025-12-19 23:48:55.082322: val_loss -0.8697\n",
      "2025-12-19 23:48:55.088328: Pseudo dice [0.9245, 0.9575, 0.9313]\n",
      "2025-12-19 23:48:55.092332: Epoch time: 137.95 s\n",
      "2025-12-19 23:48:55.748244: \n",
      "2025-12-19 23:48:55.748244: Epoch 383\n",
      "2025-12-19 23:48:55.748244: Current learning rate: 0.00648\n",
      "2025-12-19 23:51:13.758383: train_loss -0.8386\n",
      "2025-12-19 23:51:13.758383: val_loss -0.8687\n",
      "2025-12-19 23:51:13.774091: Pseudo dice [0.9252, 0.9542, 0.9317]\n",
      "2025-12-19 23:51:13.774091: Epoch time: 138.01 s\n",
      "2025-12-19 23:51:14.425871: \n",
      "2025-12-19 23:51:14.425871: Epoch 384\n",
      "2025-12-19 23:51:14.425871: Current learning rate: 0.00647\n",
      "2025-12-19 23:53:32.448318: train_loss -0.8421\n",
      "2025-12-19 23:53:32.450155: val_loss -0.8632\n",
      "2025-12-19 23:53:32.452157: Pseudo dice [0.9159, 0.9494, 0.9354]\n",
      "2025-12-19 23:53:32.456786: Epoch time: 138.02 s\n",
      "2025-12-19 23:53:33.247251: \n",
      "2025-12-19 23:53:33.247251: Epoch 385\n",
      "2025-12-19 23:53:33.249254: Current learning rate: 0.00646\n",
      "2025-12-19 23:55:51.450878: train_loss -0.8442\n",
      "2025-12-19 23:55:51.450878: val_loss -0.8767\n",
      "2025-12-19 23:55:51.456886: Pseudo dice [0.927, 0.9564, 0.9459]\n",
      "2025-12-19 23:55:51.460890: Epoch time: 138.21 s\n",
      "2025-12-19 23:55:52.129565: \n",
      "2025-12-19 23:55:52.129565: Epoch 386\n",
      "2025-12-19 23:55:52.129565: Current learning rate: 0.00645\n",
      "2025-12-19 23:58:10.357249: train_loss -0.8407\n",
      "2025-12-19 23:58:10.357249: val_loss -0.8682\n",
      "2025-12-19 23:58:10.361252: Pseudo dice [0.9209, 0.9563, 0.9365]\n",
      "2025-12-19 23:58:10.363254: Epoch time: 138.23 s\n",
      "2025-12-19 23:58:11.182885: \n",
      "2025-12-19 23:58:11.182885: Epoch 387\n",
      "2025-12-19 23:58:11.182885: Current learning rate: 0.00644\n",
      "2025-12-20 00:00:29.240540: train_loss -0.8369\n",
      "2025-12-20 00:00:29.240540: val_loss -0.8756\n",
      "2025-12-20 00:00:29.247234: Pseudo dice [0.9274, 0.9586, 0.9313]\n",
      "2025-12-20 00:00:29.247234: Epoch time: 138.06 s\n",
      "2025-12-20 00:00:30.028061: \n",
      "2025-12-20 00:00:30.028061: Epoch 388\n",
      "2025-12-20 00:00:30.028061: Current learning rate: 0.00643\n",
      "2025-12-20 00:02:47.965628: train_loss -0.8428\n",
      "2025-12-20 00:02:47.965628: val_loss -0.8719\n",
      "2025-12-20 00:02:47.965628: Pseudo dice [0.9221, 0.9541, 0.9397]\n",
      "2025-12-20 00:02:47.965628: Epoch time: 137.94 s\n",
      "2025-12-20 00:02:48.617888: \n",
      "2025-12-20 00:02:48.617888: Epoch 389\n",
      "2025-12-20 00:02:48.617888: Current learning rate: 0.00642\n",
      "2025-12-20 00:05:06.629512: train_loss -0.8441\n",
      "2025-12-20 00:05:06.629512: val_loss -0.874\n",
      "2025-12-20 00:05:06.645271: Pseudo dice [0.9246, 0.9541, 0.9414]\n",
      "2025-12-20 00:05:06.645271: Epoch time: 138.01 s\n",
      "2025-12-20 00:05:07.294608: \n",
      "2025-12-20 00:05:07.294608: Epoch 390\n",
      "2025-12-20 00:05:07.294608: Current learning rate: 0.00641\n",
      "2025-12-20 00:07:25.342449: train_loss -0.844\n",
      "2025-12-20 00:07:25.344451: val_loss -0.8642\n",
      "2025-12-20 00:07:25.344451: Pseudo dice [0.9183, 0.9533, 0.9292]\n",
      "2025-12-20 00:07:25.344451: Epoch time: 138.05 s\n",
      "2025-12-20 00:07:26.008406: \n",
      "2025-12-20 00:07:26.008406: Epoch 391\n",
      "2025-12-20 00:07:26.008406: Current learning rate: 0.0064\n",
      "2025-12-20 00:09:44.433871: train_loss -0.8434\n",
      "2025-12-20 00:09:44.433871: val_loss -0.8733\n",
      "2025-12-20 00:09:44.433871: Pseudo dice [0.9291, 0.96, 0.9337]\n",
      "2025-12-20 00:09:44.433871: Epoch time: 138.44 s\n",
      "2025-12-20 00:09:45.084725: \n",
      "2025-12-20 00:09:45.084725: Epoch 392\n",
      "2025-12-20 00:09:45.084725: Current learning rate: 0.00639\n",
      "2025-12-20 00:12:03.215966: train_loss -0.8428\n",
      "2025-12-20 00:12:03.217969: val_loss -0.8744\n",
      "2025-12-20 00:12:03.221973: Pseudo dice [0.9233, 0.9578, 0.9429]\n",
      "2025-12-20 00:12:03.223975: Epoch time: 138.13 s\n",
      "2025-12-20 00:12:04.056415: \n",
      "2025-12-20 00:12:04.056415: Epoch 393\n",
      "2025-12-20 00:12:04.056415: Current learning rate: 0.00638\n",
      "2025-12-20 00:14:22.176271: train_loss -0.8424\n",
      "2025-12-20 00:14:22.176271: val_loss -0.8674\n",
      "2025-12-20 00:14:22.176271: Pseudo dice [0.9212, 0.9515, 0.9379]\n",
      "2025-12-20 00:14:22.192283: Epoch time: 138.12 s\n",
      "2025-12-20 00:14:22.840421: \n",
      "2025-12-20 00:14:22.840421: Epoch 394\n",
      "2025-12-20 00:14:22.840421: Current learning rate: 0.00637\n",
      "2025-12-20 00:16:40.917974: train_loss -0.8428\n",
      "2025-12-20 00:16:40.919976: val_loss -0.8756\n",
      "2025-12-20 00:16:40.919976: Pseudo dice [0.9263, 0.9555, 0.9404]\n",
      "2025-12-20 00:16:40.919976: Epoch time: 138.08 s\n",
      "2025-12-20 00:16:41.583121: \n",
      "2025-12-20 00:16:41.583121: Epoch 395\n",
      "2025-12-20 00:16:41.583121: Current learning rate: 0.00636\n",
      "2025-12-20 00:18:59.459708: train_loss -0.838\n",
      "2025-12-20 00:18:59.459708: val_loss -0.8687\n",
      "2025-12-20 00:18:59.463713: Pseudo dice [0.9225, 0.9597, 0.9336]\n",
      "2025-12-20 00:18:59.465715: Epoch time: 137.88 s\n",
      "2025-12-20 00:19:00.124600: \n",
      "2025-12-20 00:19:00.124600: Epoch 396\n",
      "2025-12-20 00:19:00.124600: Current learning rate: 0.00635\n",
      "2025-12-20 00:21:18.081242: train_loss -0.8414\n",
      "2025-12-20 00:21:18.081242: val_loss -0.8712\n",
      "2025-12-20 00:21:18.083244: Pseudo dice [0.9248, 0.9618, 0.9318]\n",
      "2025-12-20 00:21:18.083244: Epoch time: 137.96 s\n",
      "2025-12-20 00:21:18.747111: \n",
      "2025-12-20 00:21:18.747111: Epoch 397\n",
      "2025-12-20 00:21:18.747111: Current learning rate: 0.00634\n",
      "2025-12-20 00:23:36.927116: train_loss -0.8394\n",
      "2025-12-20 00:23:36.928857: val_loss -0.8669\n",
      "2025-12-20 00:23:36.932862: Pseudo dice [0.9218, 0.9502, 0.9322]\n",
      "2025-12-20 00:23:36.936866: Epoch time: 138.18 s\n",
      "2025-12-20 00:23:37.592983: \n",
      "2025-12-20 00:23:37.592983: Epoch 398\n",
      "2025-12-20 00:23:37.592983: Current learning rate: 0.00633\n",
      "2025-12-20 00:25:55.716574: train_loss -0.8399\n",
      "2025-12-20 00:25:55.716574: val_loss -0.8722\n",
      "2025-12-20 00:25:55.722583: Pseudo dice [0.9219, 0.9567, 0.9385]\n",
      "2025-12-20 00:25:55.727365: Epoch time: 138.12 s\n",
      "2025-12-20 00:25:56.538836: \n",
      "2025-12-20 00:25:56.554699: Epoch 399\n",
      "2025-12-20 00:25:56.557188: Current learning rate: 0.00632\n",
      "2025-12-20 00:28:14.736046: train_loss -0.8409\n",
      "2025-12-20 00:28:14.736046: val_loss -0.8785\n",
      "2025-12-20 00:28:14.740050: Pseudo dice [0.9292, 0.9588, 0.9364]\n",
      "2025-12-20 00:28:14.744054: Epoch time: 138.2 s\n",
      "2025-12-20 00:28:15.638770: \n",
      "2025-12-20 00:28:15.638770: Epoch 400\n",
      "2025-12-20 00:28:15.654416: Current learning rate: 0.00631\n",
      "2025-12-20 00:30:33.879994: train_loss -0.8418\n",
      "2025-12-20 00:30:33.879994: val_loss -0.86\n",
      "2025-12-20 00:30:33.885862: Pseudo dice [0.9115, 0.9528, 0.9384]\n",
      "2025-12-20 00:30:33.889309: Epoch time: 138.24 s\n",
      "2025-12-20 00:30:34.536494: \n",
      "2025-12-20 00:30:34.536494: Epoch 401\n",
      "2025-12-20 00:30:34.536494: Current learning rate: 0.0063\n",
      "2025-12-20 00:32:52.761075: train_loss -0.8485\n",
      "2025-12-20 00:32:52.761075: val_loss -0.8636\n",
      "2025-12-20 00:32:52.763078: Pseudo dice [0.9164, 0.9481, 0.9326]\n",
      "2025-12-20 00:32:52.763078: Epoch time: 138.22 s\n",
      "2025-12-20 00:32:53.422839: \n",
      "2025-12-20 00:32:53.422839: Epoch 402\n",
      "2025-12-20 00:32:53.422839: Current learning rate: 0.0063\n",
      "2025-12-20 00:35:11.447914: train_loss -0.838\n",
      "2025-12-20 00:35:11.447914: val_loss -0.8763\n",
      "2025-12-20 00:35:11.453662: Pseudo dice [0.9289, 0.959, 0.937]\n",
      "2025-12-20 00:35:11.457667: Epoch time: 138.03 s\n",
      "2025-12-20 00:35:12.103657: \n",
      "2025-12-20 00:35:12.103657: Epoch 403\n",
      "2025-12-20 00:35:12.121442: Current learning rate: 0.00629\n",
      "2025-12-20 00:37:30.023002: train_loss -0.8406\n",
      "2025-12-20 00:37:30.023002: val_loss -0.8659\n",
      "2025-12-20 00:37:30.037733: Pseudo dice [0.9197, 0.9536, 0.9356]\n",
      "2025-12-20 00:37:30.038736: Epoch time: 137.92 s\n",
      "2025-12-20 00:37:30.691642: \n",
      "2025-12-20 00:37:30.691642: Epoch 404\n",
      "2025-12-20 00:37:30.699634: Current learning rate: 0.00628\n",
      "2025-12-20 00:39:50.459366: train_loss -0.8452\n",
      "2025-12-20 00:39:50.459366: val_loss -0.8737\n",
      "2025-12-20 00:39:50.475334: Pseudo dice [0.9248, 0.961, 0.939]\n",
      "2025-12-20 00:39:50.475334: Epoch time: 139.77 s\n",
      "2025-12-20 00:39:51.321328: \n",
      "2025-12-20 00:39:51.321328: Epoch 405\n",
      "2025-12-20 00:39:51.321328: Current learning rate: 0.00627\n",
      "2025-12-20 00:42:09.758632: train_loss -0.8364\n",
      "2025-12-20 00:42:09.758632: val_loss -0.8702\n",
      "2025-12-20 00:42:09.758632: Pseudo dice [0.9204, 0.9561, 0.936]\n",
      "2025-12-20 00:42:09.758632: Epoch time: 138.44 s\n",
      "2025-12-20 00:42:10.421695: \n",
      "2025-12-20 00:42:10.421695: Epoch 406\n",
      "2025-12-20 00:42:10.439536: Current learning rate: 0.00626\n",
      "2025-12-20 00:44:28.796391: train_loss -0.8433\n",
      "2025-12-20 00:44:28.796391: val_loss -0.8791\n",
      "2025-12-20 00:44:28.801975: Pseudo dice [0.929, 0.9572, 0.9396]\n",
      "2025-12-20 00:44:28.803977: Epoch time: 138.37 s\n",
      "2025-12-20 00:44:29.462353: \n",
      "2025-12-20 00:44:29.462353: Epoch 407\n",
      "2025-12-20 00:44:29.462353: Current learning rate: 0.00625\n",
      "2025-12-20 00:46:47.853631: train_loss -0.8426\n",
      "2025-12-20 00:46:47.853631: val_loss -0.8643\n",
      "2025-12-20 00:46:47.869411: Pseudo dice [0.9177, 0.9535, 0.9406]\n",
      "2025-12-20 00:46:47.869411: Epoch time: 138.39 s\n",
      "2025-12-20 00:46:48.515079: \n",
      "2025-12-20 00:46:48.515079: Epoch 408\n",
      "2025-12-20 00:46:48.530249: Current learning rate: 0.00624\n",
      "2025-12-20 00:49:06.945567: train_loss -0.8444\n",
      "2025-12-20 00:49:06.945567: val_loss -0.8724\n",
      "2025-12-20 00:49:06.949311: Pseudo dice [0.9249, 0.9561, 0.941]\n",
      "2025-12-20 00:49:06.954086: Epoch time: 138.43 s\n",
      "2025-12-20 00:49:06.958091: Yayy! New best EMA pseudo Dice: 0.9385\n",
      "2025-12-20 00:49:08.018978: \n",
      "2025-12-20 00:49:08.018978: Epoch 409\n",
      "2025-12-20 00:49:08.034852: Current learning rate: 0.00623\n",
      "2025-12-20 00:51:26.350278: train_loss -0.8477\n",
      "2025-12-20 00:51:26.352282: val_loss -0.8685\n",
      "2025-12-20 00:51:26.359210: Pseudo dice [0.9206, 0.9499, 0.9404]\n",
      "2025-12-20 00:51:26.364024: Epoch time: 138.33 s\n",
      "2025-12-20 00:51:27.193568: \n",
      "2025-12-20 00:51:27.193568: Epoch 410\n",
      "2025-12-20 00:51:27.193568: Current learning rate: 0.00622\n",
      "2025-12-20 00:53:45.294161: train_loss -0.8412\n",
      "2025-12-20 00:53:45.294161: val_loss -0.866\n",
      "2025-12-20 00:53:45.294161: Pseudo dice [0.9226, 0.957, 0.9293]\n",
      "2025-12-20 00:53:45.300767: Epoch time: 138.1 s\n",
      "2025-12-20 00:53:45.911534: \n",
      "2025-12-20 00:53:45.911534: Epoch 411\n",
      "2025-12-20 00:53:45.927443: Current learning rate: 0.00621\n",
      "2025-12-20 00:56:04.253210: train_loss -0.8341\n",
      "2025-12-20 00:56:04.253210: val_loss -0.8651\n",
      "2025-12-20 00:56:04.258955: Pseudo dice [0.9179, 0.9524, 0.9379]\n",
      "2025-12-20 00:56:04.262960: Epoch time: 138.34 s\n",
      "2025-12-20 00:56:04.999504: \n",
      "2025-12-20 00:56:04.999504: Epoch 412\n",
      "2025-12-20 00:56:05.015551: Current learning rate: 0.0062\n",
      "2025-12-20 00:58:23.136585: train_loss -0.8217\n",
      "2025-12-20 00:58:23.152556: val_loss -0.8468\n",
      "2025-12-20 00:58:23.156562: Pseudo dice [0.9186, 0.9529, 0.9171]\n",
      "2025-12-20 00:58:23.158565: Epoch time: 138.14 s\n",
      "2025-12-20 00:58:23.785631: \n",
      "2025-12-20 00:58:23.785631: Epoch 413\n",
      "2025-12-20 00:58:23.793684: Current learning rate: 0.00619\n",
      "2025-12-20 01:00:41.836487: train_loss -0.7831\n",
      "2025-12-20 01:00:41.836487: val_loss -0.8353\n",
      "2025-12-20 01:00:41.839412: Pseudo dice [0.9107, 0.9442, 0.9204]\n",
      "2025-12-20 01:00:41.839412: Epoch time: 138.05 s\n",
      "2025-12-20 01:00:42.454328: \n",
      "2025-12-20 01:00:42.454328: Epoch 414\n",
      "2025-12-20 01:00:42.469986: Current learning rate: 0.00618\n",
      "2025-12-20 01:03:00.615193: train_loss -0.8072\n",
      "2025-12-20 01:03:00.617196: val_loss -0.8488\n",
      "2025-12-20 01:03:00.620940: Pseudo dice [0.919, 0.9571, 0.9172]\n",
      "2025-12-20 01:03:00.624945: Epoch time: 138.16 s\n",
      "2025-12-20 01:03:01.365067: \n",
      "2025-12-20 01:03:01.380901: Epoch 415\n",
      "2025-12-20 01:03:01.384403: Current learning rate: 0.00617\n",
      "2025-12-20 01:05:19.446683: train_loss -0.8271\n",
      "2025-12-20 01:05:19.446683: val_loss -0.8558\n",
      "2025-12-20 01:05:19.464900: Pseudo dice [0.9161, 0.948, 0.933]\n",
      "2025-12-20 01:05:19.464900: Epoch time: 138.08 s\n",
      "2025-12-20 01:05:20.098339: \n",
      "2025-12-20 01:05:20.098339: Epoch 416\n",
      "2025-12-20 01:05:20.098339: Current learning rate: 0.00616\n",
      "2025-12-20 01:07:38.110997: train_loss -0.8376\n",
      "2025-12-20 01:07:38.110997: val_loss -0.8643\n",
      "2025-12-20 01:07:38.126806: Pseudo dice [0.9191, 0.9522, 0.9356]\n",
      "2025-12-20 01:07:38.126806: Epoch time: 138.01 s\n",
      "2025-12-20 01:07:38.920165: \n",
      "2025-12-20 01:07:38.920165: Epoch 417\n",
      "2025-12-20 01:07:38.935799: Current learning rate: 0.00615\n",
      "2025-12-20 01:09:57.326030: train_loss -0.8365\n",
      "2025-12-20 01:09:57.326030: val_loss -0.8649\n",
      "2025-12-20 01:09:57.331372: Pseudo dice [0.9213, 0.9525, 0.9314]\n",
      "2025-12-20 01:09:57.331372: Epoch time: 138.41 s\n",
      "2025-12-20 01:09:58.035182: \n",
      "2025-12-20 01:09:58.035182: Epoch 418\n",
      "2025-12-20 01:09:58.051298: Current learning rate: 0.00614\n",
      "2025-12-20 01:12:16.056901: train_loss -0.8339\n",
      "2025-12-20 01:12:16.058903: val_loss -0.8628\n",
      "2025-12-20 01:12:16.063748: Pseudo dice [0.9177, 0.9558, 0.9356]\n",
      "2025-12-20 01:12:16.067755: Epoch time: 138.02 s\n",
      "2025-12-20 01:12:16.695864: \n",
      "2025-12-20 01:12:16.695864: Epoch 419\n",
      "2025-12-20 01:12:16.705914: Current learning rate: 0.00613\n",
      "2025-12-20 01:14:34.904638: train_loss -0.8356\n",
      "2025-12-20 01:14:34.904638: val_loss -0.8637\n",
      "2025-12-20 01:14:34.904638: Pseudo dice [0.9187, 0.9487, 0.9401]\n",
      "2025-12-20 01:14:34.910809: Epoch time: 138.21 s\n",
      "2025-12-20 01:14:35.544259: \n",
      "2025-12-20 01:14:35.544259: Epoch 420\n",
      "2025-12-20 01:14:35.544259: Current learning rate: 0.00612\n",
      "2025-12-20 01:16:53.595437: train_loss -0.8366\n",
      "2025-12-20 01:16:53.595437: val_loss -0.8588\n",
      "2025-12-20 01:16:53.611408: Pseudo dice [0.9123, 0.952, 0.9317]\n",
      "2025-12-20 01:16:53.611408: Epoch time: 138.05 s\n",
      "2025-12-20 01:16:54.228269: \n",
      "2025-12-20 01:16:54.228269: Epoch 421\n",
      "2025-12-20 01:16:54.228269: Current learning rate: 0.00612\n",
      "2025-12-20 01:19:12.415139: train_loss -0.8407\n",
      "2025-12-20 01:19:12.415139: val_loss -0.8673\n",
      "2025-12-20 01:19:12.419144: Pseudo dice [0.9214, 0.9577, 0.9327]\n",
      "2025-12-20 01:19:12.423982: Epoch time: 138.19 s\n",
      "2025-12-20 01:19:13.047648: \n",
      "2025-12-20 01:19:13.047648: Epoch 422\n",
      "2025-12-20 01:19:13.063464: Current learning rate: 0.00611\n",
      "2025-12-20 01:21:31.313727: train_loss -0.8415\n",
      "2025-12-20 01:21:31.313727: val_loss -0.8715\n",
      "2025-12-20 01:21:31.313727: Pseudo dice [0.9246, 0.9577, 0.9401]\n",
      "2025-12-20 01:21:31.320678: Epoch time: 138.27 s\n",
      "2025-12-20 01:21:32.112983: \n",
      "2025-12-20 01:21:32.112983: Epoch 423\n",
      "2025-12-20 01:21:32.129015: Current learning rate: 0.0061\n",
      "2025-12-20 01:23:50.233562: train_loss -0.841\n",
      "2025-12-20 01:23:50.233562: val_loss -0.8729\n",
      "2025-12-20 01:23:50.233562: Pseudo dice [0.9263, 0.9535, 0.9359]\n",
      "2025-12-20 01:23:50.249632: Epoch time: 138.12 s\n",
      "2025-12-20 01:23:50.870022: \n",
      "2025-12-20 01:23:50.870022: Epoch 424\n",
      "2025-12-20 01:23:50.883996: Current learning rate: 0.00609\n",
      "2025-12-20 01:26:08.917786: train_loss -0.8367\n",
      "2025-12-20 01:26:08.917786: val_loss -0.866\n",
      "2025-12-20 01:26:08.924576: Pseudo dice [0.9154, 0.955, 0.9434]\n",
      "2025-12-20 01:26:08.926578: Epoch time: 138.05 s\n",
      "2025-12-20 01:26:09.549192: \n",
      "2025-12-20 01:26:09.549192: Epoch 425\n",
      "2025-12-20 01:26:09.564952: Current learning rate: 0.00608\n",
      "2025-12-20 01:28:27.787633: train_loss -0.8391\n",
      "2025-12-20 01:28:27.787633: val_loss -0.869\n",
      "2025-12-20 01:28:27.793096: Pseudo dice [0.9231, 0.9585, 0.9322]\n",
      "2025-12-20 01:28:27.793096: Epoch time: 138.24 s\n",
      "2025-12-20 01:28:28.421255: \n",
      "2025-12-20 01:28:28.421255: Epoch 426\n",
      "2025-12-20 01:28:28.421255: Current learning rate: 0.00607\n",
      "2025-12-20 01:30:46.374070: train_loss -0.8406\n",
      "2025-12-20 01:30:46.376073: val_loss -0.8697\n",
      "2025-12-20 01:30:46.380078: Pseudo dice [0.9221, 0.9559, 0.9409]\n",
      "2025-12-20 01:30:46.384083: Epoch time: 137.95 s\n",
      "2025-12-20 01:30:47.053433: \n",
      "2025-12-20 01:30:47.053433: Epoch 427\n",
      "2025-12-20 01:30:47.065554: Current learning rate: 0.00606\n",
      "2025-12-20 01:33:04.966595: train_loss -0.8447\n",
      "2025-12-20 01:33:04.968596: val_loss -0.8723\n",
      "2025-12-20 01:33:04.972601: Pseudo dice [0.923, 0.9554, 0.942]\n",
      "2025-12-20 01:33:04.976192: Epoch time: 137.91 s\n",
      "2025-12-20 01:33:05.606354: \n",
      "2025-12-20 01:33:05.606354: Epoch 428\n",
      "2025-12-20 01:33:05.606354: Current learning rate: 0.00605\n",
      "2025-12-20 01:35:23.711015: train_loss -0.8385\n",
      "2025-12-20 01:35:23.711015: val_loss -0.8662\n",
      "2025-12-20 01:35:23.721955: Pseudo dice [0.9205, 0.9496, 0.934]\n",
      "2025-12-20 01:35:23.724402: Epoch time: 138.1 s\n",
      "2025-12-20 01:35:24.551085: \n",
      "2025-12-20 01:35:24.551085: Epoch 429\n",
      "2025-12-20 01:35:24.566788: Current learning rate: 0.00604\n",
      "2025-12-20 01:37:42.588019: train_loss -0.8421\n",
      "2025-12-20 01:37:42.588019: val_loss -0.8684\n",
      "2025-12-20 01:37:42.588019: Pseudo dice [0.9181, 0.9572, 0.9383]\n",
      "2025-12-20 01:37:42.603790: Epoch time: 138.04 s\n",
      "2025-12-20 01:37:43.221492: \n",
      "2025-12-20 01:37:43.221492: Epoch 430\n",
      "2025-12-20 01:37:43.221492: Current learning rate: 0.00603\n",
      "2025-12-20 01:40:01.108428: train_loss -0.8427\n",
      "2025-12-20 01:40:01.108428: val_loss -0.8734\n",
      "2025-12-20 01:40:01.124141: Pseudo dice [0.928, 0.955, 0.9385]\n",
      "2025-12-20 01:40:01.126144: Epoch time: 137.89 s\n",
      "2025-12-20 01:40:01.756027: \n",
      "2025-12-20 01:40:01.756027: Epoch 431\n",
      "2025-12-20 01:40:01.756027: Current learning rate: 0.00602\n",
      "2025-12-20 01:42:19.776234: train_loss -0.8414\n",
      "2025-12-20 01:42:19.776234: val_loss -0.8653\n",
      "2025-12-20 01:42:19.777237: Pseudo dice [0.9202, 0.9512, 0.9377]\n",
      "2025-12-20 01:42:19.782749: Epoch time: 138.02 s\n",
      "2025-12-20 01:42:20.537609: \n",
      "2025-12-20 01:42:20.537609: Epoch 432\n",
      "2025-12-20 01:42:20.537609: Current learning rate: 0.00601\n",
      "2025-12-20 01:44:38.577883: train_loss -0.8411\n",
      "2025-12-20 01:44:38.579885: val_loss -0.8542\n",
      "2025-12-20 01:44:38.587046: Pseudo dice [0.9063, 0.9407, 0.9399]\n",
      "2025-12-20 01:44:38.592791: Epoch time: 138.04 s\n",
      "2025-12-20 01:44:39.231873: \n",
      "2025-12-20 01:44:39.231873: Epoch 433\n",
      "2025-12-20 01:44:39.231873: Current learning rate: 0.006\n",
      "2025-12-20 01:46:57.048637: train_loss -0.8411\n",
      "2025-12-20 01:46:57.048637: val_loss -0.8683\n",
      "2025-12-20 01:46:57.048637: Pseudo dice [0.9212, 0.9532, 0.9353]\n",
      "2025-12-20 01:46:57.060438: Epoch time: 137.82 s\n",
      "2025-12-20 01:46:57.677442: \n",
      "2025-12-20 01:46:57.677442: Epoch 434\n",
      "2025-12-20 01:46:57.677442: Current learning rate: 0.00599\n",
      "2025-12-20 01:49:15.622234: train_loss -0.8458\n",
      "2025-12-20 01:49:15.622234: val_loss -0.8638\n",
      "2025-12-20 01:49:15.629928: Pseudo dice [0.9207, 0.9536, 0.9299]\n",
      "2025-12-20 01:49:15.635186: Epoch time: 137.94 s\n",
      "2025-12-20 01:49:16.565262: \n",
      "2025-12-20 01:49:16.565262: Epoch 435\n",
      "2025-12-20 01:49:16.583485: Current learning rate: 0.00598\n",
      "2025-12-20 01:51:34.586885: train_loss -0.8436\n",
      "2025-12-20 01:51:34.588887: val_loss -0.8779\n",
      "2025-12-20 01:51:34.592893: Pseudo dice [0.9272, 0.9588, 0.9435]\n",
      "2025-12-20 01:51:34.592893: Epoch time: 138.02 s\n",
      "2025-12-20 01:51:35.231614: \n",
      "2025-12-20 01:51:35.231614: Epoch 436\n",
      "2025-12-20 01:51:35.231614: Current learning rate: 0.00597\n",
      "2025-12-20 01:53:53.306787: train_loss -0.8412\n",
      "2025-12-20 01:53:53.306787: val_loss -0.884\n",
      "2025-12-20 01:53:53.312532: Pseudo dice [0.9333, 0.9601, 0.9433]\n",
      "2025-12-20 01:53:53.314534: Epoch time: 138.08 s\n",
      "2025-12-20 01:53:53.957223: \n",
      "2025-12-20 01:53:53.957223: Epoch 437\n",
      "2025-12-20 01:53:53.957223: Current learning rate: 0.00596\n",
      "2025-12-20 01:56:12.102123: train_loss -0.8493\n",
      "2025-12-20 01:56:12.103863: val_loss -0.8614\n",
      "2025-12-20 01:56:12.105716: Pseudo dice [0.9146, 0.9523, 0.9339]\n",
      "2025-12-20 01:56:12.107718: Epoch time: 138.14 s\n",
      "2025-12-20 01:56:12.831997: \n",
      "2025-12-20 01:56:12.831997: Epoch 438\n",
      "2025-12-20 01:56:12.831997: Current learning rate: 0.00595\n",
      "2025-12-20 01:58:30.980229: train_loss -0.8397\n",
      "2025-12-20 01:58:30.980229: val_loss -0.8663\n",
      "2025-12-20 01:58:30.980229: Pseudo dice [0.9231, 0.9564, 0.9312]\n",
      "2025-12-20 01:58:30.995867: Epoch time: 138.15 s\n",
      "2025-12-20 01:58:31.628363: \n",
      "2025-12-20 01:58:31.628363: Epoch 439\n",
      "2025-12-20 01:58:31.628363: Current learning rate: 0.00594\n",
      "2025-12-20 02:00:49.655829: train_loss -0.8426\n",
      "2025-12-20 02:00:49.657832: val_loss -0.8604\n",
      "2025-12-20 02:00:49.659835: Pseudo dice [0.9194, 0.9466, 0.9355]\n",
      "2025-12-20 02:00:49.663845: Epoch time: 138.03 s\n",
      "2025-12-20 02:00:50.297938: \n",
      "2025-12-20 02:00:50.297938: Epoch 440\n",
      "2025-12-20 02:00:50.297938: Current learning rate: 0.00593\n",
      "2025-12-20 02:03:08.338158: train_loss -0.8253\n",
      "2025-12-20 02:03:08.341666: val_loss -0.8624\n",
      "2025-12-20 02:03:08.345670: Pseudo dice [0.9204, 0.9555, 0.9295]\n",
      "2025-12-20 02:03:08.349830: Epoch time: 138.04 s\n",
      "2025-12-20 02:03:09.355284: \n",
      "2025-12-20 02:03:09.355284: Epoch 441\n",
      "2025-12-20 02:03:09.355284: Current learning rate: 0.00592\n",
      "2025-12-20 02:05:27.489790: train_loss -0.8301\n",
      "2025-12-20 02:05:27.489790: val_loss -0.8633\n",
      "2025-12-20 02:05:27.489790: Pseudo dice [0.9215, 0.953, 0.9311]\n",
      "2025-12-20 02:05:27.489790: Epoch time: 138.14 s\n",
      "2025-12-20 02:05:28.137403: \n",
      "2025-12-20 02:05:28.137403: Epoch 442\n",
      "2025-12-20 02:05:28.137403: Current learning rate: 0.00592\n",
      "2025-12-20 02:07:46.171952: train_loss -0.8312\n",
      "2025-12-20 02:07:46.171952: val_loss -0.8671\n",
      "2025-12-20 02:07:46.180682: Pseudo dice [0.9219, 0.9563, 0.9355]\n",
      "2025-12-20 02:07:46.180682: Epoch time: 138.03 s\n",
      "2025-12-20 02:07:46.836065: \n",
      "2025-12-20 02:07:46.851794: Epoch 443\n",
      "2025-12-20 02:07:46.851794: Current learning rate: 0.00591\n",
      "2025-12-20 02:10:05.460033: train_loss -0.8312\n",
      "2025-12-20 02:10:05.460033: val_loss -0.8789\n",
      "2025-12-20 02:10:05.460033: Pseudo dice [0.9296, 0.9586, 0.9377]\n",
      "2025-12-20 02:10:05.460033: Epoch time: 138.62 s\n",
      "2025-12-20 02:10:06.184394: \n",
      "2025-12-20 02:10:06.184394: Epoch 444\n",
      "2025-12-20 02:10:06.184394: Current learning rate: 0.0059\n",
      "2025-12-20 02:12:23.987038: train_loss -0.8438\n",
      "2025-12-20 02:12:23.989047: val_loss -0.8811\n",
      "2025-12-20 02:12:23.995057: Pseudo dice [0.9312, 0.9641, 0.9379]\n",
      "2025-12-20 02:12:23.999063: Epoch time: 137.8 s\n",
      "2025-12-20 02:12:24.616557: \n",
      "2025-12-20 02:12:24.616557: Epoch 445\n",
      "2025-12-20 02:12:24.634669: Current learning rate: 0.00589\n",
      "2025-12-20 02:14:42.671228: train_loss -0.8439\n",
      "2025-12-20 02:14:42.671228: val_loss -0.8746\n",
      "2025-12-20 02:14:42.685097: Pseudo dice [0.925, 0.9607, 0.9303]\n",
      "2025-12-20 02:14:42.687103: Epoch time: 138.05 s\n",
      "2025-12-20 02:14:43.289629: \n",
      "2025-12-20 02:14:43.289629: Epoch 446\n",
      "2025-12-20 02:14:43.305548: Current learning rate: 0.00588\n",
      "2025-12-20 02:17:01.261843: train_loss -0.843\n",
      "2025-12-20 02:17:01.277780: val_loss -0.8733\n",
      "2025-12-20 02:17:01.282289: Pseudo dice [0.9254, 0.9593, 0.9335]\n",
      "2025-12-20 02:17:01.282289: Epoch time: 137.97 s\n",
      "2025-12-20 02:17:01.978168: \n",
      "2025-12-20 02:17:01.978168: Epoch 447\n",
      "2025-12-20 02:17:01.982399: Current learning rate: 0.00587\n",
      "2025-12-20 02:19:19.976720: train_loss -0.8456\n",
      "2025-12-20 02:19:19.976720: val_loss -0.8686\n",
      "2025-12-20 02:19:19.995525: Pseudo dice [0.9194, 0.9553, 0.9414]\n",
      "2025-12-20 02:19:19.995525: Epoch time: 138.0 s\n",
      "2025-12-20 02:19:20.849012: \n",
      "2025-12-20 02:19:20.849012: Epoch 448\n",
      "2025-12-20 02:19:20.855440: Current learning rate: 0.00586\n",
      "2025-12-20 02:21:38.811360: train_loss -0.8464\n",
      "2025-12-20 02:21:38.811360: val_loss -0.875\n",
      "2025-12-20 02:21:38.815364: Pseudo dice [0.9252, 0.9569, 0.9433]\n",
      "2025-12-20 02:21:38.815364: Epoch time: 137.96 s\n",
      "2025-12-20 02:21:38.815364: Yayy! New best EMA pseudo Dice: 0.9386\n",
      "2025-12-20 02:21:39.682495: \n",
      "2025-12-20 02:21:39.682495: Epoch 449\n",
      "2025-12-20 02:21:39.682495: Current learning rate: 0.00585\n",
      "2025-12-20 02:23:57.781404: train_loss -0.8418\n",
      "2025-12-20 02:23:57.781404: val_loss -0.873\n",
      "2025-12-20 02:23:57.787410: Pseudo dice [0.9223, 0.9589, 0.9429]\n",
      "2025-12-20 02:23:57.791414: Epoch time: 138.1 s\n",
      "2025-12-20 02:23:58.055307: Yayy! New best EMA pseudo Dice: 0.9389\n",
      "2025-12-20 02:23:59.043510: \n",
      "2025-12-20 02:23:59.043510: Epoch 450\n",
      "2025-12-20 02:23:59.043510: Current learning rate: 0.00584\n",
      "2025-12-20 02:26:17.111605: train_loss -0.8447\n",
      "2025-12-20 02:26:17.111605: val_loss -0.8659\n",
      "2025-12-20 02:26:17.111605: Pseudo dice [0.9158, 0.9517, 0.9399]\n",
      "2025-12-20 02:26:17.111605: Epoch time: 138.07 s\n",
      "2025-12-20 02:26:17.733217: \n",
      "2025-12-20 02:26:17.733217: Epoch 451\n",
      "2025-12-20 02:26:17.737767: Current learning rate: 0.00583\n",
      "2025-12-20 02:28:35.635605: train_loss -0.8373\n",
      "2025-12-20 02:28:35.639147: val_loss -0.8712\n",
      "2025-12-20 02:28:35.639147: Pseudo dice [0.9266, 0.9566, 0.9402]\n",
      "2025-12-20 02:28:35.639147: Epoch time: 137.9 s\n",
      "2025-12-20 02:28:36.259946: \n",
      "2025-12-20 02:28:36.261686: Epoch 452\n",
      "2025-12-20 02:28:36.264611: Current learning rate: 0.00582\n",
      "2025-12-20 02:30:54.325801: train_loss -0.8378\n",
      "2025-12-20 02:30:54.325801: val_loss -0.866\n",
      "2025-12-20 02:30:54.325801: Pseudo dice [0.919, 0.9586, 0.9316]\n",
      "2025-12-20 02:30:54.325801: Epoch time: 138.07 s\n",
      "2025-12-20 02:30:54.952527: \n",
      "2025-12-20 02:30:54.952527: Epoch 453\n",
      "2025-12-20 02:30:54.952527: Current learning rate: 0.00581\n",
      "2025-12-20 02:33:13.015297: train_loss -0.8427\n",
      "2025-12-20 02:33:13.015297: val_loss -0.8746\n",
      "2025-12-20 02:33:13.022889: Pseudo dice [0.9214, 0.9538, 0.9453]\n",
      "2025-12-20 02:33:13.026894: Epoch time: 138.06 s\n",
      "2025-12-20 02:33:13.864294: \n",
      "2025-12-20 02:33:13.864294: Epoch 454\n",
      "2025-12-20 02:33:13.864294: Current learning rate: 0.0058\n",
      "2025-12-20 02:35:32.061806: train_loss -0.8347\n",
      "2025-12-20 02:35:32.063548: val_loss -0.8602\n",
      "2025-12-20 02:35:32.067994: Pseudo dice [0.9145, 0.9531, 0.935]\n",
      "2025-12-20 02:35:32.071998: Epoch time: 138.2 s\n",
      "2025-12-20 02:35:32.682451: \n",
      "2025-12-20 02:35:32.682451: Epoch 455\n",
      "2025-12-20 02:35:32.682451: Current learning rate: 0.00579\n",
      "2025-12-20 02:37:50.797759: train_loss -0.8389\n",
      "2025-12-20 02:37:50.797759: val_loss -0.8694\n",
      "2025-12-20 02:37:50.797759: Pseudo dice [0.9227, 0.9546, 0.9384]\n",
      "2025-12-20 02:37:50.813084: Epoch time: 138.12 s\n",
      "2025-12-20 02:37:51.434383: \n",
      "2025-12-20 02:37:51.434383: Epoch 456\n",
      "2025-12-20 02:37:51.434383: Current learning rate: 0.00578\n",
      "2025-12-20 02:40:09.471555: train_loss -0.8363\n",
      "2025-12-20 02:40:09.473559: val_loss -0.866\n",
      "2025-12-20 02:40:09.477564: Pseudo dice [0.9156, 0.9544, 0.9399]\n",
      "2025-12-20 02:40:09.481568: Epoch time: 138.04 s\n",
      "2025-12-20 02:40:10.105984: \n",
      "2025-12-20 02:40:10.105984: Epoch 457\n",
      "2025-12-20 02:40:10.105984: Current learning rate: 0.00577\n",
      "2025-12-20 02:42:28.135794: train_loss -0.8454\n",
      "2025-12-20 02:42:28.137796: val_loss -0.8747\n",
      "2025-12-20 02:42:28.143541: Pseudo dice [0.9255, 0.957, 0.9395]\n",
      "2025-12-20 02:42:28.147546: Epoch time: 138.03 s\n",
      "2025-12-20 02:42:28.762409: \n",
      "2025-12-20 02:42:28.762409: Epoch 458\n",
      "2025-12-20 02:42:28.762409: Current learning rate: 0.00576\n",
      "2025-12-20 02:44:46.836051: train_loss -0.843\n",
      "2025-12-20 02:44:46.838054: val_loss -0.8677\n",
      "2025-12-20 02:44:46.842059: Pseudo dice [0.9174, 0.9553, 0.9422]\n",
      "2025-12-20 02:44:46.846064: Epoch time: 138.07 s\n",
      "2025-12-20 02:44:47.467302: \n",
      "2025-12-20 02:44:47.467302: Epoch 459\n",
      "2025-12-20 02:44:47.478108: Current learning rate: 0.00575\n",
      "2025-12-20 02:47:05.661570: train_loss -0.8417\n",
      "2025-12-20 02:47:05.661570: val_loss -0.8759\n",
      "2025-12-20 02:47:05.667577: Pseudo dice [0.9268, 0.9576, 0.9401]\n",
      "2025-12-20 02:47:05.673586: Epoch time: 138.19 s\n",
      "2025-12-20 02:47:06.450249: \n",
      "2025-12-20 02:47:06.450249: Epoch 460\n",
      "2025-12-20 02:47:06.468035: Current learning rate: 0.00574\n",
      "2025-12-20 02:49:24.580888: train_loss -0.8471\n",
      "2025-12-20 02:49:24.580888: val_loss -0.8728\n",
      "2025-12-20 02:49:24.580888: Pseudo dice [0.9264, 0.9552, 0.9389]\n",
      "2025-12-20 02:49:24.600299: Epoch time: 138.13 s\n",
      "2025-12-20 02:49:25.214663: \n",
      "2025-12-20 02:49:25.214663: Epoch 461\n",
      "2025-12-20 02:49:25.227875: Current learning rate: 0.00573\n",
      "2025-12-20 02:51:43.173583: train_loss -0.8407\n",
      "2025-12-20 02:51:43.173583: val_loss -0.8721\n",
      "2025-12-20 02:51:43.181833: Pseudo dice [0.9243, 0.9569, 0.9375]\n",
      "2025-12-20 02:51:43.185838: Epoch time: 137.96 s\n",
      "2025-12-20 02:51:43.191586: Yayy! New best EMA pseudo Dice: 0.9389\n",
      "2025-12-20 02:51:44.055810: \n",
      "2025-12-20 02:51:44.055810: Epoch 462\n",
      "2025-12-20 02:51:44.055810: Current learning rate: 0.00572\n",
      "2025-12-20 02:54:02.248348: train_loss -0.8426\n",
      "2025-12-20 02:54:02.248348: val_loss -0.8684\n",
      "2025-12-20 02:54:02.264370: Pseudo dice [0.9245, 0.959, 0.9337]\n",
      "2025-12-20 02:54:02.268992: Epoch time: 138.19 s\n",
      "2025-12-20 02:54:02.273615: Yayy! New best EMA pseudo Dice: 0.9389\n",
      "2025-12-20 02:54:03.168739: \n",
      "2025-12-20 02:54:03.170742: Epoch 463\n",
      "2025-12-20 02:54:03.170742: Current learning rate: 0.00571\n",
      "2025-12-20 02:56:21.165015: train_loss -0.8345\n",
      "2025-12-20 02:56:21.165015: val_loss -0.8685\n",
      "2025-12-20 02:56:21.180874: Pseudo dice [0.9226, 0.9571, 0.9362]\n",
      "2025-12-20 02:56:21.184579: Epoch time: 138.0 s\n",
      "2025-12-20 02:56:21.812680: \n",
      "2025-12-20 02:56:21.812680: Epoch 464\n",
      "2025-12-20 02:56:21.812680: Current learning rate: 0.0057\n",
      "2025-12-20 02:58:39.925413: train_loss -0.8354\n",
      "2025-12-20 02:58:39.925413: val_loss -0.8678\n",
      "2025-12-20 02:58:39.929526: Pseudo dice [0.9236, 0.9571, 0.9327]\n",
      "2025-12-20 02:58:39.935022: Epoch time: 138.11 s\n",
      "2025-12-20 02:58:40.543024: \n",
      "2025-12-20 02:58:40.543024: Epoch 465\n",
      "2025-12-20 02:58:40.558880: Current learning rate: 0.0057\n",
      "2025-12-20 03:00:58.587508: train_loss -0.8368\n",
      "2025-12-20 03:00:58.589248: val_loss -0.8738\n",
      "2025-12-20 03:00:58.593441: Pseudo dice [0.9262, 0.9575, 0.938]\n",
      "2025-12-20 03:00:58.599449: Epoch time: 138.04 s\n",
      "2025-12-20 03:00:58.601451: Yayy! New best EMA pseudo Dice: 0.939\n",
      "2025-12-20 03:00:59.481791: \n",
      "2025-12-20 03:00:59.481791: Epoch 466\n",
      "2025-12-20 03:00:59.484297: Current learning rate: 0.00569\n",
      "2025-12-20 03:03:17.609788: train_loss -0.8449\n",
      "2025-12-20 03:03:17.609788: val_loss -0.8627\n",
      "2025-12-20 03:03:17.615285: Pseudo dice [0.9137, 0.953, 0.9372]\n",
      "2025-12-20 03:03:17.619289: Epoch time: 138.13 s\n",
      "2025-12-20 03:03:18.420033: \n",
      "2025-12-20 03:03:18.420033: Epoch 467\n",
      "2025-12-20 03:03:18.424473: Current learning rate: 0.00568\n",
      "2025-12-20 03:05:36.656946: train_loss -0.8413\n",
      "2025-12-20 03:05:36.658949: val_loss -0.8815\n",
      "2025-12-20 03:05:36.658949: Pseudo dice [0.9295, 0.9614, 0.9431]\n",
      "2025-12-20 03:05:36.658949: Epoch time: 138.24 s\n",
      "2025-12-20 03:05:36.668302: Yayy! New best EMA pseudo Dice: 0.9391\n",
      "2025-12-20 03:05:37.550908: \n",
      "2025-12-20 03:05:37.550908: Epoch 468\n",
      "2025-12-20 03:05:37.564760: Current learning rate: 0.00567\n",
      "2025-12-20 03:07:55.700963: train_loss -0.8424\n",
      "2025-12-20 03:07:55.700963: val_loss -0.8738\n",
      "2025-12-20 03:07:55.709684: Pseudo dice [0.925, 0.9539, 0.9396]\n",
      "2025-12-20 03:07:55.711686: Epoch time: 138.15 s\n",
      "2025-12-20 03:07:55.716692: Yayy! New best EMA pseudo Dice: 0.9392\n",
      "2025-12-20 03:07:56.618064: \n",
      "2025-12-20 03:07:56.618064: Epoch 469\n",
      "2025-12-20 03:07:56.618064: Current learning rate: 0.00566\n",
      "2025-12-20 03:10:14.975722: train_loss -0.8454\n",
      "2025-12-20 03:10:14.975722: val_loss -0.874\n",
      "2025-12-20 03:10:14.991382: Pseudo dice [0.9215, 0.9578, 0.9424]\n",
      "2025-12-20 03:10:14.991382: Epoch time: 138.36 s\n",
      "2025-12-20 03:10:14.991382: Yayy! New best EMA pseudo Dice: 0.9393\n",
      "2025-12-20 03:10:15.882515: \n",
      "2025-12-20 03:10:15.882515: Epoch 470\n",
      "2025-12-20 03:10:15.882515: Current learning rate: 0.00565\n",
      "2025-12-20 03:12:33.875691: train_loss -0.843\n",
      "2025-12-20 03:12:33.875691: val_loss -0.8766\n",
      "2025-12-20 03:12:33.875691: Pseudo dice [0.9267, 0.9541, 0.9405]\n",
      "2025-12-20 03:12:33.875691: Epoch time: 137.99 s\n",
      "2025-12-20 03:12:33.875691: Yayy! New best EMA pseudo Dice: 0.9394\n",
      "2025-12-20 03:12:34.767461: \n",
      "2025-12-20 03:12:34.767461: Epoch 471\n",
      "2025-12-20 03:12:34.779598: Current learning rate: 0.00564\n",
      "2025-12-20 03:14:52.777380: train_loss -0.8418\n",
      "2025-12-20 03:14:52.777380: val_loss -0.8697\n",
      "2025-12-20 03:14:52.777380: Pseudo dice [0.921, 0.9544, 0.9385]\n",
      "2025-12-20 03:14:52.777380: Epoch time: 138.01 s\n",
      "2025-12-20 03:14:53.555265: \n",
      "2025-12-20 03:14:53.555265: Epoch 472\n",
      "2025-12-20 03:14:53.555265: Current learning rate: 0.00563\n",
      "2025-12-20 03:17:11.433466: train_loss -0.8509\n",
      "2025-12-20 03:17:11.435468: val_loss -0.8732\n",
      "2025-12-20 03:17:11.439472: Pseudo dice [0.9242, 0.9574, 0.9379]\n",
      "2025-12-20 03:17:11.443476: Epoch time: 137.88 s\n",
      "2025-12-20 03:17:12.117394: \n",
      "2025-12-20 03:17:12.117394: Epoch 473\n",
      "2025-12-20 03:17:12.117394: Current learning rate: 0.00562\n",
      "2025-12-20 03:19:30.174171: train_loss -0.8484\n",
      "2025-12-20 03:19:30.174171: val_loss -0.8771\n",
      "2025-12-20 03:19:30.178165: Pseudo dice [0.9247, 0.9572, 0.9451]\n",
      "2025-12-20 03:19:30.178165: Epoch time: 138.06 s\n",
      "2025-12-20 03:19:30.184604: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-20 03:19:31.060227: \n",
      "2025-12-20 03:19:31.060227: Epoch 474\n",
      "2025-12-20 03:19:31.060227: Current learning rate: 0.00561\n",
      "2025-12-20 03:21:49.208982: train_loss -0.8412\n",
      "2025-12-20 03:21:49.208982: val_loss -0.8763\n",
      "2025-12-20 03:21:49.224214: Pseudo dice [0.9282, 0.9588, 0.9335]\n",
      "2025-12-20 03:21:49.224214: Epoch time: 138.16 s\n",
      "2025-12-20 03:21:49.231720: Yayy! New best EMA pseudo Dice: 0.9397\n",
      "2025-12-20 03:21:50.133472: \n",
      "2025-12-20 03:21:50.133472: Epoch 475\n",
      "2025-12-20 03:21:50.133472: Current learning rate: 0.0056\n",
      "2025-12-20 03:24:08.190958: train_loss -0.849\n",
      "2025-12-20 03:24:08.190958: val_loss -0.8805\n",
      "2025-12-20 03:24:08.203948: Pseudo dice [0.93, 0.9571, 0.9401]\n",
      "2025-12-20 03:24:08.206952: Epoch time: 138.06 s\n",
      "2025-12-20 03:24:08.206952: Yayy! New best EMA pseudo Dice: 0.94\n",
      "2025-12-20 03:24:09.114700: \n",
      "2025-12-20 03:24:09.114700: Epoch 476\n",
      "2025-12-20 03:24:09.114700: Current learning rate: 0.00559\n",
      "2025-12-20 03:26:27.181305: train_loss -0.8451\n",
      "2025-12-20 03:26:27.181305: val_loss -0.8793\n",
      "2025-12-20 03:26:27.185309: Pseudo dice [0.9293, 0.9597, 0.9402]\n",
      "2025-12-20 03:26:27.189313: Epoch time: 138.07 s\n",
      "2025-12-20 03:26:27.191054: Yayy! New best EMA pseudo Dice: 0.9403\n",
      "2025-12-20 03:26:28.095594: \n",
      "2025-12-20 03:26:28.097597: Epoch 477\n",
      "2025-12-20 03:26:28.097597: Current learning rate: 0.00558\n",
      "2025-12-20 03:28:46.062678: train_loss -0.8452\n",
      "2025-12-20 03:28:46.062678: val_loss -0.8675\n",
      "2025-12-20 03:28:46.062678: Pseudo dice [0.9204, 0.9517, 0.9401]\n",
      "2025-12-20 03:28:46.062678: Epoch time: 137.97 s\n",
      "2025-12-20 03:28:46.913070: \n",
      "2025-12-20 03:28:46.913070: Epoch 478\n",
      "2025-12-20 03:28:46.913070: Current learning rate: 0.00557\n",
      "2025-12-20 03:31:05.054070: train_loss -0.8448\n",
      "2025-12-20 03:31:05.054070: val_loss -0.8707\n",
      "2025-12-20 03:31:05.060076: Pseudo dice [0.9212, 0.9522, 0.9398]\n",
      "2025-12-20 03:31:05.065273: Epoch time: 138.14 s\n",
      "2025-12-20 03:31:05.694059: \n",
      "2025-12-20 03:31:05.694059: Epoch 479\n",
      "2025-12-20 03:31:05.694059: Current learning rate: 0.00556\n",
      "2025-12-20 03:33:23.632998: train_loss -0.8473\n",
      "2025-12-20 03:33:23.632998: val_loss -0.8772\n",
      "2025-12-20 03:33:23.652284: Pseudo dice [0.9252, 0.9571, 0.9434]\n",
      "2025-12-20 03:33:23.656289: Epoch time: 137.94 s\n",
      "2025-12-20 03:33:24.281331: \n",
      "2025-12-20 03:33:24.281331: Epoch 480\n",
      "2025-12-20 03:33:24.281331: Current learning rate: 0.00555\n",
      "2025-12-20 03:35:42.363601: train_loss -0.8389\n",
      "2025-12-20 03:35:42.363601: val_loss -0.8755\n",
      "2025-12-20 03:35:42.369607: Pseudo dice [0.926, 0.9574, 0.9464]\n",
      "2025-12-20 03:35:42.374990: Epoch time: 138.08 s\n",
      "2025-12-20 03:35:42.379394: Yayy! New best EMA pseudo Dice: 0.9403\n",
      "2025-12-20 03:35:43.251659: \n",
      "2025-12-20 03:35:43.251659: Epoch 481\n",
      "2025-12-20 03:35:43.251659: Current learning rate: 0.00554\n",
      "2025-12-20 03:38:01.213293: train_loss -0.8472\n",
      "2025-12-20 03:38:01.213293: val_loss -0.8692\n",
      "2025-12-20 03:38:01.213293: Pseudo dice [0.9218, 0.9542, 0.9383]\n",
      "2025-12-20 03:38:01.226789: Epoch time: 137.96 s\n",
      "2025-12-20 03:38:01.847696: \n",
      "2025-12-20 03:38:01.847696: Epoch 482\n",
      "2025-12-20 03:38:01.852841: Current learning rate: 0.00553\n",
      "2025-12-20 03:40:19.897020: train_loss -0.8505\n",
      "2025-12-20 03:40:19.897020: val_loss -0.8722\n",
      "2025-12-20 03:40:19.912733: Pseudo dice [0.9242, 0.9549, 0.9387]\n",
      "2025-12-20 03:40:19.912733: Epoch time: 138.07 s\n",
      "2025-12-20 03:40:20.545927: \n",
      "2025-12-20 03:40:20.545927: Epoch 483\n",
      "2025-12-20 03:40:20.545927: Current learning rate: 0.00552\n",
      "2025-12-20 03:42:38.737684: train_loss -0.8455\n",
      "2025-12-20 03:42:38.739687: val_loss -0.8793\n",
      "2025-12-20 03:42:38.747700: Pseudo dice [0.9296, 0.9548, 0.9445]\n",
      "2025-12-20 03:42:38.751704: Epoch time: 138.19 s\n",
      "2025-12-20 03:42:39.549144: \n",
      "2025-12-20 03:42:39.549144: Epoch 484\n",
      "2025-12-20 03:42:39.563319: Current learning rate: 0.00551\n",
      "2025-12-20 03:44:57.776716: train_loss -0.8332\n",
      "2025-12-20 03:44:57.776716: val_loss -0.8637\n",
      "2025-12-20 03:44:57.782724: Pseudo dice [0.9209, 0.9522, 0.9259]\n",
      "2025-12-20 03:44:57.784727: Epoch time: 138.23 s\n",
      "2025-12-20 03:44:58.409364: \n",
      "2025-12-20 03:44:58.409364: Epoch 485\n",
      "2025-12-20 03:44:58.425234: Current learning rate: 0.0055\n",
      "2025-12-20 03:47:16.300217: train_loss -0.8384\n",
      "2025-12-20 03:47:16.304665: val_loss -0.8661\n",
      "2025-12-20 03:47:16.308669: Pseudo dice [0.9204, 0.953, 0.9354]\n",
      "2025-12-20 03:47:16.312411: Epoch time: 137.89 s\n",
      "2025-12-20 03:47:16.948724: \n",
      "2025-12-20 03:47:16.948724: Epoch 486\n",
      "2025-12-20 03:47:16.948724: Current learning rate: 0.00549\n",
      "2025-12-20 03:49:34.988985: train_loss -0.8394\n",
      "2025-12-20 03:49:34.988985: val_loss -0.866\n",
      "2025-12-20 03:49:34.988985: Pseudo dice [0.9206, 0.9518, 0.939]\n",
      "2025-12-20 03:49:35.004725: Epoch time: 138.04 s\n",
      "2025-12-20 03:49:35.623981: \n",
      "2025-12-20 03:49:35.623981: Epoch 487\n",
      "2025-12-20 03:49:35.623981: Current learning rate: 0.00548\n",
      "2025-12-20 03:51:53.598946: train_loss -0.8476\n",
      "2025-12-20 03:51:53.600688: val_loss -0.8656\n",
      "2025-12-20 03:51:53.604694: Pseudo dice [0.9212, 0.9561, 0.9288]\n",
      "2025-12-20 03:51:53.610711: Epoch time: 137.97 s\n",
      "2025-12-20 03:51:54.375808: \n",
      "2025-12-20 03:51:54.375808: Epoch 488\n",
      "2025-12-20 03:51:54.380536: Current learning rate: 0.00547\n",
      "2025-12-20 03:54:12.481799: train_loss -0.8479\n",
      "2025-12-20 03:54:12.481799: val_loss -0.8656\n",
      "2025-12-20 03:54:12.497900: Pseudo dice [0.9183, 0.9502, 0.9422]\n",
      "2025-12-20 03:54:12.497900: Epoch time: 138.12 s\n",
      "2025-12-20 03:54:13.118345: \n",
      "2025-12-20 03:54:13.118345: Epoch 489\n",
      "2025-12-20 03:54:13.118345: Current learning rate: 0.00546\n",
      "2025-12-20 03:56:31.186208: train_loss -0.8428\n",
      "2025-12-20 03:56:31.188214: val_loss -0.8695\n",
      "2025-12-20 03:56:31.194224: Pseudo dice [0.9204, 0.9518, 0.9342]\n",
      "2025-12-20 03:56:31.199970: Epoch time: 138.07 s\n",
      "2025-12-20 03:56:31.831885: \n",
      "2025-12-20 03:56:31.831885: Epoch 490\n",
      "2025-12-20 03:56:31.831885: Current learning rate: 0.00546\n",
      "2025-12-20 03:58:49.871129: train_loss -0.8459\n",
      "2025-12-20 03:58:49.871129: val_loss -0.871\n",
      "2025-12-20 03:58:49.879146: Pseudo dice [0.9235, 0.9583, 0.9221]\n",
      "2025-12-20 03:58:49.883153: Epoch time: 138.04 s\n",
      "2025-12-20 03:58:50.886209: \n",
      "2025-12-20 03:58:50.886209: Epoch 491\n",
      "2025-12-20 03:58:50.895847: Current learning rate: 0.00545\n",
      "2025-12-20 04:01:08.883306: train_loss -0.8506\n",
      "2025-12-20 04:01:08.883306: val_loss -0.87\n",
      "2025-12-20 04:01:08.887311: Pseudo dice [0.9207, 0.9508, 0.941]\n",
      "2025-12-20 04:01:08.887311: Epoch time: 138.0 s\n",
      "2025-12-20 04:01:09.576423: \n",
      "2025-12-20 04:01:09.576423: Epoch 492\n",
      "2025-12-20 04:01:09.576423: Current learning rate: 0.00544\n",
      "2025-12-20 04:03:27.643111: train_loss -0.8442\n",
      "2025-12-20 04:03:27.645113: val_loss -0.8775\n",
      "2025-12-20 04:03:27.649118: Pseudo dice [0.9264, 0.9572, 0.947]\n",
      "2025-12-20 04:03:27.650859: Epoch time: 138.07 s\n",
      "2025-12-20 04:03:28.321736: \n",
      "2025-12-20 04:03:28.321736: Epoch 493\n",
      "2025-12-20 04:03:28.323739: Current learning rate: 0.00543\n",
      "2025-12-20 04:05:46.212973: train_loss -0.8504\n",
      "2025-12-20 04:05:46.212973: val_loss -0.8774\n",
      "2025-12-20 04:05:46.218476: Pseudo dice [0.9249, 0.958, 0.9421]\n",
      "2025-12-20 04:05:46.222481: Epoch time: 137.89 s\n",
      "2025-12-20 04:05:46.968853: \n",
      "2025-12-20 04:05:46.968853: Epoch 494\n",
      "2025-12-20 04:05:46.968853: Current learning rate: 0.00542\n",
      "2025-12-20 04:08:04.922029: train_loss -0.8487\n",
      "2025-12-20 04:08:04.924031: val_loss -0.868\n",
      "2025-12-20 04:08:04.928035: Pseudo dice [0.9161, 0.9543, 0.9427]\n",
      "2025-12-20 04:08:04.932039: Epoch time: 137.95 s\n",
      "2025-12-20 04:08:05.553934: \n",
      "2025-12-20 04:08:05.553934: Epoch 495\n",
      "2025-12-20 04:08:05.569692: Current learning rate: 0.00541\n",
      "2025-12-20 04:10:24.224448: train_loss -0.8449\n",
      "2025-12-20 04:10:24.224448: val_loss -0.8744\n",
      "2025-12-20 04:10:24.230198: Pseudo dice [0.923, 0.9611, 0.9383]\n",
      "2025-12-20 04:10:24.235675: Epoch time: 138.67 s\n",
      "2025-12-20 04:10:24.925370: \n",
      "2025-12-20 04:10:24.925370: Epoch 496\n",
      "2025-12-20 04:10:24.925370: Current learning rate: 0.0054\n",
      "2025-12-20 04:12:43.072690: train_loss -0.8383\n",
      "2025-12-20 04:12:43.072690: val_loss -0.8734\n",
      "2025-12-20 04:12:43.078435: Pseudo dice [0.9245, 0.9543, 0.9416]\n",
      "2025-12-20 04:12:43.084445: Epoch time: 138.15 s\n",
      "2025-12-20 04:12:43.804906: \n",
      "2025-12-20 04:12:43.804906: Epoch 497\n",
      "2025-12-20 04:12:43.804906: Current learning rate: 0.00539\n",
      "2025-12-20 04:15:01.675604: train_loss -0.8365\n",
      "2025-12-20 04:15:01.675604: val_loss -0.8764\n",
      "2025-12-20 04:15:01.682794: Pseudo dice [0.9317, 0.9611, 0.9311]\n",
      "2025-12-20 04:15:01.682794: Epoch time: 137.89 s\n",
      "2025-12-20 04:15:02.484408: \n",
      "2025-12-20 04:15:02.484408: Epoch 498\n",
      "2025-12-20 04:15:02.484408: Current learning rate: 0.00538\n",
      "2025-12-20 04:17:20.225926: train_loss -0.8535\n",
      "2025-12-20 04:17:20.225926: val_loss -0.8701\n",
      "2025-12-20 04:17:20.232695: Pseudo dice [0.9221, 0.9521, 0.9395]\n",
      "2025-12-20 04:17:20.232695: Epoch time: 137.74 s\n",
      "2025-12-20 04:17:20.852279: \n",
      "2025-12-20 04:17:20.852279: Epoch 499\n",
      "2025-12-20 04:17:20.868086: Current learning rate: 0.00537\n",
      "2025-12-20 04:19:38.882654: train_loss -0.8524\n",
      "2025-12-20 04:19:38.882654: val_loss -0.8748\n",
      "2025-12-20 04:19:38.882654: Pseudo dice [0.9212, 0.9579, 0.9424]\n",
      "2025-12-20 04:19:38.896030: Epoch time: 138.03 s\n",
      "2025-12-20 04:19:39.916162: \n",
      "2025-12-20 04:19:39.916162: Epoch 500\n",
      "2025-12-20 04:19:39.929922: Current learning rate: 0.00536\n",
      "2025-12-20 04:21:57.919581: train_loss -0.8477\n",
      "2025-12-20 04:21:57.919581: val_loss -0.8738\n",
      "2025-12-20 04:21:57.925343: Pseudo dice [0.9254, 0.954, 0.9373]\n",
      "2025-12-20 04:21:57.929348: Epoch time: 138.0 s\n",
      "2025-12-20 04:21:58.555254: \n",
      "2025-12-20 04:21:58.555254: Epoch 501\n",
      "2025-12-20 04:21:58.564697: Current learning rate: 0.00535\n",
      "2025-12-20 04:24:16.540455: train_loss -0.8457\n",
      "2025-12-20 04:24:16.540455: val_loss -0.8883\n",
      "2025-12-20 04:24:16.545424: Pseudo dice [0.9332, 0.9669, 0.9425]\n",
      "2025-12-20 04:24:16.550130: Epoch time: 137.99 s\n",
      "2025-12-20 04:24:17.183595: \n",
      "2025-12-20 04:24:17.183595: Epoch 502\n",
      "2025-12-20 04:24:17.187646: Current learning rate: 0.00534\n",
      "2025-12-20 04:26:35.190441: train_loss -0.8427\n",
      "2025-12-20 04:26:35.190441: val_loss -0.8679\n",
      "2025-12-20 04:26:35.190441: Pseudo dice [0.9189, 0.9541, 0.9404]\n",
      "2025-12-20 04:26:35.190441: Epoch time: 138.02 s\n",
      "2025-12-20 04:26:35.852804: \n",
      "2025-12-20 04:26:35.854808: Epoch 503\n",
      "2025-12-20 04:26:35.856551: Current learning rate: 0.00533\n",
      "2025-12-20 04:28:53.883154: train_loss -0.8457\n",
      "2025-12-20 04:28:53.885158: val_loss -0.8718\n",
      "2025-12-20 04:28:53.890904: Pseudo dice [0.9219, 0.9568, 0.934]\n",
      "2025-12-20 04:28:53.892906: Epoch time: 138.03 s\n",
      "2025-12-20 04:28:54.678660: \n",
      "2025-12-20 04:28:54.678660: Epoch 504\n",
      "2025-12-20 04:28:54.678660: Current learning rate: 0.00532\n",
      "2025-12-20 04:31:12.571975: train_loss -0.8452\n",
      "2025-12-20 04:31:12.571975: val_loss -0.8753\n",
      "2025-12-20 04:31:12.571975: Pseudo dice [0.9232, 0.9582, 0.9418]\n",
      "2025-12-20 04:31:12.571975: Epoch time: 137.89 s\n",
      "2025-12-20 04:31:13.220352: \n",
      "2025-12-20 04:31:13.220352: Epoch 505\n",
      "2025-12-20 04:31:13.220352: Current learning rate: 0.00531\n",
      "2025-12-20 04:33:31.240924: train_loss -0.852\n",
      "2025-12-20 04:33:31.240924: val_loss -0.8774\n",
      "2025-12-20 04:33:31.256674: Pseudo dice [0.9268, 0.959, 0.9389]\n",
      "2025-12-20 04:33:31.256674: Epoch time: 138.02 s\n",
      "2025-12-20 04:33:31.891360: \n",
      "2025-12-20 04:33:31.891360: Epoch 506\n",
      "2025-12-20 04:33:31.891360: Current learning rate: 0.0053\n",
      "2025-12-20 04:35:49.965917: train_loss -0.8453\n",
      "2025-12-20 04:35:49.967658: val_loss -0.867\n",
      "2025-12-20 04:35:49.973664: Pseudo dice [0.9196, 0.9518, 0.9397]\n",
      "2025-12-20 04:35:49.977669: Epoch time: 138.09 s\n",
      "2025-12-20 04:35:50.599905: \n",
      "2025-12-20 04:35:50.599905: Epoch 507\n",
      "2025-12-20 04:35:50.599905: Current learning rate: 0.00529\n",
      "2025-12-20 04:38:08.730893: train_loss -0.8432\n",
      "2025-12-20 04:38:08.730893: val_loss -0.8801\n",
      "2025-12-20 04:38:08.732633: Pseudo dice [0.9317, 0.9572, 0.9356]\n",
      "2025-12-20 04:38:08.738933: Epoch time: 138.13 s\n",
      "2025-12-20 04:38:09.370784: \n",
      "2025-12-20 04:38:09.370784: Epoch 508\n",
      "2025-12-20 04:38:09.370784: Current learning rate: 0.00528\n",
      "2025-12-20 04:40:27.265511: train_loss -0.8534\n",
      "2025-12-20 04:40:27.267514: val_loss -0.8725\n",
      "2025-12-20 04:40:27.271301: Pseudo dice [0.9235, 0.9581, 0.9368]\n",
      "2025-12-20 04:40:27.273116: Epoch time: 137.89 s\n",
      "2025-12-20 04:40:27.904870: \n",
      "2025-12-20 04:40:27.904870: Epoch 509\n",
      "2025-12-20 04:40:27.904870: Current learning rate: 0.00527\n",
      "2025-12-20 04:42:45.883068: train_loss -0.8451\n",
      "2025-12-20 04:42:45.883068: val_loss -0.8665\n",
      "2025-12-20 04:42:45.883068: Pseudo dice [0.9205, 0.952, 0.9369]\n",
      "2025-12-20 04:42:45.898759: Epoch time: 137.98 s\n",
      "2025-12-20 04:42:46.676052: \n",
      "2025-12-20 04:42:46.676052: Epoch 510\n",
      "2025-12-20 04:42:46.691927: Current learning rate: 0.00526\n",
      "2025-12-20 04:45:04.821715: train_loss -0.8405\n",
      "2025-12-20 04:45:04.821715: val_loss -0.8762\n",
      "2025-12-20 04:45:04.828209: Pseudo dice [0.9257, 0.9534, 0.9411]\n",
      "2025-12-20 04:45:04.832216: Epoch time: 138.15 s\n",
      "2025-12-20 04:45:05.457386: \n",
      "2025-12-20 04:45:05.457386: Epoch 511\n",
      "2025-12-20 04:45:05.473249: Current learning rate: 0.00525\n",
      "2025-12-20 04:47:23.667875: train_loss -0.8471\n",
      "2025-12-20 04:47:23.667875: val_loss -0.8752\n",
      "2025-12-20 04:47:23.667875: Pseudo dice [0.9264, 0.957, 0.9436]\n",
      "2025-12-20 04:47:23.667875: Epoch time: 138.21 s\n",
      "2025-12-20 04:47:24.317055: \n",
      "2025-12-20 04:47:24.317055: Epoch 512\n",
      "2025-12-20 04:47:24.322831: Current learning rate: 0.00524\n",
      "2025-12-20 04:49:42.295432: train_loss -0.8481\n",
      "2025-12-20 04:49:42.295432: val_loss -0.8763\n",
      "2025-12-20 04:49:42.300044: Pseudo dice [0.9243, 0.9551, 0.9453]\n",
      "2025-12-20 04:49:42.303787: Epoch time: 137.98 s\n",
      "2025-12-20 04:49:42.978551: \n",
      "2025-12-20 04:49:42.978551: Epoch 513\n",
      "2025-12-20 04:49:42.987342: Current learning rate: 0.00523\n",
      "2025-12-20 04:52:01.184268: train_loss -0.8481\n",
      "2025-12-20 04:52:01.186271: val_loss -0.87\n",
      "2025-12-20 04:52:01.186271: Pseudo dice [0.9232, 0.955, 0.9381]\n",
      "2025-12-20 04:52:01.193403: Epoch time: 138.21 s\n",
      "2025-12-20 04:52:01.987817: \n",
      "2025-12-20 04:52:01.987817: Epoch 514\n",
      "2025-12-20 04:52:01.993960: Current learning rate: 0.00522\n",
      "2025-12-20 04:54:20.209428: train_loss -0.8401\n",
      "2025-12-20 04:54:20.209428: val_loss -0.8668\n",
      "2025-12-20 04:54:20.217961: Pseudo dice [0.9168, 0.953, 0.9382]\n",
      "2025-12-20 04:54:20.222726: Epoch time: 138.22 s\n",
      "2025-12-20 04:54:20.858178: \n",
      "2025-12-20 04:54:20.858178: Epoch 515\n",
      "2025-12-20 04:54:20.858178: Current learning rate: 0.00521\n",
      "2025-12-20 04:56:38.940004: train_loss -0.848\n",
      "2025-12-20 04:56:38.940004: val_loss -0.8754\n",
      "2025-12-20 04:56:38.944010: Pseudo dice [0.9244, 0.9614, 0.9347]\n",
      "2025-12-20 04:56:38.950056: Epoch time: 138.08 s\n",
      "2025-12-20 04:56:39.751786: \n",
      "2025-12-20 04:56:39.751786: Epoch 516\n",
      "2025-12-20 04:56:39.760785: Current learning rate: 0.0052\n",
      "2025-12-20 04:58:57.737394: train_loss -0.8483\n",
      "2025-12-20 04:58:57.737394: val_loss -0.8765\n",
      "2025-12-20 04:58:57.745839: Pseudo dice [0.9233, 0.9565, 0.9423]\n",
      "2025-12-20 04:58:57.748942: Epoch time: 137.99 s\n",
      "2025-12-20 04:58:58.513075: \n",
      "2025-12-20 04:58:58.513075: Epoch 517\n",
      "2025-12-20 04:58:58.525999: Current learning rate: 0.00519\n",
      "2025-12-20 05:01:16.628542: train_loss -0.8479\n",
      "2025-12-20 05:01:16.628542: val_loss -0.88\n",
      "2025-12-20 05:01:16.635916: Pseudo dice [0.9274, 0.9613, 0.9427]\n",
      "2025-12-20 05:01:16.640668: Epoch time: 138.12 s\n",
      "2025-12-20 05:01:17.308076: \n",
      "2025-12-20 05:01:17.308076: Epoch 518\n",
      "2025-12-20 05:01:17.309818: Current learning rate: 0.00518\n",
      "2025-12-20 05:03:35.344792: train_loss -0.8492\n",
      "2025-12-20 05:03:35.344792: val_loss -0.8815\n",
      "2025-12-20 05:03:35.350433: Pseudo dice [0.9317, 0.9577, 0.9339]\n",
      "2025-12-20 05:03:35.354437: Epoch time: 138.04 s\n",
      "2025-12-20 05:03:35.999500: \n",
      "2025-12-20 05:03:35.999500: Epoch 519\n",
      "2025-12-20 05:03:36.002038: Current learning rate: 0.00518\n",
      "2025-12-20 05:05:54.149917: train_loss -0.8499\n",
      "2025-12-20 05:05:54.149917: val_loss -0.8772\n",
      "2025-12-20 05:05:54.155926: Pseudo dice [0.9267, 0.9601, 0.9342]\n",
      "2025-12-20 05:05:54.161936: Epoch time: 138.15 s\n",
      "2025-12-20 05:05:54.962203: \n",
      "2025-12-20 05:05:54.962203: Epoch 520\n",
      "2025-12-20 05:05:54.968059: Current learning rate: 0.00517\n",
      "2025-12-20 05:08:13.078436: train_loss -0.8499\n",
      "2025-12-20 05:08:13.078436: val_loss -0.8756\n",
      "2025-12-20 05:08:13.085067: Pseudo dice [0.9245, 0.9566, 0.9438]\n",
      "2025-12-20 05:08:13.089072: Epoch time: 138.12 s\n",
      "2025-12-20 05:08:13.093076: Yayy! New best EMA pseudo Dice: 0.9403\n",
      "2025-12-20 05:08:13.989722: \n",
      "2025-12-20 05:08:13.993225: Epoch 521\n",
      "2025-12-20 05:08:13.993225: Current learning rate: 0.00516\n",
      "2025-12-20 05:10:32.435256: train_loss -0.8526\n",
      "2025-12-20 05:10:32.435256: val_loss -0.8667\n",
      "2025-12-20 05:10:32.452965: Pseudo dice [0.9199, 0.9529, 0.9362]\n",
      "2025-12-20 05:10:32.456970: Epoch time: 138.45 s\n",
      "2025-12-20 05:10:33.257944: \n",
      "2025-12-20 05:10:33.257944: Epoch 522\n",
      "2025-12-20 05:10:33.257944: Current learning rate: 0.00515\n",
      "2025-12-20 05:12:51.154889: train_loss -0.8465\n",
      "2025-12-20 05:12:51.154889: val_loss -0.8812\n",
      "2025-12-20 05:12:51.170995: Pseudo dice [0.9294, 0.9606, 0.9371]\n",
      "2025-12-20 05:12:51.170995: Epoch time: 137.9 s\n",
      "2025-12-20 05:12:51.932090: \n",
      "2025-12-20 05:12:51.932090: Epoch 523\n",
      "2025-12-20 05:12:51.932090: Current learning rate: 0.00514\n",
      "2025-12-20 05:15:10.075998: train_loss -0.85\n",
      "2025-12-20 05:15:10.075998: val_loss -0.8739\n",
      "2025-12-20 05:15:10.092042: Pseudo dice [0.9238, 0.9553, 0.9414]\n",
      "2025-12-20 05:15:10.092042: Epoch time: 138.14 s\n",
      "2025-12-20 05:15:10.710516: \n",
      "2025-12-20 05:15:10.710516: Epoch 524\n",
      "2025-12-20 05:15:10.726149: Current learning rate: 0.00513\n",
      "2025-12-20 05:17:28.729890: train_loss -0.8487\n",
      "2025-12-20 05:17:28.729890: val_loss -0.8754\n",
      "2025-12-20 05:17:28.738011: Pseudo dice [0.9234, 0.9551, 0.9459]\n",
      "2025-12-20 05:17:28.742016: Epoch time: 138.02 s\n",
      "2025-12-20 05:17:29.365522: \n",
      "2025-12-20 05:17:29.365522: Epoch 525\n",
      "2025-12-20 05:17:29.381624: Current learning rate: 0.00512\n",
      "2025-12-20 05:19:47.291307: train_loss -0.8516\n",
      "2025-12-20 05:19:47.291307: val_loss -0.8717\n",
      "2025-12-20 05:19:47.295634: Pseudo dice [0.9206, 0.9546, 0.945]\n",
      "2025-12-20 05:19:47.299638: Epoch time: 137.93 s\n",
      "2025-12-20 05:19:48.066238: \n",
      "2025-12-20 05:19:48.066238: Epoch 526\n",
      "2025-12-20 05:19:48.066238: Current learning rate: 0.00511\n",
      "2025-12-20 05:22:06.180334: train_loss -0.8514\n",
      "2025-12-20 05:22:06.180334: val_loss -0.8736\n",
      "2025-12-20 05:22:06.185789: Pseudo dice [0.9224, 0.9554, 0.9399]\n",
      "2025-12-20 05:22:06.189794: Epoch time: 138.13 s\n",
      "2025-12-20 05:22:06.820165: \n",
      "2025-12-20 05:22:06.822167: Epoch 527\n",
      "2025-12-20 05:22:06.826483: Current learning rate: 0.0051\n",
      "2025-12-20 05:24:24.882377: train_loss -0.8534\n",
      "2025-12-20 05:24:24.882377: val_loss -0.8745\n",
      "2025-12-20 05:24:24.894058: Pseudo dice [0.9216, 0.9573, 0.9438]\n",
      "2025-12-20 05:24:24.898061: Epoch time: 138.06 s\n",
      "2025-12-20 05:24:25.697913: \n",
      "2025-12-20 05:24:25.697913: Epoch 528\n",
      "2025-12-20 05:24:25.714005: Current learning rate: 0.00509\n",
      "2025-12-20 05:26:43.560711: train_loss -0.8542\n",
      "2025-12-20 05:26:43.560711: val_loss -0.8806\n",
      "2025-12-20 05:26:43.576757: Pseudo dice [0.9309, 0.9571, 0.9409]\n",
      "2025-12-20 05:26:43.576757: Epoch time: 137.86 s\n",
      "2025-12-20 05:26:43.576757: Yayy! New best EMA pseudo Dice: 0.9405\n",
      "2025-12-20 05:26:44.610021: \n",
      "2025-12-20 05:26:44.610021: Epoch 529\n",
      "2025-12-20 05:26:44.614677: Current learning rate: 0.00508\n",
      "2025-12-20 05:29:02.753589: train_loss -0.8483\n",
      "2025-12-20 05:29:02.753589: val_loss -0.8743\n",
      "2025-12-20 05:29:02.768768: Pseudo dice [0.9206, 0.9634, 0.9422]\n",
      "2025-12-20 05:29:02.771886: Epoch time: 138.15 s\n",
      "2025-12-20 05:29:02.776612: Yayy! New best EMA pseudo Dice: 0.9407\n",
      "2025-12-20 05:29:03.697057: \n",
      "2025-12-20 05:29:03.697057: Epoch 530\n",
      "2025-12-20 05:29:03.697057: Current learning rate: 0.00507\n",
      "2025-12-20 05:31:21.972911: train_loss -0.8465\n",
      "2025-12-20 05:31:21.972911: val_loss -0.8699\n",
      "2025-12-20 05:31:21.979638: Pseudo dice [0.921, 0.9585, 0.9419]\n",
      "2025-12-20 05:31:21.983642: Epoch time: 138.28 s\n",
      "2025-12-20 05:31:22.606193: \n",
      "2025-12-20 05:31:22.606193: Epoch 531\n",
      "2025-12-20 05:31:22.606193: Current learning rate: 0.00506\n",
      "2025-12-20 05:33:40.736748: train_loss -0.8452\n",
      "2025-12-20 05:33:40.736748: val_loss -0.8703\n",
      "2025-12-20 05:33:40.740753: Pseudo dice [0.9191, 0.9583, 0.9459]\n",
      "2025-12-20 05:33:40.746759: Epoch time: 138.13 s\n",
      "2025-12-20 05:33:40.750501: Yayy! New best EMA pseudo Dice: 0.9407\n",
      "2025-12-20 05:33:41.817244: \n",
      "2025-12-20 05:33:41.817244: Epoch 532\n",
      "2025-12-20 05:33:41.823252: Current learning rate: 0.00505\n",
      "2025-12-20 05:35:59.921025: train_loss -0.8485\n",
      "2025-12-20 05:35:59.921025: val_loss -0.8784\n",
      "2025-12-20 05:35:59.927037: Pseudo dice [0.9305, 0.961, 0.9336]\n",
      "2025-12-20 05:35:59.930783: Epoch time: 138.11 s\n",
      "2025-12-20 05:35:59.937487: Yayy! New best EMA pseudo Dice: 0.9408\n",
      "2025-12-20 05:36:00.879388: \n",
      "2025-12-20 05:36:00.879388: Epoch 533\n",
      "2025-12-20 05:36:00.879388: Current learning rate: 0.00504\n",
      "2025-12-20 05:38:19.079039: train_loss -0.8507\n",
      "2025-12-20 05:38:19.079039: val_loss -0.8738\n",
      "2025-12-20 05:38:19.085045: Pseudo dice [0.9209, 0.9559, 0.9438]\n",
      "2025-12-20 05:38:19.089049: Epoch time: 138.2 s\n",
      "2025-12-20 05:38:19.897220: \n",
      "2025-12-20 05:38:19.897220: Epoch 534\n",
      "2025-12-20 05:38:19.912467: Current learning rate: 0.00503\n",
      "2025-12-20 05:40:38.003081: train_loss -0.8511\n",
      "2025-12-20 05:40:38.005084: val_loss -0.8751\n",
      "2025-12-20 05:40:38.006825: Pseudo dice [0.9235, 0.9559, 0.9408]\n",
      "2025-12-20 05:40:38.013679: Epoch time: 138.11 s\n",
      "2025-12-20 05:40:38.828842: \n",
      "2025-12-20 05:40:38.828842: Epoch 535\n",
      "2025-12-20 05:40:38.828842: Current learning rate: 0.00502\n",
      "2025-12-20 05:42:56.821275: train_loss -0.8507\n",
      "2025-12-20 05:42:56.821275: val_loss -0.877\n",
      "2025-12-20 05:42:56.827017: Pseudo dice [0.9258, 0.9543, 0.944]\n",
      "2025-12-20 05:42:56.831021: Epoch time: 137.99 s\n",
      "2025-12-20 05:42:57.457266: \n",
      "2025-12-20 05:42:57.457266: Epoch 536\n",
      "2025-12-20 05:42:57.457266: Current learning rate: 0.00501\n",
      "2025-12-20 05:45:15.676650: train_loss -0.8529\n",
      "2025-12-20 05:45:15.676650: val_loss -0.8697\n",
      "2025-12-20 05:45:15.676650: Pseudo dice [0.9207, 0.9524, 0.9431]\n",
      "2025-12-20 05:45:15.676650: Epoch time: 138.22 s\n",
      "2025-12-20 05:45:16.310101: \n",
      "2025-12-20 05:45:16.312103: Epoch 537\n",
      "2025-12-20 05:45:16.312103: Current learning rate: 0.005\n",
      "2025-12-20 05:47:34.347543: train_loss -0.8548\n",
      "2025-12-20 05:47:34.347543: val_loss -0.8735\n",
      "2025-12-20 05:47:34.352154: Pseudo dice [0.922, 0.9551, 0.9428]\n",
      "2025-12-20 05:47:34.355658: Epoch time: 138.04 s\n",
      "2025-12-20 05:47:35.146605: \n",
      "2025-12-20 05:47:35.146605: Epoch 538\n",
      "2025-12-20 05:47:35.162380: Current learning rate: 0.00499\n",
      "2025-12-20 05:49:53.145115: train_loss -0.8516\n",
      "2025-12-20 05:49:53.147117: val_loss -0.8713\n",
      "2025-12-20 05:49:53.151274: Pseudo dice [0.9223, 0.9561, 0.9377]\n",
      "2025-12-20 05:49:53.155279: Epoch time: 138.0 s\n",
      "2025-12-20 05:49:53.795977: \n",
      "2025-12-20 05:49:53.795977: Epoch 539\n",
      "2025-12-20 05:49:53.800461: Current learning rate: 0.00498\n",
      "2025-12-20 05:52:11.932406: train_loss -0.8493\n",
      "2025-12-20 05:52:11.932406: val_loss -0.8848\n",
      "2025-12-20 05:52:11.948427: Pseudo dice [0.9306, 0.9584, 0.9489]\n",
      "2025-12-20 05:52:11.952489: Epoch time: 138.14 s\n",
      "2025-12-20 05:52:11.956493: Yayy! New best EMA pseudo Dice: 0.9409\n",
      "2025-12-20 05:52:13.017658: \n",
      "2025-12-20 05:52:13.017658: Epoch 540\n",
      "2025-12-20 05:52:13.033441: Current learning rate: 0.00497\n",
      "2025-12-20 05:54:31.116719: train_loss -0.8477\n",
      "2025-12-20 05:54:31.116719: val_loss -0.8751\n",
      "2025-12-20 05:54:31.118222: Pseudo dice [0.9261, 0.9597, 0.935]\n",
      "2025-12-20 05:54:31.124900: Epoch time: 138.1 s\n",
      "2025-12-20 05:54:31.865421: \n",
      "2025-12-20 05:54:31.865421: Epoch 541\n",
      "2025-12-20 05:54:31.865421: Current learning rate: 0.00496\n",
      "2025-12-20 05:56:49.905093: train_loss -0.8466\n",
      "2025-12-20 05:56:49.906095: val_loss -0.8669\n",
      "2025-12-20 05:56:49.918121: Pseudo dice [0.9199, 0.9544, 0.9335]\n",
      "2025-12-20 05:56:49.921868: Epoch time: 138.04 s\n",
      "2025-12-20 05:56:50.539121: \n",
      "2025-12-20 05:56:50.539121: Epoch 542\n",
      "2025-12-20 05:56:50.539121: Current learning rate: 0.00495\n",
      "2025-12-20 05:59:08.668796: train_loss -0.8551\n",
      "2025-12-20 05:59:08.668796: val_loss -0.8708\n",
      "2025-12-20 05:59:08.668796: Pseudo dice [0.9211, 0.9552, 0.9456]\n",
      "2025-12-20 05:59:08.668796: Epoch time: 138.11 s\n",
      "2025-12-20 05:59:09.301117: \n",
      "2025-12-20 05:59:09.301117: Epoch 543\n",
      "2025-12-20 05:59:09.301117: Current learning rate: 0.00494\n",
      "2025-12-20 06:01:27.288595: train_loss -0.8429\n",
      "2025-12-20 06:01:27.288595: val_loss -0.8747\n",
      "2025-12-20 06:01:27.294601: Pseudo dice [0.9253, 0.954, 0.9448]\n",
      "2025-12-20 06:01:27.298606: Epoch time: 137.99 s\n",
      "2025-12-20 06:01:28.076528: \n",
      "2025-12-20 06:01:28.076528: Epoch 544\n",
      "2025-12-20 06:01:28.092328: Current learning rate: 0.00493\n",
      "2025-12-20 06:03:46.256121: train_loss -0.8462\n",
      "2025-12-20 06:03:46.258124: val_loss -0.8822\n",
      "2025-12-20 06:03:46.258124: Pseudo dice [0.9291, 0.9579, 0.9387]\n",
      "2025-12-20 06:03:46.265862: Epoch time: 138.18 s\n",
      "2025-12-20 06:03:46.884863: \n",
      "2025-12-20 06:03:46.884863: Epoch 545\n",
      "2025-12-20 06:03:46.884863: Current learning rate: 0.00492\n",
      "2025-12-20 06:06:05.062620: train_loss -0.8479\n",
      "2025-12-20 06:06:05.062620: val_loss -0.8728\n",
      "2025-12-20 06:06:05.062620: Pseudo dice [0.9225, 0.954, 0.9417]\n",
      "2025-12-20 06:06:05.062620: Epoch time: 138.18 s\n",
      "2025-12-20 06:06:05.854736: \n",
      "2025-12-20 06:06:05.854736: Epoch 546\n",
      "2025-12-20 06:06:05.854736: Current learning rate: 0.00491\n",
      "2025-12-20 06:08:23.865292: train_loss -0.8524\n",
      "2025-12-20 06:08:23.865292: val_loss -0.8748\n",
      "2025-12-20 06:08:23.877302: Pseudo dice [0.9268, 0.9593, 0.9384]\n",
      "2025-12-20 06:08:23.883153: Epoch time: 138.01 s\n",
      "2025-12-20 06:08:24.578242: \n",
      "2025-12-20 06:08:24.578242: Epoch 547\n",
      "2025-12-20 06:08:24.578242: Current learning rate: 0.0049\n",
      "2025-12-20 06:10:43.018114: train_loss -0.8539\n",
      "2025-12-20 06:10:43.018114: val_loss -0.8725\n",
      "2025-12-20 06:10:43.024590: Pseudo dice [0.9214, 0.9559, 0.9364]\n",
      "2025-12-20 06:10:43.026092: Epoch time: 138.44 s\n",
      "2025-12-20 06:10:43.659075: \n",
      "2025-12-20 06:10:43.659075: Epoch 548\n",
      "2025-12-20 06:10:43.659075: Current learning rate: 0.00489\n",
      "2025-12-20 06:13:01.715605: train_loss -0.8474\n",
      "2025-12-20 06:13:01.717345: val_loss -0.8712\n",
      "2025-12-20 06:13:01.719348: Pseudo dice [0.918, 0.9559, 0.9471]\n",
      "2025-12-20 06:13:01.726599: Epoch time: 138.06 s\n",
      "2025-12-20 06:13:02.364566: \n",
      "2025-12-20 06:13:02.364566: Epoch 549\n",
      "2025-12-20 06:13:02.364566: Current learning rate: 0.00488\n",
      "2025-12-20 06:15:20.409644: train_loss -0.8374\n",
      "2025-12-20 06:15:20.411646: val_loss -0.8623\n",
      "2025-12-20 06:15:20.415651: Pseudo dice [0.9204, 0.9547, 0.9277]\n",
      "2025-12-20 06:15:20.417653: Epoch time: 138.05 s\n",
      "2025-12-20 06:15:21.393366: \n",
      "2025-12-20 06:15:21.393366: Epoch 550\n",
      "2025-12-20 06:15:21.407375: Current learning rate: 0.00487\n",
      "2025-12-20 06:17:39.385518: train_loss -0.8355\n",
      "2025-12-20 06:17:39.385518: val_loss -0.8693\n",
      "2025-12-20 06:17:39.385518: Pseudo dice [0.9251, 0.9582, 0.9316]\n",
      "2025-12-20 06:17:39.385518: Epoch time: 137.99 s\n",
      "2025-12-20 06:17:40.019329: \n",
      "2025-12-20 06:17:40.019329: Epoch 551\n",
      "2025-12-20 06:17:40.019329: Current learning rate: 0.00486\n",
      "2025-12-20 06:19:58.252126: train_loss -0.8336\n",
      "2025-12-20 06:19:58.252126: val_loss -0.868\n",
      "2025-12-20 06:19:58.255868: Pseudo dice [0.9212, 0.9535, 0.9368]\n",
      "2025-12-20 06:19:58.255868: Epoch time: 138.23 s\n",
      "2025-12-20 06:19:59.045825: \n",
      "2025-12-20 06:19:59.045825: Epoch 552\n",
      "2025-12-20 06:19:59.061509: Current learning rate: 0.00485\n",
      "2025-12-20 06:22:17.119558: train_loss -0.8426\n",
      "2025-12-20 06:22:17.119558: val_loss -0.8736\n",
      "2025-12-20 06:22:17.125698: Pseudo dice [0.9262, 0.9608, 0.9289]\n",
      "2025-12-20 06:22:17.129441: Epoch time: 138.07 s\n",
      "2025-12-20 06:22:17.917343: \n",
      "2025-12-20 06:22:17.917343: Epoch 553\n",
      "2025-12-20 06:22:17.917343: Current learning rate: 0.00484\n",
      "2025-12-20 06:24:36.066044: train_loss -0.8474\n",
      "2025-12-20 06:24:36.066044: val_loss -0.8718\n",
      "2025-12-20 06:24:36.066044: Pseudo dice [0.9229, 0.9573, 0.9394]\n",
      "2025-12-20 06:24:36.083848: Epoch time: 138.15 s\n",
      "2025-12-20 06:24:36.714981: \n",
      "2025-12-20 06:24:36.714981: Epoch 554\n",
      "2025-12-20 06:24:36.714981: Current learning rate: 0.00484\n",
      "2025-12-20 06:26:54.691179: train_loss -0.8481\n",
      "2025-12-20 06:26:54.691179: val_loss -0.8722\n",
      "2025-12-20 06:26:54.691179: Pseudo dice [0.9215, 0.9516, 0.9452]\n",
      "2025-12-20 06:26:54.691179: Epoch time: 137.98 s\n",
      "2025-12-20 06:26:55.325495: \n",
      "2025-12-20 06:26:55.325495: Epoch 555\n",
      "2025-12-20 06:26:55.325495: Current learning rate: 0.00483\n",
      "2025-12-20 06:29:13.372214: train_loss -0.8447\n",
      "2025-12-20 06:29:13.374217: val_loss -0.8724\n",
      "2025-12-20 06:29:13.380225: Pseudo dice [0.9234, 0.9577, 0.9334]\n",
      "2025-12-20 06:29:13.385969: Epoch time: 138.05 s\n",
      "2025-12-20 06:29:14.048492: \n",
      "2025-12-20 06:29:14.048492: Epoch 556\n",
      "2025-12-20 06:29:14.048492: Current learning rate: 0.00482\n",
      "2025-12-20 06:31:32.151443: train_loss -0.8465\n",
      "2025-12-20 06:31:32.153449: val_loss -0.8788\n",
      "2025-12-20 06:31:32.159462: Pseudo dice [0.9261, 0.9582, 0.9434]\n",
      "2025-12-20 06:31:32.165206: Epoch time: 138.1 s\n",
      "2025-12-20 06:31:32.795701: \n",
      "2025-12-20 06:31:32.795701: Epoch 557\n",
      "2025-12-20 06:31:32.795701: Current learning rate: 0.00481\n",
      "2025-12-20 06:33:50.960618: train_loss -0.848\n",
      "2025-12-20 06:33:50.960618: val_loss -0.8669\n",
      "2025-12-20 06:33:50.966624: Pseudo dice [0.9182, 0.9469, 0.9421]\n",
      "2025-12-20 06:33:50.972368: Epoch time: 138.16 s\n",
      "2025-12-20 06:33:51.768030: \n",
      "2025-12-20 06:33:51.768030: Epoch 558\n",
      "2025-12-20 06:33:51.768030: Current learning rate: 0.0048\n",
      "2025-12-20 06:36:09.781299: train_loss -0.845\n",
      "2025-12-20 06:36:09.781299: val_loss -0.8829\n",
      "2025-12-20 06:36:09.787311: Pseudo dice [0.9317, 0.9612, 0.9422]\n",
      "2025-12-20 06:36:09.793317: Epoch time: 138.01 s\n",
      "2025-12-20 06:36:10.412234: \n",
      "2025-12-20 06:36:10.412234: Epoch 559\n",
      "2025-12-20 06:36:10.427956: Current learning rate: 0.00479\n",
      "2025-12-20 06:38:28.561064: train_loss -0.854\n",
      "2025-12-20 06:38:28.561064: val_loss -0.8785\n",
      "2025-12-20 06:38:28.565068: Pseudo dice [0.9324, 0.9629, 0.9245]\n",
      "2025-12-20 06:38:28.568548: Epoch time: 138.15 s\n",
      "2025-12-20 06:38:29.201359: \n",
      "2025-12-20 06:38:29.201359: Epoch 560\n",
      "2025-12-20 06:38:29.201359: Current learning rate: 0.00478\n",
      "2025-12-20 06:40:47.174463: train_loss -0.8491\n",
      "2025-12-20 06:40:47.174463: val_loss -0.8745\n",
      "2025-12-20 06:40:47.180471: Pseudo dice [0.9227, 0.9572, 0.943]\n",
      "2025-12-20 06:40:47.184475: Epoch time: 137.99 s\n",
      "2025-12-20 06:40:47.838452: \n",
      "2025-12-20 06:40:47.838452: Epoch 561\n",
      "2025-12-20 06:40:47.838452: Current learning rate: 0.00477\n",
      "2025-12-20 06:43:05.932801: train_loss -0.8487\n",
      "2025-12-20 06:43:05.948761: val_loss -0.8771\n",
      "2025-12-20 06:43:05.948761: Pseudo dice [0.9238, 0.957, 0.9418]\n",
      "2025-12-20 06:43:05.948761: Epoch time: 138.09 s\n",
      "2025-12-20 06:43:06.581819: \n",
      "2025-12-20 06:43:06.581819: Epoch 562\n",
      "2025-12-20 06:43:06.581819: Current learning rate: 0.00476\n",
      "2025-12-20 06:45:24.635612: train_loss -0.8448\n",
      "2025-12-20 06:45:24.637614: val_loss -0.8801\n",
      "2025-12-20 06:45:24.639616: Pseudo dice [0.9273, 0.9626, 0.944]\n",
      "2025-12-20 06:45:24.645432: Epoch time: 138.07 s\n",
      "2025-12-20 06:45:25.281275: \n",
      "2025-12-20 06:45:25.281275: Epoch 563\n",
      "2025-12-20 06:45:25.281275: Current learning rate: 0.00475\n",
      "2025-12-20 06:47:43.139745: train_loss -0.8451\n",
      "2025-12-20 06:47:43.141747: val_loss -0.8738\n",
      "2025-12-20 06:47:43.146753: Pseudo dice [0.9257, 0.9555, 0.936]\n",
      "2025-12-20 06:47:43.150757: Epoch time: 137.86 s\n",
      "2025-12-20 06:47:43.970536: \n",
      "2025-12-20 06:47:43.970536: Epoch 564\n",
      "2025-12-20 06:47:43.970536: Current learning rate: 0.00474\n",
      "2025-12-20 06:50:01.944138: train_loss -0.8498\n",
      "2025-12-20 06:50:01.946141: val_loss -0.8835\n",
      "2025-12-20 06:50:01.950632: Pseudo dice [0.9311, 0.9594, 0.9398]\n",
      "2025-12-20 06:50:01.954136: Epoch time: 137.97 s\n",
      "2025-12-20 06:50:02.586300: \n",
      "2025-12-20 06:50:02.586300: Epoch 565\n",
      "2025-12-20 06:50:02.602008: Current learning rate: 0.00473\n",
      "2025-12-20 06:52:20.561846: train_loss -0.856\n",
      "2025-12-20 06:52:20.561846: val_loss -0.8888\n",
      "2025-12-20 06:52:20.561846: Pseudo dice [0.9345, 0.9621, 0.9449]\n",
      "2025-12-20 06:52:20.570992: Epoch time: 137.98 s\n",
      "2025-12-20 06:52:20.570992: Yayy! New best EMA pseudo Dice: 0.9413\n",
      "2025-12-20 06:52:21.457887: \n",
      "2025-12-20 06:52:21.457887: Epoch 566\n",
      "2025-12-20 06:52:21.473806: Current learning rate: 0.00472\n",
      "2025-12-20 06:54:39.594690: train_loss -0.8533\n",
      "2025-12-20 06:54:39.594690: val_loss -0.8788\n",
      "2025-12-20 06:54:39.602701: Pseudo dice [0.9266, 0.9596, 0.9367]\n",
      "2025-12-20 06:54:39.608709: Epoch time: 138.14 s\n",
      "2025-12-20 06:54:40.243766: \n",
      "2025-12-20 06:54:40.243766: Epoch 567\n",
      "2025-12-20 06:54:40.243766: Current learning rate: 0.00471\n",
      "2025-12-20 06:56:58.429592: train_loss -0.849\n",
      "2025-12-20 06:56:58.429592: val_loss -0.8761\n",
      "2025-12-20 06:56:58.435335: Pseudo dice [0.9224, 0.9564, 0.9431]\n",
      "2025-12-20 06:56:58.435335: Epoch time: 138.19 s\n",
      "2025-12-20 06:56:59.070170: \n",
      "2025-12-20 06:56:59.070170: Epoch 568\n",
      "2025-12-20 06:56:59.070170: Current learning rate: 0.0047\n",
      "2025-12-20 06:59:17.101076: train_loss -0.8537\n",
      "2025-12-20 06:59:17.101076: val_loss -0.8596\n",
      "2025-12-20 06:59:17.101076: Pseudo dice [0.9179, 0.9592, 0.9181]\n",
      "2025-12-20 06:59:17.108601: Epoch time: 138.03 s\n",
      "2025-12-20 06:59:17.733458: \n",
      "2025-12-20 06:59:17.733458: Epoch 569\n",
      "2025-12-20 06:59:17.733458: Current learning rate: 0.00469\n",
      "2025-12-20 07:01:35.839551: train_loss -0.837\n",
      "2025-12-20 07:01:35.839551: val_loss -0.8758\n",
      "2025-12-20 07:01:35.839551: Pseudo dice [0.9269, 0.9583, 0.9415]\n",
      "2025-12-20 07:01:35.839551: Epoch time: 138.12 s\n",
      "2025-12-20 07:01:36.739569: \n",
      "2025-12-20 07:01:36.739569: Epoch 570\n",
      "2025-12-20 07:01:36.755261: Current learning rate: 0.00468\n",
      "2025-12-20 07:03:55.041246: train_loss -0.8448\n",
      "2025-12-20 07:03:55.043253: val_loss -0.8703\n",
      "2025-12-20 07:03:55.048998: Pseudo dice [0.9189, 0.9502, 0.944]\n",
      "2025-12-20 07:03:55.053002: Epoch time: 138.3 s\n",
      "2025-12-20 07:03:55.683048: \n",
      "2025-12-20 07:03:55.683048: Epoch 571\n",
      "2025-12-20 07:03:55.683048: Current learning rate: 0.00467\n",
      "2025-12-20 07:06:13.793588: train_loss -0.8468\n",
      "2025-12-20 07:06:13.795596: val_loss -0.8802\n",
      "2025-12-20 07:06:13.801344: Pseudo dice [0.9301, 0.9602, 0.9431]\n",
      "2025-12-20 07:06:13.805674: Epoch time: 138.11 s\n",
      "2025-12-20 07:06:14.432095: \n",
      "2025-12-20 07:06:14.432095: Epoch 572\n",
      "2025-12-20 07:06:14.432095: Current learning rate: 0.00466\n",
      "2025-12-20 07:08:32.663667: train_loss -0.8512\n",
      "2025-12-20 07:08:32.665669: val_loss -0.8767\n",
      "2025-12-20 07:08:32.671676: Pseudo dice [0.9251, 0.9567, 0.9435]\n",
      "2025-12-20 07:08:32.675679: Epoch time: 138.23 s\n",
      "2025-12-20 07:08:33.409137: \n",
      "2025-12-20 07:08:33.409137: Epoch 573\n",
      "2025-12-20 07:08:33.409137: Current learning rate: 0.00465\n",
      "2025-12-20 07:10:51.801093: train_loss -0.8539\n",
      "2025-12-20 07:10:51.801093: val_loss -0.8721\n",
      "2025-12-20 07:10:51.801093: Pseudo dice [0.9227, 0.9485, 0.9464]\n",
      "2025-12-20 07:10:51.815167: Epoch time: 138.39 s\n",
      "2025-12-20 07:10:52.446866: \n",
      "2025-12-20 07:10:52.446866: Epoch 574\n",
      "2025-12-20 07:10:52.446866: Current learning rate: 0.00464\n",
      "2025-12-20 07:13:10.508104: train_loss -0.8455\n",
      "2025-12-20 07:13:10.510106: val_loss -0.8735\n",
      "2025-12-20 07:13:10.514109: Pseudo dice [0.9238, 0.9582, 0.93]\n",
      "2025-12-20 07:13:10.518113: Epoch time: 138.06 s\n",
      "2025-12-20 07:13:11.162048: \n",
      "2025-12-20 07:13:11.162048: Epoch 575\n",
      "2025-12-20 07:13:11.162048: Current learning rate: 0.00463\n",
      "2025-12-20 07:15:29.322057: train_loss -0.8487\n",
      "2025-12-20 07:15:29.324059: val_loss -0.8747\n",
      "2025-12-20 07:15:29.329065: Pseudo dice [0.9254, 0.958, 0.9403]\n",
      "2025-12-20 07:15:29.329065: Epoch time: 138.16 s\n",
      "2025-12-20 07:15:30.134962: \n",
      "2025-12-20 07:15:30.134962: Epoch 576\n",
      "2025-12-20 07:15:30.134962: Current learning rate: 0.00462\n",
      "2025-12-20 07:17:48.347051: train_loss -0.85\n",
      "2025-12-20 07:17:48.348791: val_loss -0.8788\n",
      "2025-12-20 07:17:48.352795: Pseudo dice [0.9284, 0.9597, 0.9377]\n",
      "2025-12-20 07:17:48.352795: Epoch time: 138.21 s\n",
      "2025-12-20 07:17:49.160489: \n",
      "2025-12-20 07:17:49.160489: Epoch 577\n",
      "2025-12-20 07:17:49.160489: Current learning rate: 0.00461\n",
      "2025-12-20 07:20:07.321343: train_loss -0.8565\n",
      "2025-12-20 07:20:07.321343: val_loss -0.8843\n",
      "2025-12-20 07:20:07.327411: Pseudo dice [0.9305, 0.9646, 0.9467]\n",
      "2025-12-20 07:20:07.331416: Epoch time: 138.16 s\n",
      "2025-12-20 07:20:07.959716: \n",
      "2025-12-20 07:20:07.959716: Epoch 578\n",
      "2025-12-20 07:20:07.959716: Current learning rate: 0.0046\n",
      "2025-12-20 07:22:26.199988: train_loss -0.8484\n",
      "2025-12-20 07:22:26.201730: val_loss -0.8778\n",
      "2025-12-20 07:22:26.207738: Pseudo dice [0.9257, 0.9548, 0.945]\n",
      "2025-12-20 07:22:26.211741: Epoch time: 138.24 s\n",
      "2025-12-20 07:22:27.008518: \n",
      "2025-12-20 07:22:27.008518: Epoch 579\n",
      "2025-12-20 07:22:27.024292: Current learning rate: 0.00459\n",
      "2025-12-20 07:24:45.003505: train_loss -0.8462\n",
      "2025-12-20 07:24:45.003505: val_loss -0.8742\n",
      "2025-12-20 07:24:45.003505: Pseudo dice [0.9205, 0.9556, 0.94]\n",
      "2025-12-20 07:24:45.014570: Epoch time: 137.99 s\n",
      "2025-12-20 07:24:45.654102: \n",
      "2025-12-20 07:24:45.654102: Epoch 580\n",
      "2025-12-20 07:24:45.654102: Current learning rate: 0.00458\n",
      "2025-12-20 07:27:03.620139: train_loss -0.8479\n",
      "2025-12-20 07:27:03.620139: val_loss -0.8802\n",
      "2025-12-20 07:27:03.622141: Pseudo dice [0.9296, 0.9545, 0.9437]\n",
      "2025-12-20 07:27:03.622141: Epoch time: 137.97 s\n",
      "2025-12-20 07:27:04.248909: \n",
      "2025-12-20 07:27:04.248909: Epoch 581\n",
      "2025-12-20 07:27:04.265019: Current learning rate: 0.00457\n",
      "2025-12-20 07:29:22.279929: train_loss -0.8494\n",
      "2025-12-20 07:29:22.279929: val_loss -0.8748\n",
      "2025-12-20 07:29:22.285938: Pseudo dice [0.9223, 0.954, 0.9444]\n",
      "2025-12-20 07:29:22.290354: Epoch time: 138.03 s\n",
      "2025-12-20 07:29:23.083496: \n",
      "2025-12-20 07:29:23.083496: Epoch 582\n",
      "2025-12-20 07:29:23.083496: Current learning rate: 0.00456\n",
      "2025-12-20 07:31:41.178644: train_loss -0.8512\n",
      "2025-12-20 07:31:41.178644: val_loss -0.8779\n",
      "2025-12-20 07:31:41.178644: Pseudo dice [0.9261, 0.9563, 0.9429]\n",
      "2025-12-20 07:31:41.178644: Epoch time: 138.1 s\n",
      "2025-12-20 07:31:41.983925: \n",
      "2025-12-20 07:31:41.983925: Epoch 583\n",
      "2025-12-20 07:31:41.983925: Current learning rate: 0.00455\n",
      "2025-12-20 07:34:00.124198: train_loss -0.8462\n",
      "2025-12-20 07:34:00.124198: val_loss -0.8875\n",
      "2025-12-20 07:34:00.124198: Pseudo dice [0.9326, 0.9607, 0.9442]\n",
      "2025-12-20 07:34:00.124198: Epoch time: 138.14 s\n",
      "2025-12-20 07:34:00.124198: Yayy! New best EMA pseudo Dice: 0.9416\n",
      "2025-12-20 07:34:01.011765: \n",
      "2025-12-20 07:34:01.011765: Epoch 584\n",
      "2025-12-20 07:34:01.011765: Current learning rate: 0.00454\n",
      "2025-12-20 07:36:19.062769: train_loss -0.8483\n",
      "2025-12-20 07:36:19.062769: val_loss -0.8742\n",
      "2025-12-20 07:36:19.068775: Pseudo dice [0.9247, 0.9535, 0.9434]\n",
      "2025-12-20 07:36:19.073342: Epoch time: 138.05 s\n",
      "2025-12-20 07:36:19.908315: \n",
      "2025-12-20 07:36:19.908315: Epoch 585\n",
      "2025-12-20 07:36:19.908315: Current learning rate: 0.00453\n",
      "2025-12-20 07:38:37.857488: train_loss -0.8567\n",
      "2025-12-20 07:38:37.857488: val_loss -0.8782\n",
      "2025-12-20 07:38:37.865498: Pseudo dice [0.9258, 0.9581, 0.9436]\n",
      "2025-12-20 07:38:37.869502: Epoch time: 137.95 s\n",
      "2025-12-20 07:38:38.505718: \n",
      "2025-12-20 07:38:38.505718: Epoch 586\n",
      "2025-12-20 07:38:38.505718: Current learning rate: 0.00452\n",
      "2025-12-20 07:40:56.632332: train_loss -0.8503\n",
      "2025-12-20 07:40:56.632332: val_loss -0.8789\n",
      "2025-12-20 07:40:56.638338: Pseudo dice [0.9271, 0.9572, 0.9423]\n",
      "2025-12-20 07:40:56.643963: Epoch time: 138.13 s\n",
      "2025-12-20 07:40:56.647923: Yayy! New best EMA pseudo Dice: 0.9417\n",
      "2025-12-20 07:40:57.542360: \n",
      "2025-12-20 07:40:57.542360: Epoch 587\n",
      "2025-12-20 07:40:57.542360: Current learning rate: 0.00451\n",
      "2025-12-20 07:43:15.815485: train_loss -0.8529\n",
      "2025-12-20 07:43:15.815485: val_loss -0.8752\n",
      "2025-12-20 07:43:15.825501: Pseudo dice [0.9231, 0.9581, 0.9423]\n",
      "2025-12-20 07:43:15.831248: Epoch time: 138.28 s\n",
      "2025-12-20 07:43:16.809891: \n",
      "2025-12-20 07:43:16.809891: Epoch 588\n",
      "2025-12-20 07:43:16.809891: Current learning rate: 0.0045\n",
      "2025-12-20 07:45:34.929405: train_loss -0.8478\n",
      "2025-12-20 07:45:34.929405: val_loss -0.881\n",
      "2025-12-20 07:45:34.945243: Pseudo dice [0.9274, 0.9626, 0.9368]\n",
      "2025-12-20 07:45:34.945243: Epoch time: 138.12 s\n",
      "2025-12-20 07:45:34.955839: Yayy! New best EMA pseudo Dice: 0.9417\n",
      "2025-12-20 07:45:35.867284: \n",
      "2025-12-20 07:45:35.867284: Epoch 589\n",
      "2025-12-20 07:45:35.867284: Current learning rate: 0.00449\n",
      "2025-12-20 07:47:53.855821: train_loss -0.8466\n",
      "2025-12-20 07:47:53.857823: val_loss -0.8666\n",
      "2025-12-20 07:47:53.863831: Pseudo dice [0.9183, 0.9586, 0.9355]\n",
      "2025-12-20 07:47:53.869838: Epoch time: 137.99 s\n",
      "2025-12-20 07:47:54.512224: \n",
      "2025-12-20 07:47:54.512224: Epoch 590\n",
      "2025-12-20 07:47:54.512224: Current learning rate: 0.00448\n",
      "2025-12-20 07:50:12.720040: train_loss -0.8462\n",
      "2025-12-20 07:50:12.720040: val_loss -0.8766\n",
      "2025-12-20 07:50:12.724804: Pseudo dice [0.9274, 0.9569, 0.9378]\n",
      "2025-12-20 07:50:12.730811: Epoch time: 138.21 s\n",
      "2025-12-20 07:50:13.506596: \n",
      "2025-12-20 07:50:13.506596: Epoch 591\n",
      "2025-12-20 07:50:13.506596: Current learning rate: 0.00447\n",
      "2025-12-20 07:52:31.398237: train_loss -0.8472\n",
      "2025-12-20 07:52:31.398237: val_loss -0.8711\n",
      "2025-12-20 07:52:31.400849: Pseudo dice [0.923, 0.9511, 0.9391]\n",
      "2025-12-20 07:52:31.407898: Epoch time: 137.89 s\n",
      "2025-12-20 07:52:32.082712: \n",
      "2025-12-20 07:52:32.082712: Epoch 592\n",
      "2025-12-20 07:52:32.082712: Current learning rate: 0.00446\n",
      "2025-12-20 07:54:50.088590: train_loss -0.8483\n",
      "2025-12-20 07:54:50.088590: val_loss -0.8803\n",
      "2025-12-20 07:54:50.094596: Pseudo dice [0.9283, 0.961, 0.9386]\n",
      "2025-12-20 07:54:50.098599: Epoch time: 138.01 s\n",
      "2025-12-20 07:54:50.745996: \n",
      "2025-12-20 07:54:50.745996: Epoch 593\n",
      "2025-12-20 07:54:50.745996: Current learning rate: 0.00445\n",
      "2025-12-20 07:57:08.788661: train_loss -0.8486\n",
      "2025-12-20 07:57:08.788661: val_loss -0.8733\n",
      "2025-12-20 07:57:08.792667: Pseudo dice [0.9234, 0.9604, 0.9326]\n",
      "2025-12-20 07:57:08.796671: Epoch time: 138.06 s\n",
      "2025-12-20 07:57:09.543432: \n",
      "2025-12-20 07:57:09.543432: Epoch 594\n",
      "2025-12-20 07:57:09.543432: Current learning rate: 0.00444\n",
      "2025-12-20 07:59:27.480364: train_loss -0.8519\n",
      "2025-12-20 07:59:27.482365: val_loss -0.8751\n",
      "2025-12-20 07:59:27.488371: Pseudo dice [0.9225, 0.9552, 0.9402]\n",
      "2025-12-20 07:59:27.492972: Epoch time: 137.94 s\n",
      "2025-12-20 07:59:28.344584: \n",
      "2025-12-20 07:59:28.344584: Epoch 595\n",
      "2025-12-20 07:59:28.347136: Current learning rate: 0.00443\n",
      "2025-12-20 08:01:46.373286: train_loss -0.8542\n",
      "2025-12-20 08:01:46.375026: val_loss -0.8755\n",
      "2025-12-20 08:01:46.381035: Pseudo dice [0.9234, 0.9572, 0.9416]\n",
      "2025-12-20 08:01:46.389047: Epoch time: 138.03 s\n",
      "2025-12-20 08:01:47.037590: \n",
      "2025-12-20 08:01:47.037590: Epoch 596\n",
      "2025-12-20 08:01:47.037590: Current learning rate: 0.00442\n",
      "2025-12-20 08:04:05.114873: train_loss -0.8508\n",
      "2025-12-20 08:04:05.114873: val_loss -0.8826\n",
      "2025-12-20 08:04:05.122202: Pseudo dice [0.9316, 0.9579, 0.9383]\n",
      "2025-12-20 08:04:05.126208: Epoch time: 138.08 s\n",
      "2025-12-20 08:04:05.929883: \n",
      "2025-12-20 08:04:05.931886: Epoch 597\n",
      "2025-12-20 08:04:05.931886: Current learning rate: 0.00441\n",
      "2025-12-20 08:06:24.098948: train_loss -0.8456\n",
      "2025-12-20 08:06:24.100950: val_loss -0.8731\n",
      "2025-12-20 08:06:24.106956: Pseudo dice [0.9208, 0.9526, 0.9459]\n",
      "2025-12-20 08:06:24.112463: Epoch time: 138.17 s\n",
      "2025-12-20 08:06:24.762993: \n",
      "2025-12-20 08:06:24.762993: Epoch 598\n",
      "2025-12-20 08:06:24.764734: Current learning rate: 0.0044\n",
      "2025-12-20 08:08:42.759120: train_loss -0.8481\n",
      "2025-12-20 08:08:42.759120: val_loss -0.8732\n",
      "2025-12-20 08:08:42.765126: Pseudo dice [0.9225, 0.9527, 0.9402]\n",
      "2025-12-20 08:08:42.768970: Epoch time: 138.0 s\n",
      "2025-12-20 08:08:43.422864: \n",
      "2025-12-20 08:08:43.422864: Epoch 599\n",
      "2025-12-20 08:08:43.422864: Current learning rate: 0.00439\n",
      "2025-12-20 08:11:01.780877: train_loss -0.848\n",
      "2025-12-20 08:11:01.780877: val_loss -0.8695\n",
      "2025-12-20 08:11:01.786214: Pseudo dice [0.918, 0.9517, 0.9441]\n",
      "2025-12-20 08:11:01.789717: Epoch time: 138.36 s\n",
      "2025-12-20 08:11:03.064128: \n",
      "2025-12-20 08:11:03.064128: Epoch 600\n",
      "2025-12-20 08:11:03.068194: Current learning rate: 0.00438\n",
      "2025-12-20 08:13:21.171978: train_loss -0.8501\n",
      "2025-12-20 08:13:21.173981: val_loss -0.884\n",
      "2025-12-20 08:13:21.177985: Pseudo dice [0.9308, 0.9549, 0.9446]\n",
      "2025-12-20 08:13:21.181989: Epoch time: 138.11 s\n",
      "2025-12-20 08:13:21.820939: \n",
      "2025-12-20 08:13:21.820939: Epoch 601\n",
      "2025-12-20 08:13:21.820939: Current learning rate: 0.00437\n",
      "2025-12-20 08:15:39.985811: train_loss -0.8444\n",
      "2025-12-20 08:15:39.985811: val_loss -0.8717\n",
      "2025-12-20 08:15:39.985811: Pseudo dice [0.9199, 0.9559, 0.9427]\n",
      "2025-12-20 08:15:39.985811: Epoch time: 138.16 s\n",
      "2025-12-20 08:15:40.620462: \n",
      "2025-12-20 08:15:40.620462: Epoch 602\n",
      "2025-12-20 08:15:40.636517: Current learning rate: 0.00436\n",
      "2025-12-20 08:17:58.722180: train_loss -0.8476\n",
      "2025-12-20 08:17:58.722180: val_loss -0.8786\n",
      "2025-12-20 08:17:58.728188: Pseudo dice [0.9318, 0.9608, 0.9344]\n",
      "2025-12-20 08:17:58.730190: Epoch time: 138.1 s\n",
      "2025-12-20 08:17:59.541130: \n",
      "2025-12-20 08:17:59.541130: Epoch 603\n",
      "2025-12-20 08:17:59.544349: Current learning rate: 0.00435\n",
      "2025-12-20 08:20:17.667493: train_loss -0.8442\n",
      "2025-12-20 08:20:17.669495: val_loss -0.8795\n",
      "2025-12-20 08:20:17.675239: Pseudo dice [0.926, 0.9601, 0.9374]\n",
      "2025-12-20 08:20:17.681354: Epoch time: 138.14 s\n",
      "2025-12-20 08:20:18.312508: \n",
      "2025-12-20 08:20:18.312508: Epoch 604\n",
      "2025-12-20 08:20:18.330215: Current learning rate: 0.00434\n",
      "2025-12-20 08:22:36.549278: train_loss -0.8498\n",
      "2025-12-20 08:22:36.551281: val_loss -0.8732\n",
      "2025-12-20 08:22:36.557288: Pseudo dice [0.9222, 0.9594, 0.9438]\n",
      "2025-12-20 08:22:36.559291: Epoch time: 138.24 s\n",
      "2025-12-20 08:22:37.195988: \n",
      "2025-12-20 08:22:37.195988: Epoch 605\n",
      "2025-12-20 08:22:37.211711: Current learning rate: 0.00433\n",
      "2025-12-20 08:24:55.443949: train_loss -0.8492\n",
      "2025-12-20 08:24:55.443949: val_loss -0.8792\n",
      "2025-12-20 08:24:55.454506: Pseudo dice [0.93, 0.961, 0.9319]\n",
      "2025-12-20 08:24:55.460011: Epoch time: 138.25 s\n",
      "2025-12-20 08:24:56.256892: \n",
      "2025-12-20 08:24:56.256892: Epoch 606\n",
      "2025-12-20 08:24:56.262901: Current learning rate: 0.00432\n",
      "2025-12-20 08:27:14.451339: train_loss -0.8494\n",
      "2025-12-20 08:27:14.451339: val_loss -0.8795\n",
      "2025-12-20 08:27:14.456908: Pseudo dice [0.9284, 0.9575, 0.9356]\n",
      "2025-12-20 08:27:14.460912: Epoch time: 138.2 s\n",
      "2025-12-20 08:27:15.278088: \n",
      "2025-12-20 08:27:15.278088: Epoch 607\n",
      "2025-12-20 08:27:15.278088: Current learning rate: 0.00431\n",
      "2025-12-20 08:29:33.233061: train_loss -0.8535\n",
      "2025-12-20 08:29:33.233061: val_loss -0.8723\n",
      "2025-12-20 08:29:33.240581: Pseudo dice [0.9241, 0.9517, 0.9427]\n",
      "2025-12-20 08:29:33.244585: Epoch time: 137.95 s\n",
      "2025-12-20 08:29:33.932026: \n",
      "2025-12-20 08:29:33.932026: Epoch 608\n",
      "2025-12-20 08:29:33.932026: Current learning rate: 0.0043\n",
      "2025-12-20 08:31:52.064850: train_loss -0.8542\n",
      "2025-12-20 08:31:52.066852: val_loss -0.8764\n",
      "2025-12-20 08:31:52.068593: Pseudo dice [0.9226, 0.9546, 0.9429]\n",
      "2025-12-20 08:31:52.068593: Epoch time: 138.13 s\n",
      "2025-12-20 08:31:52.874495: \n",
      "2025-12-20 08:31:52.874495: Epoch 609\n",
      "2025-12-20 08:31:52.874495: Current learning rate: 0.00429\n",
      "2025-12-20 08:34:11.270897: train_loss -0.853\n",
      "2025-12-20 08:34:11.272899: val_loss -0.8835\n",
      "2025-12-20 08:34:11.278766: Pseudo dice [0.9301, 0.9629, 0.9435]\n",
      "2025-12-20 08:34:11.282770: Epoch time: 138.4 s\n",
      "2025-12-20 08:34:11.925833: \n",
      "2025-12-20 08:34:11.925833: Epoch 610\n",
      "2025-12-20 08:34:11.925833: Current learning rate: 0.00429\n",
      "2025-12-20 08:36:30.087703: train_loss -0.854\n",
      "2025-12-20 08:36:30.087703: val_loss -0.87\n",
      "2025-12-20 08:36:30.103620: Pseudo dice [0.9207, 0.9525, 0.9407]\n",
      "2025-12-20 08:36:30.103620: Epoch time: 138.18 s\n",
      "2025-12-20 08:36:30.736470: \n",
      "2025-12-20 08:36:30.736470: Epoch 611\n",
      "2025-12-20 08:36:30.736470: Current learning rate: 0.00428\n",
      "2025-12-20 08:38:48.775743: train_loss -0.8559\n",
      "2025-12-20 08:38:48.775743: val_loss -0.8738\n",
      "2025-12-20 08:38:48.777746: Pseudo dice [0.922, 0.954, 0.9386]\n",
      "2025-12-20 08:38:48.785641: Epoch time: 138.04 s\n",
      "2025-12-20 08:38:49.546402: \n",
      "2025-12-20 08:38:49.546402: Epoch 612\n",
      "2025-12-20 08:38:49.562064: Current learning rate: 0.00427\n",
      "2025-12-20 08:41:07.723074: train_loss -0.8515\n",
      "2025-12-20 08:41:07.723074: val_loss -0.8806\n",
      "2025-12-20 08:41:07.728820: Pseudo dice [0.9276, 0.9615, 0.9439]\n",
      "2025-12-20 08:41:07.733378: Epoch time: 138.18 s\n",
      "2025-12-20 08:41:08.534873: \n",
      "2025-12-20 08:41:08.534873: Epoch 613\n",
      "2025-12-20 08:41:08.550643: Current learning rate: 0.00426\n",
      "2025-12-20 08:43:26.790736: train_loss -0.8577\n",
      "2025-12-20 08:43:26.790736: val_loss -0.8773\n",
      "2025-12-20 08:43:26.810013: Pseudo dice [0.9253, 0.9575, 0.9386]\n",
      "2025-12-20 08:43:26.814942: Epoch time: 138.26 s\n",
      "2025-12-20 08:43:27.461270: \n",
      "2025-12-20 08:43:27.461270: Epoch 614\n",
      "2025-12-20 08:43:27.469446: Current learning rate: 0.00425\n",
      "2025-12-20 08:45:45.626266: train_loss -0.8537\n",
      "2025-12-20 08:45:45.626266: val_loss -0.8735\n",
      "2025-12-20 08:45:45.643804: Pseudo dice [0.9225, 0.9565, 0.9392]\n",
      "2025-12-20 08:45:45.646971: Epoch time: 138.16 s\n",
      "2025-12-20 08:45:46.434370: \n",
      "2025-12-20 08:45:46.434370: Epoch 615\n",
      "2025-12-20 08:45:46.450433: Current learning rate: 0.00424\n",
      "2025-12-20 08:48:04.402196: train_loss -0.8528\n",
      "2025-12-20 08:48:04.402196: val_loss -0.8767\n",
      "2025-12-20 08:48:04.418017: Pseudo dice [0.923, 0.9559, 0.9458]\n",
      "2025-12-20 08:48:04.418017: Epoch time: 137.97 s\n",
      "2025-12-20 08:48:05.082521: \n",
      "2025-12-20 08:48:05.082521: Epoch 616\n",
      "2025-12-20 08:48:05.096277: Current learning rate: 0.00423\n",
      "2025-12-20 08:50:23.156187: train_loss -0.8508\n",
      "2025-12-20 08:50:23.156187: val_loss -0.876\n",
      "2025-12-20 08:50:23.166677: Pseudo dice [0.9264, 0.9555, 0.9448]\n",
      "2025-12-20 08:50:23.169856: Epoch time: 138.07 s\n",
      "2025-12-20 08:50:23.790139: \n",
      "2025-12-20 08:50:23.790139: Epoch 617\n",
      "2025-12-20 08:50:23.807913: Current learning rate: 0.00422\n",
      "2025-12-20 08:52:41.785609: train_loss -0.8557\n",
      "2025-12-20 08:52:41.785609: val_loss -0.8846\n",
      "2025-12-20 08:52:41.791615: Pseudo dice [0.9311, 0.9615, 0.9382]\n",
      "2025-12-20 08:52:41.795357: Epoch time: 138.0 s\n",
      "2025-12-20 08:52:42.525365: \n",
      "2025-12-20 08:52:42.525365: Epoch 618\n",
      "2025-12-20 08:52:42.525365: Current learning rate: 0.00421\n",
      "2025-12-20 08:55:00.457793: train_loss -0.8547\n",
      "2025-12-20 08:55:00.457793: val_loss -0.8846\n",
      "2025-12-20 08:55:00.473794: Pseudo dice [0.9311, 0.9581, 0.9434]\n",
      "2025-12-20 08:55:00.473794: Epoch time: 137.95 s\n",
      "2025-12-20 08:55:01.266807: \n",
      "2025-12-20 08:55:01.266807: Epoch 619\n",
      "2025-12-20 08:55:01.282451: Current learning rate: 0.0042\n",
      "2025-12-20 08:57:19.376797: train_loss -0.855\n",
      "2025-12-20 08:57:19.376797: val_loss -0.8768\n",
      "2025-12-20 08:57:19.379800: Pseudo dice [0.9257, 0.954, 0.9451]\n",
      "2025-12-20 08:57:19.384023: Epoch time: 138.11 s\n",
      "2025-12-20 08:57:20.080534: \n",
      "2025-12-20 08:57:20.080534: Epoch 620\n",
      "2025-12-20 08:57:20.086575: Current learning rate: 0.00419\n",
      "2025-12-20 08:59:38.039706: train_loss -0.8518\n",
      "2025-12-20 08:59:38.039706: val_loss -0.8738\n",
      "2025-12-20 08:59:38.045712: Pseudo dice [0.9219, 0.9574, 0.9395]\n",
      "2025-12-20 08:59:38.051441: Epoch time: 137.97 s\n",
      "2025-12-20 08:59:38.828065: \n",
      "2025-12-20 08:59:38.828065: Epoch 621\n",
      "2025-12-20 08:59:38.839454: Current learning rate: 0.00418\n",
      "2025-12-20 09:01:56.799944: train_loss -0.8539\n",
      "2025-12-20 09:01:56.801947: val_loss -0.8794\n",
      "2025-12-20 09:01:56.815728: Pseudo dice [0.9244, 0.9579, 0.9423]\n",
      "2025-12-20 09:01:56.821737: Epoch time: 137.97 s\n",
      "2025-12-20 09:01:57.449349: \n",
      "2025-12-20 09:01:57.449349: Epoch 622\n",
      "2025-12-20 09:01:57.462186: Current learning rate: 0.00417\n",
      "2025-12-20 09:04:15.593747: train_loss -0.8553\n",
      "2025-12-20 09:04:15.593747: val_loss -0.8798\n",
      "2025-12-20 09:04:15.599493: Pseudo dice [0.9266, 0.9561, 0.9483]\n",
      "2025-12-20 09:04:15.605499: Epoch time: 138.14 s\n",
      "2025-12-20 09:04:16.241553: \n",
      "2025-12-20 09:04:16.241553: Epoch 623\n",
      "2025-12-20 09:04:16.257548: Current learning rate: 0.00416\n",
      "2025-12-20 09:06:34.376127: train_loss -0.8494\n",
      "2025-12-20 09:06:34.376127: val_loss -0.8714\n",
      "2025-12-20 09:06:34.384147: Pseudo dice [0.9208, 0.9552, 0.9351]\n",
      "2025-12-20 09:06:34.389893: Epoch time: 138.13 s\n",
      "2025-12-20 09:06:35.039380: \n",
      "2025-12-20 09:06:35.041383: Epoch 624\n",
      "2025-12-20 09:06:35.044917: Current learning rate: 0.00415\n",
      "2025-12-20 09:08:53.123295: train_loss -0.8532\n",
      "2025-12-20 09:08:53.123295: val_loss -0.8736\n",
      "2025-12-20 09:08:53.143179: Pseudo dice [0.9225, 0.9549, 0.9427]\n",
      "2025-12-20 09:08:53.143179: Epoch time: 138.08 s\n",
      "2025-12-20 09:08:53.774848: \n",
      "2025-12-20 09:08:53.774848: Epoch 625\n",
      "2025-12-20 09:08:53.790746: Current learning rate: 0.00414\n",
      "2025-12-20 09:11:12.157449: train_loss -0.8556\n",
      "2025-12-20 09:11:12.157449: val_loss -0.8756\n",
      "2025-12-20 09:11:12.157449: Pseudo dice [0.9224, 0.9617, 0.9368]\n",
      "2025-12-20 09:11:12.157449: Epoch time: 138.38 s\n",
      "2025-12-20 09:11:12.855479: \n",
      "2025-12-20 09:11:12.855479: Epoch 626\n",
      "2025-12-20 09:11:12.869434: Current learning rate: 0.00413\n",
      "2025-12-20 09:13:30.998241: train_loss -0.8515\n",
      "2025-12-20 09:13:30.999244: val_loss -0.8884\n",
      "2025-12-20 09:13:31.005254: Pseudo dice [0.9328, 0.9629, 0.9412]\n",
      "2025-12-20 09:13:31.012223: Epoch time: 138.14 s\n",
      "2025-12-20 09:13:31.648470: \n",
      "2025-12-20 09:13:31.648470: Epoch 627\n",
      "2025-12-20 09:13:31.664155: Current learning rate: 0.00412\n",
      "2025-12-20 09:15:49.651903: train_loss -0.8476\n",
      "2025-12-20 09:15:49.651903: val_loss -0.8733\n",
      "2025-12-20 09:15:49.657647: Pseudo dice [0.9226, 0.9542, 0.9403]\n",
      "2025-12-20 09:15:49.662304: Epoch time: 138.0 s\n",
      "2025-12-20 09:15:50.307352: \n",
      "2025-12-20 09:15:50.307352: Epoch 628\n",
      "2025-12-20 09:15:50.319028: Current learning rate: 0.00411\n",
      "2025-12-20 09:18:08.472856: train_loss -0.8555\n",
      "2025-12-20 09:18:08.472856: val_loss -0.8811\n",
      "2025-12-20 09:18:08.477341: Pseudo dice [0.9278, 0.9589, 0.9434]\n",
      "2025-12-20 09:18:08.477341: Epoch time: 138.17 s\n",
      "2025-12-20 09:18:09.122720: \n",
      "2025-12-20 09:18:09.122720: Epoch 629\n",
      "2025-12-20 09:18:09.138230: Current learning rate: 0.0041\n",
      "2025-12-20 09:20:27.170232: train_loss -0.8525\n",
      "2025-12-20 09:20:27.170232: val_loss -0.8728\n",
      "2025-12-20 09:20:27.174999: Pseudo dice [0.9229, 0.9541, 0.9418]\n",
      "2025-12-20 09:20:27.179554: Epoch time: 138.05 s\n",
      "2025-12-20 09:20:27.979279: \n",
      "2025-12-20 09:20:27.979279: Epoch 630\n",
      "2025-12-20 09:20:27.992538: Current learning rate: 0.00409\n",
      "2025-12-20 09:22:45.975928: train_loss -0.8474\n",
      "2025-12-20 09:22:45.975928: val_loss -0.871\n",
      "2025-12-20 09:22:45.988696: Pseudo dice [0.9208, 0.9539, 0.9364]\n",
      "2025-12-20 09:22:45.991692: Epoch time: 138.0 s\n",
      "2025-12-20 09:22:46.625933: \n",
      "2025-12-20 09:22:46.625933: Epoch 631\n",
      "2025-12-20 09:22:46.640305: Current learning rate: 0.00408\n",
      "2025-12-20 09:25:04.766542: train_loss -0.8516\n",
      "2025-12-20 09:25:04.766542: val_loss -0.877\n",
      "2025-12-20 09:25:04.783922: Pseudo dice [0.9268, 0.9572, 0.9373]\n",
      "2025-12-20 09:25:04.787608: Epoch time: 138.14 s\n",
      "2025-12-20 09:25:05.478509: \n",
      "2025-12-20 09:25:05.478509: Epoch 632\n",
      "2025-12-20 09:25:05.494222: Current learning rate: 0.00407\n",
      "2025-12-20 09:27:23.372964: train_loss -0.8597\n",
      "2025-12-20 09:27:23.372964: val_loss -0.8756\n",
      "2025-12-20 09:27:23.378042: Pseudo dice [0.9251, 0.9566, 0.938]\n",
      "2025-12-20 09:27:23.382046: Epoch time: 137.89 s\n",
      "2025-12-20 09:27:24.021065: \n",
      "2025-12-20 09:27:24.021065: Epoch 633\n",
      "2025-12-20 09:27:24.021065: Current learning rate: 0.00406\n",
      "2025-12-20 09:29:42.101487: train_loss -0.8532\n",
      "2025-12-20 09:29:42.101487: val_loss -0.8719\n",
      "2025-12-20 09:29:42.117602: Pseudo dice [0.9226, 0.9587, 0.9405]\n",
      "2025-12-20 09:29:42.117602: Epoch time: 138.08 s\n",
      "2025-12-20 09:29:42.799084: \n",
      "2025-12-20 09:29:42.799084: Epoch 634\n",
      "2025-12-20 09:29:42.799084: Current learning rate: 0.00405\n",
      "2025-12-20 09:32:00.888407: train_loss -0.8494\n",
      "2025-12-20 09:32:00.888407: val_loss -0.873\n",
      "2025-12-20 09:32:00.888407: Pseudo dice [0.9248, 0.9553, 0.9366]\n",
      "2025-12-20 09:32:00.888407: Epoch time: 138.11 s\n",
      "2025-12-20 09:32:01.522135: \n",
      "2025-12-20 09:32:01.522135: Epoch 635\n",
      "2025-12-20 09:32:01.538031: Current learning rate: 0.00404\n",
      "2025-12-20 09:34:19.640880: train_loss -0.8486\n",
      "2025-12-20 09:34:19.640880: val_loss -0.8808\n",
      "2025-12-20 09:34:19.649156: Pseudo dice [0.9292, 0.9578, 0.9454]\n",
      "2025-12-20 09:34:19.654768: Epoch time: 138.12 s\n",
      "2025-12-20 09:34:20.291164: \n",
      "2025-12-20 09:34:20.291164: Epoch 636\n",
      "2025-12-20 09:34:20.303422: Current learning rate: 0.00403\n",
      "2025-12-20 09:36:38.421099: train_loss -0.8453\n",
      "2025-12-20 09:36:38.421099: val_loss -0.8836\n",
      "2025-12-20 09:36:38.434582: Pseudo dice [0.9323, 0.9604, 0.9437]\n",
      "2025-12-20 09:36:38.434582: Epoch time: 138.13 s\n",
      "2025-12-20 09:36:39.274289: \n",
      "2025-12-20 09:36:39.274289: Epoch 637\n",
      "2025-12-20 09:36:39.274289: Current learning rate: 0.00402\n",
      "2025-12-20 09:38:57.282971: train_loss -0.8599\n",
      "2025-12-20 09:38:57.282971: val_loss -0.8781\n",
      "2025-12-20 09:38:57.298645: Pseudo dice [0.9248, 0.9571, 0.9432]\n",
      "2025-12-20 09:38:57.300915: Epoch time: 138.02 s\n",
      "2025-12-20 09:38:58.012266: \n",
      "2025-12-20 09:38:58.012266: Epoch 638\n",
      "2025-12-20 09:38:58.028381: Current learning rate: 0.00401\n",
      "2025-12-20 09:41:15.910965: train_loss -0.8547\n",
      "2025-12-20 09:41:15.912967: val_loss -0.8785\n",
      "2025-12-20 09:41:15.916723: Pseudo dice [0.9258, 0.9552, 0.9452]\n",
      "2025-12-20 09:41:15.916723: Epoch time: 137.9 s\n",
      "2025-12-20 09:41:16.549389: \n",
      "2025-12-20 09:41:16.549389: Epoch 639\n",
      "2025-12-20 09:41:16.565427: Current learning rate: 0.004\n",
      "2025-12-20 09:43:34.473485: train_loss -0.8489\n",
      "2025-12-20 09:43:34.473485: val_loss -0.8776\n",
      "2025-12-20 09:43:34.489506: Pseudo dice [0.922, 0.9562, 0.9456]\n",
      "2025-12-20 09:43:34.497423: Epoch time: 137.92 s\n",
      "2025-12-20 09:43:35.139200: \n",
      "2025-12-20 09:43:35.139200: Epoch 640\n",
      "2025-12-20 09:43:35.154970: Current learning rate: 0.00399\n",
      "2025-12-20 09:45:53.176759: train_loss -0.857\n",
      "2025-12-20 09:45:53.176759: val_loss -0.8815\n",
      "2025-12-20 09:45:53.176759: Pseudo dice [0.9272, 0.9573, 0.948]\n",
      "2025-12-20 09:45:53.176759: Epoch time: 138.04 s\n",
      "2025-12-20 09:45:53.192867: Yayy! New best EMA pseudo Dice: 0.9417\n",
      "2025-12-20 09:45:54.214466: \n",
      "2025-12-20 09:45:54.214466: Epoch 641\n",
      "2025-12-20 09:45:54.219761: Current learning rate: 0.00398\n",
      "2025-12-20 09:48:12.323338: train_loss -0.8481\n",
      "2025-12-20 09:48:12.323338: val_loss -0.8823\n",
      "2025-12-20 09:48:12.343437: Pseudo dice [0.9317, 0.9608, 0.9377]\n",
      "2025-12-20 09:48:12.347441: Epoch time: 138.11 s\n",
      "2025-12-20 09:48:12.351445: Yayy! New best EMA pseudo Dice: 0.9419\n",
      "2025-12-20 09:48:13.433893: \n",
      "2025-12-20 09:48:13.433893: Epoch 642\n",
      "2025-12-20 09:48:13.433893: Current learning rate: 0.00397\n",
      "2025-12-20 09:50:31.723417: train_loss -0.8529\n",
      "2025-12-20 09:50:31.723417: val_loss -0.8724\n",
      "2025-12-20 09:50:31.736969: Pseudo dice [0.921, 0.9563, 0.94]\n",
      "2025-12-20 09:50:31.741840: Epoch time: 138.29 s\n",
      "2025-12-20 09:50:32.386868: \n",
      "2025-12-20 09:50:32.386868: Epoch 643\n",
      "2025-12-20 09:50:32.402009: Current learning rate: 0.00396\n",
      "2025-12-20 09:52:50.489766: train_loss -0.855\n",
      "2025-12-20 09:52:50.489766: val_loss -0.8702\n",
      "2025-12-20 09:52:50.499756: Pseudo dice [0.9189, 0.9537, 0.9376]\n",
      "2025-12-20 09:52:50.505762: Epoch time: 138.1 s\n",
      "2025-12-20 09:52:51.201483: \n",
      "2025-12-20 09:52:51.201483: Epoch 644\n",
      "2025-12-20 09:52:51.207448: Current learning rate: 0.00395\n",
      "2025-12-20 09:55:09.254817: train_loss -0.855\n",
      "2025-12-20 09:55:09.254817: val_loss -0.8842\n",
      "2025-12-20 09:55:09.254817: Pseudo dice [0.9322, 0.9622, 0.939]\n",
      "2025-12-20 09:55:09.270619: Epoch time: 138.05 s\n",
      "2025-12-20 09:55:09.904859: \n",
      "2025-12-20 09:55:09.904859: Epoch 645\n",
      "2025-12-20 09:55:09.904859: Current learning rate: 0.00394\n",
      "2025-12-20 09:57:27.957416: train_loss -0.8553\n",
      "2025-12-20 09:57:27.957416: val_loss -0.8749\n",
      "2025-12-20 09:57:27.971071: Pseudo dice [0.9225, 0.9499, 0.9409]\n",
      "2025-12-20 09:57:27.973075: Epoch time: 138.05 s\n",
      "2025-12-20 09:57:28.605850: \n",
      "2025-12-20 09:57:28.605850: Epoch 646\n",
      "2025-12-20 09:57:28.617538: Current learning rate: 0.00393\n",
      "2025-12-20 09:59:46.661238: train_loss -0.8499\n",
      "2025-12-20 09:59:46.661238: val_loss -0.8831\n",
      "2025-12-20 09:59:46.680627: Pseudo dice [0.9278, 0.9589, 0.9513]\n",
      "2025-12-20 09:59:46.684237: Epoch time: 138.06 s\n",
      "2025-12-20 09:59:47.454589: \n",
      "2025-12-20 09:59:47.454589: Epoch 647\n",
      "2025-12-20 09:59:47.472494: Current learning rate: 0.00392\n",
      "2025-12-20 10:02:05.654564: train_loss -0.8586\n",
      "2025-12-20 10:02:05.656565: val_loss -0.8769\n",
      "2025-12-20 10:02:05.660570: Pseudo dice [0.9251, 0.9603, 0.9367]\n",
      "2025-12-20 10:02:05.664574: Epoch time: 138.2 s\n",
      "2025-12-20 10:02:06.315589: \n",
      "2025-12-20 10:02:06.315589: Epoch 648\n",
      "2025-12-20 10:02:06.320508: Current learning rate: 0.00391\n",
      "2025-12-20 10:04:24.344389: train_loss -0.8485\n",
      "2025-12-20 10:04:24.344389: val_loss -0.875\n",
      "2025-12-20 10:04:24.344389: Pseudo dice [0.924, 0.9588, 0.9409]\n",
      "2025-12-20 10:04:24.362187: Epoch time: 138.04 s\n",
      "2025-12-20 10:04:25.168620: \n",
      "2025-12-20 10:04:25.168620: Epoch 649\n",
      "2025-12-20 10:04:25.182680: Current learning rate: 0.0039\n",
      "2025-12-20 10:06:43.210988: train_loss -0.8503\n",
      "2025-12-20 10:06:43.212990: val_loss -0.8815\n",
      "2025-12-20 10:06:43.214730: Pseudo dice [0.9284, 0.9608, 0.949]\n",
      "2025-12-20 10:06:43.214730: Epoch time: 138.04 s\n",
      "2025-12-20 10:06:43.499274: Yayy! New best EMA pseudo Dice: 0.9419\n",
      "2025-12-20 10:06:44.558439: \n",
      "2025-12-20 10:06:44.558439: Epoch 650\n",
      "2025-12-20 10:06:44.574320: Current learning rate: 0.00389\n",
      "2025-12-20 10:09:02.761572: train_loss -0.8544\n",
      "2025-12-20 10:09:02.761572: val_loss -0.8828\n",
      "2025-12-20 10:09:02.761572: Pseudo dice [0.9282, 0.9596, 0.9447]\n",
      "2025-12-20 10:09:02.777248: Epoch time: 138.2 s\n",
      "2025-12-20 10:09:02.777248: Yayy! New best EMA pseudo Dice: 0.9421\n",
      "2025-12-20 10:09:03.699204: \n",
      "2025-12-20 10:09:03.699204: Epoch 651\n",
      "2025-12-20 10:09:03.713214: Current learning rate: 0.00388\n",
      "2025-12-20 10:11:22.097693: train_loss -0.8502\n",
      "2025-12-20 10:11:22.097693: val_loss -0.8754\n",
      "2025-12-20 10:11:22.113002: Pseudo dice [0.9249, 0.9573, 0.9413]\n",
      "2025-12-20 10:11:22.120244: Epoch time: 138.4 s\n",
      "2025-12-20 10:11:22.758041: \n",
      "2025-12-20 10:11:22.758041: Epoch 652\n",
      "2025-12-20 10:11:22.769780: Current learning rate: 0.00387\n",
      "2025-12-20 10:13:41.721793: train_loss -0.8484\n",
      "2025-12-20 10:13:41.721793: val_loss -0.8795\n",
      "2025-12-20 10:13:41.729473: Pseudo dice [0.9299, 0.962, 0.9262]\n",
      "2025-12-20 10:13:41.735479: Epoch time: 138.96 s\n",
      "2025-12-20 10:13:42.376698: \n",
      "2025-12-20 10:13:42.376698: Epoch 653\n",
      "2025-12-20 10:13:42.392780: Current learning rate: 0.00386\n",
      "2025-12-20 10:16:00.757727: train_loss -0.86\n",
      "2025-12-20 10:16:00.757727: val_loss -0.8869\n",
      "2025-12-20 10:16:00.769662: Pseudo dice [0.9323, 0.9608, 0.9406]\n",
      "2025-12-20 10:16:00.773748: Epoch time: 138.38 s\n",
      "2025-12-20 10:16:01.558800: \n",
      "2025-12-20 10:16:01.558800: Epoch 654\n",
      "2025-12-20 10:16:01.576592: Current learning rate: 0.00385\n",
      "2025-12-20 10:18:20.055763: train_loss -0.8537\n",
      "2025-12-20 10:18:20.055763: val_loss -0.8731\n",
      "2025-12-20 10:18:20.073715: Pseudo dice [0.9222, 0.9575, 0.9372]\n",
      "2025-12-20 10:18:20.078028: Epoch time: 138.5 s\n",
      "2025-12-20 10:18:20.717139: \n",
      "2025-12-20 10:18:20.717139: Epoch 655\n",
      "2025-12-20 10:18:20.717139: Current learning rate: 0.00384\n",
      "2025-12-20 10:20:39.249569: train_loss -0.854\n",
      "2025-12-20 10:20:39.251572: val_loss -0.8799\n",
      "2025-12-20 10:20:39.256352: Pseudo dice [0.9265, 0.9593, 0.9481]\n",
      "2025-12-20 10:20:39.261348: Epoch time: 138.53 s\n",
      "2025-12-20 10:20:39.930908: \n",
      "2025-12-20 10:20:39.930908: Epoch 656\n",
      "2025-12-20 10:20:39.944788: Current learning rate: 0.00383\n",
      "2025-12-20 10:22:58.422811: train_loss -0.8508\n",
      "2025-12-20 10:22:58.422811: val_loss -0.875\n",
      "2025-12-20 10:22:58.428816: Pseudo dice [0.9274, 0.9586, 0.9328]\n",
      "2025-12-20 10:22:58.432559: Epoch time: 138.49 s\n",
      "2025-12-20 10:22:59.066822: \n",
      "2025-12-20 10:22:59.066822: Epoch 657\n",
      "2025-12-20 10:22:59.082529: Current learning rate: 0.00382\n",
      "2025-12-20 10:25:17.065034: train_loss -0.847\n",
      "2025-12-20 10:25:17.067037: val_loss -0.8739\n",
      "2025-12-20 10:25:17.071043: Pseudo dice [0.9188, 0.9559, 0.9472]\n",
      "2025-12-20 10:25:17.076846: Epoch time: 138.0 s\n",
      "2025-12-20 10:25:17.720055: \n",
      "2025-12-20 10:25:17.720055: Epoch 658\n",
      "2025-12-20 10:25:17.726472: Current learning rate: 0.00381\n",
      "2025-12-20 10:27:35.797031: train_loss -0.8487\n",
      "2025-12-20 10:27:35.797031: val_loss -0.8754\n",
      "2025-12-20 10:27:35.810214: Pseudo dice [0.9201, 0.9528, 0.9456]\n",
      "2025-12-20 10:27:35.812943: Epoch time: 138.08 s\n",
      "2025-12-20 10:27:36.447607: \n",
      "2025-12-20 10:27:36.447607: Epoch 659\n",
      "2025-12-20 10:27:36.463493: Current learning rate: 0.0038\n",
      "2025-12-20 10:29:54.382855: train_loss -0.8547\n",
      "2025-12-20 10:29:54.384856: val_loss -0.886\n",
      "2025-12-20 10:29:54.390602: Pseudo dice [0.9332, 0.9603, 0.9427]\n",
      "2025-12-20 10:29:54.394606: Epoch time: 137.94 s\n",
      "2025-12-20 10:29:55.038023: \n",
      "2025-12-20 10:29:55.038023: Epoch 660\n",
      "2025-12-20 10:29:55.040026: Current learning rate: 0.00379\n",
      "2025-12-20 10:32:13.111892: train_loss -0.8512\n",
      "2025-12-20 10:32:13.111892: val_loss -0.8841\n",
      "2025-12-20 10:32:13.117897: Pseudo dice [0.9302, 0.9581, 0.9465]\n",
      "2025-12-20 10:32:13.119899: Epoch time: 138.07 s\n",
      "2025-12-20 10:32:13.126748: Yayy! New best EMA pseudo Dice: 0.9422\n",
      "2025-12-20 10:32:14.212988: \n",
      "2025-12-20 10:32:14.212988: Epoch 661\n",
      "2025-12-20 10:32:14.221562: Current learning rate: 0.00378\n",
      "2025-12-20 10:34:32.312948: train_loss -0.8544\n",
      "2025-12-20 10:34:32.314951: val_loss -0.872\n",
      "2025-12-20 10:34:32.318694: Pseudo dice [0.9196, 0.9515, 0.9463]\n",
      "2025-12-20 10:34:32.324702: Epoch time: 138.1 s\n",
      "2025-12-20 10:34:32.967373: \n",
      "2025-12-20 10:34:32.967373: Epoch 662\n",
      "2025-12-20 10:34:32.967373: Current learning rate: 0.00377\n",
      "2025-12-20 10:36:51.056367: train_loss -0.8537\n",
      "2025-12-20 10:36:51.056367: val_loss -0.8839\n",
      "2025-12-20 10:36:51.069165: Pseudo dice [0.9265, 0.9603, 0.9412]\n",
      "2025-12-20 10:36:51.073996: Epoch time: 138.1 s\n",
      "2025-12-20 10:36:51.707064: \n",
      "2025-12-20 10:36:51.707064: Epoch 663\n",
      "2025-12-20 10:36:51.723069: Current learning rate: 0.00376\n",
      "2025-12-20 10:39:09.853371: train_loss -0.8561\n",
      "2025-12-20 10:39:09.853371: val_loss -0.8747\n",
      "2025-12-20 10:39:09.869181: Pseudo dice [0.9231, 0.9578, 0.9415]\n",
      "2025-12-20 10:39:09.869181: Epoch time: 138.15 s\n",
      "2025-12-20 10:39:10.504812: \n",
      "2025-12-20 10:39:10.504812: Epoch 664\n",
      "2025-12-20 10:39:10.504812: Current learning rate: 0.00375\n",
      "2025-12-20 10:41:28.434887: train_loss -0.8556\n",
      "2025-12-20 10:41:28.436890: val_loss -0.8792\n",
      "2025-12-20 10:41:28.436890: Pseudo dice [0.9278, 0.9628, 0.9364]\n",
      "2025-12-20 10:41:28.443418: Epoch time: 137.93 s\n",
      "2025-12-20 10:41:29.077975: \n",
      "2025-12-20 10:41:29.077975: Epoch 665\n",
      "2025-12-20 10:41:29.077975: Current learning rate: 0.00374\n",
      "2025-12-20 10:43:47.072241: train_loss -0.8517\n",
      "2025-12-20 10:43:47.072241: val_loss -0.8785\n",
      "2025-12-20 10:43:47.072241: Pseudo dice [0.925, 0.9605, 0.9404]\n",
      "2025-12-20 10:43:47.082016: Epoch time: 138.0 s\n",
      "2025-12-20 10:43:47.719577: \n",
      "2025-12-20 10:43:47.719577: Epoch 666\n",
      "2025-12-20 10:43:47.719577: Current learning rate: 0.00373\n",
      "2025-12-20 10:46:05.875400: train_loss -0.86\n",
      "2025-12-20 10:46:05.875400: val_loss -0.8727\n",
      "2025-12-20 10:46:05.879950: Pseudo dice [0.9224, 0.9565, 0.9421]\n",
      "2025-12-20 10:46:05.879950: Epoch time: 138.16 s\n",
      "2025-12-20 10:46:06.510365: \n",
      "2025-12-20 10:46:06.510365: Epoch 667\n",
      "2025-12-20 10:46:06.510365: Current learning rate: 0.00372\n",
      "2025-12-20 10:48:24.534376: train_loss -0.8556\n",
      "2025-12-20 10:48:24.534376: val_loss -0.8725\n",
      "2025-12-20 10:48:24.544390: Pseudo dice [0.9217, 0.9492, 0.9431]\n",
      "2025-12-20 10:48:24.552139: Epoch time: 138.02 s\n",
      "2025-12-20 10:48:25.290807: \n",
      "2025-12-20 10:48:25.290807: Epoch 668\n",
      "2025-12-20 10:48:25.302288: Current learning rate: 0.00371\n",
      "2025-12-20 10:50:43.341460: train_loss -0.8579\n",
      "2025-12-20 10:50:43.341460: val_loss -0.8767\n",
      "2025-12-20 10:50:43.344687: Pseudo dice [0.9235, 0.9577, 0.9419]\n",
      "2025-12-20 10:50:43.353085: Epoch time: 138.05 s\n",
      "2025-12-20 10:50:44.083757: \n",
      "2025-12-20 10:50:44.083757: Epoch 669\n",
      "2025-12-20 10:50:44.083757: Current learning rate: 0.0037\n",
      "2025-12-20 10:53:02.068502: train_loss -0.855\n",
      "2025-12-20 10:53:02.068502: val_loss -0.8728\n",
      "2025-12-20 10:53:02.079804: Pseudo dice [0.9233, 0.9573, 0.9351]\n",
      "2025-12-20 10:53:02.083809: Epoch time: 137.99 s\n",
      "2025-12-20 10:53:02.732997: \n",
      "2025-12-20 10:53:02.732997: Epoch 670\n",
      "2025-12-20 10:53:02.732997: Current learning rate: 0.00369\n",
      "2025-12-20 10:55:20.677520: train_loss -0.8574\n",
      "2025-12-20 10:55:20.677520: val_loss -0.8749\n",
      "2025-12-20 10:55:20.689862: Pseudo dice [0.9238, 0.9562, 0.9415]\n",
      "2025-12-20 10:55:20.689862: Epoch time: 137.94 s\n",
      "2025-12-20 10:55:21.478934: \n",
      "2025-12-20 10:55:21.478934: Epoch 671\n",
      "2025-12-20 10:55:21.478934: Current learning rate: 0.00368\n",
      "2025-12-20 10:57:39.539032: train_loss -0.8514\n",
      "2025-12-20 10:57:39.539032: val_loss -0.8829\n",
      "2025-12-20 10:57:39.547740: Pseudo dice [0.9282, 0.9626, 0.9419]\n",
      "2025-12-20 10:57:39.551744: Epoch time: 138.08 s\n",
      "2025-12-20 10:57:40.409234: \n",
      "2025-12-20 10:57:40.409234: Epoch 672\n",
      "2025-12-20 10:57:40.409234: Current learning rate: 0.00367\n",
      "2025-12-20 10:59:58.373185: train_loss -0.8491\n",
      "2025-12-20 10:59:58.375187: val_loss -0.8781\n",
      "2025-12-20 10:59:58.379191: Pseudo dice [0.9276, 0.9553, 0.9423]\n",
      "2025-12-20 10:59:58.385197: Epoch time: 137.96 s\n",
      "2025-12-20 10:59:59.033033: \n",
      "2025-12-20 10:59:59.033033: Epoch 673\n",
      "2025-12-20 10:59:59.033033: Current learning rate: 0.00366\n",
      "2025-12-20 11:02:16.985818: train_loss -0.8547\n",
      "2025-12-20 11:02:16.987820: val_loss -0.8799\n",
      "2025-12-20 11:02:16.993564: Pseudo dice [0.9273, 0.9589, 0.9418]\n",
      "2025-12-20 11:02:16.999572: Epoch time: 137.95 s\n",
      "2025-12-20 11:02:17.752821: \n",
      "2025-12-20 11:02:17.752821: Epoch 674\n",
      "2025-12-20 11:02:17.752821: Current learning rate: 0.00365\n",
      "2025-12-20 11:04:35.907326: train_loss -0.8535\n",
      "2025-12-20 11:04:35.907326: val_loss -0.884\n",
      "2025-12-20 11:04:35.918362: Pseudo dice [0.9302, 0.96, 0.9426]\n",
      "2025-12-20 11:04:35.922366: Epoch time: 138.15 s\n",
      "2025-12-20 11:04:36.585066: \n",
      "2025-12-20 11:04:36.585066: Epoch 675\n",
      "2025-12-20 11:04:36.585066: Current learning rate: 0.00364\n",
      "2025-12-20 11:06:54.538559: train_loss -0.8572\n",
      "2025-12-20 11:06:54.540562: val_loss -0.8825\n",
      "2025-12-20 11:06:54.545649: Pseudo dice [0.9284, 0.9602, 0.9442]\n",
      "2025-12-20 11:06:54.549654: Epoch time: 137.96 s\n",
      "2025-12-20 11:06:55.202313: \n",
      "2025-12-20 11:06:55.202313: Epoch 676\n",
      "2025-12-20 11:06:55.204316: Current learning rate: 0.00363\n",
      "2025-12-20 11:09:13.324776: train_loss -0.8544\n",
      "2025-12-20 11:09:13.324776: val_loss -0.8785\n",
      "2025-12-20 11:09:13.332531: Pseudo dice [0.9259, 0.9574, 0.9392]\n",
      "2025-12-20 11:09:13.338541: Epoch time: 138.12 s\n",
      "2025-12-20 11:09:14.162101: \n",
      "2025-12-20 11:09:14.162101: Epoch 677\n",
      "2025-12-20 11:09:14.165181: Current learning rate: 0.00362\n",
      "2025-12-20 11:11:32.412444: train_loss -0.8532\n",
      "2025-12-20 11:11:32.412444: val_loss -0.8832\n",
      "2025-12-20 11:11:32.426965: Pseudo dice [0.9317, 0.9589, 0.9373]\n",
      "2025-12-20 11:11:32.430469: Epoch time: 138.25 s\n",
      "2025-12-20 11:11:33.249154: \n",
      "2025-12-20 11:11:33.249154: Epoch 678\n",
      "2025-12-20 11:11:33.249154: Current learning rate: 0.00361\n",
      "2025-12-20 11:13:51.369081: train_loss -0.852\n",
      "2025-12-20 11:13:51.369081: val_loss -0.8801\n",
      "2025-12-20 11:13:51.377092: Pseudo dice [0.9279, 0.9582, 0.9388]\n",
      "2025-12-20 11:13:51.383012: Epoch time: 138.12 s\n",
      "2025-12-20 11:13:52.040764: \n",
      "2025-12-20 11:13:52.040764: Epoch 679\n",
      "2025-12-20 11:13:52.040764: Current learning rate: 0.0036\n",
      "2025-12-20 11:16:10.123301: train_loss -0.8501\n",
      "2025-12-20 11:16:10.123301: val_loss -0.8685\n",
      "2025-12-20 11:16:10.131312: Pseudo dice [0.9214, 0.9517, 0.9406]\n",
      "2025-12-20 11:16:10.137057: Epoch time: 138.08 s\n",
      "2025-12-20 11:16:10.927148: \n",
      "2025-12-20 11:16:10.927148: Epoch 680\n",
      "2025-12-20 11:16:10.927148: Current learning rate: 0.00359\n",
      "2025-12-20 11:18:29.158195: train_loss -0.8535\n",
      "2025-12-20 11:18:29.158195: val_loss -0.8663\n",
      "2025-12-20 11:18:29.158195: Pseudo dice [0.9173, 0.9474, 0.9469]\n",
      "2025-12-20 11:18:29.158195: Epoch time: 138.23 s\n",
      "2025-12-20 11:18:29.807811: \n",
      "2025-12-20 11:18:29.807811: Epoch 681\n",
      "2025-12-20 11:18:29.807811: Current learning rate: 0.00358\n",
      "2025-12-20 11:20:48.003384: train_loss -0.8508\n",
      "2025-12-20 11:20:48.005124: val_loss -0.8746\n",
      "2025-12-20 11:20:48.011131: Pseudo dice [0.9232, 0.9524, 0.9508]\n",
      "2025-12-20 11:20:48.015134: Epoch time: 138.2 s\n",
      "2025-12-20 11:20:48.655537: \n",
      "2025-12-20 11:20:48.655537: Epoch 682\n",
      "2025-12-20 11:20:48.671618: Current learning rate: 0.00357\n",
      "2025-12-20 11:23:06.777499: train_loss -0.8531\n",
      "2025-12-20 11:23:06.777499: val_loss -0.8799\n",
      "2025-12-20 11:23:06.785247: Pseudo dice [0.9265, 0.959, 0.9394]\n",
      "2025-12-20 11:23:06.791254: Epoch time: 138.12 s\n",
      "2025-12-20 11:23:07.521891: \n",
      "2025-12-20 11:23:07.521891: Epoch 683\n",
      "2025-12-20 11:23:07.537799: Current learning rate: 0.00356\n",
      "2025-12-20 11:25:25.551266: train_loss -0.8512\n",
      "2025-12-20 11:25:25.551266: val_loss -0.8713\n",
      "2025-12-20 11:25:25.558770: Pseudo dice [0.9218, 0.9531, 0.9405]\n",
      "2025-12-20 11:25:25.562774: Epoch time: 138.03 s\n",
      "2025-12-20 11:25:26.212729: \n",
      "2025-12-20 11:25:26.212729: Epoch 684\n",
      "2025-12-20 11:25:26.218056: Current learning rate: 0.00355\n",
      "2025-12-20 11:27:44.277249: train_loss -0.8527\n",
      "2025-12-20 11:27:44.277249: val_loss -0.8742\n",
      "2025-12-20 11:27:44.277249: Pseudo dice [0.9233, 0.9573, 0.935]\n",
      "2025-12-20 11:27:44.293201: Epoch time: 138.06 s\n",
      "2025-12-20 11:27:44.928549: \n",
      "2025-12-20 11:27:44.928549: Epoch 685\n",
      "2025-12-20 11:27:44.928549: Current learning rate: 0.00354\n",
      "2025-12-20 11:30:03.116243: train_loss -0.8433\n",
      "2025-12-20 11:30:03.116243: val_loss -0.8878\n",
      "2025-12-20 11:30:03.122249: Pseudo dice [0.9329, 0.9653, 0.9411]\n",
      "2025-12-20 11:30:03.127993: Epoch time: 138.19 s\n",
      "2025-12-20 11:30:03.872487: \n",
      "2025-12-20 11:30:03.872487: Epoch 686\n",
      "2025-12-20 11:30:03.872487: Current learning rate: 0.00353\n",
      "2025-12-20 11:32:21.937225: train_loss -0.853\n",
      "2025-12-20 11:32:21.939227: val_loss -0.8714\n",
      "2025-12-20 11:32:21.950981: Pseudo dice [0.919, 0.9517, 0.9468]\n",
      "2025-12-20 11:32:21.954987: Epoch time: 138.08 s\n",
      "2025-12-20 11:32:22.598822: \n",
      "2025-12-20 11:32:22.598822: Epoch 687\n",
      "2025-12-20 11:32:22.598822: Current learning rate: 0.00352\n",
      "2025-12-20 11:34:40.671720: train_loss -0.8523\n",
      "2025-12-20 11:34:40.671720: val_loss -0.8792\n",
      "2025-12-20 11:34:40.687470: Pseudo dice [0.9264, 0.9549, 0.9473]\n",
      "2025-12-20 11:34:40.687470: Epoch time: 138.07 s\n",
      "2025-12-20 11:34:41.337191: \n",
      "2025-12-20 11:34:41.337191: Epoch 688\n",
      "2025-12-20 11:34:41.339979: Current learning rate: 0.00351\n",
      "2025-12-20 11:36:59.418574: train_loss -0.851\n",
      "2025-12-20 11:36:59.420577: val_loss -0.8839\n",
      "2025-12-20 11:36:59.426278: Pseudo dice [0.93, 0.9616, 0.9425]\n",
      "2025-12-20 11:36:59.428828: Epoch time: 138.1 s\n",
      "2025-12-20 11:37:00.216938: \n",
      "2025-12-20 11:37:00.216938: Epoch 689\n",
      "2025-12-20 11:37:00.216938: Current learning rate: 0.0035\n",
      "2025-12-20 11:39:18.438739: train_loss -0.8517\n",
      "2025-12-20 11:39:18.440741: val_loss -0.8788\n",
      "2025-12-20 11:39:18.446287: Pseudo dice [0.924, 0.9582, 0.9447]\n",
      "2025-12-20 11:39:18.450291: Epoch time: 138.22 s\n",
      "2025-12-20 11:39:19.266390: \n",
      "2025-12-20 11:39:19.266390: Epoch 690\n",
      "2025-12-20 11:39:19.270360: Current learning rate: 0.00349\n",
      "2025-12-20 11:41:37.225680: train_loss -0.8563\n",
      "2025-12-20 11:41:37.225680: val_loss -0.885\n",
      "2025-12-20 11:41:37.231685: Pseudo dice [0.9269, 0.9604, 0.9497]\n",
      "2025-12-20 11:41:37.235689: Epoch time: 137.96 s\n",
      "2025-12-20 11:41:37.893911: \n",
      "2025-12-20 11:41:37.893911: Epoch 691\n",
      "2025-12-20 11:41:37.893911: Current learning rate: 0.00348\n",
      "2025-12-20 11:43:56.012832: train_loss -0.8491\n",
      "2025-12-20 11:43:56.014834: val_loss -0.8795\n",
      "2025-12-20 11:43:56.022592: Pseudo dice [0.9271, 0.9606, 0.9394]\n",
      "2025-12-20 11:43:56.026596: Epoch time: 138.13 s\n",
      "2025-12-20 11:43:56.800170: \n",
      "2025-12-20 11:43:56.800170: Epoch 692\n",
      "2025-12-20 11:43:56.800170: Current learning rate: 0.00346\n",
      "2025-12-20 11:46:14.844098: train_loss -0.8421\n",
      "2025-12-20 11:46:14.844098: val_loss -0.8665\n",
      "2025-12-20 11:46:14.844098: Pseudo dice [0.9235, 0.956, 0.9331]\n",
      "2025-12-20 11:46:14.859930: Epoch time: 138.06 s\n",
      "2025-12-20 11:46:15.509727: \n",
      "2025-12-20 11:46:15.509727: Epoch 693\n",
      "2025-12-20 11:46:15.509727: Current learning rate: 0.00345\n",
      "2025-12-20 11:48:33.478800: train_loss -0.8313\n",
      "2025-12-20 11:48:33.478800: val_loss -0.8639\n",
      "2025-12-20 11:48:33.490335: Pseudo dice [0.9215, 0.9554, 0.9261]\n",
      "2025-12-20 11:48:33.496340: Epoch time: 137.97 s\n",
      "2025-12-20 11:48:34.136973: \n",
      "2025-12-20 11:48:34.136973: Epoch 694\n",
      "2025-12-20 11:48:34.150986: Current learning rate: 0.00344\n",
      "2025-12-20 11:50:52.202619: train_loss -0.8456\n",
      "2025-12-20 11:50:52.202619: val_loss -0.8744\n",
      "2025-12-20 11:50:52.202619: Pseudo dice [0.9226, 0.957, 0.9376]\n",
      "2025-12-20 11:50:52.218679: Epoch time: 138.07 s\n",
      "2025-12-20 11:50:52.884381: \n",
      "2025-12-20 11:50:52.884381: Epoch 695\n",
      "2025-12-20 11:50:52.884381: Current learning rate: 0.00343\n",
      "2025-12-20 11:53:10.811520: train_loss -0.8377\n",
      "2025-12-20 11:53:10.811520: val_loss -0.8646\n",
      "2025-12-20 11:53:10.817527: Pseudo dice [0.921, 0.9518, 0.9318]\n",
      "2025-12-20 11:53:10.823272: Epoch time: 137.93 s\n",
      "2025-12-20 11:53:11.635625: \n",
      "2025-12-20 11:53:11.635625: Epoch 696\n",
      "2025-12-20 11:53:11.649284: Current learning rate: 0.00342\n",
      "2025-12-20 11:55:29.703792: train_loss -0.8224\n",
      "2025-12-20 11:55:29.703792: val_loss -0.855\n",
      "2025-12-20 11:55:29.703792: Pseudo dice [0.918, 0.9516, 0.9331]\n",
      "2025-12-20 11:55:29.703792: Epoch time: 138.07 s\n",
      "2025-12-20 11:55:30.398877: \n",
      "2025-12-20 11:55:30.398877: Epoch 697\n",
      "2025-12-20 11:55:30.398877: Current learning rate: 0.00341\n",
      "2025-12-20 11:57:48.397119: train_loss -0.827\n",
      "2025-12-20 11:57:48.399121: val_loss -0.8611\n",
      "2025-12-20 11:57:48.406867: Pseudo dice [0.9168, 0.9513, 0.9343]\n",
      "2025-12-20 11:57:48.410872: Epoch time: 138.0 s\n",
      "2025-12-20 11:57:49.055346: \n",
      "2025-12-20 11:57:49.055346: Epoch 698\n",
      "2025-12-20 11:57:49.071433: Current learning rate: 0.0034\n",
      "2025-12-20 12:00:07.242042: train_loss -0.8398\n",
      "2025-12-20 12:00:07.242042: val_loss -0.8695\n",
      "2025-12-20 12:00:07.257788: Pseudo dice [0.9238, 0.9569, 0.9288]\n",
      "2025-12-20 12:00:07.257788: Epoch time: 138.19 s\n",
      "2025-12-20 12:00:07.939024: \n",
      "2025-12-20 12:00:07.939024: Epoch 699\n",
      "2025-12-20 12:00:07.955069: Current learning rate: 0.00339\n",
      "2025-12-20 12:02:25.947741: train_loss -0.8466\n",
      "2025-12-20 12:02:25.947741: val_loss -0.8769\n",
      "2025-12-20 12:02:25.954191: Pseudo dice [0.9266, 0.9584, 0.9398]\n",
      "2025-12-20 12:02:25.956193: Epoch time: 138.01 s\n",
      "2025-12-20 12:02:26.864824: \n",
      "2025-12-20 12:02:26.864824: Epoch 700\n",
      "2025-12-20 12:02:26.864824: Current learning rate: 0.00338\n",
      "2025-12-20 12:04:44.904006: train_loss -0.8507\n",
      "2025-12-20 12:04:44.905747: val_loss -0.8787\n",
      "2025-12-20 12:04:44.913483: Pseudo dice [0.9267, 0.9577, 0.9421]\n",
      "2025-12-20 12:04:44.919227: Epoch time: 138.04 s\n",
      "2025-12-20 12:04:45.625446: \n",
      "2025-12-20 12:04:45.625446: Epoch 701\n",
      "2025-12-20 12:04:45.631100: Current learning rate: 0.00337\n",
      "2025-12-20 12:07:03.698797: train_loss -0.8525\n",
      "2025-12-20 12:07:03.698797: val_loss -0.8832\n",
      "2025-12-20 12:07:03.706806: Pseudo dice [0.9285, 0.9631, 0.9443]\n",
      "2025-12-20 12:07:03.712550: Epoch time: 138.08 s\n",
      "2025-12-20 12:07:04.545019: \n",
      "2025-12-20 12:07:04.545019: Epoch 702\n",
      "2025-12-20 12:07:04.545522: Current learning rate: 0.00336\n",
      "2025-12-20 12:09:22.733805: train_loss -0.8508\n",
      "2025-12-20 12:09:22.733805: val_loss -0.8706\n",
      "2025-12-20 12:09:22.743818: Pseudo dice [0.9226, 0.9554, 0.9394]\n",
      "2025-12-20 12:09:22.751832: Epoch time: 138.19 s\n",
      "2025-12-20 12:09:23.486382: \n",
      "2025-12-20 12:09:23.486382: Epoch 703\n",
      "2025-12-20 12:09:23.491656: Current learning rate: 0.00335\n",
      "2025-12-20 12:11:41.561013: train_loss -0.845\n",
      "2025-12-20 12:11:41.563016: val_loss -0.8714\n",
      "2025-12-20 12:11:41.568826: Pseudo dice [0.9205, 0.959, 0.9379]\n",
      "2025-12-20 12:11:41.572830: Epoch time: 138.08 s\n",
      "2025-12-20 12:11:42.218113: \n",
      "2025-12-20 12:11:42.218113: Epoch 704\n",
      "2025-12-20 12:11:42.218113: Current learning rate: 0.00334\n",
      "2025-12-20 12:14:00.534228: train_loss -0.8479\n",
      "2025-12-20 12:14:00.534228: val_loss -0.8743\n",
      "2025-12-20 12:14:00.543618: Pseudo dice [0.9234, 0.96, 0.9376]\n",
      "2025-12-20 12:14:00.547622: Epoch time: 138.32 s\n",
      "2025-12-20 12:14:01.199623: \n",
      "2025-12-20 12:14:01.199623: Epoch 705\n",
      "2025-12-20 12:14:01.199623: Current learning rate: 0.00333\n",
      "2025-12-20 12:16:19.099265: train_loss -0.8521\n",
      "2025-12-20 12:16:19.099265: val_loss -0.8837\n",
      "2025-12-20 12:16:19.115119: Pseudo dice [0.9279, 0.9606, 0.9466]\n",
      "2025-12-20 12:16:19.115119: Epoch time: 137.9 s\n",
      "2025-12-20 12:16:19.795349: \n",
      "2025-12-20 12:16:19.795349: Epoch 706\n",
      "2025-12-20 12:16:19.795349: Current learning rate: 0.00332\n",
      "2025-12-20 12:18:37.868659: train_loss -0.8515\n",
      "2025-12-20 12:18:37.868659: val_loss -0.8695\n",
      "2025-12-20 12:18:37.874664: Pseudo dice [0.9215, 0.9539, 0.9378]\n",
      "2025-12-20 12:18:37.877668: Epoch time: 138.07 s\n",
      "2025-12-20 12:18:38.524531: \n",
      "2025-12-20 12:18:38.524531: Epoch 707\n",
      "2025-12-20 12:18:38.540630: Current learning rate: 0.00331\n",
      "2025-12-20 12:20:56.847115: train_loss -0.8477\n",
      "2025-12-20 12:20:56.847115: val_loss -0.8878\n",
      "2025-12-20 12:20:56.854194: Pseudo dice [0.9358, 0.9636, 0.9357]\n",
      "2025-12-20 12:20:56.858198: Epoch time: 138.32 s\n",
      "2025-12-20 12:20:57.508783: \n",
      "2025-12-20 12:20:57.508783: Epoch 708\n",
      "2025-12-20 12:20:57.524711: Current learning rate: 0.0033\n",
      "2025-12-20 12:23:15.576871: train_loss -0.844\n",
      "2025-12-20 12:23:15.576871: val_loss -0.8736\n",
      "2025-12-20 12:23:15.584417: Pseudo dice [0.9252, 0.9585, 0.9327]\n",
      "2025-12-20 12:23:15.588421: Epoch time: 138.07 s\n",
      "2025-12-20 12:23:16.287025: \n",
      "2025-12-20 12:23:16.287025: Epoch 709\n",
      "2025-12-20 12:23:16.287025: Current learning rate: 0.00329\n",
      "2025-12-20 12:25:34.510626: train_loss -0.8506\n",
      "2025-12-20 12:25:34.510626: val_loss -0.8828\n",
      "2025-12-20 12:25:34.510626: Pseudo dice [0.9283, 0.9639, 0.9435]\n",
      "2025-12-20 12:25:34.510626: Epoch time: 138.22 s\n",
      "2025-12-20 12:25:35.159884: \n",
      "2025-12-20 12:25:35.159884: Epoch 710\n",
      "2025-12-20 12:25:35.159884: Current learning rate: 0.00328\n",
      "2025-12-20 12:27:53.190661: train_loss -0.8546\n",
      "2025-12-20 12:27:53.192664: val_loss -0.8712\n",
      "2025-12-20 12:27:53.198669: Pseudo dice [0.9166, 0.9528, 0.9417]\n",
      "2025-12-20 12:27:53.204675: Epoch time: 138.03 s\n",
      "2025-12-20 12:27:53.861900: \n",
      "2025-12-20 12:27:53.861900: Epoch 711\n",
      "2025-12-20 12:27:53.861900: Current learning rate: 0.00327\n",
      "2025-12-20 12:30:11.986415: train_loss -0.8555\n",
      "2025-12-20 12:30:11.988417: val_loss -0.8718\n",
      "2025-12-20 12:30:11.994422: Pseudo dice [0.9217, 0.9577, 0.9402]\n",
      "2025-12-20 12:30:12.000166: Epoch time: 138.12 s\n",
      "2025-12-20 12:30:12.696981: \n",
      "2025-12-20 12:30:12.696981: Epoch 712\n",
      "2025-12-20 12:30:12.696981: Current learning rate: 0.00326\n",
      "2025-12-20 12:32:30.775149: train_loss -0.8435\n",
      "2025-12-20 12:32:30.775149: val_loss -0.8753\n",
      "2025-12-20 12:32:30.775149: Pseudo dice [0.9258, 0.9576, 0.9393]\n",
      "2025-12-20 12:32:30.791178: Epoch time: 138.08 s\n",
      "2025-12-20 12:32:31.440741: \n",
      "2025-12-20 12:32:31.440741: Epoch 713\n",
      "2025-12-20 12:32:31.440741: Current learning rate: 0.00325\n",
      "2025-12-20 12:34:49.544451: train_loss -0.8529\n",
      "2025-12-20 12:34:49.546454: val_loss -0.882\n",
      "2025-12-20 12:34:49.550460: Pseudo dice [0.9291, 0.9574, 0.9453]\n",
      "2025-12-20 12:34:49.557053: Epoch time: 138.12 s\n",
      "2025-12-20 12:34:50.403765: \n",
      "2025-12-20 12:34:50.403765: Epoch 714\n",
      "2025-12-20 12:34:50.403765: Current learning rate: 0.00324\n",
      "2025-12-20 12:37:08.347538: train_loss -0.8544\n",
      "2025-12-20 12:37:08.363254: val_loss -0.882\n",
      "2025-12-20 12:37:08.367260: Pseudo dice [0.9278, 0.9644, 0.9365]\n",
      "2025-12-20 12:37:08.371265: Epoch time: 137.96 s\n",
      "2025-12-20 12:37:09.073939: \n",
      "2025-12-20 12:37:09.073939: Epoch 715\n",
      "2025-12-20 12:37:09.073939: Current learning rate: 0.00323\n",
      "2025-12-20 12:39:27.258789: train_loss -0.8515\n",
      "2025-12-20 12:39:27.258789: val_loss -0.8895\n",
      "2025-12-20 12:39:27.258789: Pseudo dice [0.9353, 0.9623, 0.9425]\n",
      "2025-12-20 12:39:27.267495: Epoch time: 138.18 s\n",
      "2025-12-20 12:39:27.969152: \n",
      "2025-12-20 12:39:27.969152: Epoch 716\n",
      "2025-12-20 12:39:27.969152: Current learning rate: 0.00322\n",
      "2025-12-20 12:41:45.996862: train_loss -0.8553\n",
      "2025-12-20 12:41:45.998864: val_loss -0.881\n",
      "2025-12-20 12:41:46.000604: Pseudo dice [0.9268, 0.959, 0.9414]\n",
      "2025-12-20 12:41:46.006172: Epoch time: 138.03 s\n",
      "2025-12-20 12:41:46.665748: \n",
      "2025-12-20 12:41:46.665748: Epoch 717\n",
      "2025-12-20 12:41:46.665748: Current learning rate: 0.00321\n",
      "2025-12-20 12:44:04.646192: train_loss -0.8511\n",
      "2025-12-20 12:44:04.646192: val_loss -0.8799\n",
      "2025-12-20 12:44:04.657953: Pseudo dice [0.9238, 0.9593, 0.9458]\n",
      "2025-12-20 12:44:04.665963: Epoch time: 137.98 s\n",
      "2025-12-20 12:44:05.418628: \n",
      "2025-12-20 12:44:05.418628: Epoch 718\n",
      "2025-12-20 12:44:05.418628: Current learning rate: 0.0032\n",
      "2025-12-20 12:46:23.409055: train_loss -0.8556\n",
      "2025-12-20 12:46:23.409055: val_loss -0.8813\n",
      "2025-12-20 12:46:23.418485: Pseudo dice [0.9273, 0.9608, 0.9415]\n",
      "2025-12-20 12:46:23.424345: Epoch time: 137.99 s\n",
      "2025-12-20 12:46:24.073660: \n",
      "2025-12-20 12:46:24.073660: Epoch 719\n",
      "2025-12-20 12:46:24.087679: Current learning rate: 0.00319\n",
      "2025-12-20 12:48:42.360923: train_loss -0.8537\n",
      "2025-12-20 12:48:42.360923: val_loss -0.8769\n",
      "2025-12-20 12:48:42.368930: Pseudo dice [0.9252, 0.9583, 0.9396]\n",
      "2025-12-20 12:48:42.374937: Epoch time: 138.29 s\n",
      "2025-12-20 12:48:43.203321: \n",
      "2025-12-20 12:48:43.203321: Epoch 720\n",
      "2025-12-20 12:48:43.203321: Current learning rate: 0.00318\n",
      "2025-12-20 12:51:01.136116: train_loss -0.8512\n",
      "2025-12-20 12:51:01.136116: val_loss -0.8779\n",
      "2025-12-20 12:51:01.145869: Pseudo dice [0.9254, 0.9568, 0.9409]\n",
      "2025-12-20 12:51:01.153883: Epoch time: 137.93 s\n",
      "2025-12-20 12:51:01.985339: \n",
      "2025-12-20 12:51:01.985339: Epoch 721\n",
      "2025-12-20 12:51:02.001182: Current learning rate: 0.00317\n",
      "2025-12-20 12:53:20.172255: train_loss -0.8574\n",
      "2025-12-20 12:53:20.172255: val_loss -0.8801\n",
      "2025-12-20 12:53:20.172255: Pseudo dice [0.9292, 0.9559, 0.9395]\n",
      "2025-12-20 12:53:20.172255: Epoch time: 138.19 s\n",
      "2025-12-20 12:53:20.823129: \n",
      "2025-12-20 12:53:20.823129: Epoch 722\n",
      "2025-12-20 12:53:20.823129: Current learning rate: 0.00316\n",
      "2025-12-20 12:55:39.039046: train_loss -0.857\n",
      "2025-12-20 12:55:39.039046: val_loss -0.8757\n",
      "2025-12-20 12:55:39.044791: Pseudo dice [0.9221, 0.9516, 0.9463]\n",
      "2025-12-20 12:55:39.048797: Epoch time: 138.22 s\n",
      "2025-12-20 12:55:39.710023: \n",
      "2025-12-20 12:55:39.710023: Epoch 723\n",
      "2025-12-20 12:55:39.710023: Current learning rate: 0.00315\n",
      "2025-12-20 12:57:57.762695: train_loss -0.8535\n",
      "2025-12-20 12:57:57.764698: val_loss -0.8774\n",
      "2025-12-20 12:57:57.768443: Pseudo dice [0.9233, 0.9555, 0.9437]\n",
      "2025-12-20 12:57:57.768443: Epoch time: 138.05 s\n",
      "2025-12-20 12:57:58.589631: \n",
      "2025-12-20 12:57:58.589631: Epoch 724\n",
      "2025-12-20 12:57:58.589631: Current learning rate: 0.00314\n",
      "2025-12-20 13:00:16.733475: train_loss -0.8549\n",
      "2025-12-20 13:00:16.733475: val_loss -0.8856\n",
      "2025-12-20 13:00:16.733475: Pseudo dice [0.9316, 0.9647, 0.9376]\n",
      "2025-12-20 13:00:16.749412: Epoch time: 138.16 s\n",
      "2025-12-20 13:00:17.397516: \n",
      "2025-12-20 13:00:17.397516: Epoch 725\n",
      "2025-12-20 13:00:17.397516: Current learning rate: 0.00313\n",
      "2025-12-20 13:02:35.415499: train_loss -0.8584\n",
      "2025-12-20 13:02:35.415499: val_loss -0.8824\n",
      "2025-12-20 13:02:35.415499: Pseudo dice [0.9296, 0.9609, 0.9434]\n",
      "2025-12-20 13:02:35.429508: Epoch time: 138.02 s\n",
      "2025-12-20 13:02:36.238572: \n",
      "2025-12-20 13:02:36.238572: Epoch 726\n",
      "2025-12-20 13:02:36.238572: Current learning rate: 0.00312\n",
      "2025-12-20 13:04:54.297189: train_loss -0.85\n",
      "2025-12-20 13:04:54.299191: val_loss -0.8769\n",
      "2025-12-20 13:04:54.303195: Pseudo dice [0.9229, 0.9557, 0.9455]\n",
      "2025-12-20 13:04:54.309202: Epoch time: 138.06 s\n",
      "2025-12-20 13:04:55.080178: \n",
      "2025-12-20 13:04:55.080178: Epoch 727\n",
      "2025-12-20 13:04:55.095922: Current learning rate: 0.00311\n",
      "2025-12-20 13:07:13.251102: train_loss -0.8543\n",
      "2025-12-20 13:07:13.251102: val_loss -0.8866\n",
      "2025-12-20 13:07:13.257108: Pseudo dice [0.9323, 0.9615, 0.9444]\n",
      "2025-12-20 13:07:13.262590: Epoch time: 138.17 s\n",
      "2025-12-20 13:07:13.266594: Yayy! New best EMA pseudo Dice: 0.9424\n",
      "2025-12-20 13:07:14.199147: \n",
      "2025-12-20 13:07:14.199147: Epoch 728\n",
      "2025-12-20 13:07:14.199147: Current learning rate: 0.0031\n",
      "2025-12-20 13:09:32.455013: train_loss -0.8544\n",
      "2025-12-20 13:09:32.456755: val_loss -0.8759\n",
      "2025-12-20 13:09:32.456755: Pseudo dice [0.923, 0.9548, 0.941]\n",
      "2025-12-20 13:09:32.456755: Epoch time: 138.27 s\n",
      "2025-12-20 13:09:33.123105: \n",
      "2025-12-20 13:09:33.123105: Epoch 729\n",
      "2025-12-20 13:09:33.123105: Current learning rate: 0.00309\n",
      "2025-12-20 13:11:51.355152: train_loss -0.8501\n",
      "2025-12-20 13:11:51.355152: val_loss -0.8862\n",
      "2025-12-20 13:11:51.361158: Pseudo dice [0.9308, 0.9595, 0.9469]\n",
      "2025-12-20 13:11:51.366786: Epoch time: 138.23 s\n",
      "2025-12-20 13:11:51.370789: Yayy! New best EMA pseudo Dice: 0.9425\n",
      "2025-12-20 13:11:52.501979: \n",
      "2025-12-20 13:11:52.501979: Epoch 730\n",
      "2025-12-20 13:11:52.517945: Current learning rate: 0.00308\n",
      "2025-12-20 13:14:10.548421: train_loss -0.8529\n",
      "2025-12-20 13:14:10.550423: val_loss -0.8795\n",
      "2025-12-20 13:14:10.556428: Pseudo dice [0.9254, 0.9554, 0.9454]\n",
      "2025-12-20 13:14:10.564175: Epoch time: 138.05 s\n",
      "2025-12-20 13:14:11.210001: \n",
      "2025-12-20 13:14:11.210001: Epoch 731\n",
      "2025-12-20 13:14:11.225888: Current learning rate: 0.00307\n",
      "2025-12-20 13:16:29.554237: train_loss -0.859\n",
      "2025-12-20 13:16:29.556239: val_loss -0.8806\n",
      "2025-12-20 13:16:29.563985: Pseudo dice [0.9264, 0.9593, 0.9407]\n",
      "2025-12-20 13:16:29.565988: Epoch time: 138.34 s\n",
      "2025-12-20 13:16:30.433880: \n",
      "2025-12-20 13:16:30.433880: Epoch 732\n",
      "2025-12-20 13:16:30.433880: Current learning rate: 0.00306\n",
      "2025-12-20 13:18:48.557934: train_loss -0.8558\n",
      "2025-12-20 13:18:48.557934: val_loss -0.8785\n",
      "2025-12-20 13:18:48.565682: Pseudo dice [0.9273, 0.959, 0.9411]\n",
      "2025-12-20 13:18:48.571688: Epoch time: 138.12 s\n",
      "2025-12-20 13:18:49.333551: \n",
      "2025-12-20 13:18:49.333551: Epoch 733\n",
      "2025-12-20 13:18:49.333551: Current learning rate: 0.00305\n",
      "2025-12-20 13:21:07.371895: train_loss -0.8589\n",
      "2025-12-20 13:21:07.371895: val_loss -0.8875\n",
      "2025-12-20 13:21:07.381596: Pseudo dice [0.9322, 0.96, 0.9432]\n",
      "2025-12-20 13:21:07.387602: Epoch time: 138.05 s\n",
      "2025-12-20 13:21:07.393347: Yayy! New best EMA pseudo Dice: 0.9427\n",
      "2025-12-20 13:21:08.316865: \n",
      "2025-12-20 13:21:08.318868: Epoch 734\n",
      "2025-12-20 13:21:08.324386: Current learning rate: 0.00304\n",
      "2025-12-20 13:23:26.481874: train_loss -0.8575\n",
      "2025-12-20 13:23:26.481874: val_loss -0.8816\n",
      "2025-12-20 13:23:26.491727: Pseudo dice [0.9269, 0.9592, 0.9492]\n",
      "2025-12-20 13:23:26.495518: Epoch time: 138.17 s\n",
      "2025-12-20 13:23:26.497522: Yayy! New best EMA pseudo Dice: 0.9429\n",
      "2025-12-20 13:23:27.434525: \n",
      "2025-12-20 13:23:27.434525: Epoch 735\n",
      "2025-12-20 13:23:27.434525: Current learning rate: 0.00303\n",
      "2025-12-20 13:25:45.625238: train_loss -0.8545\n",
      "2025-12-20 13:25:45.627242: val_loss -0.87\n",
      "2025-12-20 13:25:45.634997: Pseudo dice [0.9176, 0.9586, 0.942]\n",
      "2025-12-20 13:25:45.643010: Epoch time: 138.19 s\n",
      "2025-12-20 13:25:46.406384: \n",
      "2025-12-20 13:25:46.406384: Epoch 736\n",
      "2025-12-20 13:25:46.406384: Current learning rate: 0.00302\n",
      "2025-12-20 13:28:04.531561: train_loss -0.8524\n",
      "2025-12-20 13:28:04.531561: val_loss -0.8776\n",
      "2025-12-20 13:28:04.539310: Pseudo dice [0.9222, 0.9562, 0.9485]\n",
      "2025-12-20 13:28:04.541169: Epoch time: 138.13 s\n",
      "2025-12-20 13:28:05.188049: \n",
      "2025-12-20 13:28:05.188049: Epoch 737\n",
      "2025-12-20 13:28:05.204054: Current learning rate: 0.00301\n",
      "2025-12-20 13:30:23.276527: train_loss -0.8552\n",
      "2025-12-20 13:30:23.278529: val_loss -0.8769\n",
      "2025-12-20 13:30:23.282533: Pseudo dice [0.9259, 0.9575, 0.9422]\n",
      "2025-12-20 13:30:23.290378: Epoch time: 138.09 s\n",
      "2025-12-20 13:30:24.115816: \n",
      "2025-12-20 13:30:24.115816: Epoch 738\n",
      "2025-12-20 13:30:24.115816: Current learning rate: 0.003\n",
      "2025-12-20 13:32:42.108106: train_loss -0.8553\n",
      "2025-12-20 13:32:42.108106: val_loss -0.88\n",
      "2025-12-20 13:32:42.113595: Pseudo dice [0.9256, 0.9574, 0.9434]\n",
      "2025-12-20 13:32:42.119256: Epoch time: 137.99 s\n",
      "2025-12-20 13:32:42.877380: \n",
      "2025-12-20 13:32:42.877380: Epoch 739\n",
      "2025-12-20 13:32:42.882994: Current learning rate: 0.00299\n",
      "2025-12-20 13:35:01.174031: train_loss -0.8555\n",
      "2025-12-20 13:35:01.174031: val_loss -0.883\n",
      "2025-12-20 13:35:01.195974: Pseudo dice [0.9283, 0.9573, 0.9416]\n",
      "2025-12-20 13:35:01.199978: Epoch time: 138.3 s\n",
      "2025-12-20 13:35:01.839293: \n",
      "2025-12-20 13:35:01.839293: Epoch 740\n",
      "2025-12-20 13:35:01.858921: Current learning rate: 0.00297\n",
      "2025-12-20 13:37:20.070302: train_loss -0.8518\n",
      "2025-12-20 13:37:20.070302: val_loss -0.8847\n",
      "2025-12-20 13:37:20.086243: Pseudo dice [0.9266, 0.9559, 0.9542]\n",
      "2025-12-20 13:37:20.086243: Epoch time: 138.23 s\n",
      "2025-12-20 13:37:20.752360: \n",
      "2025-12-20 13:37:20.752360: Epoch 741\n",
      "2025-12-20 13:37:20.752360: Current learning rate: 0.00296\n",
      "2025-12-20 13:39:39.063837: train_loss -0.8525\n",
      "2025-12-20 13:39:39.063837: val_loss -0.8808\n",
      "2025-12-20 13:39:39.071848: Pseudo dice [0.9256, 0.9619, 0.9412]\n",
      "2025-12-20 13:39:39.079598: Epoch time: 138.31 s\n",
      "2025-12-20 13:39:39.867180: \n",
      "2025-12-20 13:39:39.867180: Epoch 742\n",
      "2025-12-20 13:39:39.875579: Current learning rate: 0.00295\n",
      "2025-12-20 13:41:57.866086: train_loss -0.8572\n",
      "2025-12-20 13:41:57.866086: val_loss -0.8817\n",
      "2025-12-20 13:41:57.876358: Pseudo dice [0.9262, 0.9575, 0.9498]\n",
      "2025-12-20 13:41:57.878099: Epoch time: 138.0 s\n",
      "2025-12-20 13:41:57.884935: Yayy! New best EMA pseudo Dice: 0.943\n",
      "2025-12-20 13:41:58.781836: \n",
      "2025-12-20 13:41:58.781836: Epoch 743\n",
      "2025-12-20 13:41:58.790769: Current learning rate: 0.00294\n",
      "2025-12-20 13:44:16.858382: train_loss -0.8528\n",
      "2025-12-20 13:44:16.860390: val_loss -0.8844\n",
      "2025-12-20 13:44:16.868405: Pseudo dice [0.9309, 0.9575, 0.9409]\n",
      "2025-12-20 13:44:16.872149: Epoch time: 138.08 s\n",
      "2025-12-20 13:44:16.879695: Yayy! New best EMA pseudo Dice: 0.943\n",
      "2025-12-20 13:44:18.012297: \n",
      "2025-12-20 13:44:18.012297: Epoch 744\n",
      "2025-12-20 13:44:18.028042: Current learning rate: 0.00293\n",
      "2025-12-20 13:46:36.117593: train_loss -0.8585\n",
      "2025-12-20 13:46:36.117593: val_loss -0.8808\n",
      "2025-12-20 13:46:36.117593: Pseudo dice [0.9254, 0.9583, 0.9491]\n",
      "2025-12-20 13:46:36.117593: Epoch time: 138.11 s\n",
      "2025-12-20 13:46:36.135627: Yayy! New best EMA pseudo Dice: 0.9431\n",
      "2025-12-20 13:46:37.188149: \n",
      "2025-12-20 13:46:37.188149: Epoch 745\n",
      "2025-12-20 13:46:37.193309: Current learning rate: 0.00292\n",
      "2025-12-20 13:48:55.326654: train_loss -0.8468\n",
      "2025-12-20 13:48:55.328657: val_loss -0.8685\n",
      "2025-12-20 13:48:55.334401: Pseudo dice [0.9187, 0.9541, 0.9363]\n",
      "2025-12-20 13:48:55.342106: Epoch time: 138.14 s\n",
      "2025-12-20 13:48:55.998838: \n",
      "2025-12-20 13:48:55.998838: Epoch 746\n",
      "2025-12-20 13:48:55.998838: Current learning rate: 0.00291\n",
      "2025-12-20 13:51:14.117292: train_loss -0.8476\n",
      "2025-12-20 13:51:14.119294: val_loss -0.8776\n",
      "2025-12-20 13:51:14.125302: Pseudo dice [0.9242, 0.9594, 0.9374]\n",
      "2025-12-20 13:51:14.131114: Epoch time: 138.12 s\n",
      "2025-12-20 13:51:14.774115: \n",
      "2025-12-20 13:51:14.774115: Epoch 747\n",
      "2025-12-20 13:51:14.790026: Current learning rate: 0.0029\n",
      "2025-12-20 13:53:32.989619: train_loss -0.8476\n",
      "2025-12-20 13:53:32.989619: val_loss -0.8827\n",
      "2025-12-20 13:53:32.989619: Pseudo dice [0.9277, 0.9596, 0.9453]\n",
      "2025-12-20 13:53:32.989619: Epoch time: 138.22 s\n",
      "2025-12-20 13:53:33.785385: \n",
      "2025-12-20 13:53:33.785385: Epoch 748\n",
      "2025-12-20 13:53:33.799482: Current learning rate: 0.00289\n",
      "2025-12-20 13:55:51.718128: train_loss -0.8537\n",
      "2025-12-20 13:55:51.720131: val_loss -0.8795\n",
      "2025-12-20 13:55:51.725378: Pseudo dice [0.9283, 0.9575, 0.9403]\n",
      "2025-12-20 13:55:51.730209: Epoch time: 137.93 s\n",
      "2025-12-20 13:55:52.418731: \n",
      "2025-12-20 13:55:52.418731: Epoch 749\n",
      "2025-12-20 13:55:52.418731: Current learning rate: 0.00288\n",
      "2025-12-20 13:58:10.333911: train_loss -0.858\n",
      "2025-12-20 13:58:10.333911: val_loss -0.8811\n",
      "2025-12-20 13:58:10.349602: Pseudo dice [0.9287, 0.9593, 0.9432]\n",
      "2025-12-20 13:58:10.356096: Epoch time: 137.92 s\n",
      "2025-12-20 13:58:11.491960: \n",
      "2025-12-20 13:58:11.491960: Epoch 750\n",
      "2025-12-20 13:58:11.499260: Current learning rate: 0.00287\n",
      "2025-12-20 14:00:29.463365: train_loss -0.855\n",
      "2025-12-20 14:00:29.463365: val_loss -0.8784\n",
      "2025-12-20 14:00:29.479023: Pseudo dice [0.9236, 0.9582, 0.9428]\n",
      "2025-12-20 14:00:29.486554: Epoch time: 137.97 s\n",
      "2025-12-20 14:00:30.284944: \n",
      "2025-12-20 14:00:30.284944: Epoch 751\n",
      "2025-12-20 14:00:30.304474: Current learning rate: 0.00286\n",
      "2025-12-20 14:02:48.399189: train_loss -0.8583\n",
      "2025-12-20 14:02:48.401193: val_loss -0.8818\n",
      "2025-12-20 14:02:48.405199: Pseudo dice [0.9255, 0.9604, 0.9406]\n",
      "2025-12-20 14:02:48.411077: Epoch time: 138.11 s\n",
      "2025-12-20 14:02:49.063054: \n",
      "2025-12-20 14:02:49.063054: Epoch 752\n",
      "2025-12-20 14:02:49.077309: Current learning rate: 0.00285\n",
      "2025-12-20 14:05:06.972090: train_loss -0.8582\n",
      "2025-12-20 14:05:06.972090: val_loss -0.8783\n",
      "2025-12-20 14:05:06.985774: Pseudo dice [0.923, 0.9506, 0.9435]\n",
      "2025-12-20 14:05:06.987777: Epoch time: 137.91 s\n",
      "2025-12-20 14:05:07.717969: \n",
      "2025-12-20 14:05:07.717969: Epoch 753\n",
      "2025-12-20 14:05:07.717969: Current learning rate: 0.00284\n",
      "2025-12-20 14:07:25.729402: train_loss -0.8625\n",
      "2025-12-20 14:07:25.729402: val_loss -0.8875\n",
      "2025-12-20 14:07:25.739756: Pseudo dice [0.9332, 0.9579, 0.9457]\n",
      "2025-12-20 14:07:25.745073: Epoch time: 138.01 s\n",
      "2025-12-20 14:07:26.542304: \n",
      "2025-12-20 14:07:26.542304: Epoch 754\n",
      "2025-12-20 14:07:26.548049: Current learning rate: 0.00283\n",
      "2025-12-20 14:09:44.807103: train_loss -0.8581\n",
      "2025-12-20 14:09:44.807103: val_loss -0.8827\n",
      "2025-12-20 14:09:44.811505: Pseudo dice [0.9311, 0.9598, 0.9356]\n",
      "2025-12-20 14:09:44.817002: Epoch time: 138.27 s\n",
      "2025-12-20 14:09:45.676718: \n",
      "2025-12-20 14:09:45.676718: Epoch 755\n",
      "2025-12-20 14:09:45.692698: Current learning rate: 0.00282\n",
      "2025-12-20 14:12:03.774703: train_loss -0.8584\n",
      "2025-12-20 14:12:03.774703: val_loss -0.8826\n",
      "2025-12-20 14:12:03.782711: Pseudo dice [0.9288, 0.9608, 0.9396]\n",
      "2025-12-20 14:12:03.790460: Epoch time: 138.1 s\n",
      "2025-12-20 14:12:04.465190: \n",
      "2025-12-20 14:12:04.465190: Epoch 756\n",
      "2025-12-20 14:12:04.481017: Current learning rate: 0.00281\n",
      "2025-12-20 14:14:22.765359: train_loss -0.8547\n",
      "2025-12-20 14:14:22.765359: val_loss -0.8805\n",
      "2025-12-20 14:14:22.765359: Pseudo dice [0.925, 0.957, 0.9462]\n",
      "2025-12-20 14:14:22.776113: Epoch time: 138.3 s\n",
      "2025-12-20 14:14:23.589211: \n",
      "2025-12-20 14:14:23.589211: Epoch 757\n",
      "2025-12-20 14:14:23.604559: Current learning rate: 0.0028\n",
      "2025-12-20 14:16:41.611803: train_loss -0.8616\n",
      "2025-12-20 14:16:41.611803: val_loss -0.8807\n",
      "2025-12-20 14:16:41.627869: Pseudo dice [0.9283, 0.9587, 0.9405]\n",
      "2025-12-20 14:16:41.632792: Epoch time: 138.02 s\n",
      "2025-12-20 14:16:42.276840: \n",
      "2025-12-20 14:16:42.276840: Epoch 758\n",
      "2025-12-20 14:16:42.296258: Current learning rate: 0.00279\n",
      "2025-12-20 14:19:00.263978: train_loss -0.8568\n",
      "2025-12-20 14:19:00.265980: val_loss -0.8854\n",
      "2025-12-20 14:19:00.271726: Pseudo dice [0.9325, 0.9635, 0.9411]\n",
      "2025-12-20 14:19:00.275564: Epoch time: 137.99 s\n",
      "2025-12-20 14:19:00.921633: \n",
      "2025-12-20 14:19:00.921633: Epoch 759\n",
      "2025-12-20 14:19:00.930710: Current learning rate: 0.00278\n",
      "2025-12-20 14:21:18.971635: train_loss -0.858\n",
      "2025-12-20 14:21:18.971635: val_loss -0.8802\n",
      "2025-12-20 14:21:18.977019: Pseudo dice [0.9288, 0.9576, 0.9432]\n",
      "2025-12-20 14:21:18.981023: Epoch time: 138.05 s\n",
      "2025-12-20 14:21:19.713550: \n",
      "2025-12-20 14:21:19.713550: Epoch 760\n",
      "2025-12-20 14:21:19.722067: Current learning rate: 0.00277\n",
      "2025-12-20 14:23:37.758742: train_loss -0.8543\n",
      "2025-12-20 14:23:37.758742: val_loss -0.8701\n",
      "2025-12-20 14:23:37.764880: Pseudo dice [0.9204, 0.9512, 0.943]\n",
      "2025-12-20 14:23:37.770397: Epoch time: 138.05 s\n",
      "2025-12-20 14:23:38.407445: \n",
      "2025-12-20 14:23:38.423243: Epoch 761\n",
      "2025-12-20 14:23:38.427718: Current learning rate: 0.00276\n",
      "2025-12-20 14:25:56.548466: train_loss -0.8513\n",
      "2025-12-20 14:25:56.548466: val_loss -0.8799\n",
      "2025-12-20 14:25:56.558071: Pseudo dice [0.9285, 0.9562, 0.9376]\n",
      "2025-12-20 14:25:56.563641: Epoch time: 138.14 s\n",
      "2025-12-20 14:25:57.387865: \n",
      "2025-12-20 14:25:57.387865: Epoch 762\n",
      "2025-12-20 14:25:57.401264: Current learning rate: 0.00275\n",
      "2025-12-20 14:28:15.158332: train_loss -0.8546\n",
      "2025-12-20 14:28:15.158332: val_loss -0.8791\n",
      "2025-12-20 14:28:15.174194: Pseudo dice [0.9272, 0.9583, 0.9407]\n",
      "2025-12-20 14:28:15.179580: Epoch time: 137.77 s\n",
      "2025-12-20 14:28:15.945149: \n",
      "2025-12-20 14:28:15.945149: Epoch 763\n",
      "2025-12-20 14:28:15.949695: Current learning rate: 0.00274\n",
      "2025-12-20 14:30:33.859614: train_loss -0.8564\n",
      "2025-12-20 14:30:33.859614: val_loss -0.8876\n",
      "2025-12-20 14:30:33.867625: Pseudo dice [0.9343, 0.9608, 0.9407]\n",
      "2025-12-20 14:30:33.873372: Epoch time: 137.92 s\n",
      "2025-12-20 14:30:34.538076: \n",
      "2025-12-20 14:30:34.538076: Epoch 764\n",
      "2025-12-20 14:30:34.550110: Current learning rate: 0.00273\n",
      "2025-12-20 14:32:52.581288: train_loss -0.8549\n",
      "2025-12-20 14:32:52.581288: val_loss -0.8826\n",
      "2025-12-20 14:32:52.597265: Pseudo dice [0.9313, 0.9561, 0.9454]\n",
      "2025-12-20 14:32:52.603831: Epoch time: 138.04 s\n",
      "2025-12-20 14:32:53.246777: \n",
      "2025-12-20 14:32:53.246777: Epoch 765\n",
      "2025-12-20 14:32:53.262434: Current learning rate: 0.00272\n",
      "2025-12-20 14:35:11.458480: train_loss -0.8518\n",
      "2025-12-20 14:35:11.458480: val_loss -0.8756\n",
      "2025-12-20 14:35:11.458480: Pseudo dice [0.9193, 0.9532, 0.9452]\n",
      "2025-12-20 14:35:11.458480: Epoch time: 138.21 s\n",
      "2025-12-20 14:35:12.179239: \n",
      "2025-12-20 14:35:12.179239: Epoch 766\n",
      "2025-12-20 14:35:12.183963: Current learning rate: 0.00271\n",
      "2025-12-20 14:37:30.232085: train_loss -0.8533\n",
      "2025-12-20 14:37:30.232085: val_loss -0.8795\n",
      "2025-12-20 14:37:30.247898: Pseudo dice [0.9272, 0.9564, 0.9447]\n",
      "2025-12-20 14:37:30.255299: Epoch time: 138.05 s\n",
      "2025-12-20 14:37:31.089896: \n",
      "2025-12-20 14:37:31.091858: Epoch 767\n",
      "2025-12-20 14:37:31.095741: Current learning rate: 0.0027\n",
      "2025-12-20 14:39:49.029176: train_loss -0.8573\n",
      "2025-12-20 14:39:49.029176: val_loss -0.8821\n",
      "2025-12-20 14:39:49.048718: Pseudo dice [0.9248, 0.9592, 0.9472]\n",
      "2025-12-20 14:39:49.054186: Epoch time: 137.94 s\n",
      "2025-12-20 14:39:49.740389: \n",
      "2025-12-20 14:39:49.740389: Epoch 768\n",
      "2025-12-20 14:39:49.759858: Current learning rate: 0.00268\n",
      "2025-12-20 14:42:07.815009: train_loss -0.8545\n",
      "2025-12-20 14:42:07.815009: val_loss -0.8826\n",
      "2025-12-20 14:42:07.831022: Pseudo dice [0.9303, 0.9597, 0.9393]\n",
      "2025-12-20 14:42:07.831022: Epoch time: 138.07 s\n",
      "2025-12-20 14:42:08.496095: \n",
      "2025-12-20 14:42:08.496095: Epoch 769\n",
      "2025-12-20 14:42:08.508449: Current learning rate: 0.00267\n",
      "2025-12-20 14:44:26.687568: train_loss -0.8541\n",
      "2025-12-20 14:44:26.689570: val_loss -0.8888\n",
      "2025-12-20 14:44:26.697318: Pseudo dice [0.931, 0.962, 0.9457]\n",
      "2025-12-20 14:44:26.703630: Epoch time: 138.19 s\n",
      "2025-12-20 14:44:27.357125: \n",
      "2025-12-20 14:44:27.357125: Epoch 770\n",
      "2025-12-20 14:44:27.370901: Current learning rate: 0.00266\n",
      "2025-12-20 14:46:45.376624: train_loss -0.8566\n",
      "2025-12-20 14:46:45.376624: val_loss -0.885\n",
      "2025-12-20 14:46:45.385001: Pseudo dice [0.929, 0.9611, 0.9479]\n",
      "2025-12-20 14:46:45.389462: Epoch time: 138.02 s\n",
      "2025-12-20 14:46:45.392627: Yayy! New best EMA pseudo Dice: 0.9432\n",
      "2025-12-20 14:46:46.323695: \n",
      "2025-12-20 14:46:46.323695: Epoch 771\n",
      "2025-12-20 14:46:46.327500: Current learning rate: 0.00265\n",
      "2025-12-20 14:49:04.445377: train_loss -0.8548\n",
      "2025-12-20 14:49:04.447117: val_loss -0.88\n",
      "2025-12-20 14:49:04.455128: Pseudo dice [0.9246, 0.9605, 0.9413]\n",
      "2025-12-20 14:49:04.461137: Epoch time: 138.13 s\n",
      "2025-12-20 14:49:05.216545: \n",
      "2025-12-20 14:49:05.216545: Epoch 772\n",
      "2025-12-20 14:49:05.218547: Current learning rate: 0.00264\n",
      "2025-12-20 14:51:23.142985: train_loss -0.8605\n",
      "2025-12-20 14:51:23.144993: val_loss -0.8749\n",
      "2025-12-20 14:51:23.153004: Pseudo dice [0.9223, 0.9535, 0.935]\n",
      "2025-12-20 14:51:23.160220: Epoch time: 137.93 s\n",
      "2025-12-20 14:51:23.978525: \n",
      "2025-12-20 14:51:23.978525: Epoch 773\n",
      "2025-12-20 14:51:23.994190: Current learning rate: 0.00263\n",
      "2025-12-20 14:53:41.943992: train_loss -0.8517\n",
      "2025-12-20 14:53:41.943992: val_loss -0.8768\n",
      "2025-12-20 14:53:41.953815: Pseudo dice [0.9212, 0.9529, 0.945]\n",
      "2025-12-20 14:53:41.957883: Epoch time: 137.97 s\n",
      "2025-12-20 14:53:42.608272: \n",
      "2025-12-20 14:53:42.608272: Epoch 774\n",
      "2025-12-20 14:53:42.608272: Current learning rate: 0.00262\n",
      "2025-12-20 14:56:00.510549: train_loss -0.8534\n",
      "2025-12-20 14:56:00.512552: val_loss -0.8768\n",
      "2025-12-20 14:56:00.520423: Pseudo dice [0.925, 0.9583, 0.9393]\n",
      "2025-12-20 14:56:00.526430: Epoch time: 137.9 s\n",
      "2025-12-20 14:56:01.248181: \n",
      "2025-12-20 14:56:01.248181: Epoch 775\n",
      "2025-12-20 14:56:01.254105: Current learning rate: 0.00261\n",
      "2025-12-20 14:58:19.227237: train_loss -0.8554\n",
      "2025-12-20 14:58:19.227237: val_loss -0.8765\n",
      "2025-12-20 14:58:19.236143: Pseudo dice [0.9231, 0.9533, 0.9493]\n",
      "2025-12-20 14:58:19.240204: Epoch time: 137.99 s\n",
      "2025-12-20 14:58:19.892168: \n",
      "2025-12-20 14:58:19.892168: Epoch 776\n",
      "2025-12-20 14:58:19.900548: Current learning rate: 0.0026\n",
      "2025-12-20 15:00:37.875919: train_loss -0.8575\n",
      "2025-12-20 15:00:37.875919: val_loss -0.874\n",
      "2025-12-20 15:00:37.875919: Pseudo dice [0.9201, 0.9537, 0.9443]\n",
      "2025-12-20 15:00:37.888494: Epoch time: 137.98 s\n",
      "2025-12-20 15:00:38.586996: \n",
      "2025-12-20 15:00:38.586996: Epoch 777\n",
      "2025-12-20 15:00:38.603053: Current learning rate: 0.00259\n",
      "2025-12-20 15:02:56.513498: train_loss -0.8584\n",
      "2025-12-20 15:02:56.513498: val_loss -0.8816\n",
      "2025-12-20 15:02:56.513498: Pseudo dice [0.929, 0.9568, 0.9398]\n",
      "2025-12-20 15:02:56.525974: Epoch time: 137.93 s\n",
      "2025-12-20 15:02:57.211431: \n",
      "2025-12-20 15:02:57.211431: Epoch 778\n",
      "2025-12-20 15:02:57.229291: Current learning rate: 0.00258\n",
      "2025-12-20 15:05:15.413027: train_loss -0.8599\n",
      "2025-12-20 15:05:15.413027: val_loss -0.8804\n",
      "2025-12-20 15:05:15.433006: Pseudo dice [0.9203, 0.9598, 0.9523]\n",
      "2025-12-20 15:05:15.440037: Epoch time: 138.2 s\n",
      "2025-12-20 15:05:16.330878: \n",
      "2025-12-20 15:05:16.330878: Epoch 779\n",
      "2025-12-20 15:05:16.340945: Current learning rate: 0.00257\n",
      "2025-12-20 15:07:34.442351: train_loss -0.8573\n",
      "2025-12-20 15:07:34.442351: val_loss -0.8789\n",
      "2025-12-20 15:07:34.448358: Pseudo dice [0.9248, 0.9573, 0.9401]\n",
      "2025-12-20 15:07:34.450360: Epoch time: 138.11 s\n",
      "2025-12-20 15:07:35.107390: \n",
      "2025-12-20 15:07:35.107390: Epoch 780\n",
      "2025-12-20 15:07:35.123222: Current learning rate: 0.00256\n",
      "2025-12-20 15:09:53.672300: train_loss -0.8549\n",
      "2025-12-20 15:09:53.672300: val_loss -0.8892\n",
      "2025-12-20 15:09:53.686399: Pseudo dice [0.9322, 0.9638, 0.9483]\n",
      "2025-12-20 15:09:53.691203: Epoch time: 138.56 s\n",
      "2025-12-20 15:09:54.397243: \n",
      "2025-12-20 15:09:54.397243: Epoch 781\n",
      "2025-12-20 15:09:54.414948: Current learning rate: 0.00255\n",
      "2025-12-20 15:12:12.384889: train_loss -0.8598\n",
      "2025-12-20 15:12:12.384889: val_loss -0.8861\n",
      "2025-12-20 15:12:12.400107: Pseudo dice [0.9328, 0.9577, 0.9489]\n",
      "2025-12-20 15:12:12.405359: Epoch time: 137.99 s\n",
      "2025-12-20 15:12:13.066515: \n",
      "2025-12-20 15:12:13.066515: Epoch 782\n",
      "2025-12-20 15:12:13.082167: Current learning rate: 0.00254\n",
      "2025-12-20 15:14:31.278823: train_loss -0.8545\n",
      "2025-12-20 15:14:31.278823: val_loss -0.8842\n",
      "2025-12-20 15:14:31.283829: Pseudo dice [0.9315, 0.9595, 0.9441]\n",
      "2025-12-20 15:14:31.289512: Epoch time: 138.21 s\n",
      "2025-12-20 15:14:32.004252: \n",
      "2025-12-20 15:14:32.004252: Epoch 783\n",
      "2025-12-20 15:14:32.013560: Current learning rate: 0.00253\n",
      "2025-12-20 15:16:50.220163: train_loss -0.8558\n",
      "2025-12-20 15:16:50.222165: val_loss -0.887\n",
      "2025-12-20 15:16:50.227909: Pseudo dice [0.9307, 0.9619, 0.9468]\n",
      "2025-12-20 15:16:50.229736: Epoch time: 138.22 s\n",
      "2025-12-20 15:16:50.229736: Yayy! New best EMA pseudo Dice: 0.9435\n",
      "2025-12-20 15:16:51.170816: \n",
      "2025-12-20 15:16:51.174327: Epoch 784\n",
      "2025-12-20 15:16:51.174327: Current learning rate: 0.00252\n",
      "2025-12-20 15:19:09.155258: train_loss -0.8616\n",
      "2025-12-20 15:19:09.155258: val_loss -0.881\n",
      "2025-12-20 15:19:09.161264: Pseudo dice [0.9259, 0.9596, 0.9447]\n",
      "2025-12-20 15:19:09.166762: Epoch time: 137.98 s\n",
      "2025-12-20 15:19:09.838334: \n",
      "2025-12-20 15:19:09.838334: Epoch 785\n",
      "2025-12-20 15:19:09.846776: Current learning rate: 0.00251\n",
      "2025-12-20 15:21:27.874882: train_loss -0.8585\n",
      "2025-12-20 15:21:27.874882: val_loss -0.88\n",
      "2025-12-20 15:21:27.892393: Pseudo dice [0.9269, 0.9583, 0.9406]\n",
      "2025-12-20 15:21:27.898399: Epoch time: 138.04 s\n",
      "2025-12-20 15:21:28.626913: \n",
      "2025-12-20 15:21:28.626913: Epoch 786\n",
      "2025-12-20 15:21:28.630000: Current learning rate: 0.0025\n",
      "2025-12-20 15:23:46.683985: train_loss -0.8577\n",
      "2025-12-20 15:23:46.683985: val_loss -0.8851\n",
      "2025-12-20 15:23:46.689867: Pseudo dice [0.9289, 0.9584, 0.9464]\n",
      "2025-12-20 15:23:46.693871: Epoch time: 138.06 s\n",
      "2025-12-20 15:23:47.367542: \n",
      "2025-12-20 15:23:47.367542: Epoch 787\n",
      "2025-12-20 15:23:47.373830: Current learning rate: 0.00249\n",
      "2025-12-20 15:26:05.520485: train_loss -0.8544\n",
      "2025-12-20 15:26:05.520485: val_loss -0.884\n",
      "2025-12-20 15:26:05.536121: Pseudo dice [0.9259, 0.9625, 0.9512]\n",
      "2025-12-20 15:26:05.536121: Epoch time: 138.15 s\n",
      "2025-12-20 15:26:05.536121: Yayy! New best EMA pseudo Dice: 0.9437\n",
      "2025-12-20 15:26:06.532803: \n",
      "2025-12-20 15:26:06.532803: Epoch 788\n",
      "2025-12-20 15:26:06.532803: Current learning rate: 0.00248\n",
      "2025-12-20 15:28:24.525072: train_loss -0.8568\n",
      "2025-12-20 15:28:24.525072: val_loss -0.8822\n",
      "2025-12-20 15:28:24.540828: Pseudo dice [0.926, 0.9605, 0.9428]\n",
      "2025-12-20 15:28:24.548424: Epoch time: 137.99 s\n",
      "2025-12-20 15:28:25.266603: \n",
      "2025-12-20 15:28:25.266603: Epoch 789\n",
      "2025-12-20 15:28:25.282355: Current learning rate: 0.00247\n",
      "2025-12-20 15:30:43.233191: train_loss -0.8544\n",
      "2025-12-20 15:30:43.233191: val_loss -0.8857\n",
      "2025-12-20 15:30:43.248482: Pseudo dice [0.9319, 0.9641, 0.9406]\n",
      "2025-12-20 15:30:43.256447: Epoch time: 137.97 s\n",
      "2025-12-20 15:30:43.260451: Yayy! New best EMA pseudo Dice: 0.9439\n",
      "2025-12-20 15:30:44.371929: \n",
      "2025-12-20 15:30:44.371929: Epoch 790\n",
      "2025-12-20 15:30:44.392781: Current learning rate: 0.00245\n",
      "2025-12-20 15:33:02.499780: train_loss -0.8592\n",
      "2025-12-20 15:33:02.499780: val_loss -0.875\n",
      "2025-12-20 15:33:02.507789: Pseudo dice [0.9208, 0.9587, 0.9443]\n",
      "2025-12-20 15:33:02.513714: Epoch time: 138.13 s\n",
      "2025-12-20 15:33:03.177292: \n",
      "2025-12-20 15:33:03.177292: Epoch 791\n",
      "2025-12-20 15:33:03.177292: Current learning rate: 0.00244\n",
      "2025-12-20 15:35:21.294551: train_loss -0.8595\n",
      "2025-12-20 15:35:21.294551: val_loss -0.885\n",
      "2025-12-20 15:35:21.302383: Pseudo dice [0.9288, 0.9628, 0.9427]\n",
      "2025-12-20 15:35:21.308127: Epoch time: 138.12 s\n",
      "2025-12-20 15:35:22.028202: \n",
      "2025-12-20 15:35:22.028202: Epoch 792\n",
      "2025-12-20 15:35:22.043930: Current learning rate: 0.00243\n",
      "2025-12-20 15:37:40.118672: train_loss -0.8592\n",
      "2025-12-20 15:37:40.118672: val_loss -0.8826\n",
      "2025-12-20 15:37:40.126419: Pseudo dice [0.9297, 0.9606, 0.9413]\n",
      "2025-12-20 15:37:40.129190: Epoch time: 138.09 s\n",
      "2025-12-20 15:37:40.789864: \n",
      "2025-12-20 15:37:40.789864: Epoch 793\n",
      "2025-12-20 15:37:40.789864: Current learning rate: 0.00242\n",
      "2025-12-20 15:39:58.865561: train_loss -0.8574\n",
      "2025-12-20 15:39:58.865561: val_loss -0.889\n",
      "2025-12-20 15:39:58.873307: Pseudo dice [0.9322, 0.9615, 0.9459]\n",
      "2025-12-20 15:39:58.881320: Epoch time: 138.07 s\n",
      "2025-12-20 15:39:58.887065: Yayy! New best EMA pseudo Dice: 0.944\n",
      "2025-12-20 15:39:59.799165: \n",
      "2025-12-20 15:39:59.799165: Epoch 794\n",
      "2025-12-20 15:39:59.799165: Current learning rate: 0.00241\n",
      "2025-12-20 15:42:17.891381: train_loss -0.859\n",
      "2025-12-20 15:42:17.891381: val_loss -0.8842\n",
      "2025-12-20 15:42:17.899009: Pseudo dice [0.93, 0.9625, 0.9416]\n",
      "2025-12-20 15:42:17.905015: Epoch time: 138.09 s\n",
      "2025-12-20 15:42:17.910519: Yayy! New best EMA pseudo Dice: 0.9441\n",
      "2025-12-20 15:42:18.905742: \n",
      "2025-12-20 15:42:18.905742: Epoch 795\n",
      "2025-12-20 15:42:18.905742: Current learning rate: 0.0024\n",
      "2025-12-20 15:44:37.138199: train_loss -0.8558\n",
      "2025-12-20 15:44:37.138199: val_loss -0.8852\n",
      "2025-12-20 15:44:37.143993: Pseudo dice [0.9297, 0.9608, 0.9473]\n",
      "2025-12-20 15:44:37.143993: Epoch time: 138.23 s\n",
      "2025-12-20 15:44:37.143993: Yayy! New best EMA pseudo Dice: 0.9443\n",
      "2025-12-20 15:44:38.292227: \n",
      "2025-12-20 15:44:38.294229: Epoch 796\n",
      "2025-12-20 15:44:38.300236: Current learning rate: 0.00239\n",
      "2025-12-20 15:46:56.396148: train_loss -0.8592\n",
      "2025-12-20 15:46:56.396148: val_loss -0.8841\n",
      "2025-12-20 15:46:56.404014: Pseudo dice [0.9291, 0.9604, 0.9427]\n",
      "2025-12-20 15:46:56.408018: Epoch time: 138.1 s\n",
      "2025-12-20 15:46:57.071592: \n",
      "2025-12-20 15:46:57.071592: Epoch 797\n",
      "2025-12-20 15:46:57.071592: Current learning rate: 0.00238\n",
      "2025-12-20 15:49:15.070291: train_loss -0.859\n",
      "2025-12-20 15:49:15.072295: val_loss -0.8829\n",
      "2025-12-20 15:49:15.080369: Pseudo dice [0.9319, 0.9564, 0.9402]\n",
      "2025-12-20 15:49:15.087378: Epoch time: 138.0 s\n",
      "2025-12-20 15:49:15.817276: \n",
      "2025-12-20 15:49:15.819278: Epoch 798\n",
      "2025-12-20 15:49:15.819278: Current learning rate: 0.00237\n",
      "2025-12-20 15:51:34.000936: train_loss -0.858\n",
      "2025-12-20 15:51:34.000936: val_loss -0.8888\n",
      "2025-12-20 15:51:34.000936: Pseudo dice [0.9316, 0.9593, 0.9506]\n",
      "2025-12-20 15:51:34.017048: Epoch time: 138.18 s\n",
      "2025-12-20 15:51:34.017048: Yayy! New best EMA pseudo Dice: 0.9444\n",
      "2025-12-20 15:51:34.928728: \n",
      "2025-12-20 15:51:34.928728: Epoch 799\n",
      "2025-12-20 15:51:34.944596: Current learning rate: 0.00236\n",
      "2025-12-20 15:53:52.988953: train_loss -0.8635\n",
      "2025-12-20 15:53:52.988953: val_loss -0.8854\n",
      "2025-12-20 15:53:52.998514: Pseudo dice [0.9288, 0.961, 0.9437]\n",
      "2025-12-20 15:53:53.002518: Epoch time: 138.06 s\n",
      "2025-12-20 15:53:53.283852: Yayy! New best EMA pseudo Dice: 0.9444\n",
      "2025-12-20 15:53:54.266744: \n",
      "2025-12-20 15:53:54.266744: Epoch 800\n",
      "2025-12-20 15:53:54.268747: Current learning rate: 0.00235\n",
      "2025-12-20 15:56:12.340430: train_loss -0.8596\n",
      "2025-12-20 15:56:12.340430: val_loss -0.8896\n",
      "2025-12-20 15:56:12.348335: Pseudo dice [0.9339, 0.9618, 0.9453]\n",
      "2025-12-20 15:56:12.352339: Epoch time: 138.07 s\n",
      "2025-12-20 15:56:12.358083: Yayy! New best EMA pseudo Dice: 0.9447\n",
      "2025-12-20 15:56:13.507982: \n",
      "2025-12-20 15:56:13.507982: Epoch 801\n",
      "2025-12-20 15:56:13.507982: Current learning rate: 0.00234\n",
      "2025-12-20 15:58:31.654366: train_loss -0.8601\n",
      "2025-12-20 15:58:31.654366: val_loss -0.8847\n",
      "2025-12-20 15:58:31.662373: Pseudo dice [0.9305, 0.96, 0.941]\n",
      "2025-12-20 15:58:31.667173: Epoch time: 138.15 s\n",
      "2025-12-20 15:58:32.332367: \n",
      "2025-12-20 15:58:32.332367: Epoch 802\n",
      "2025-12-20 15:58:32.332367: Current learning rate: 0.00233\n",
      "2025-12-20 16:00:50.313720: train_loss -0.863\n",
      "2025-12-20 16:00:50.313720: val_loss -0.8892\n",
      "2025-12-20 16:00:50.329692: Pseudo dice [0.9348, 0.9652, 0.9416]\n",
      "2025-12-20 16:00:50.329692: Epoch time: 137.98 s\n",
      "2025-12-20 16:00:50.342247: Yayy! New best EMA pseudo Dice: 0.9449\n",
      "2025-12-20 16:00:51.328614: \n",
      "2025-12-20 16:00:51.328614: Epoch 803\n",
      "2025-12-20 16:00:51.328614: Current learning rate: 0.00232\n",
      "2025-12-20 16:03:09.357543: train_loss -0.8542\n",
      "2025-12-20 16:03:09.357543: val_loss -0.8756\n",
      "2025-12-20 16:03:09.375463: Pseudo dice [0.9219, 0.9568, 0.9486]\n",
      "2025-12-20 16:03:09.381469: Epoch time: 138.03 s\n",
      "2025-12-20 16:03:10.086004: \n",
      "2025-12-20 16:03:10.086004: Epoch 804\n",
      "2025-12-20 16:03:10.086004: Current learning rate: 0.00231\n",
      "2025-12-20 16:05:28.314963: train_loss -0.856\n",
      "2025-12-20 16:05:28.314963: val_loss -0.8836\n",
      "2025-12-20 16:05:28.314963: Pseudo dice [0.9278, 0.9601, 0.9493]\n",
      "2025-12-20 16:05:28.328778: Epoch time: 138.23 s\n",
      "2025-12-20 16:05:29.055785: \n",
      "2025-12-20 16:05:29.055785: Epoch 805\n",
      "2025-12-20 16:05:29.071849: Current learning rate: 0.0023\n",
      "2025-12-20 16:07:47.143931: train_loss -0.8547\n",
      "2025-12-20 16:07:47.143931: val_loss -0.8812\n",
      "2025-12-20 16:07:47.150918: Pseudo dice [0.9257, 0.9549, 0.9436]\n",
      "2025-12-20 16:07:47.156925: Epoch time: 138.09 s\n",
      "2025-12-20 16:07:48.049387: \n",
      "2025-12-20 16:07:48.049387: Epoch 806\n",
      "2025-12-20 16:07:48.061149: Current learning rate: 0.00229\n",
      "2025-12-20 16:10:06.537634: train_loss -0.8575\n",
      "2025-12-20 16:10:06.539636: val_loss -0.8826\n",
      "2025-12-20 16:10:06.549389: Pseudo dice [0.9284, 0.9599, 0.9466]\n",
      "2025-12-20 16:10:06.559145: Epoch time: 138.49 s\n",
      "2025-12-20 16:10:07.222026: \n",
      "2025-12-20 16:10:07.222026: Epoch 807\n",
      "2025-12-20 16:10:07.222026: Current learning rate: 0.00228\n",
      "2025-12-20 16:12:25.548936: train_loss -0.86\n",
      "2025-12-20 16:12:25.550677: val_loss -0.8824\n",
      "2025-12-20 16:12:25.562693: Pseudo dice [0.925, 0.9595, 0.949]\n",
      "2025-12-20 16:12:25.568439: Epoch time: 138.33 s\n",
      "2025-12-20 16:12:26.230541: \n",
      "2025-12-20 16:12:26.230541: Epoch 808\n",
      "2025-12-20 16:12:26.246541: Current learning rate: 0.00226\n",
      "2025-12-20 16:14:44.324690: train_loss -0.8578\n",
      "2025-12-20 16:14:44.326692: val_loss -0.8818\n",
      "2025-12-20 16:14:44.334702: Pseudo dice [0.9255, 0.9588, 0.9469]\n",
      "2025-12-20 16:14:44.340722: Epoch time: 138.09 s\n",
      "2025-12-20 16:14:45.056878: \n",
      "2025-12-20 16:14:45.056878: Epoch 809\n",
      "2025-12-20 16:14:45.070861: Current learning rate: 0.00225\n",
      "2025-12-20 16:17:03.196537: train_loss -0.855\n",
      "2025-12-20 16:17:03.196537: val_loss -0.8847\n",
      "2025-12-20 16:17:03.212319: Pseudo dice [0.9318, 0.9596, 0.942]\n",
      "2025-12-20 16:17:03.212319: Epoch time: 138.14 s\n",
      "2025-12-20 16:17:03.924192: \n",
      "2025-12-20 16:17:03.924192: Epoch 810\n",
      "2025-12-20 16:17:03.934790: Current learning rate: 0.00224\n",
      "2025-12-20 16:19:21.964597: train_loss -0.8591\n",
      "2025-12-20 16:19:21.964597: val_loss -0.8792\n",
      "2025-12-20 16:19:21.980589: Pseudo dice [0.924, 0.9558, 0.9445]\n",
      "2025-12-20 16:19:21.986468: Epoch time: 138.04 s\n",
      "2025-12-20 16:19:22.653120: \n",
      "2025-12-20 16:19:22.653120: Epoch 811\n",
      "2025-12-20 16:19:22.653120: Current learning rate: 0.00223\n",
      "2025-12-20 16:21:40.614940: train_loss -0.8576\n",
      "2025-12-20 16:21:40.614940: val_loss -0.8828\n",
      "2025-12-20 16:21:40.626301: Pseudo dice [0.9302, 0.9589, 0.9396]\n",
      "2025-12-20 16:21:40.630631: Epoch time: 137.96 s\n",
      "2025-12-20 16:21:41.502267: \n",
      "2025-12-20 16:21:41.502267: Epoch 812\n",
      "2025-12-20 16:21:41.502267: Current learning rate: 0.00222\n",
      "2025-12-20 16:23:59.560456: train_loss -0.8612\n",
      "2025-12-20 16:23:59.560456: val_loss -0.8879\n",
      "2025-12-20 16:23:59.576541: Pseudo dice [0.9332, 0.9599, 0.9461]\n",
      "2025-12-20 16:23:59.576541: Epoch time: 138.06 s\n",
      "2025-12-20 16:24:00.260021: \n",
      "2025-12-20 16:24:00.260021: Epoch 813\n",
      "2025-12-20 16:24:00.274040: Current learning rate: 0.00221\n",
      "2025-12-20 16:26:18.209503: train_loss -0.8571\n",
      "2025-12-20 16:26:18.211505: val_loss -0.8821\n",
      "2025-12-20 16:26:18.219251: Pseudo dice [0.9275, 0.9556, 0.9432]\n",
      "2025-12-20 16:26:18.225260: Epoch time: 137.95 s\n",
      "2025-12-20 16:26:18.897020: \n",
      "2025-12-20 16:26:18.897020: Epoch 814\n",
      "2025-12-20 16:26:18.897020: Current learning rate: 0.0022\n",
      "2025-12-20 16:28:36.863390: train_loss -0.8572\n",
      "2025-12-20 16:28:36.863390: val_loss -0.8937\n",
      "2025-12-20 16:28:36.873400: Pseudo dice [0.9357, 0.967, 0.9444]\n",
      "2025-12-20 16:28:36.879144: Epoch time: 137.97 s\n",
      "2025-12-20 16:28:37.560287: \n",
      "2025-12-20 16:28:37.560287: Epoch 815\n",
      "2025-12-20 16:28:37.560287: Current learning rate: 0.00219\n",
      "2025-12-20 16:30:55.563529: train_loss -0.861\n",
      "2025-12-20 16:30:55.563529: val_loss -0.88\n",
      "2025-12-20 16:30:55.567416: Pseudo dice [0.9262, 0.9552, 0.9454]\n",
      "2025-12-20 16:30:55.567416: Epoch time: 138.0 s\n",
      "2025-12-20 16:30:56.228349: \n",
      "2025-12-20 16:30:56.244139: Epoch 816\n",
      "2025-12-20 16:30:56.244139: Current learning rate: 0.00218\n",
      "2025-12-20 16:33:14.561657: train_loss -0.8571\n",
      "2025-12-20 16:33:14.563660: val_loss -0.877\n",
      "2025-12-20 16:33:14.575418: Pseudo dice [0.9233, 0.957, 0.9428]\n",
      "2025-12-20 16:33:14.583434: Epoch time: 138.33 s\n",
      "2025-12-20 16:33:15.254363: \n",
      "2025-12-20 16:33:15.254363: Epoch 817\n",
      "2025-12-20 16:33:15.254363: Current learning rate: 0.00217\n",
      "2025-12-20 16:35:33.481679: train_loss -0.86\n",
      "2025-12-20 16:35:33.481679: val_loss -0.8839\n",
      "2025-12-20 16:35:33.491697: Pseudo dice [0.9283, 0.9579, 0.9431]\n",
      "2025-12-20 16:35:33.498986: Epoch time: 138.23 s\n",
      "2025-12-20 16:35:34.321158: \n",
      "2025-12-20 16:35:34.321158: Epoch 818\n",
      "2025-12-20 16:35:34.337022: Current learning rate: 0.00216\n",
      "2025-12-20 16:37:52.443794: train_loss -0.8541\n",
      "2025-12-20 16:37:52.443794: val_loss -0.8783\n",
      "2025-12-20 16:37:52.443794: Pseudo dice [0.9257, 0.9561, 0.9397]\n",
      "2025-12-20 16:37:52.443794: Epoch time: 138.12 s\n",
      "2025-12-20 16:37:53.105699: \n",
      "2025-12-20 16:37:53.105699: Epoch 819\n",
      "2025-12-20 16:37:53.121492: Current learning rate: 0.00215\n",
      "2025-12-20 16:40:11.179782: train_loss -0.8558\n",
      "2025-12-20 16:40:11.179782: val_loss -0.88\n",
      "2025-12-20 16:40:11.181785: Pseudo dice [0.9256, 0.9522, 0.9457]\n",
      "2025-12-20 16:40:11.181785: Epoch time: 138.07 s\n",
      "2025-12-20 16:40:11.815697: \n",
      "2025-12-20 16:40:11.815697: Epoch 820\n",
      "2025-12-20 16:40:11.815697: Current learning rate: 0.00214\n",
      "2025-12-20 16:42:29.725370: train_loss -0.8629\n",
      "2025-12-20 16:42:29.725370: val_loss -0.8828\n",
      "2025-12-20 16:42:29.735452: Pseudo dice [0.9276, 0.9571, 0.9414]\n",
      "2025-12-20 16:42:29.735452: Epoch time: 137.91 s\n",
      "2025-12-20 16:42:30.370586: \n",
      "2025-12-20 16:42:30.370586: Epoch 821\n",
      "2025-12-20 16:42:30.370586: Current learning rate: 0.00213\n",
      "2025-12-20 16:44:48.468851: train_loss -0.8581\n",
      "2025-12-20 16:44:48.468851: val_loss -0.8807\n",
      "2025-12-20 16:44:48.468851: Pseudo dice [0.9298, 0.956, 0.9416]\n",
      "2025-12-20 16:44:48.484694: Epoch time: 138.1 s\n",
      "2025-12-20 16:44:49.181342: \n",
      "2025-12-20 16:44:49.181342: Epoch 822\n",
      "2025-12-20 16:44:49.197255: Current learning rate: 0.00212\n",
      "2025-12-20 16:47:07.122765: train_loss -0.8609\n",
      "2025-12-20 16:47:07.122765: val_loss -0.8833\n",
      "2025-12-20 16:47:07.122765: Pseudo dice [0.9287, 0.9581, 0.9482]\n",
      "2025-12-20 16:47:07.138495: Epoch time: 137.94 s\n",
      "2025-12-20 16:47:07.775278: \n",
      "2025-12-20 16:47:07.775278: Epoch 823\n",
      "2025-12-20 16:47:07.789107: Current learning rate: 0.0021\n",
      "2025-12-20 16:49:25.781119: train_loss -0.8562\n",
      "2025-12-20 16:49:25.781119: val_loss -0.8883\n",
      "2025-12-20 16:49:25.796798: Pseudo dice [0.9316, 0.9627, 0.9469]\n",
      "2025-12-20 16:49:25.796798: Epoch time: 138.01 s\n",
      "2025-12-20 16:49:26.590274: \n",
      "2025-12-20 16:49:26.590274: Epoch 824\n",
      "2025-12-20 16:49:26.606327: Current learning rate: 0.00209\n",
      "2025-12-20 16:51:44.681063: train_loss -0.8613\n",
      "2025-12-20 16:51:44.681063: val_loss -0.8787\n",
      "2025-12-20 16:51:44.687069: Pseudo dice [0.9252, 0.9593, 0.9383]\n",
      "2025-12-20 16:51:44.692075: Epoch time: 138.09 s\n",
      "2025-12-20 16:51:45.433770: \n",
      "2025-12-20 16:51:45.433770: Epoch 825\n",
      "2025-12-20 16:51:45.449532: Current learning rate: 0.00208\n",
      "2025-12-20 16:54:03.461803: train_loss -0.855\n",
      "2025-12-20 16:54:03.461803: val_loss -0.8779\n",
      "2025-12-20 16:54:03.481583: Pseudo dice [0.922, 0.9519, 0.9507]\n",
      "2025-12-20 16:54:03.481583: Epoch time: 138.03 s\n",
      "2025-12-20 16:54:04.110618: \n",
      "2025-12-20 16:54:04.110618: Epoch 826\n",
      "2025-12-20 16:54:04.117434: Current learning rate: 0.00207\n",
      "2025-12-20 16:56:22.131028: train_loss -0.8606\n",
      "2025-12-20 16:56:22.131028: val_loss -0.8822\n",
      "2025-12-20 16:56:22.131028: Pseudo dice [0.9272, 0.9576, 0.9473]\n",
      "2025-12-20 16:56:22.140783: Epoch time: 138.02 s\n",
      "2025-12-20 16:56:22.774212: \n",
      "2025-12-20 16:56:22.774212: Epoch 827\n",
      "2025-12-20 16:56:22.774212: Current learning rate: 0.00206\n",
      "2025-12-20 16:58:40.839595: train_loss -0.8619\n",
      "2025-12-20 16:58:40.839595: val_loss -0.8866\n",
      "2025-12-20 16:58:40.854752: Pseudo dice [0.9296, 0.9623, 0.9431]\n",
      "2025-12-20 16:58:40.854752: Epoch time: 138.07 s\n",
      "2025-12-20 16:58:41.663401: \n",
      "2025-12-20 16:58:41.663401: Epoch 828\n",
      "2025-12-20 16:58:41.663401: Current learning rate: 0.00205\n",
      "2025-12-20 17:00:59.632170: train_loss -0.8627\n",
      "2025-12-20 17:00:59.632170: val_loss -0.8767\n",
      "2025-12-20 17:00:59.648140: Pseudo dice [0.9234, 0.9564, 0.9443]\n",
      "2025-12-20 17:00:59.648140: Epoch time: 137.98 s\n",
      "2025-12-20 17:01:00.282030: \n",
      "2025-12-20 17:01:00.282030: Epoch 829\n",
      "2025-12-20 17:01:00.282030: Current learning rate: 0.00204\n",
      "2025-12-20 17:03:18.339465: train_loss -0.8547\n",
      "2025-12-20 17:03:18.341468: val_loss -0.8797\n",
      "2025-12-20 17:03:18.349218: Pseudo dice [0.9256, 0.9582, 0.9433]\n",
      "2025-12-20 17:03:18.357229: Epoch time: 138.07 s\n",
      "2025-12-20 17:03:18.994640: \n",
      "2025-12-20 17:03:18.994640: Epoch 830\n",
      "2025-12-20 17:03:18.994640: Current learning rate: 0.00203\n",
      "2025-12-20 17:05:36.984564: train_loss -0.8602\n",
      "2025-12-20 17:05:36.984564: val_loss -0.8873\n",
      "2025-12-20 17:05:36.988306: Pseudo dice [0.934, 0.9601, 0.9371]\n",
      "2025-12-20 17:05:36.988306: Epoch time: 137.99 s\n",
      "2025-12-20 17:05:37.858418: \n",
      "2025-12-20 17:05:37.858418: Epoch 831\n",
      "2025-12-20 17:05:37.874105: Current learning rate: 0.00202\n",
      "2025-12-20 17:07:55.971631: train_loss -0.8624\n",
      "2025-12-20 17:07:55.973634: val_loss -0.8872\n",
      "2025-12-20 17:07:55.979642: Pseudo dice [0.931, 0.9593, 0.9486]\n",
      "2025-12-20 17:07:55.987279: Epoch time: 138.11 s\n",
      "2025-12-20 17:07:56.617817: \n",
      "2025-12-20 17:07:56.617817: Epoch 832\n",
      "2025-12-20 17:07:56.617817: Current learning rate: 0.00201\n",
      "2025-12-20 17:10:15.071943: train_loss -0.8596\n",
      "2025-12-20 17:10:15.071943: val_loss -0.8811\n",
      "2025-12-20 17:10:15.071943: Pseudo dice [0.9266, 0.9561, 0.9457]\n",
      "2025-12-20 17:10:15.087792: Epoch time: 138.45 s\n",
      "2025-12-20 17:10:15.818838: \n",
      "2025-12-20 17:10:15.818838: Epoch 833\n",
      "2025-12-20 17:10:15.832817: Current learning rate: 0.002\n",
      "2025-12-20 17:12:33.972508: train_loss -0.8613\n",
      "2025-12-20 17:12:33.974511: val_loss -0.8869\n",
      "2025-12-20 17:12:33.988276: Pseudo dice [0.9317, 0.9592, 0.9468]\n",
      "2025-12-20 17:12:33.996287: Epoch time: 138.15 s\n",
      "2025-12-20 17:12:34.777900: \n",
      "2025-12-20 17:12:34.777900: Epoch 834\n",
      "2025-12-20 17:12:34.793577: Current learning rate: 0.00199\n",
      "2025-12-20 17:14:53.058279: train_loss -0.858\n",
      "2025-12-20 17:14:53.058279: val_loss -0.8872\n",
      "2025-12-20 17:14:53.074361: Pseudo dice [0.9312, 0.9629, 0.9429]\n",
      "2025-12-20 17:14:53.074361: Epoch time: 138.28 s\n",
      "2025-12-20 17:14:53.710527: \n",
      "2025-12-20 17:14:53.710527: Epoch 835\n",
      "2025-12-20 17:14:53.724538: Current learning rate: 0.00198\n",
      "2025-12-20 17:17:12.568939: train_loss -0.858\n",
      "2025-12-20 17:17:12.568939: val_loss -0.8888\n",
      "2025-12-20 17:17:12.577952: Pseudo dice [0.9325, 0.9622, 0.944]\n",
      "2025-12-20 17:17:12.583958: Epoch time: 138.86 s\n",
      "2025-12-20 17:17:13.251589: \n",
      "2025-12-20 17:17:13.251589: Epoch 836\n",
      "2025-12-20 17:17:13.267366: Current learning rate: 0.00196\n",
      "2025-12-20 17:19:31.757209: train_loss -0.864\n",
      "2025-12-20 17:19:31.757209: val_loss -0.8885\n",
      "2025-12-20 17:19:31.766720: Pseudo dice [0.9305, 0.9593, 0.9466]\n",
      "2025-12-20 17:19:31.770724: Epoch time: 138.51 s\n",
      "2025-12-20 17:19:32.716488: \n",
      "2025-12-20 17:19:32.716488: Epoch 837\n",
      "2025-12-20 17:19:32.716488: Current learning rate: 0.00195\n",
      "2025-12-20 17:21:51.240426: train_loss -0.861\n",
      "2025-12-20 17:21:51.240426: val_loss -0.8841\n",
      "2025-12-20 17:21:51.240426: Pseudo dice [0.9269, 0.9607, 0.9428]\n",
      "2025-12-20 17:21:51.256060: Epoch time: 138.52 s\n",
      "2025-12-20 17:21:51.878429: \n",
      "2025-12-20 17:21:51.878429: Epoch 838\n",
      "2025-12-20 17:21:51.894287: Current learning rate: 0.00194\n",
      "2025-12-20 17:24:10.508769: train_loss -0.859\n",
      "2025-12-20 17:24:10.508769: val_loss -0.8885\n",
      "2025-12-20 17:24:10.514695: Pseudo dice [0.9313, 0.9618, 0.9458]\n",
      "2025-12-20 17:24:10.514695: Epoch time: 138.63 s\n",
      "2025-12-20 17:24:11.132258: \n",
      "2025-12-20 17:24:11.132258: Epoch 839\n",
      "2025-12-20 17:24:11.148178: Current learning rate: 0.00193\n",
      "2025-12-20 17:26:29.739135: train_loss -0.8596\n",
      "2025-12-20 17:26:29.741138: val_loss -0.8858\n",
      "2025-12-20 17:26:29.748756: Pseudo dice [0.9318, 0.9592, 0.9403]\n",
      "2025-12-20 17:26:29.754762: Epoch time: 138.61 s\n",
      "2025-12-20 17:26:30.470134: \n",
      "2025-12-20 17:26:30.470134: Epoch 840\n",
      "2025-12-20 17:26:30.470134: Current learning rate: 0.00192\n",
      "2025-12-20 17:28:48.873329: train_loss -0.858\n",
      "2025-12-20 17:28:48.873329: val_loss -0.8763\n",
      "2025-12-20 17:28:48.883086: Pseudo dice [0.9228, 0.9537, 0.9448]\n",
      "2025-12-20 17:28:48.889091: Epoch time: 138.4 s\n",
      "2025-12-20 17:28:49.514097: \n",
      "2025-12-20 17:28:49.514097: Epoch 841\n",
      "2025-12-20 17:28:49.529776: Current learning rate: 0.00191\n",
      "2025-12-20 17:31:07.609922: train_loss -0.8619\n",
      "2025-12-20 17:31:07.609922: val_loss -0.8796\n",
      "2025-12-20 17:31:07.624650: Pseudo dice [0.9261, 0.9534, 0.9421]\n",
      "2025-12-20 17:31:07.629334: Epoch time: 138.1 s\n",
      "2025-12-20 17:31:08.267561: \n",
      "2025-12-20 17:31:08.269563: Epoch 842\n",
      "2025-12-20 17:31:08.269563: Current learning rate: 0.0019\n",
      "2025-12-20 17:33:26.288713: train_loss -0.8615\n",
      "2025-12-20 17:33:26.288713: val_loss -0.8826\n",
      "2025-12-20 17:33:26.298226: Pseudo dice [0.9275, 0.9597, 0.9431]\n",
      "2025-12-20 17:33:26.304232: Epoch time: 138.02 s\n",
      "2025-12-20 17:33:27.165583: \n",
      "2025-12-20 17:33:27.165583: Epoch 843\n",
      "2025-12-20 17:33:27.171953: Current learning rate: 0.00189\n",
      "2025-12-20 17:35:45.215116: train_loss -0.8583\n",
      "2025-12-20 17:35:45.215116: val_loss -0.8838\n",
      "2025-12-20 17:35:45.222625: Pseudo dice [0.9289, 0.9589, 0.9409]\n",
      "2025-12-20 17:35:45.228631: Epoch time: 138.05 s\n",
      "2025-12-20 17:35:45.862047: \n",
      "2025-12-20 17:35:45.871910: Epoch 844\n",
      "2025-12-20 17:35:45.871910: Current learning rate: 0.00188\n",
      "2025-12-20 17:38:03.781865: train_loss -0.8646\n",
      "2025-12-20 17:38:03.781865: val_loss -0.8816\n",
      "2025-12-20 17:38:03.791367: Pseudo dice [0.9282, 0.9554, 0.9439]\n",
      "2025-12-20 17:38:03.795371: Epoch time: 137.92 s\n",
      "2025-12-20 17:38:04.425291: \n",
      "2025-12-20 17:38:04.425291: Epoch 845\n",
      "2025-12-20 17:38:04.425291: Current learning rate: 0.00187\n",
      "2025-12-20 17:40:22.466938: train_loss -0.8612\n",
      "2025-12-20 17:40:22.466938: val_loss -0.876\n",
      "2025-12-20 17:40:22.479048: Pseudo dice [0.9214, 0.9573, 0.939]\n",
      "2025-12-20 17:40:22.482946: Epoch time: 138.04 s\n",
      "2025-12-20 17:40:23.258491: \n",
      "2025-12-20 17:40:23.258491: Epoch 846\n",
      "2025-12-20 17:40:23.270741: Current learning rate: 0.00186\n",
      "2025-12-20 17:42:41.340969: train_loss -0.8611\n",
      "2025-12-20 17:42:41.356950: val_loss -0.8811\n",
      "2025-12-20 17:42:41.356950: Pseudo dice [0.9235, 0.9589, 0.9458]\n",
      "2025-12-20 17:42:41.356950: Epoch time: 138.08 s\n",
      "2025-12-20 17:42:42.070059: \n",
      "2025-12-20 17:42:42.070059: Epoch 847\n",
      "2025-12-20 17:42:42.070059: Current learning rate: 0.00185\n",
      "2025-12-20 17:45:00.326506: train_loss -0.8612\n",
      "2025-12-20 17:45:00.326506: val_loss -0.8782\n",
      "2025-12-20 17:45:00.334041: Pseudo dice [0.9237, 0.9592, 0.9479]\n",
      "2025-12-20 17:45:00.340842: Epoch time: 138.26 s\n",
      "2025-12-20 17:45:01.059330: \n",
      "2025-12-20 17:45:01.059330: Epoch 848\n",
      "2025-12-20 17:45:01.069890: Current learning rate: 0.00184\n",
      "2025-12-20 17:47:19.290649: train_loss -0.8562\n",
      "2025-12-20 17:47:19.290649: val_loss -0.8877\n",
      "2025-12-20 17:47:19.290649: Pseudo dice [0.9323, 0.963, 0.9408]\n",
      "2025-12-20 17:47:19.290649: Epoch time: 138.23 s\n",
      "2025-12-20 17:47:20.049690: \n",
      "2025-12-20 17:47:20.049690: Epoch 849\n",
      "2025-12-20 17:47:20.065386: Current learning rate: 0.00182\n",
      "2025-12-20 17:49:38.141606: train_loss -0.857\n",
      "2025-12-20 17:49:38.143608: val_loss -0.8854\n",
      "2025-12-20 17:49:38.151118: Pseudo dice [0.9308, 0.96, 0.9402]\n",
      "2025-12-20 17:49:38.158102: Epoch time: 138.09 s\n",
      "2025-12-20 17:49:39.315016: \n",
      "2025-12-20 17:49:39.315016: Epoch 850\n",
      "2025-12-20 17:49:39.315016: Current learning rate: 0.00181\n",
      "2025-12-20 17:51:57.423789: train_loss -0.8612\n",
      "2025-12-20 17:51:57.423789: val_loss -0.8913\n",
      "2025-12-20 17:51:57.431798: Pseudo dice [0.9351, 0.9671, 0.9421]\n",
      "2025-12-20 17:51:57.435540: Epoch time: 138.11 s\n",
      "2025-12-20 17:51:58.066732: \n",
      "2025-12-20 17:51:58.066732: Epoch 851\n",
      "2025-12-20 17:51:58.066732: Current learning rate: 0.0018\n",
      "2025-12-20 17:54:16.166536: train_loss -0.8583\n",
      "2025-12-20 17:54:16.168539: val_loss -0.8849\n",
      "2025-12-20 17:54:16.174046: Pseudo dice [0.9287, 0.961, 0.9479]\n",
      "2025-12-20 17:54:16.180052: Epoch time: 138.1 s\n",
      "2025-12-20 17:54:16.900048: \n",
      "2025-12-20 17:54:16.900048: Epoch 852\n",
      "2025-12-20 17:54:16.906249: Current learning rate: 0.00179\n",
      "2025-12-20 17:56:34.930665: train_loss -0.8563\n",
      "2025-12-20 17:56:34.930665: val_loss -0.8809\n",
      "2025-12-20 17:56:34.930665: Pseudo dice [0.9241, 0.9584, 0.9501]\n",
      "2025-12-20 17:56:34.946652: Epoch time: 138.03 s\n",
      "2025-12-20 17:56:35.624871: \n",
      "2025-12-20 17:56:35.624871: Epoch 853\n",
      "2025-12-20 17:56:35.640881: Current learning rate: 0.00178\n",
      "2025-12-20 17:58:53.614961: train_loss -0.8599\n",
      "2025-12-20 17:58:53.616964: val_loss -0.8907\n",
      "2025-12-20 17:58:53.626980: Pseudo dice [0.9345, 0.9634, 0.9407]\n",
      "2025-12-20 17:58:53.632967: Epoch time: 137.99 s\n",
      "2025-12-20 17:58:54.253305: \n",
      "2025-12-20 17:58:54.253305: Epoch 854\n",
      "2025-12-20 17:58:54.253305: Current learning rate: 0.00177\n",
      "2025-12-20 18:01:12.350026: train_loss -0.8597\n",
      "2025-12-20 18:01:12.350026: val_loss -0.8849\n",
      "2025-12-20 18:01:12.360036: Pseudo dice [0.9277, 0.9602, 0.948]\n",
      "2025-12-20 18:01:12.367047: Epoch time: 138.1 s\n",
      "2025-12-20 18:01:13.150576: \n",
      "2025-12-20 18:01:13.150576: Epoch 855\n",
      "2025-12-20 18:01:13.166150: Current learning rate: 0.00176\n",
      "2025-12-20 18:03:31.350320: train_loss -0.8605\n",
      "2025-12-20 18:03:31.350320: val_loss -0.8836\n",
      "2025-12-20 18:03:31.358185: Pseudo dice [0.9278, 0.9621, 0.9417]\n",
      "2025-12-20 18:03:31.364191: Epoch time: 138.2 s\n",
      "2025-12-20 18:03:32.144636: \n",
      "2025-12-20 18:03:32.144636: Epoch 856\n",
      "2025-12-20 18:03:32.144636: Current learning rate: 0.00175\n",
      "2025-12-20 18:05:50.199310: train_loss -0.8602\n",
      "2025-12-20 18:05:50.199310: val_loss -0.8805\n",
      "2025-12-20 18:05:50.204479: Pseudo dice [0.9267, 0.9546, 0.9457]\n",
      "2025-12-20 18:05:50.209979: Epoch time: 138.05 s\n",
      "2025-12-20 18:05:50.822765: \n",
      "2025-12-20 18:05:50.822765: Epoch 857\n",
      "2025-12-20 18:05:50.838735: Current learning rate: 0.00174\n",
      "2025-12-20 18:08:08.807696: train_loss -0.863\n",
      "2025-12-20 18:08:08.807696: val_loss -0.8824\n",
      "2025-12-20 18:08:08.818943: Pseudo dice [0.927, 0.9588, 0.9448]\n",
      "2025-12-20 18:08:08.822947: Epoch time: 137.98 s\n",
      "2025-12-20 18:08:09.579297: \n",
      "2025-12-20 18:08:09.579297: Epoch 858\n",
      "2025-12-20 18:08:09.586801: Current learning rate: 0.00173\n",
      "2025-12-20 18:10:28.024916: train_loss -0.8581\n",
      "2025-12-20 18:10:28.024916: val_loss -0.8761\n",
      "2025-12-20 18:10:28.032923: Pseudo dice [0.919, 0.9541, 0.9521]\n",
      "2025-12-20 18:10:28.040885: Epoch time: 138.45 s\n",
      "2025-12-20 18:10:28.656055: \n",
      "2025-12-20 18:10:28.656055: Epoch 859\n",
      "2025-12-20 18:10:28.671847: Current learning rate: 0.00172\n",
      "2025-12-20 18:12:46.801306: train_loss -0.8615\n",
      "2025-12-20 18:12:46.803308: val_loss -0.885\n",
      "2025-12-20 18:12:46.809314: Pseudo dice [0.9305, 0.9585, 0.9457]\n",
      "2025-12-20 18:12:46.815320: Epoch time: 138.15 s\n",
      "2025-12-20 18:12:47.439987: \n",
      "2025-12-20 18:12:47.439987: Epoch 860\n",
      "2025-12-20 18:12:47.439987: Current learning rate: 0.0017\n",
      "2025-12-20 18:15:05.572770: train_loss -0.8587\n",
      "2025-12-20 18:15:05.572770: val_loss -0.8796\n",
      "2025-12-20 18:15:05.578721: Pseudo dice [0.9238, 0.9548, 0.9451]\n",
      "2025-12-20 18:15:05.578721: Epoch time: 138.13 s\n",
      "2025-12-20 18:15:06.333406: \n",
      "2025-12-20 18:15:06.333406: Epoch 861\n",
      "2025-12-20 18:15:06.339836: Current learning rate: 0.00169\n",
      "2025-12-20 18:17:24.238978: train_loss -0.8685\n",
      "2025-12-20 18:17:24.238978: val_loss -0.8785\n",
      "2025-12-20 18:17:24.244984: Pseudo dice [0.9269, 0.9592, 0.9389]\n",
      "2025-12-20 18:17:24.252486: Epoch time: 137.91 s\n",
      "2025-12-20 18:17:25.042305: \n",
      "2025-12-20 18:17:25.045073: Epoch 862\n",
      "2025-12-20 18:17:25.045073: Current learning rate: 0.00168\n",
      "2025-12-20 18:19:42.961570: train_loss -0.8549\n",
      "2025-12-20 18:19:42.963572: val_loss -0.8785\n",
      "2025-12-20 18:19:42.969315: Pseudo dice [0.9238, 0.9544, 0.9472]\n",
      "2025-12-20 18:19:42.975322: Epoch time: 137.92 s\n",
      "2025-12-20 18:19:43.595361: \n",
      "2025-12-20 18:19:43.595361: Epoch 863\n",
      "2025-12-20 18:19:43.595361: Current learning rate: 0.00167\n",
      "2025-12-20 18:22:01.589487: train_loss -0.8559\n",
      "2025-12-20 18:22:01.589487: val_loss -0.893\n",
      "2025-12-20 18:22:01.594714: Pseudo dice [0.9348, 0.9619, 0.9522]\n",
      "2025-12-20 18:22:01.600286: Epoch time: 137.99 s\n",
      "2025-12-20 18:22:02.339596: \n",
      "2025-12-20 18:22:02.339596: Epoch 864\n",
      "2025-12-20 18:22:02.339596: Current learning rate: 0.00166\n",
      "2025-12-20 18:24:20.400176: train_loss -0.8621\n",
      "2025-12-20 18:24:20.400176: val_loss -0.8835\n",
      "2025-12-20 18:24:20.403996: Pseudo dice [0.9268, 0.9575, 0.9479]\n",
      "2025-12-20 18:24:20.403996: Epoch time: 138.06 s\n",
      "2025-12-20 18:24:21.021839: \n",
      "2025-12-20 18:24:21.021839: Epoch 865\n",
      "2025-12-20 18:24:21.037608: Current learning rate: 0.00165\n",
      "2025-12-20 18:26:38.982465: train_loss -0.8584\n",
      "2025-12-20 18:26:38.982465: val_loss -0.8771\n",
      "2025-12-20 18:26:38.990041: Pseudo dice [0.924, 0.9563, 0.9404]\n",
      "2025-12-20 18:26:38.992043: Epoch time: 137.96 s\n",
      "2025-12-20 18:26:39.618857: \n",
      "2025-12-20 18:26:39.618857: Epoch 866\n",
      "2025-12-20 18:26:39.624741: Current learning rate: 0.00164\n",
      "2025-12-20 18:28:57.717548: train_loss -0.8607\n",
      "2025-12-20 18:28:57.717548: val_loss -0.8778\n",
      "2025-12-20 18:28:57.717548: Pseudo dice [0.9224, 0.9541, 0.9434]\n",
      "2025-12-20 18:28:57.733269: Epoch time: 138.11 s\n",
      "2025-12-20 18:28:58.493468: \n",
      "2025-12-20 18:28:58.493468: Epoch 867\n",
      "2025-12-20 18:28:58.493468: Current learning rate: 0.00163\n",
      "2025-12-20 18:31:16.369200: train_loss -0.8606\n",
      "2025-12-20 18:31:16.369200: val_loss -0.8807\n",
      "2025-12-20 18:31:16.369200: Pseudo dice [0.925, 0.9576, 0.9454]\n",
      "2025-12-20 18:31:16.381490: Epoch time: 137.88 s\n",
      "2025-12-20 18:31:17.173626: \n",
      "2025-12-20 18:31:17.173626: Epoch 868\n",
      "2025-12-20 18:31:17.173626: Current learning rate: 0.00162\n",
      "2025-12-20 18:33:35.254579: train_loss -0.8656\n",
      "2025-12-20 18:33:35.256581: val_loss -0.8875\n",
      "2025-12-20 18:33:35.260325: Pseudo dice [0.9287, 0.9579, 0.9486]\n",
      "2025-12-20 18:33:35.260325: Epoch time: 138.08 s\n",
      "2025-12-20 18:33:35.875503: \n",
      "2025-12-20 18:33:35.875503: Epoch 869\n",
      "2025-12-20 18:33:35.891424: Current learning rate: 0.00161\n",
      "2025-12-20 18:35:53.964348: train_loss -0.8633\n",
      "2025-12-20 18:35:53.964348: val_loss -0.8877\n",
      "2025-12-20 18:35:53.964348: Pseudo dice [0.93, 0.9625, 0.945]\n",
      "2025-12-20 18:35:53.980265: Epoch time: 138.09 s\n",
      "2025-12-20 18:35:54.581881: \n",
      "2025-12-20 18:35:54.581881: Epoch 870\n",
      "2025-12-20 18:35:54.597639: Current learning rate: 0.00159\n",
      "2025-12-20 18:38:12.628829: train_loss -0.8628\n",
      "2025-12-20 18:38:12.628829: val_loss -0.8771\n",
      "2025-12-20 18:38:12.638317: Pseudo dice [0.9237, 0.9557, 0.9422]\n",
      "2025-12-20 18:38:12.644782: Epoch time: 138.05 s\n",
      "2025-12-20 18:38:13.263847: \n",
      "2025-12-20 18:38:13.263847: Epoch 871\n",
      "2025-12-20 18:38:13.277861: Current learning rate: 0.00158\n",
      "2025-12-20 18:40:31.452509: train_loss -0.8623\n",
      "2025-12-20 18:40:31.454512: val_loss -0.8778\n",
      "2025-12-20 18:40:31.462521: Pseudo dice [0.9241, 0.9543, 0.9434]\n",
      "2025-12-20 18:40:31.470270: Epoch time: 138.19 s\n",
      "2025-12-20 18:40:32.115997: \n",
      "2025-12-20 18:40:32.115997: Epoch 872\n",
      "2025-12-20 18:40:32.115997: Current learning rate: 0.00157\n",
      "2025-12-20 18:42:50.165324: train_loss -0.8622\n",
      "2025-12-20 18:42:50.165324: val_loss -0.8828\n",
      "2025-12-20 18:42:50.175348: Pseudo dice [0.9278, 0.9564, 0.9447]\n",
      "2025-12-20 18:42:50.181092: Epoch time: 138.05 s\n",
      "2025-12-20 18:42:50.798782: \n",
      "2025-12-20 18:42:50.798782: Epoch 873\n",
      "2025-12-20 18:42:50.798782: Current learning rate: 0.00156\n",
      "2025-12-20 18:45:09.052568: train_loss -0.8564\n",
      "2025-12-20 18:45:09.054071: val_loss -0.8847\n",
      "2025-12-20 18:45:09.062081: Pseudo dice [0.9282, 0.9591, 0.9443]\n",
      "2025-12-20 18:45:09.069828: Epoch time: 138.25 s\n",
      "2025-12-20 18:45:09.687364: \n",
      "2025-12-20 18:45:09.687364: Epoch 874\n",
      "2025-12-20 18:45:09.703136: Current learning rate: 0.00155\n",
      "2025-12-20 18:47:27.680302: train_loss -0.86\n",
      "2025-12-20 18:47:27.682043: val_loss -0.891\n",
      "2025-12-20 18:47:27.690054: Pseudo dice [0.9335, 0.9631, 0.9469]\n",
      "2025-12-20 18:47:27.697801: Epoch time: 137.99 s\n",
      "2025-12-20 18:47:28.549968: \n",
      "2025-12-20 18:47:28.549968: Epoch 875\n",
      "2025-12-20 18:47:28.553800: Current learning rate: 0.00154\n",
      "2025-12-20 18:49:46.577920: train_loss -0.863\n",
      "2025-12-20 18:49:46.577920: val_loss -0.8895\n",
      "2025-12-20 18:49:46.593803: Pseudo dice [0.9325, 0.9602, 0.9452]\n",
      "2025-12-20 18:49:46.593803: Epoch time: 138.03 s\n",
      "2025-12-20 18:49:47.228376: \n",
      "2025-12-20 18:49:47.228376: Epoch 876\n",
      "2025-12-20 18:49:47.228376: Current learning rate: 0.00153\n",
      "2025-12-20 18:52:04.994286: train_loss -0.8633\n",
      "2025-12-20 18:52:04.994286: val_loss -0.8772\n",
      "2025-12-20 18:52:05.009979: Pseudo dice [0.9201, 0.9571, 0.9457]\n",
      "2025-12-20 18:52:05.009979: Epoch time: 137.78 s\n",
      "2025-12-20 18:52:05.691524: \n",
      "2025-12-20 18:52:05.691524: Epoch 877\n",
      "2025-12-20 18:52:05.691524: Current learning rate: 0.00152\n",
      "2025-12-20 18:54:23.826407: train_loss -0.8603\n",
      "2025-12-20 18:54:23.826407: val_loss -0.8816\n",
      "2025-12-20 18:54:23.842164: Pseudo dice [0.926, 0.9567, 0.9459]\n",
      "2025-12-20 18:54:23.842164: Epoch time: 138.13 s\n",
      "2025-12-20 18:54:24.476253: \n",
      "2025-12-20 18:54:24.476253: Epoch 878\n",
      "2025-12-20 18:54:24.478459: Current learning rate: 0.00151\n",
      "2025-12-20 18:56:42.488692: train_loss -0.8632\n",
      "2025-12-20 18:56:42.488692: val_loss -0.889\n",
      "2025-12-20 18:56:42.488692: Pseudo dice [0.9335, 0.9643, 0.9423]\n",
      "2025-12-20 18:56:42.504814: Epoch time: 138.03 s\n",
      "2025-12-20 18:56:43.200796: \n",
      "2025-12-20 18:56:43.200796: Epoch 879\n",
      "2025-12-20 18:56:43.216569: Current learning rate: 0.00149\n",
      "2025-12-20 18:59:01.211201: train_loss -0.8635\n",
      "2025-12-20 18:59:01.211201: val_loss -0.8894\n",
      "2025-12-20 18:59:01.218617: Pseudo dice [0.9336, 0.9626, 0.9398]\n",
      "2025-12-20 18:59:01.222623: Epoch time: 138.01 s\n",
      "2025-12-20 18:59:01.937023: \n",
      "2025-12-20 18:59:01.952794: Epoch 880\n",
      "2025-12-20 18:59:01.957049: Current learning rate: 0.00148\n",
      "2025-12-20 19:01:20.037885: train_loss -0.8602\n",
      "2025-12-20 19:01:20.037885: val_loss -0.8893\n",
      "2025-12-20 19:01:20.048394: Pseudo dice [0.9321, 0.9617, 0.9415]\n",
      "2025-12-20 19:01:20.053905: Epoch time: 138.1 s\n",
      "2025-12-20 19:01:20.891999: \n",
      "2025-12-20 19:01:20.891999: Epoch 881\n",
      "2025-12-20 19:01:20.891999: Current learning rate: 0.00147\n",
      "2025-12-20 19:03:39.008248: train_loss -0.8591\n",
      "2025-12-20 19:03:39.008248: val_loss -0.8813\n",
      "2025-12-20 19:03:39.018134: Pseudo dice [0.9266, 0.9552, 0.9451]\n",
      "2025-12-20 19:03:39.026155: Epoch time: 138.12 s\n",
      "2025-12-20 19:03:39.718178: \n",
      "2025-12-20 19:03:39.718178: Epoch 882\n",
      "2025-12-20 19:03:39.739683: Current learning rate: 0.00146\n",
      "2025-12-20 19:05:57.652364: train_loss -0.8631\n",
      "2025-12-20 19:05:57.652364: val_loss -0.8817\n",
      "2025-12-20 19:05:57.664390: Pseudo dice [0.9262, 0.9593, 0.9402]\n",
      "2025-12-20 19:05:57.668134: Epoch time: 137.93 s\n",
      "2025-12-20 19:05:58.298396: \n",
      "2025-12-20 19:05:58.298396: Epoch 883\n",
      "2025-12-20 19:05:58.305034: Current learning rate: 0.00145\n",
      "2025-12-20 19:08:16.298904: train_loss -0.858\n",
      "2025-12-20 19:08:16.298904: val_loss -0.8871\n",
      "2025-12-20 19:08:16.304835: Pseudo dice [0.9346, 0.9611, 0.9441]\n",
      "2025-12-20 19:08:16.304835: Epoch time: 138.0 s\n",
      "2025-12-20 19:08:17.032161: \n",
      "2025-12-20 19:08:17.032161: Epoch 884\n",
      "2025-12-20 19:08:17.032161: Current learning rate: 0.00144\n",
      "2025-12-20 19:10:35.449748: train_loss -0.8619\n",
      "2025-12-20 19:10:35.449748: val_loss -0.8856\n",
      "2025-12-20 19:10:35.469256: Pseudo dice [0.9246, 0.963, 0.9482]\n",
      "2025-12-20 19:10:35.469256: Epoch time: 138.42 s\n",
      "2025-12-20 19:10:36.083866: \n",
      "2025-12-20 19:10:36.083866: Epoch 885\n",
      "2025-12-20 19:10:36.103633: Current learning rate: 0.00143\n",
      "2025-12-20 19:12:54.190701: train_loss -0.866\n",
      "2025-12-20 19:12:54.190701: val_loss -0.8854\n",
      "2025-12-20 19:12:54.205208: Pseudo dice [0.9269, 0.9579, 0.9529]\n",
      "2025-12-20 19:12:54.210468: Epoch time: 138.11 s\n",
      "2025-12-20 19:12:54.839689: \n",
      "2025-12-20 19:12:54.839689: Epoch 886\n",
      "2025-12-20 19:12:54.845023: Current learning rate: 0.00142\n",
      "2025-12-20 19:15:12.698195: train_loss -0.8626\n",
      "2025-12-20 19:15:12.698195: val_loss -0.8898\n",
      "2025-12-20 19:15:12.713875: Pseudo dice [0.9332, 0.9648, 0.9425]\n",
      "2025-12-20 19:15:12.713875: Epoch time: 137.87 s\n",
      "2025-12-20 19:15:13.426744: \n",
      "2025-12-20 19:15:13.426744: Epoch 887\n",
      "2025-12-20 19:15:13.426744: Current learning rate: 0.00141\n",
      "2025-12-20 19:17:31.397548: train_loss -0.8638\n",
      "2025-12-20 19:17:31.399550: val_loss -0.8904\n",
      "2025-12-20 19:17:31.407087: Pseudo dice [0.9312, 0.9612, 0.9476]\n",
      "2025-12-20 19:17:31.407087: Epoch time: 137.97 s\n",
      "2025-12-20 19:17:32.211123: \n",
      "2025-12-20 19:17:32.211123: Epoch 888\n",
      "2025-12-20 19:17:32.211123: Current learning rate: 0.00139\n",
      "2025-12-20 19:19:50.085397: train_loss -0.8641\n",
      "2025-12-20 19:19:50.087398: val_loss -0.8813\n",
      "2025-12-20 19:19:50.087398: Pseudo dice [0.9262, 0.9549, 0.9452]\n",
      "2025-12-20 19:19:50.099176: Epoch time: 137.87 s\n",
      "2025-12-20 19:19:50.730723: \n",
      "2025-12-20 19:19:50.730723: Epoch 889\n",
      "2025-12-20 19:19:50.735626: Current learning rate: 0.00138\n",
      "2025-12-20 19:22:08.814063: train_loss -0.8614\n",
      "2025-12-20 19:22:08.815804: val_loss -0.8876\n",
      "2025-12-20 19:22:08.825816: Pseudo dice [0.9292, 0.9596, 0.9491]\n",
      "2025-12-20 19:22:08.833565: Epoch time: 138.1 s\n",
      "2025-12-20 19:22:09.559074: \n",
      "2025-12-20 19:22:09.559074: Epoch 890\n",
      "2025-12-20 19:22:09.572086: Current learning rate: 0.00137\n",
      "2025-12-20 19:24:27.523082: train_loss -0.8603\n",
      "2025-12-20 19:24:27.523082: val_loss -0.8785\n",
      "2025-12-20 19:24:27.531090: Pseudo dice [0.923, 0.955, 0.9427]\n",
      "2025-12-20 19:24:27.540846: Epoch time: 137.96 s\n",
      "2025-12-20 19:24:28.168445: \n",
      "2025-12-20 19:24:28.168445: Epoch 891\n",
      "2025-12-20 19:24:28.168445: Current learning rate: 0.00136\n",
      "2025-12-20 19:26:46.292395: train_loss -0.8637\n",
      "2025-12-20 19:26:46.292395: val_loss -0.8847\n",
      "2025-12-20 19:26:46.298810: Pseudo dice [0.928, 0.9604, 0.9439]\n",
      "2025-12-20 19:26:46.298810: Epoch time: 138.12 s\n",
      "2025-12-20 19:26:46.931103: \n",
      "2025-12-20 19:26:46.931103: Epoch 892\n",
      "2025-12-20 19:26:46.931103: Current learning rate: 0.00135\n",
      "2025-12-20 19:29:04.920588: train_loss -0.8665\n",
      "2025-12-20 19:29:04.920588: val_loss -0.8866\n",
      "2025-12-20 19:29:04.931526: Pseudo dice [0.9283, 0.9589, 0.9515]\n",
      "2025-12-20 19:29:04.936388: Epoch time: 137.99 s\n",
      "2025-12-20 19:29:05.648223: \n",
      "2025-12-20 19:29:05.648223: Epoch 893\n",
      "2025-12-20 19:29:05.665875: Current learning rate: 0.00134\n",
      "2025-12-20 19:31:23.728668: train_loss -0.8574\n",
      "2025-12-20 19:31:23.728668: val_loss -0.8908\n",
      "2025-12-20 19:31:23.728668: Pseudo dice [0.9355, 0.9595, 0.9478]\n",
      "2025-12-20 19:31:23.748599: Epoch time: 138.08 s\n",
      "2025-12-20 19:31:24.377565: \n",
      "2025-12-20 19:31:24.377565: Epoch 894\n",
      "2025-12-20 19:31:24.377565: Current learning rate: 0.00133\n",
      "2025-12-20 19:33:42.443533: train_loss -0.8603\n",
      "2025-12-20 19:33:42.443533: val_loss -0.8855\n",
      "2025-12-20 19:33:42.457206: Pseudo dice [0.9307, 0.9585, 0.9491]\n",
      "2025-12-20 19:33:42.463696: Epoch time: 138.07 s\n",
      "2025-12-20 19:33:43.269268: \n",
      "2025-12-20 19:33:43.269268: Epoch 895\n",
      "2025-12-20 19:33:43.273909: Current learning rate: 0.00132\n",
      "2025-12-20 19:36:01.437981: train_loss -0.8598\n",
      "2025-12-20 19:36:01.437981: val_loss -0.887\n",
      "2025-12-20 19:36:01.441726: Pseudo dice [0.9283, 0.9575, 0.9426]\n",
      "2025-12-20 19:36:01.441726: Epoch time: 138.17 s\n",
      "2025-12-20 19:36:02.169208: \n",
      "2025-12-20 19:36:02.169208: Epoch 896\n",
      "2025-12-20 19:36:02.169208: Current learning rate: 0.0013\n",
      "2025-12-20 19:38:20.082545: train_loss -0.864\n",
      "2025-12-20 19:38:20.082545: val_loss -0.8904\n",
      "2025-12-20 19:38:20.098188: Pseudo dice [0.9309, 0.9587, 0.9452]\n",
      "2025-12-20 19:38:20.098188: Epoch time: 137.91 s\n",
      "2025-12-20 19:38:20.718451: \n",
      "2025-12-20 19:38:20.718451: Epoch 897\n",
      "2025-12-20 19:38:20.718451: Current learning rate: 0.00129\n",
      "2025-12-20 19:40:38.849594: train_loss -0.8614\n",
      "2025-12-20 19:40:38.849594: val_loss -0.8829\n",
      "2025-12-20 19:40:38.851597: Pseudo dice [0.9276, 0.9579, 0.9429]\n",
      "2025-12-20 19:40:38.857606: Epoch time: 138.13 s\n",
      "2025-12-20 19:40:39.492976: \n",
      "2025-12-20 19:40:39.492976: Epoch 898\n",
      "2025-12-20 19:40:39.492976: Current learning rate: 0.00128\n",
      "2025-12-20 19:42:57.566516: train_loss -0.8598\n",
      "2025-12-20 19:42:57.568518: val_loss -0.8878\n",
      "2025-12-20 19:42:57.575527: Pseudo dice [0.9288, 0.9637, 0.9504]\n",
      "2025-12-20 19:42:57.583282: Epoch time: 138.07 s\n",
      "2025-12-20 19:42:58.258666: \n",
      "2025-12-20 19:42:58.258666: Epoch 899\n",
      "2025-12-20 19:42:58.264682: Current learning rate: 0.00127\n",
      "2025-12-20 19:45:16.428352: train_loss -0.8571\n",
      "2025-12-20 19:45:16.428352: val_loss -0.8849\n",
      "2025-12-20 19:45:16.428352: Pseudo dice [0.9293, 0.9593, 0.9414]\n",
      "2025-12-20 19:45:16.444459: Epoch time: 138.17 s\n",
      "2025-12-20 19:45:17.316680: \n",
      "2025-12-20 19:45:17.316680: Epoch 900\n",
      "2025-12-20 19:45:17.332365: Current learning rate: 0.00126\n",
      "2025-12-20 19:47:35.444332: train_loss -0.8589\n",
      "2025-12-20 19:47:35.444332: val_loss -0.8839\n",
      "2025-12-20 19:47:35.444332: Pseudo dice [0.9274, 0.9558, 0.945]\n",
      "2025-12-20 19:47:35.460121: Epoch time: 138.13 s\n",
      "2025-12-20 19:47:36.253591: \n",
      "2025-12-20 19:47:36.253591: Epoch 901\n",
      "2025-12-20 19:47:36.261836: Current learning rate: 0.00125\n",
      "2025-12-20 19:49:54.281679: train_loss -0.8623\n",
      "2025-12-20 19:49:54.281679: val_loss -0.8879\n",
      "2025-12-20 19:49:54.295488: Pseudo dice [0.9312, 0.9578, 0.9482]\n",
      "2025-12-20 19:49:54.299526: Epoch time: 138.03 s\n",
      "2025-12-20 19:49:55.068241: \n",
      "2025-12-20 19:49:55.068241: Epoch 902\n",
      "2025-12-20 19:49:55.088174: Current learning rate: 0.00124\n",
      "2025-12-20 19:52:12.988345: train_loss -0.8663\n",
      "2025-12-20 19:52:12.988345: val_loss -0.8816\n",
      "2025-12-20 19:52:13.004465: Pseudo dice [0.9247, 0.9565, 0.9474]\n",
      "2025-12-20 19:52:13.008784: Epoch time: 137.92 s\n",
      "2025-12-20 19:52:13.685077: \n",
      "2025-12-20 19:52:13.685077: Epoch 903\n",
      "2025-12-20 19:52:13.685077: Current learning rate: 0.00122\n",
      "2025-12-20 19:54:31.636149: train_loss -0.8638\n",
      "2025-12-20 19:54:31.636149: val_loss -0.893\n",
      "2025-12-20 19:54:31.649085: Pseudo dice [0.9348, 0.962, 0.9487]\n",
      "2025-12-20 19:54:31.656071: Epoch time: 137.95 s\n",
      "2025-12-20 19:54:32.348009: \n",
      "2025-12-20 19:54:32.348009: Epoch 904\n",
      "2025-12-20 19:54:32.363734: Current learning rate: 0.00121\n",
      "2025-12-20 19:56:50.201045: train_loss -0.8642\n",
      "2025-12-20 19:56:50.203046: val_loss -0.8827\n",
      "2025-12-20 19:56:50.210363: Pseudo dice [0.9243, 0.9565, 0.9486]\n",
      "2025-12-20 19:56:50.218026: Epoch time: 137.85 s\n",
      "2025-12-20 19:56:50.871771: \n",
      "2025-12-20 19:56:50.871771: Epoch 905\n",
      "2025-12-20 19:56:50.884497: Current learning rate: 0.0012\n",
      "2025-12-20 19:59:08.803279: train_loss -0.8638\n",
      "2025-12-20 19:59:08.803279: val_loss -0.8921\n",
      "2025-12-20 19:59:08.822924: Pseudo dice [0.9342, 0.9637, 0.944]\n",
      "2025-12-20 19:59:08.828875: Epoch time: 137.93 s\n",
      "2025-12-20 19:59:08.828875: Yayy! New best EMA pseudo Dice: 0.9449\n",
      "2025-12-20 19:59:09.797299: \n",
      "2025-12-20 19:59:09.797299: Epoch 906\n",
      "2025-12-20 19:59:09.797299: Current learning rate: 0.00119\n",
      "2025-12-20 20:01:27.824582: train_loss -0.8653\n",
      "2025-12-20 20:01:27.824582: val_loss -0.8883\n",
      "2025-12-20 20:01:27.832783: Pseudo dice [0.9323, 0.9614, 0.9469]\n",
      "2025-12-20 20:01:27.838789: Epoch time: 138.03 s\n",
      "2025-12-20 20:01:27.844295: Yayy! New best EMA pseudo Dice: 0.9451\n",
      "2025-12-20 20:01:28.827501: \n",
      "2025-12-20 20:01:28.827501: Epoch 907\n",
      "2025-12-20 20:01:28.832292: Current learning rate: 0.00118\n",
      "2025-12-20 20:03:46.939206: train_loss -0.866\n",
      "2025-12-20 20:03:46.946704: val_loss -0.8847\n",
      "2025-12-20 20:03:46.952710: Pseudo dice [0.9315, 0.9569, 0.9452]\n",
      "2025-12-20 20:03:46.958210: Epoch time: 138.11 s\n",
      "2025-12-20 20:03:47.849730: \n",
      "2025-12-20 20:03:47.849730: Epoch 908\n",
      "2025-12-20 20:03:47.863067: Current learning rate: 0.00117\n",
      "2025-12-20 20:06:05.926234: train_loss -0.8678\n",
      "2025-12-20 20:06:05.926234: val_loss -0.891\n",
      "2025-12-20 20:06:05.926234: Pseudo dice [0.9352, 0.963, 0.9466]\n",
      "2025-12-20 20:06:05.942291: Epoch time: 138.08 s\n",
      "2025-12-20 20:06:05.942291: Yayy! New best EMA pseudo Dice: 0.9454\n",
      "2025-12-20 20:06:06.855398: \n",
      "2025-12-20 20:06:06.855398: Epoch 909\n",
      "2025-12-20 20:06:06.855398: Current learning rate: 0.00116\n",
      "2025-12-20 20:08:24.917067: train_loss -0.8633\n",
      "2025-12-20 20:08:24.917067: val_loss -0.8868\n",
      "2025-12-20 20:08:24.932977: Pseudo dice [0.9332, 0.9627, 0.9381]\n",
      "2025-12-20 20:08:24.932977: Epoch time: 138.06 s\n",
      "2025-12-20 20:08:25.625723: \n",
      "2025-12-20 20:08:25.625723: Epoch 910\n",
      "2025-12-20 20:08:25.641663: Current learning rate: 0.00115\n",
      "2025-12-20 20:10:44.066970: train_loss -0.8623\n",
      "2025-12-20 20:10:44.066970: val_loss -0.8897\n",
      "2025-12-20 20:10:44.082947: Pseudo dice [0.9323, 0.9611, 0.9516]\n",
      "2025-12-20 20:10:44.088485: Epoch time: 138.44 s\n",
      "2025-12-20 20:10:44.092490: Yayy! New best EMA pseudo Dice: 0.9456\n",
      "2025-12-20 20:10:45.070924: \n",
      "2025-12-20 20:10:45.072665: Epoch 911\n",
      "2025-12-20 20:10:45.079094: Current learning rate: 0.00113\n",
      "2025-12-20 20:13:03.080240: train_loss -0.8639\n",
      "2025-12-20 20:13:03.080240: val_loss -0.8921\n",
      "2025-12-20 20:13:03.096105: Pseudo dice [0.934, 0.9635, 0.9454]\n",
      "2025-12-20 20:13:03.105604: Epoch time: 138.01 s\n",
      "2025-12-20 20:13:03.111610: Yayy! New best EMA pseudo Dice: 0.9458\n",
      "2025-12-20 20:13:04.023639: \n",
      "2025-12-20 20:13:04.027143: Epoch 912\n",
      "2025-12-20 20:13:04.027143: Current learning rate: 0.00112\n",
      "2025-12-20 20:15:22.093951: train_loss -0.8624\n",
      "2025-12-20 20:15:22.095954: val_loss -0.8909\n",
      "2025-12-20 20:15:22.101960: Pseudo dice [0.9321, 0.9596, 0.9517]\n",
      "2025-12-20 20:15:22.105703: Epoch time: 138.07 s\n",
      "2025-12-20 20:15:22.113202: Yayy! New best EMA pseudo Dice: 0.946\n",
      "2025-12-20 20:15:23.222607: \n",
      "2025-12-20 20:15:23.222607: Epoch 913\n",
      "2025-12-20 20:15:23.228622: Current learning rate: 0.00111\n",
      "2025-12-20 20:17:41.303140: train_loss -0.8649\n",
      "2025-12-20 20:17:41.303140: val_loss -0.8838\n",
      "2025-12-20 20:17:41.303140: Pseudo dice [0.9291, 0.9589, 0.9397]\n",
      "2025-12-20 20:17:41.303140: Epoch time: 138.1 s\n",
      "2025-12-20 20:17:42.120919: \n",
      "2025-12-20 20:17:42.120919: Epoch 914\n",
      "2025-12-20 20:17:42.126335: Current learning rate: 0.0011\n",
      "2025-12-20 20:19:59.937377: train_loss -0.8677\n",
      "2025-12-20 20:19:59.937377: val_loss -0.8908\n",
      "2025-12-20 20:19:59.937377: Pseudo dice [0.9312, 0.9632, 0.9462]\n",
      "2025-12-20 20:19:59.945780: Epoch time: 137.82 s\n",
      "2025-12-20 20:20:00.566249: \n",
      "2025-12-20 20:20:00.566249: Epoch 915\n",
      "2025-12-20 20:20:00.566249: Current learning rate: 0.00109\n",
      "2025-12-20 20:22:18.485902: train_loss -0.8665\n",
      "2025-12-20 20:22:18.485902: val_loss -0.8923\n",
      "2025-12-20 20:22:18.492856: Pseudo dice [0.9327, 0.9601, 0.9505]\n",
      "2025-12-20 20:22:18.498862: Epoch time: 137.92 s\n",
      "2025-12-20 20:22:19.202545: \n",
      "2025-12-20 20:22:19.202545: Epoch 916\n",
      "2025-12-20 20:22:19.218582: Current learning rate: 0.00108\n",
      "2025-12-20 20:24:37.240157: train_loss -0.8628\n",
      "2025-12-20 20:24:37.240157: val_loss -0.8796\n",
      "2025-12-20 20:24:37.259983: Pseudo dice [0.9224, 0.9571, 0.9487]\n",
      "2025-12-20 20:24:37.265989: Epoch time: 138.04 s\n",
      "2025-12-20 20:24:37.889121: \n",
      "2025-12-20 20:24:37.889121: Epoch 917\n",
      "2025-12-20 20:24:37.905015: Current learning rate: 0.00106\n",
      "2025-12-20 20:26:55.913757: train_loss -0.8587\n",
      "2025-12-20 20:26:55.913757: val_loss -0.887\n",
      "2025-12-20 20:26:55.921075: Pseudo dice [0.9284, 0.9616, 0.9522]\n",
      "2025-12-20 20:26:55.927082: Epoch time: 138.02 s\n",
      "2025-12-20 20:26:56.550634: \n",
      "2025-12-20 20:26:56.550634: Epoch 918\n",
      "2025-12-20 20:26:56.550634: Current learning rate: 0.00105\n",
      "2025-12-20 20:29:14.623445: train_loss -0.8615\n",
      "2025-12-20 20:29:14.623445: val_loss -0.8933\n",
      "2025-12-20 20:29:14.629451: Pseudo dice [0.9331, 0.962, 0.9492]\n",
      "2025-12-20 20:29:14.631191: Epoch time: 138.07 s\n",
      "2025-12-20 20:29:14.631191: Yayy! New best EMA pseudo Dice: 0.9461\n",
      "2025-12-20 20:29:15.552764: \n",
      "2025-12-20 20:29:15.554766: Epoch 919\n",
      "2025-12-20 20:29:15.560772: Current learning rate: 0.00104\n",
      "2025-12-20 20:31:33.755164: train_loss -0.8526\n",
      "2025-12-20 20:31:33.755164: val_loss -0.8866\n",
      "2025-12-20 20:31:33.761169: Pseudo dice [0.9319, 0.96, 0.9469]\n",
      "2025-12-20 20:31:33.771641: Epoch time: 138.2 s\n",
      "2025-12-20 20:31:33.775570: Yayy! New best EMA pseudo Dice: 0.9461\n",
      "2025-12-20 20:31:34.889881: \n",
      "2025-12-20 20:31:34.889881: Epoch 920\n",
      "2025-12-20 20:31:34.905544: Current learning rate: 0.00103\n",
      "2025-12-20 20:33:53.066538: train_loss -0.8644\n",
      "2025-12-20 20:33:53.066538: val_loss -0.8863\n",
      "2025-12-20 20:33:53.074510: Pseudo dice [0.9284, 0.9613, 0.9427]\n",
      "2025-12-20 20:33:53.082191: Epoch time: 138.18 s\n",
      "2025-12-20 20:33:53.737296: \n",
      "2025-12-20 20:33:53.737296: Epoch 921\n",
      "2025-12-20 20:33:53.737296: Current learning rate: 0.00102\n",
      "2025-12-20 20:36:11.820783: train_loss -0.861\n",
      "2025-12-20 20:36:11.822785: val_loss -0.8888\n",
      "2025-12-20 20:36:11.828792: Pseudo dice [0.9327, 0.9634, 0.9427]\n",
      "2025-12-20 20:36:11.832796: Epoch time: 138.08 s\n",
      "2025-12-20 20:36:12.514593: \n",
      "2025-12-20 20:36:12.516595: Epoch 922\n",
      "2025-12-20 20:36:12.516595: Current learning rate: 0.00101\n",
      "2025-12-20 20:38:30.560704: train_loss -0.8631\n",
      "2025-12-20 20:38:30.560704: val_loss -0.8891\n",
      "2025-12-20 20:38:30.578691: Pseudo dice [0.9335, 0.962, 0.9437]\n",
      "2025-12-20 20:38:30.584697: Epoch time: 138.05 s\n",
      "2025-12-20 20:38:31.262923: \n",
      "2025-12-20 20:38:31.262923: Epoch 923\n",
      "2025-12-20 20:38:31.267100: Current learning rate: 0.001\n",
      "2025-12-20 20:40:49.291725: train_loss -0.8652\n",
      "2025-12-20 20:40:49.291725: val_loss -0.8881\n",
      "2025-12-20 20:40:49.297731: Pseudo dice [0.9287, 0.9608, 0.9525]\n",
      "2025-12-20 20:40:49.303237: Epoch time: 138.03 s\n",
      "2025-12-20 20:40:49.309243: Yayy! New best EMA pseudo Dice: 0.9461\n",
      "2025-12-20 20:40:50.205288: \n",
      "2025-12-20 20:40:50.205288: Epoch 924\n",
      "2025-12-20 20:40:50.205288: Current learning rate: 0.00098\n",
      "2025-12-20 20:43:08.224635: train_loss -0.8639\n",
      "2025-12-20 20:43:08.224635: val_loss -0.8765\n",
      "2025-12-20 20:43:08.230641: Pseudo dice [0.9219, 0.9522, 0.9462]\n",
      "2025-12-20 20:43:08.236145: Epoch time: 138.02 s\n",
      "2025-12-20 20:43:08.975576: \n",
      "2025-12-20 20:43:08.975576: Epoch 925\n",
      "2025-12-20 20:43:08.977578: Current learning rate: 0.00097\n",
      "2025-12-20 20:45:27.179584: train_loss -0.8611\n",
      "2025-12-20 20:45:27.179584: val_loss -0.8816\n",
      "2025-12-20 20:45:27.187150: Pseudo dice [0.9246, 0.958, 0.9502]\n",
      "2025-12-20 20:45:27.193156: Epoch time: 138.2 s\n",
      "2025-12-20 20:45:27.970116: \n",
      "2025-12-20 20:45:27.970116: Epoch 926\n",
      "2025-12-20 20:45:27.970116: Current learning rate: 0.00096\n",
      "2025-12-20 20:47:46.095320: train_loss -0.8626\n",
      "2025-12-20 20:47:46.095320: val_loss -0.8936\n",
      "2025-12-20 20:47:46.103328: Pseudo dice [0.9355, 0.9642, 0.9453]\n",
      "2025-12-20 20:47:46.109161: Epoch time: 138.13 s\n",
      "2025-12-20 20:47:46.734770: \n",
      "2025-12-20 20:47:46.734770: Epoch 927\n",
      "2025-12-20 20:47:46.734770: Current learning rate: 0.00095\n",
      "2025-12-20 20:50:04.636847: train_loss -0.8623\n",
      "2025-12-20 20:50:04.636847: val_loss -0.8911\n",
      "2025-12-20 20:50:04.650354: Pseudo dice [0.934, 0.9663, 0.945]\n",
      "2025-12-20 20:50:04.656161: Epoch time: 137.9 s\n",
      "2025-12-20 20:50:05.269473: \n",
      "2025-12-20 20:50:05.269473: Epoch 928\n",
      "2025-12-20 20:50:05.285465: Current learning rate: 0.00094\n",
      "2025-12-20 20:52:23.551421: train_loss -0.8631\n",
      "2025-12-20 20:52:23.551421: val_loss -0.8854\n",
      "2025-12-20 20:52:23.560664: Pseudo dice [0.9321, 0.96, 0.9375]\n",
      "2025-12-20 20:52:23.566670: Epoch time: 138.28 s\n",
      "2025-12-20 20:52:24.181255: \n",
      "2025-12-20 20:52:24.181255: Epoch 929\n",
      "2025-12-20 20:52:24.181255: Current learning rate: 0.00092\n",
      "2025-12-20 20:54:42.337018: train_loss -0.8586\n",
      "2025-12-20 20:54:42.339021: val_loss -0.8908\n",
      "2025-12-20 20:54:42.344766: Pseudo dice [0.9337, 0.9655, 0.9414]\n",
      "2025-12-20 20:54:42.354274: Epoch time: 138.16 s\n",
      "2025-12-20 20:54:42.977577: \n",
      "2025-12-20 20:54:42.977577: Epoch 930\n",
      "2025-12-20 20:54:42.977577: Current learning rate: 0.00091\n",
      "2025-12-20 20:57:00.928333: train_loss -0.863\n",
      "2025-12-20 20:57:00.930336: val_loss -0.8885\n",
      "2025-12-20 20:57:00.937905: Pseudo dice [0.9298, 0.9583, 0.9488]\n",
      "2025-12-20 20:57:00.941909: Epoch time: 137.95 s\n",
      "2025-12-20 20:57:01.623502: \n",
      "2025-12-20 20:57:01.623502: Epoch 931\n",
      "2025-12-20 20:57:01.623502: Current learning rate: 0.0009\n",
      "2025-12-20 20:59:19.622019: train_loss -0.8626\n",
      "2025-12-20 20:59:19.624022: val_loss -0.8867\n",
      "2025-12-20 20:59:19.630471: Pseudo dice [0.9318, 0.9595, 0.9477]\n",
      "2025-12-20 20:59:19.630471: Epoch time: 138.01 s\n",
      "2025-12-20 20:59:20.415466: \n",
      "2025-12-20 20:59:20.415466: Epoch 932\n",
      "2025-12-20 20:59:20.420500: Current learning rate: 0.00089\n",
      "2025-12-20 21:01:38.633115: train_loss -0.8601\n",
      "2025-12-20 21:01:38.633115: val_loss -0.8908\n",
      "2025-12-20 21:01:38.640232: Pseudo dice [0.9343, 0.9632, 0.9382]\n",
      "2025-12-20 21:01:38.647653: Epoch time: 138.22 s\n",
      "2025-12-20 21:01:39.266835: \n",
      "2025-12-20 21:01:39.266835: Epoch 933\n",
      "2025-12-20 21:01:39.277420: Current learning rate: 0.00088\n",
      "2025-12-20 21:03:57.304623: train_loss -0.8651\n",
      "2025-12-20 21:03:57.304623: val_loss -0.8868\n",
      "2025-12-20 21:03:57.310630: Pseudo dice [0.929, 0.96, 0.9497]\n",
      "2025-12-20 21:03:57.315375: Epoch time: 138.04 s\n",
      "2025-12-20 21:03:57.947468: \n",
      "2025-12-20 21:03:57.947468: Epoch 934\n",
      "2025-12-20 21:03:57.947468: Current learning rate: 0.00087\n",
      "2025-12-20 21:06:15.930010: train_loss -0.8678\n",
      "2025-12-20 21:06:15.930010: val_loss -0.8904\n",
      "2025-12-20 21:06:15.937337: Pseudo dice [0.9323, 0.9621, 0.9487]\n",
      "2025-12-20 21:06:15.943343: Epoch time: 137.98 s\n",
      "2025-12-20 21:06:16.569424: \n",
      "2025-12-20 21:06:16.569424: Epoch 935\n",
      "2025-12-20 21:06:16.569424: Current learning rate: 0.00085\n",
      "2025-12-20 21:08:34.774137: train_loss -0.8592\n",
      "2025-12-20 21:08:34.774137: val_loss -0.8926\n",
      "2025-12-20 21:08:34.774137: Pseudo dice [0.9351, 0.9637, 0.9464]\n",
      "2025-12-20 21:08:34.792068: Epoch time: 138.2 s\n",
      "2025-12-20 21:08:34.792068: Yayy! New best EMA pseudo Dice: 0.9462\n",
      "2025-12-20 21:08:35.730163: \n",
      "2025-12-20 21:08:35.730163: Epoch 936\n",
      "2025-12-20 21:08:35.736435: Current learning rate: 0.00084\n",
      "2025-12-20 21:10:54.119030: train_loss -0.8649\n",
      "2025-12-20 21:10:54.119030: val_loss -0.8833\n",
      "2025-12-20 21:10:54.128901: Pseudo dice [0.9283, 0.9607, 0.9424]\n",
      "2025-12-20 21:10:54.132904: Epoch time: 138.39 s\n",
      "2025-12-20 21:10:54.778760: \n",
      "2025-12-20 21:10:54.778760: Epoch 937\n",
      "2025-12-20 21:10:54.778760: Current learning rate: 0.00083\n",
      "2025-12-20 21:13:12.869257: train_loss -0.8612\n",
      "2025-12-20 21:13:12.871261: val_loss -0.8927\n",
      "2025-12-20 21:13:12.885056: Pseudo dice [0.937, 0.9601, 0.9487]\n",
      "2025-12-20 21:13:12.891063: Epoch time: 138.09 s\n",
      "2025-12-20 21:13:12.900740: Yayy! New best EMA pseudo Dice: 0.9463\n",
      "2025-12-20 21:13:13.985560: \n",
      "2025-12-20 21:13:13.985560: Epoch 938\n",
      "2025-12-20 21:13:13.985560: Current learning rate: 0.00082\n",
      "2025-12-20 21:15:32.222190: train_loss -0.8657\n",
      "2025-12-20 21:15:32.222190: val_loss -0.8892\n",
      "2025-12-20 21:15:32.238166: Pseudo dice [0.9307, 0.96, 0.9501]\n",
      "2025-12-20 21:15:32.238166: Epoch time: 138.24 s\n",
      "2025-12-20 21:15:32.238166: Yayy! New best EMA pseudo Dice: 0.9463\n",
      "2025-12-20 21:15:33.158452: \n",
      "2025-12-20 21:15:33.158452: Epoch 939\n",
      "2025-12-20 21:15:33.158452: Current learning rate: 0.00081\n",
      "2025-12-20 21:17:51.260601: train_loss -0.8628\n",
      "2025-12-20 21:17:51.262603: val_loss -0.8901\n",
      "2025-12-20 21:17:51.270615: Pseudo dice [0.9323, 0.9592, 0.9526]\n",
      "2025-12-20 21:17:51.278254: Epoch time: 138.1 s\n",
      "2025-12-20 21:17:51.286001: Yayy! New best EMA pseudo Dice: 0.9465\n",
      "2025-12-20 21:17:52.235533: \n",
      "2025-12-20 21:17:52.235533: Epoch 940\n",
      "2025-12-20 21:17:52.241521: Current learning rate: 0.00079\n",
      "2025-12-20 21:20:10.325879: train_loss -0.8656\n",
      "2025-12-20 21:20:10.325879: val_loss -0.8826\n",
      "2025-12-20 21:20:10.340862: Pseudo dice [0.9267, 0.9562, 0.9468]\n",
      "2025-12-20 21:20:10.345869: Epoch time: 138.09 s\n",
      "2025-12-20 21:20:11.015202: \n",
      "2025-12-20 21:20:11.015202: Epoch 941\n",
      "2025-12-20 21:20:11.015202: Current learning rate: 0.00078\n",
      "2025-12-20 21:22:29.067829: train_loss -0.8676\n",
      "2025-12-20 21:22:29.067829: val_loss -0.8839\n",
      "2025-12-20 21:22:29.067829: Pseudo dice [0.9266, 0.9621, 0.9494]\n",
      "2025-12-20 21:22:29.067829: Epoch time: 138.05 s\n",
      "2025-12-20 21:22:29.705533: \n",
      "2025-12-20 21:22:29.705533: Epoch 942\n",
      "2025-12-20 21:22:29.711166: Current learning rate: 0.00077\n",
      "2025-12-20 21:24:47.938973: train_loss -0.8672\n",
      "2025-12-20 21:24:47.938973: val_loss -0.8896\n",
      "2025-12-20 21:24:47.946817: Pseudo dice [0.9312, 0.9658, 0.946]\n",
      "2025-12-20 21:24:47.950821: Epoch time: 138.24 s\n",
      "2025-12-20 21:24:48.656646: \n",
      "2025-12-20 21:24:48.656646: Epoch 943\n",
      "2025-12-20 21:24:48.660448: Current learning rate: 0.00076\n",
      "2025-12-20 21:27:06.764373: train_loss -0.8644\n",
      "2025-12-20 21:27:06.764373: val_loss -0.8948\n",
      "2025-12-20 21:27:06.768377: Pseudo dice [0.9353, 0.9632, 0.9516]\n",
      "2025-12-20 21:27:06.777878: Epoch time: 138.11 s\n",
      "2025-12-20 21:27:06.783884: Yayy! New best EMA pseudo Dice: 0.9467\n",
      "2025-12-20 21:27:07.846622: \n",
      "2025-12-20 21:27:07.846622: Epoch 944\n",
      "2025-12-20 21:27:07.846622: Current learning rate: 0.00075\n",
      "2025-12-20 21:29:25.852921: train_loss -0.8641\n",
      "2025-12-20 21:29:25.852921: val_loss -0.8903\n",
      "2025-12-20 21:29:25.859930: Pseudo dice [0.9336, 0.9658, 0.9416]\n",
      "2025-12-20 21:29:25.867938: Epoch time: 138.01 s\n",
      "2025-12-20 21:29:25.873819: Yayy! New best EMA pseudo Dice: 0.9467\n",
      "2025-12-20 21:29:26.801642: \n",
      "2025-12-20 21:29:26.803908: Epoch 945\n",
      "2025-12-20 21:29:26.803908: Current learning rate: 0.00074\n",
      "2025-12-20 21:31:44.997139: train_loss -0.8596\n",
      "2025-12-20 21:31:44.997139: val_loss -0.8891\n",
      "2025-12-20 21:31:45.004207: Pseudo dice [0.9304, 0.9624, 0.9484]\n",
      "2025-12-20 21:31:45.010213: Epoch time: 138.2 s\n",
      "2025-12-20 21:31:45.015046: Yayy! New best EMA pseudo Dice: 0.9468\n",
      "2025-12-20 21:31:45.946464: \n",
      "2025-12-20 21:31:45.946464: Epoch 946\n",
      "2025-12-20 21:31:45.962179: Current learning rate: 0.00072\n",
      "2025-12-20 21:34:04.183926: train_loss -0.8691\n",
      "2025-12-20 21:34:04.183926: val_loss -0.8895\n",
      "2025-12-20 21:34:04.201786: Pseudo dice [0.93, 0.96, 0.9487]\n",
      "2025-12-20 21:34:04.201786: Epoch time: 138.24 s\n",
      "2025-12-20 21:34:04.835003: \n",
      "2025-12-20 21:34:04.835003: Epoch 947\n",
      "2025-12-20 21:34:04.850766: Current learning rate: 0.00071\n",
      "2025-12-20 21:36:22.864079: train_loss -0.8624\n",
      "2025-12-20 21:36:22.864079: val_loss -0.887\n",
      "2025-12-20 21:36:22.874838: Pseudo dice [0.929, 0.9605, 0.9422]\n",
      "2025-12-20 21:36:22.879767: Epoch time: 138.03 s\n",
      "2025-12-20 21:36:23.513657: \n",
      "2025-12-20 21:36:23.513657: Epoch 948\n",
      "2025-12-20 21:36:23.522383: Current learning rate: 0.0007\n",
      "2025-12-20 21:38:41.574906: train_loss -0.8625\n",
      "2025-12-20 21:38:41.583501: val_loss -0.8823\n",
      "2025-12-20 21:38:41.591386: Pseudo dice [0.9264, 0.9589, 0.9462]\n",
      "2025-12-20 21:38:41.595390: Epoch time: 138.06 s\n",
      "2025-12-20 21:38:42.216879: \n",
      "2025-12-20 21:38:42.232661: Epoch 949\n",
      "2025-12-20 21:38:42.232661: Current learning rate: 0.00069\n",
      "2025-12-20 21:41:00.172190: train_loss -0.862\n",
      "2025-12-20 21:41:00.172190: val_loss -0.883\n",
      "2025-12-20 21:41:00.178917: Pseudo dice [0.9278, 0.9601, 0.9374]\n",
      "2025-12-20 21:41:00.183993: Epoch time: 137.96 s\n",
      "2025-12-20 21:41:01.250909: \n",
      "2025-12-20 21:41:01.250909: Epoch 950\n",
      "2025-12-20 21:41:01.270598: Current learning rate: 0.00067\n",
      "2025-12-20 21:43:19.181827: train_loss -0.8641\n",
      "2025-12-20 21:43:19.181827: val_loss -0.8868\n",
      "2025-12-20 21:43:19.199222: Pseudo dice [0.9318, 0.961, 0.9389]\n",
      "2025-12-20 21:43:19.204720: Epoch time: 137.93 s\n",
      "2025-12-20 21:43:19.831812: \n",
      "2025-12-20 21:43:19.831812: Epoch 951\n",
      "2025-12-20 21:43:19.831812: Current learning rate: 0.00066\n",
      "2025-12-20 21:45:38.006305: train_loss -0.8632\n",
      "2025-12-20 21:45:38.006305: val_loss -0.8807\n",
      "2025-12-20 21:45:38.006305: Pseudo dice [0.9236, 0.9558, 0.9434]\n",
      "2025-12-20 21:45:38.025624: Epoch time: 138.17 s\n",
      "2025-12-20 21:45:38.711571: \n",
      "2025-12-20 21:45:38.711571: Epoch 952\n",
      "2025-12-20 21:45:38.724195: Current learning rate: 0.00065\n",
      "2025-12-20 21:47:56.751210: train_loss -0.8612\n",
      "2025-12-20 21:47:56.751210: val_loss -0.8824\n",
      "2025-12-20 21:47:56.767080: Pseudo dice [0.9252, 0.9565, 0.9472]\n",
      "2025-12-20 21:47:56.773126: Epoch time: 138.04 s\n",
      "2025-12-20 21:47:57.400493: \n",
      "2025-12-20 21:47:57.400493: Epoch 953\n",
      "2025-12-20 21:47:57.410173: Current learning rate: 0.00064\n",
      "2025-12-20 21:50:15.560163: train_loss -0.8605\n",
      "2025-12-20 21:50:15.576163: val_loss -0.8862\n",
      "2025-12-20 21:50:15.582171: Pseudo dice [0.928, 0.9605, 0.9458]\n",
      "2025-12-20 21:50:15.588177: Epoch time: 138.16 s\n",
      "2025-12-20 21:50:16.224262: \n",
      "2025-12-20 21:50:16.224262: Epoch 954\n",
      "2025-12-20 21:50:16.230625: Current learning rate: 0.00063\n",
      "2025-12-20 21:52:34.411937: train_loss -0.8676\n",
      "2025-12-20 21:52:34.411937: val_loss -0.885\n",
      "2025-12-20 21:52:34.424342: Pseudo dice [0.9265, 0.9587, 0.9489]\n",
      "2025-12-20 21:52:34.431192: Epoch time: 138.2 s\n",
      "2025-12-20 21:52:35.060261: \n",
      "2025-12-20 21:52:35.060261: Epoch 955\n",
      "2025-12-20 21:52:35.071797: Current learning rate: 0.00061\n",
      "2025-12-20 21:54:53.337772: train_loss -0.8636\n",
      "2025-12-20 21:54:53.337772: val_loss -0.8965\n",
      "2025-12-20 21:54:53.353800: Pseudo dice [0.9393, 0.9672, 0.9487]\n",
      "2025-12-20 21:54:53.353800: Epoch time: 138.28 s\n",
      "2025-12-20 21:54:54.211436: \n",
      "2025-12-20 21:54:54.211436: Epoch 956\n",
      "2025-12-20 21:54:54.225305: Current learning rate: 0.0006\n",
      "2025-12-20 21:57:12.175242: train_loss -0.8618\n",
      "2025-12-20 21:57:12.177244: val_loss -0.8861\n",
      "2025-12-20 21:57:12.183642: Pseudo dice [0.9286, 0.9589, 0.9506]\n",
      "2025-12-20 21:57:12.189386: Epoch time: 137.96 s\n",
      "2025-12-20 21:57:12.883473: \n",
      "2025-12-20 21:57:12.883473: Epoch 957\n",
      "2025-12-20 21:57:12.892578: Current learning rate: 0.00059\n",
      "2025-12-20 21:59:30.899635: train_loss -0.869\n",
      "2025-12-20 21:59:30.899635: val_loss -0.8943\n",
      "2025-12-20 21:59:30.915729: Pseudo dice [0.9342, 0.9609, 0.953]\n",
      "2025-12-20 21:59:30.925803: Epoch time: 138.02 s\n",
      "2025-12-20 21:59:31.565269: \n",
      "2025-12-20 21:59:31.565269: Epoch 958\n",
      "2025-12-20 21:59:31.577475: Current learning rate: 0.00058\n",
      "2025-12-20 22:01:49.533235: train_loss -0.8653\n",
      "2025-12-20 22:01:49.533235: val_loss -0.8898\n",
      "2025-12-20 22:01:49.551097: Pseudo dice [0.9307, 0.9588, 0.9521]\n",
      "2025-12-20 22:01:49.557266: Epoch time: 137.97 s\n",
      "2025-12-20 22:01:50.184599: \n",
      "2025-12-20 22:01:50.184599: Epoch 959\n",
      "2025-12-20 22:01:50.184599: Current learning rate: 0.00056\n",
      "2025-12-20 22:04:08.252234: train_loss -0.8641\n",
      "2025-12-20 22:04:08.252234: val_loss -0.8855\n",
      "2025-12-20 22:04:08.265168: Pseudo dice [0.9279, 0.957, 0.9501]\n",
      "2025-12-20 22:04:08.265168: Epoch time: 138.07 s\n",
      "2025-12-20 22:04:08.978479: \n",
      "2025-12-20 22:04:08.978479: Epoch 960\n",
      "2025-12-20 22:04:08.978479: Current learning rate: 0.00055\n",
      "2025-12-20 22:06:27.010690: train_loss -0.8628\n",
      "2025-12-20 22:06:27.010690: val_loss -0.8925\n",
      "2025-12-20 22:06:27.016696: Pseudo dice [0.9359, 0.9633, 0.9467]\n",
      "2025-12-20 22:06:27.024202: Epoch time: 138.05 s\n",
      "2025-12-20 22:06:27.755424: \n",
      "2025-12-20 22:06:27.755424: Epoch 961\n",
      "2025-12-20 22:06:27.761336: Current learning rate: 0.00054\n",
      "2025-12-20 22:08:45.705721: train_loss -0.8631\n",
      "2025-12-20 22:08:45.705721: val_loss -0.8986\n",
      "2025-12-20 22:08:45.724955: Pseudo dice [0.9405, 0.9685, 0.9417]\n",
      "2025-12-20 22:08:45.729561: Epoch time: 137.95 s\n",
      "2025-12-20 22:08:46.356668: \n",
      "2025-12-20 22:08:46.356668: Epoch 962\n",
      "2025-12-20 22:08:46.366166: Current learning rate: 0.00053\n",
      "2025-12-20 22:11:04.772357: train_loss -0.8615\n",
      "2025-12-20 22:11:04.772357: val_loss -0.8896\n",
      "2025-12-20 22:11:04.777639: Pseudo dice [0.9313, 0.9596, 0.9463]\n",
      "2025-12-20 22:11:04.786016: Epoch time: 138.42 s\n",
      "2025-12-20 22:11:05.593730: \n",
      "2025-12-20 22:11:05.593730: Epoch 963\n",
      "2025-12-20 22:11:05.599867: Current learning rate: 0.00051\n",
      "2025-12-20 22:13:23.611684: train_loss -0.8613\n",
      "2025-12-20 22:13:23.611684: val_loss -0.883\n",
      "2025-12-20 22:13:23.611684: Pseudo dice [0.9268, 0.9587, 0.9436]\n",
      "2025-12-20 22:13:23.611684: Epoch time: 138.02 s\n",
      "2025-12-20 22:13:24.393558: \n",
      "2025-12-20 22:13:24.393558: Epoch 964\n",
      "2025-12-20 22:13:24.393558: Current learning rate: 0.0005\n",
      "2025-12-20 22:15:42.295197: train_loss -0.8691\n",
      "2025-12-20 22:15:42.297200: val_loss -0.887\n",
      "2025-12-20 22:15:42.306953: Pseudo dice [0.9306, 0.9596, 0.9473]\n",
      "2025-12-20 22:15:42.314703: Epoch time: 137.91 s\n",
      "2025-12-20 22:15:42.934272: \n",
      "2025-12-20 22:15:42.934272: Epoch 965\n",
      "2025-12-20 22:15:42.948347: Current learning rate: 0.00049\n",
      "2025-12-20 22:18:00.957935: train_loss -0.8669\n",
      "2025-12-20 22:18:00.959681: val_loss -0.8869\n",
      "2025-12-20 22:18:00.970117: Pseudo dice [0.9303, 0.9619, 0.9457]\n",
      "2025-12-20 22:18:00.977625: Epoch time: 138.02 s\n",
      "2025-12-20 22:18:01.607571: \n",
      "2025-12-20 22:18:01.607571: Epoch 966\n",
      "2025-12-20 22:18:01.607571: Current learning rate: 0.00048\n",
      "2025-12-20 22:20:19.648373: train_loss -0.8653\n",
      "2025-12-20 22:20:19.650376: val_loss -0.8864\n",
      "2025-12-20 22:20:19.656385: Pseudo dice [0.93, 0.9618, 0.9391]\n",
      "2025-12-20 22:20:19.662395: Epoch time: 138.04 s\n",
      "2025-12-20 22:20:20.418721: \n",
      "2025-12-20 22:20:20.418721: Epoch 967\n",
      "2025-12-20 22:20:20.425048: Current learning rate: 0.00046\n",
      "2025-12-20 22:22:38.384340: train_loss -0.8682\n",
      "2025-12-20 22:22:38.384340: val_loss -0.8956\n",
      "2025-12-20 22:22:38.404043: Pseudo dice [0.9367, 0.9621, 0.9459]\n",
      "2025-12-20 22:22:38.411105: Epoch time: 137.97 s\n",
      "2025-12-20 22:22:39.050085: \n",
      "2025-12-20 22:22:39.050085: Epoch 968\n",
      "2025-12-20 22:22:39.057245: Current learning rate: 0.00045\n",
      "2025-12-20 22:24:57.089278: train_loss -0.8658\n",
      "2025-12-20 22:24:57.091279: val_loss -0.8841\n",
      "2025-12-20 22:24:57.096495: Pseudo dice [0.9259, 0.9631, 0.9435]\n",
      "2025-12-20 22:24:57.103883: Epoch time: 138.04 s\n",
      "2025-12-20 22:24:57.890258: \n",
      "2025-12-20 22:24:57.890258: Epoch 969\n",
      "2025-12-20 22:24:57.906245: Current learning rate: 0.00044\n",
      "2025-12-20 22:27:15.838142: train_loss -0.8658\n",
      "2025-12-20 22:27:15.838142: val_loss -0.8837\n",
      "2025-12-20 22:27:15.853931: Pseudo dice [0.9282, 0.9594, 0.9448]\n",
      "2025-12-20 22:27:15.860670: Epoch time: 137.95 s\n",
      "2025-12-20 22:27:16.581437: \n",
      "2025-12-20 22:27:16.581437: Epoch 970\n",
      "2025-12-20 22:27:16.595806: Current learning rate: 0.00043\n",
      "2025-12-20 22:29:34.570107: train_loss -0.8639\n",
      "2025-12-20 22:29:34.586203: val_loss -0.8883\n",
      "2025-12-20 22:29:34.592414: Pseudo dice [0.929, 0.9612, 0.9471]\n",
      "2025-12-20 22:29:34.599739: Epoch time: 137.99 s\n",
      "2025-12-20 22:29:35.236775: \n",
      "2025-12-20 22:29:35.236775: Epoch 971\n",
      "2025-12-20 22:29:35.245612: Current learning rate: 0.00041\n",
      "2025-12-20 22:31:53.430212: train_loss -0.864\n",
      "2025-12-20 22:31:53.432215: val_loss -0.8883\n",
      "2025-12-20 22:31:53.437777: Pseudo dice [0.9292, 0.9602, 0.9501]\n",
      "2025-12-20 22:31:53.445245: Epoch time: 138.19 s\n",
      "2025-12-20 22:31:54.068437: \n",
      "2025-12-20 22:31:54.068437: Epoch 972\n",
      "2025-12-20 22:31:54.084471: Current learning rate: 0.0004\n",
      "2025-12-20 22:34:12.354410: train_loss -0.8611\n",
      "2025-12-20 22:34:12.370274: val_loss -0.8975\n",
      "2025-12-20 22:34:12.370274: Pseudo dice [0.9371, 0.9661, 0.9484]\n",
      "2025-12-20 22:34:12.370274: Epoch time: 138.29 s\n",
      "2025-12-20 22:34:13.112208: \n",
      "2025-12-20 22:34:13.112208: Epoch 973\n",
      "2025-12-20 22:34:13.112208: Current learning rate: 0.00039\n",
      "2025-12-20 22:36:31.148919: train_loss -0.8646\n",
      "2025-12-20 22:36:31.148919: val_loss -0.8853\n",
      "2025-12-20 22:36:31.152421: Pseudo dice [0.9277, 0.9588, 0.9488]\n",
      "2025-12-20 22:36:31.164563: Epoch time: 138.04 s\n",
      "2025-12-20 22:36:31.798745: \n",
      "2025-12-20 22:36:31.798745: Epoch 974\n",
      "2025-12-20 22:36:31.809735: Current learning rate: 0.00037\n",
      "2025-12-20 22:38:49.912365: train_loss -0.8665\n",
      "2025-12-20 22:38:49.912365: val_loss -0.883\n",
      "2025-12-20 22:38:49.924764: Pseudo dice [0.9261, 0.9599, 0.9416]\n",
      "2025-12-20 22:38:49.929685: Epoch time: 138.11 s\n",
      "2025-12-20 22:38:50.721169: \n",
      "2025-12-20 22:38:50.721169: Epoch 975\n",
      "2025-12-20 22:38:50.733733: Current learning rate: 0.00036\n",
      "2025-12-20 22:41:08.813885: train_loss -0.8643\n",
      "2025-12-20 22:41:08.830011: val_loss -0.8881\n",
      "2025-12-20 22:41:08.836063: Pseudo dice [0.9303, 0.9593, 0.9492]\n",
      "2025-12-20 22:41:08.836063: Epoch time: 138.09 s\n",
      "2025-12-20 22:41:09.513302: \n",
      "2025-12-20 22:41:09.513302: Epoch 976\n",
      "2025-12-20 22:41:09.524209: Current learning rate: 0.00035\n",
      "2025-12-20 22:43:27.741801: train_loss -0.8623\n",
      "2025-12-20 22:43:27.743542: val_loss -0.8921\n",
      "2025-12-20 22:43:27.757561: Pseudo dice [0.9349, 0.9599, 0.9459]\n",
      "2025-12-20 22:43:27.765309: Epoch time: 138.23 s\n",
      "2025-12-20 22:43:28.393222: \n",
      "2025-12-20 22:43:28.393222: Epoch 977\n",
      "2025-12-20 22:43:28.409221: Current learning rate: 0.00034\n",
      "2025-12-20 22:45:46.487280: train_loss -0.8685\n",
      "2025-12-20 22:45:46.487280: val_loss -0.8943\n",
      "2025-12-20 22:45:46.503049: Pseudo dice [0.9338, 0.9633, 0.955]\n",
      "2025-12-20 22:45:46.503049: Epoch time: 138.09 s\n",
      "2025-12-20 22:45:47.138893: \n",
      "2025-12-20 22:45:47.138893: Epoch 978\n",
      "2025-12-20 22:45:47.152903: Current learning rate: 0.00032\n",
      "2025-12-20 22:48:05.159877: train_loss -0.865\n",
      "2025-12-20 22:48:05.159877: val_loss -0.8915\n",
      "2025-12-20 22:48:05.169575: Pseudo dice [0.9314, 0.9608, 0.9467]\n",
      "2025-12-20 22:48:05.175582: Epoch time: 138.02 s\n",
      "2025-12-20 22:48:05.801365: \n",
      "2025-12-20 22:48:05.801365: Epoch 979\n",
      "2025-12-20 22:48:05.817123: Current learning rate: 0.00031\n",
      "2025-12-20 22:50:23.844872: train_loss -0.8662\n",
      "2025-12-20 22:50:23.844872: val_loss -0.887\n",
      "2025-12-20 22:50:23.844872: Pseudo dice [0.9277, 0.9585, 0.9504]\n",
      "2025-12-20 22:50:23.844872: Epoch time: 138.04 s\n",
      "2025-12-20 22:50:24.479564: \n",
      "2025-12-20 22:50:24.479564: Epoch 980\n",
      "2025-12-20 22:50:24.479564: Current learning rate: 0.0003\n",
      "2025-12-20 22:52:42.714370: train_loss -0.8624\n",
      "2025-12-20 22:52:42.714370: val_loss -0.8834\n",
      "2025-12-20 22:52:42.730249: Pseudo dice [0.9274, 0.955, 0.9509]\n",
      "2025-12-20 22:52:42.730249: Epoch time: 138.23 s\n",
      "2025-12-20 22:52:43.380025: \n",
      "2025-12-20 22:52:43.380025: Epoch 981\n",
      "2025-12-20 22:52:43.380025: Current learning rate: 0.00028\n",
      "2025-12-20 22:55:01.531827: train_loss -0.8703\n",
      "2025-12-20 22:55:01.531827: val_loss -0.8795\n",
      "2025-12-20 22:55:01.531827: Pseudo dice [0.9207, 0.9613, 0.9454]\n",
      "2025-12-20 22:55:01.547808: Epoch time: 138.15 s\n",
      "2025-12-20 22:55:02.357651: \n",
      "2025-12-20 22:55:02.357651: Epoch 982\n",
      "2025-12-20 22:55:02.357651: Current learning rate: 0.00027\n",
      "2025-12-20 22:57:20.182136: train_loss -0.8712\n",
      "2025-12-20 22:57:20.183965: val_loss -0.8944\n",
      "2025-12-20 22:57:20.183965: Pseudo dice [0.9351, 0.9606, 0.9522]\n",
      "2025-12-20 22:57:20.183965: Epoch time: 137.84 s\n",
      "2025-12-20 22:57:20.893358: \n",
      "2025-12-20 22:57:20.893358: Epoch 983\n",
      "2025-12-20 22:57:20.909455: Current learning rate: 0.00026\n",
      "2025-12-20 22:59:38.922091: train_loss -0.8647\n",
      "2025-12-20 22:59:38.922091: val_loss -0.8901\n",
      "2025-12-20 22:59:38.922091: Pseudo dice [0.9334, 0.96, 0.9476]\n",
      "2025-12-20 22:59:38.922091: Epoch time: 138.03 s\n",
      "2025-12-20 22:59:39.555791: \n",
      "2025-12-20 22:59:39.555791: Epoch 984\n",
      "2025-12-20 22:59:39.573496: Current learning rate: 0.00024\n",
      "2025-12-20 23:01:57.804012: train_loss -0.8649\n",
      "2025-12-20 23:01:57.804012: val_loss -0.8879\n",
      "2025-12-20 23:01:57.819766: Pseudo dice [0.9305, 0.9627, 0.9441]\n",
      "2025-12-20 23:01:57.826560: Epoch time: 138.25 s\n",
      "2025-12-20 23:01:58.453666: \n",
      "2025-12-20 23:01:58.453666: Epoch 985\n",
      "2025-12-20 23:01:58.469771: Current learning rate: 0.00023\n",
      "2025-12-20 23:04:16.605479: train_loss -0.8602\n",
      "2025-12-20 23:04:16.605479: val_loss -0.8898\n",
      "2025-12-20 23:04:16.613581: Pseudo dice [0.9327, 0.9624, 0.9404]\n",
      "2025-12-20 23:04:16.618864: Epoch time: 138.15 s\n",
      "2025-12-20 23:04:17.303712: \n",
      "2025-12-20 23:04:17.303712: Epoch 986\n",
      "2025-12-20 23:04:17.321643: Current learning rate: 0.00021\n",
      "2025-12-20 23:06:35.207001: train_loss -0.87\n",
      "2025-12-20 23:06:35.209003: val_loss -0.8841\n",
      "2025-12-20 23:06:35.216875: Pseudo dice [0.9286, 0.9618, 0.94]\n",
      "2025-12-20 23:06:35.224885: Epoch time: 137.9 s\n",
      "2025-12-20 23:06:35.912771: \n",
      "2025-12-20 23:06:35.912771: Epoch 987\n",
      "2025-12-20 23:06:35.912771: Current learning rate: 0.0002\n",
      "2025-12-20 23:08:54.337261: train_loss -0.8647\n",
      "2025-12-20 23:08:54.337261: val_loss -0.8797\n",
      "2025-12-20 23:08:54.353309: Pseudo dice [0.9213, 0.9557, 0.9521]\n",
      "2025-12-20 23:08:54.362561: Epoch time: 138.42 s\n",
      "2025-12-20 23:08:55.241832: \n",
      "2025-12-20 23:08:55.241832: Epoch 988\n",
      "2025-12-20 23:08:55.241832: Current learning rate: 0.00019\n",
      "2025-12-20 23:11:13.470767: train_loss -0.8642\n",
      "2025-12-20 23:11:13.472770: val_loss -0.8931\n",
      "2025-12-20 23:11:13.480330: Pseudo dice [0.9374, 0.9653, 0.9367]\n",
      "2025-12-20 23:11:13.482332: Epoch time: 138.23 s\n",
      "2025-12-20 23:11:14.206691: \n",
      "2025-12-20 23:11:14.206691: Epoch 989\n",
      "2025-12-20 23:11:14.206691: Current learning rate: 0.00017\n",
      "2025-12-20 23:13:32.267002: train_loss -0.8694\n",
      "2025-12-20 23:13:32.267002: val_loss -0.8912\n",
      "2025-12-20 23:13:32.267002: Pseudo dice [0.932, 0.9651, 0.9458]\n",
      "2025-12-20 23:13:32.282674: Epoch time: 138.08 s\n",
      "2025-12-20 23:13:32.900603: \n",
      "2025-12-20 23:13:32.900603: Epoch 990\n",
      "2025-12-20 23:13:32.916365: Current learning rate: 0.00016\n",
      "2025-12-20 23:15:51.628554: train_loss -0.8634\n",
      "2025-12-20 23:15:51.628554: val_loss -0.8897\n",
      "2025-12-20 23:15:51.638660: Pseudo dice [0.932, 0.9611, 0.9493]\n",
      "2025-12-20 23:15:51.646408: Epoch time: 138.73 s\n",
      "2025-12-20 23:15:52.274132: \n",
      "2025-12-20 23:15:52.274132: Epoch 991\n",
      "2025-12-20 23:15:52.274132: Current learning rate: 0.00014\n",
      "2025-12-20 23:18:10.892366: train_loss -0.8619\n",
      "2025-12-20 23:18:10.892366: val_loss -0.8869\n",
      "2025-12-20 23:18:10.902118: Pseudo dice [0.9302, 0.9608, 0.9428]\n",
      "2025-12-20 23:18:10.909333: Epoch time: 138.62 s\n",
      "2025-12-20 23:18:11.547107: \n",
      "2025-12-20 23:18:11.547107: Epoch 992\n",
      "2025-12-20 23:18:11.562887: Current learning rate: 0.00013\n",
      "2025-12-20 23:20:30.257386: train_loss -0.8673\n",
      "2025-12-20 23:20:30.258389: val_loss -0.8934\n",
      "2025-12-20 23:20:30.269109: Pseudo dice [0.9319, 0.9644, 0.9534]\n",
      "2025-12-20 23:20:30.274115: Epoch time: 138.71 s\n",
      "2025-12-20 23:20:30.902943: \n",
      "2025-12-20 23:20:30.902943: Epoch 993\n",
      "2025-12-20 23:20:30.902943: Current learning rate: 0.00011\n",
      "2025-12-20 23:22:49.401038: train_loss -0.8685\n",
      "2025-12-20 23:22:49.401038: val_loss -0.8933\n",
      "2025-12-20 23:22:49.409407: Pseudo dice [0.9345, 0.9623, 0.9483]\n",
      "2025-12-20 23:22:49.416730: Epoch time: 138.5 s\n",
      "2025-12-20 23:22:50.395700: \n",
      "2025-12-20 23:22:50.395700: Epoch 994\n",
      "2025-12-20 23:22:50.409516: Current learning rate: 0.0001\n",
      "2025-12-20 23:25:08.891027: train_loss -0.8679\n",
      "2025-12-20 23:25:08.891027: val_loss -0.8885\n",
      "2025-12-20 23:25:08.906720: Pseudo dice [0.9284, 0.9598, 0.9482]\n",
      "2025-12-20 23:25:08.906720: Epoch time: 138.5 s\n",
      "2025-12-20 23:25:09.539857: \n",
      "2025-12-20 23:25:09.539857: Epoch 995\n",
      "2025-12-20 23:25:09.555714: Current learning rate: 8e-05\n",
      "2025-12-20 23:27:27.615238: train_loss -0.8647\n",
      "2025-12-20 23:27:27.615238: val_loss -0.893\n",
      "2025-12-20 23:27:27.626828: Pseudo dice [0.9355, 0.9621, 0.9491]\n",
      "2025-12-20 23:27:27.634734: Epoch time: 138.08 s\n",
      "2025-12-20 23:27:28.264308: \n",
      "2025-12-20 23:27:28.264308: Epoch 996\n",
      "2025-12-20 23:27:28.280132: Current learning rate: 7e-05\n",
      "2025-12-20 23:29:46.318377: train_loss -0.8675\n",
      "2025-12-20 23:29:46.318377: val_loss -0.8803\n",
      "2025-12-20 23:29:46.318377: Pseudo dice [0.9279, 0.9586, 0.9391]\n",
      "2025-12-20 23:29:46.334049: Epoch time: 138.05 s\n",
      "2025-12-20 23:29:47.061064: \n",
      "2025-12-20 23:29:47.061064: Epoch 997\n",
      "2025-12-20 23:29:47.076965: Current learning rate: 5e-05\n",
      "2025-12-20 23:32:05.198715: train_loss -0.8674\n",
      "2025-12-20 23:32:05.200717: val_loss -0.8916\n",
      "2025-12-20 23:32:05.210204: Pseudo dice [0.9331, 0.9619, 0.9456]\n",
      "2025-12-20 23:32:05.218214: Epoch time: 138.14 s\n",
      "2025-12-20 23:32:05.859814: \n",
      "2025-12-20 23:32:05.859814: Epoch 998\n",
      "2025-12-20 23:32:05.859814: Current learning rate: 4e-05\n",
      "2025-12-20 23:34:24.213797: train_loss -0.8648\n",
      "2025-12-20 23:34:24.215800: val_loss -0.8922\n",
      "2025-12-20 23:34:24.227552: Pseudo dice [0.9327, 0.9624, 0.9495]\n",
      "2025-12-20 23:34:24.235563: Epoch time: 138.37 s\n",
      "2025-12-20 23:34:24.859725: \n",
      "2025-12-20 23:34:24.859725: Epoch 999\n",
      "2025-12-20 23:34:24.877244: Current learning rate: 2e-05\n",
      "2025-12-20 23:36:43.065101: train_loss -0.8637\n",
      "2025-12-20 23:36:43.065101: val_loss -0.8946\n",
      "2025-12-20 23:36:43.065101: Pseudo dice [0.9332, 0.9625, 0.9509]\n",
      "2025-12-20 23:36:43.065101: Epoch time: 138.21 s\n",
      "2025-12-20 23:36:44.503297: Training done.\n",
      "2025-12-20 23:36:44.561985: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-20 23:36:44.561985: The split file contains 5 splits.\n",
      "2025-12-20 23:36:44.578413: Desired fold for training: 3\n",
      "2025-12-20 23:36:44.588584: This split has 400 training and 100 validation cases.\n",
      "2025-12-20 23:36:44.601647: predicting OAS30014_MR_d0196_10\n",
      "2025-12-20 23:36:45.042761: OAS30014_MR_d0196_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:37:05.601218: predicting OAS30014_MR_d0196_2\n",
      "2025-12-20 23:37:05.601218: OAS30014_MR_d0196_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:37:23.247246: predicting OAS30014_MR_d0196_6\n",
      "2025-12-20 23:37:23.247246: OAS30014_MR_d0196_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:37:40.908992: predicting OAS30025_MR_d0210_4\n",
      "2025-12-20 23:37:40.918841: OAS30025_MR_d0210_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:37:58.598761: predicting OAS30025_MR_d0210_5\n",
      "2025-12-20 23:37:58.614868: OAS30025_MR_d0210_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:38:16.296035: predicting OAS30025_MR_d0210_6\n",
      "2025-12-20 23:38:16.306932: OAS30025_MR_d0210_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:38:33.950885: predicting OAS30036_MR_d0059_2\n",
      "2025-12-20 23:38:33.973716: OAS30036_MR_d0059_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:38:51.630678: predicting OAS30036_MR_d0059_7\n",
      "2025-12-20 23:38:51.645168: OAS30036_MR_d0059_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:39:09.300174: predicting OAS30078_MR_d0210_10\n",
      "2025-12-20 23:39:09.309643: OAS30078_MR_d0210_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:39:26.995290: predicting OAS30078_MR_d0210_5\n",
      "2025-12-20 23:39:27.007106: OAS30078_MR_d0210_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:39:44.680505: predicting OAS30078_MR_d0210_9\n",
      "2025-12-20 23:39:44.688017: OAS30078_MR_d0210_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:40:02.320188: predicting OAS30083_MR_d0465_8\n",
      "2025-12-20 23:40:02.340301: OAS30083_MR_d0465_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:40:20.004311: predicting OAS30083_MR_d0465_9\n",
      "2025-12-20 23:40:20.025390: OAS30083_MR_d0465_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:40:37.712203: predicting OAS30087_MR_d0260_6\n",
      "2025-12-20 23:40:37.732171: OAS30087_MR_d0260_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:40:55.444828: predicting OAS30099_MR_d0032_2\n",
      "2025-12-20 23:40:55.453211: OAS30099_MR_d0032_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:41:13.106510: predicting OAS30099_MR_d0032_3\n",
      "2025-12-20 23:41:13.118437: OAS30099_MR_d0032_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:41:30.769097: predicting OAS30099_MR_d0032_4\n",
      "2025-12-20 23:41:30.777854: OAS30099_MR_d0032_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:41:48.478417: predicting OAS30099_MR_d0032_6\n",
      "2025-12-20 23:41:48.488437: OAS30099_MR_d0032_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:42:06.180004: predicting OAS30099_MR_d0032_8\n",
      "2025-12-20 23:42:06.190100: OAS30099_MR_d0032_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:42:23.872886: predicting OAS30102_MR_d0024_2\n",
      "2025-12-20 23:42:23.890070: OAS30102_MR_d0024_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:42:41.534490: predicting OAS30102_MR_d0024_9\n",
      "2025-12-20 23:42:41.556763: OAS30102_MR_d0024_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:42:59.202715: predicting OAS30104_MR_d0328_10\n",
      "2025-12-20 23:42:59.212765: OAS30104_MR_d0328_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:43:16.865997: predicting OAS30104_MR_d0328_6\n",
      "2025-12-20 23:43:16.875973: OAS30104_MR_d0328_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:43:34.571984: predicting OAS30107_MR_d0387_10\n",
      "2025-12-20 23:43:34.586087: OAS30107_MR_d0387_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:43:52.284468: predicting OAS30107_MR_d0387_5\n",
      "2025-12-20 23:43:52.296877: OAS30107_MR_d0387_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:44:09.928830: predicting OAS30107_MR_d0387_6\n",
      "2025-12-20 23:44:09.939789: OAS30107_MR_d0387_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:44:27.580784: predicting OAS30125_MR_d0201_10\n",
      "2025-12-20 23:44:27.595945: OAS30125_MR_d0201_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:44:45.253545: predicting OAS30125_MR_d0201_9\n",
      "2025-12-20 23:44:45.271260: OAS30125_MR_d0201_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:45:02.910935: predicting OAS30134_MR_d0080_10\n",
      "2025-12-20 23:45:02.928969: OAS30134_MR_d0080_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:45:20.565466: predicting OAS30134_MR_d0080_3\n",
      "2025-12-20 23:45:20.581560: OAS30134_MR_d0080_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:45:38.244405: predicting OAS30140_MR_d0172_7\n",
      "2025-12-20 23:45:38.253125: OAS30140_MR_d0172_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:45:55.942660: predicting OAS30147_MR_d0048_6\n",
      "2025-12-20 23:45:55.964603: OAS30147_MR_d0048_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:46:13.604425: predicting OAS30165_MR_d1763_10\n",
      "2025-12-20 23:46:13.621959: OAS30165_MR_d1763_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:46:31.309064: predicting OAS30165_MR_d1763_7\n",
      "2025-12-20 23:46:31.315960: OAS30165_MR_d1763_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:46:49.027706: predicting OAS30167_MR_d0111_2\n",
      "2025-12-20 23:46:49.041305: OAS30167_MR_d0111_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:47:06.673852: predicting OAS30167_MR_d0111_3\n",
      "2025-12-20 23:47:06.683151: OAS30167_MR_d0111_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:47:24.399007: predicting OAS30167_MR_d0111_4\n",
      "2025-12-20 23:47:24.407022: OAS30167_MR_d0111_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:47:42.099031: predicting OAS30167_MR_d0111_8\n",
      "2025-12-20 23:47:42.104932: OAS30167_MR_d0111_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:47:59.732317: predicting OAS30167_MR_d0111_9\n",
      "2025-12-20 23:47:59.755139: OAS30167_MR_d0111_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:48:17.407233: predicting OAS30176_MR_d0000_1\n",
      "2025-12-20 23:48:17.416248: OAS30176_MR_d0000_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:48:35.087827: predicting OAS30176_MR_d0000_5\n",
      "2025-12-20 23:48:35.103510: OAS30176_MR_d0000_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:48:52.773169: predicting OAS30195_MR_d1596_10\n",
      "2025-12-20 23:48:52.793083: OAS30195_MR_d1596_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:49:10.484615: predicting OAS30195_MR_d1596_2\n",
      "2025-12-20 23:49:10.494636: OAS30195_MR_d1596_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:49:28.162644: predicting OAS30195_MR_d1596_4\n",
      "2025-12-20 23:49:28.171481: OAS30195_MR_d1596_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:49:45.811965: predicting OAS30226_MR_d0183_4\n",
      "2025-12-20 23:49:45.822317: OAS30226_MR_d0183_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:50:03.469375: predicting OAS30226_MR_d0183_5\n",
      "2025-12-20 23:50:03.487423: OAS30226_MR_d0183_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:50:21.153445: predicting OAS30226_MR_d0183_8\n",
      "2025-12-20 23:50:21.178108: OAS30226_MR_d0183_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:50:38.874767: predicting OAS30234_MR_d2098_2\n",
      "2025-12-20 23:50:38.882818: OAS30234_MR_d2098_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:50:56.540269: predicting OAS30234_MR_d2098_3\n",
      "2025-12-20 23:50:56.549178: OAS30234_MR_d2098_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:51:14.227346: predicting OAS30234_MR_d2098_6\n",
      "2025-12-20 23:51:14.242266: OAS30234_MR_d2098_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:51:31.882094: predicting OAS30234_MR_d2098_8\n",
      "2025-12-20 23:51:31.902913: OAS30234_MR_d2098_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:51:49.535115: predicting OAS30238_MR_d0037_2\n",
      "2025-12-20 23:51:49.552952: OAS30238_MR_d0037_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:52:07.242434: predicting OAS30238_MR_d0037_5\n",
      "2025-12-20 23:52:07.251928: OAS30238_MR_d0037_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:52:24.905508: predicting OAS30262_MR_d0037_1\n",
      "2025-12-20 23:52:24.917853: OAS30262_MR_d0037_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:52:42.605645: predicting OAS30262_MR_d0037_4\n",
      "2025-12-20 23:52:42.615765: OAS30262_MR_d0037_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:53:00.255426: predicting OAS30262_MR_d0037_8\n",
      "2025-12-20 23:53:00.267357: OAS30262_MR_d0037_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:53:17.920572: predicting OAS30262_MR_d0037_9\n",
      "2025-12-20 23:53:17.940011: OAS30262_MR_d0037_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:53:35.590409: predicting OAS30274_MR_d3332_7\n",
      "2025-12-20 23:53:35.610080: OAS30274_MR_d3332_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:53:53.292549: predicting OAS30292_MR_d0165_1\n",
      "2025-12-20 23:53:53.312272: OAS30292_MR_d0165_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:54:10.965639: predicting OAS30292_MR_d0165_4\n",
      "2025-12-20 23:54:10.987304: OAS30292_MR_d0165_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:54:28.681004: predicting OAS30297_MR_d1712_3\n",
      "2025-12-20 23:54:28.696620: OAS30297_MR_d1712_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:54:46.360893: predicting OAS30300_MR_d0100_10\n",
      "2025-12-20 23:54:46.384767: OAS30300_MR_d0100_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:55:04.098075: predicting OAS30300_MR_d0100_7\n",
      "2025-12-20 23:55:04.106243: OAS30300_MR_d0100_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:55:21.791752: predicting OAS30302_MR_d0262_6\n",
      "2025-12-20 23:55:21.807695: OAS30302_MR_d0262_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:55:39.452530: predicting OAS30302_MR_d0262_8\n",
      "2025-12-20 23:55:39.474714: OAS30302_MR_d0262_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:55:57.165394: predicting OAS30302_MR_d0262_9\n",
      "2025-12-20 23:55:57.176591: OAS30302_MR_d0262_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:56:14.842229: predicting OAS30306_MR_d0028_10\n",
      "2025-12-20 23:56:14.863927: OAS30306_MR_d0028_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:56:32.554965: predicting OAS30306_MR_d0028_5\n",
      "2025-12-20 23:56:32.566195: OAS30306_MR_d0028_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:56:50.196124: predicting OAS30321_MR_d3003_3\n",
      "2025-12-20 23:56:50.216301: OAS30321_MR_d3003_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:57:07.906390: predicting OAS30325_MR_d0032_1\n",
      "2025-12-20 23:57:07.926118: OAS30325_MR_d0032_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:57:25.632968: predicting OAS30325_MR_d0032_6\n",
      "2025-12-20 23:57:25.644077: OAS30325_MR_d0032_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:57:43.274571: predicting OAS30325_MR_d0032_9\n",
      "2025-12-20 23:57:43.294528: OAS30325_MR_d0032_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:58:01.009813: predicting OAS30343_MR_d4178_8\n",
      "2025-12-20 23:58:01.022568: OAS30343_MR_d4178_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:58:18.687852: predicting OAS30349_MR_d0699_6\n",
      "2025-12-20 23:58:18.697263: OAS30349_MR_d0699_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:58:36.356469: predicting OAS30350_MR_d0018_3\n",
      "2025-12-20 23:58:36.367715: OAS30350_MR_d0018_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:58:54.046596: predicting OAS30350_MR_d0018_7\n",
      "2025-12-20 23:58:54.070135: OAS30350_MR_d0018_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:59:11.722733: predicting OAS30352_MR_d0099_1\n",
      "2025-12-20 23:59:11.740082: OAS30352_MR_d0099_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:59:29.407086: predicting OAS30352_MR_d0099_3\n",
      "2025-12-20 23:59:29.417286: OAS30352_MR_d0099_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-20 23:59:47.108912: predicting OAS30352_MR_d0099_5\n",
      "2025-12-20 23:59:47.122899: OAS30352_MR_d0099_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:00:04.761037: predicting OAS30354_MR_d0056_4\n",
      "2025-12-21 00:00:04.778990: OAS30354_MR_d0056_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:00:22.457078: predicting OAS30354_MR_d0056_6\n",
      "2025-12-21 00:00:22.476323: OAS30354_MR_d0056_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:00:40.189420: predicting OAS30354_MR_d0056_7\n",
      "2025-12-21 00:00:40.202992: OAS30354_MR_d0056_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:00:57.876886: predicting OAS30355_MR_d0048_10\n",
      "2025-12-21 00:00:57.886424: OAS30355_MR_d0048_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:01:15.573910: predicting OAS30361_MR_d1457_2\n",
      "2025-12-21 00:01:15.596531: OAS30361_MR_d1457_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:01:33.249227: predicting OAS30361_MR_d1457_5\n",
      "2025-12-21 00:01:33.258104: OAS30361_MR_d1457_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:01:50.942750: predicting OAS30361_MR_d1457_9\n",
      "2025-12-21 00:01:50.968311: OAS30361_MR_d1457_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:02:08.634115: predicting OAS30367_MR_d1540_10\n",
      "2025-12-21 00:02:08.647120: OAS30367_MR_d1540_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:02:26.303789: predicting OAS30367_MR_d1540_7\n",
      "2025-12-21 00:02:26.321635: OAS30367_MR_d1540_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:02:43.967072: predicting OAS30369_MR_d4058_3\n",
      "2025-12-21 00:02:43.982732: OAS30369_MR_d4058_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:03:01.650083: predicting OAS30369_MR_d4058_6\n",
      "2025-12-21 00:03:01.667868: OAS30369_MR_d4058_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:03:19.310352: predicting OAS30369_MR_d4058_7\n",
      "2025-12-21 00:03:19.322136: OAS30369_MR_d4058_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:03:36.998822: predicting OAS30369_MR_d4058_8\n",
      "2025-12-21 00:03:37.007557: OAS30369_MR_d4058_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:03:54.666911: predicting OAS30373_MR_d1211_3\n",
      "2025-12-21 00:03:54.674663: OAS30373_MR_d1211_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:04:12.327012: predicting OAS30373_MR_d1211_7\n",
      "2025-12-21 00:04:12.346948: OAS30373_MR_d1211_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:04:30.009703: predicting OAS30379_MR_d2106_5\n",
      "2025-12-21 00:04:30.025722: OAS30379_MR_d2106_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:04:47.667935: predicting OAS30380_MR_d3446_1\n",
      "2025-12-21 00:04:47.689995: OAS30380_MR_d3446_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:05:05.379600: predicting OAS30380_MR_d3446_3\n",
      "2025-12-21 00:05:05.390335: OAS30380_MR_d3446_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:05:23.030391: predicting OAS30380_MR_d3446_9\n",
      "2025-12-21 00:05:23.048133: OAS30380_MR_d3446_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:05:40.725904: predicting OAS30383_MR_d0134_8\n",
      "2025-12-21 00:05:40.741965: OAS30383_MR_d0134_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:05:58.423394: predicting OAS30388_MR_d0073_2\n",
      "2025-12-21 00:05:58.440717: OAS30388_MR_d0073_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-21 00:06:41.531317: Validation complete\n",
      "2025-12-21 00:06:41.531317: Mean Validation Dice:  0.9355553095304664\n"
     ]
    }
   ],
   "source": [
    "#Train nnU-Net on fold 3\n",
    "!nnUNetv2_train 500 3d_lowres 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789813fc-980b-4395-8706-4737bfd365cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-12-21 00:07:01.388072: do_dummy_2d_data_aug: False\n",
      "2025-12-21 00:07:01.403735: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-21 00:07:01.403735: The split file contains 5 splits.\n",
      "2025-12-21 00:07:01.403735: Desired fold for training: 4\n",
      "2025-12-21 00:07:01.403735: This split has 400 training and 100 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2025-12-21 00:07:34.389931: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_lowres\n",
      " {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [202, 202, 202], 'spacing': [1.2667700813876164, 1.2667700813876164, 1.2667700813876164], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset500_MRI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [256, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.422696590423584, 'median': 0.4194243550300598, 'min': 0.0027002037968486547, 'percentile_00_5': 0.05628390982747078, 'percentile_99_5': 0.8565635681152344, 'std': 0.19347868859767914}}} \n",
      "\n",
      "2025-12-21 00:07:34.389931: unpacking dataset...\n",
      "2025-12-21 00:07:34.968829: unpacking done...\n",
      "2025-12-21 00:07:34.968829: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-12-21 00:07:35.007991: \n",
      "2025-12-21 00:07:35.007991: Epoch 0\n",
      "2025-12-21 00:07:35.007991: Current learning rate: 0.01\n",
      "2025-12-21 00:10:03.804550: train_loss 0.2137\n",
      "2025-12-21 00:10:03.804550: val_loss 0.0404\n",
      "2025-12-21 00:10:03.806553: Pseudo dice [0.4974, 0.5511, 0.0]\n",
      "2025-12-21 00:10:03.806553: Epoch time: 148.81 s\n",
      "2025-12-21 00:10:03.806553: Yayy! New best EMA pseudo Dice: 0.3495\n",
      "2025-12-21 00:10:04.675283: \n",
      "2025-12-21 00:10:04.675283: Epoch 1\n",
      "2025-12-21 00:10:04.675283: Current learning rate: 0.00999\n",
      "2025-12-21 00:12:22.884391: train_loss -0.0337\n",
      "2025-12-21 00:12:22.884391: val_loss -0.0878\n",
      "2025-12-21 00:12:22.886132: Pseudo dice [0.5355, 0.5695, 0.3855]\n",
      "2025-12-21 00:12:22.886132: Epoch time: 138.21 s\n",
      "2025-12-21 00:12:22.886132: Yayy! New best EMA pseudo Dice: 0.3643\n",
      "2025-12-21 00:12:23.898476: \n",
      "2025-12-21 00:12:23.898476: Epoch 2\n",
      "2025-12-21 00:12:23.898476: Current learning rate: 0.00998\n",
      "2025-12-21 00:14:42.343448: train_loss -0.1303\n",
      "2025-12-21 00:14:42.343448: val_loss -0.1565\n",
      "2025-12-21 00:14:42.345454: Pseudo dice [0.5659, 0.5627, 0.4579]\n",
      "2025-12-21 00:14:42.347462: Epoch time: 138.44 s\n",
      "2025-12-21 00:14:42.349467: Yayy! New best EMA pseudo Dice: 0.3807\n",
      "2025-12-21 00:14:43.293688: \n",
      "2025-12-21 00:14:43.293688: Epoch 3\n",
      "2025-12-21 00:14:43.293688: Current learning rate: 0.00997\n",
      "2025-12-21 00:17:01.485509: train_loss -0.1977\n",
      "2025-12-21 00:17:01.485509: val_loss -0.1733\n",
      "2025-12-21 00:17:01.487511: Pseudo dice [0.5757, 0.6109, 0.4575]\n",
      "2025-12-21 00:17:01.487511: Epoch time: 138.19 s\n",
      "2025-12-21 00:17:01.489513: Yayy! New best EMA pseudo Dice: 0.3974\n",
      "2025-12-21 00:17:02.558919: \n",
      "2025-12-21 00:17:02.558919: Epoch 4\n",
      "2025-12-21 00:17:02.558919: Current learning rate: 0.00996\n",
      "2025-12-21 00:19:20.754849: train_loss -0.2553\n",
      "2025-12-21 00:19:20.754849: val_loss -0.2672\n",
      "2025-12-21 00:19:20.754849: Pseudo dice [0.6161, 0.6664, 0.5643]\n",
      "2025-12-21 00:19:20.754849: Epoch time: 138.2 s\n",
      "2025-12-21 00:19:20.754849: Yayy! New best EMA pseudo Dice: 0.4193\n",
      "2025-12-21 00:19:21.657997: \n",
      "2025-12-21 00:19:21.657997: Epoch 5\n",
      "2025-12-21 00:19:21.657997: Current learning rate: 0.00995\n",
      "2025-12-21 00:21:39.537123: train_loss -0.2987\n",
      "2025-12-21 00:21:39.537123: val_loss -0.3387\n",
      "2025-12-21 00:21:39.537123: Pseudo dice [0.6602, 0.6868, 0.621]\n",
      "2025-12-21 00:21:39.537123: Epoch time: 137.88 s\n",
      "2025-12-21 00:21:39.537123: Yayy! New best EMA pseudo Dice: 0.4429\n",
      "2025-12-21 00:21:40.422580: \n",
      "2025-12-21 00:21:40.422580: Epoch 6\n",
      "2025-12-21 00:21:40.422580: Current learning rate: 0.00995\n",
      "2025-12-21 00:23:58.437121: train_loss -0.3725\n",
      "2025-12-21 00:23:58.439125: val_loss -0.3923\n",
      "2025-12-21 00:23:58.439125: Pseudo dice [0.6819, 0.7541, 0.6276]\n",
      "2025-12-21 00:23:58.439125: Epoch time: 138.01 s\n",
      "2025-12-21 00:23:58.439125: Yayy! New best EMA pseudo Dice: 0.4674\n",
      "2025-12-21 00:23:59.688591: \n",
      "2025-12-21 00:23:59.688591: Epoch 7\n",
      "2025-12-21 00:23:59.688591: Current learning rate: 0.00994\n",
      "2025-12-21 00:26:17.682140: train_loss -0.4071\n",
      "2025-12-21 00:26:17.684143: val_loss -0.4384\n",
      "2025-12-21 00:26:17.684143: Pseudo dice [0.7249, 0.769, 0.6648]\n",
      "2025-12-21 00:26:17.684143: Epoch time: 138.01 s\n",
      "2025-12-21 00:26:17.686409: Yayy! New best EMA pseudo Dice: 0.4926\n",
      "2025-12-21 00:26:18.598138: \n",
      "2025-12-21 00:26:18.598138: Epoch 8\n",
      "2025-12-21 00:26:18.598138: Current learning rate: 0.00993\n",
      "2025-12-21 00:28:36.732919: train_loss -0.45\n",
      "2025-12-21 00:28:36.732919: val_loss -0.4811\n",
      "2025-12-21 00:28:36.732919: Pseudo dice [0.7517, 0.8094, 0.6794]\n",
      "2025-12-21 00:28:36.732919: Epoch time: 138.13 s\n",
      "2025-12-21 00:28:36.732919: Yayy! New best EMA pseudo Dice: 0.5181\n",
      "2025-12-21 00:28:37.653718: \n",
      "2025-12-21 00:28:37.653718: Epoch 9\n",
      "2025-12-21 00:28:37.653718: Current learning rate: 0.00992\n",
      "2025-12-21 00:30:55.391314: train_loss -0.4992\n",
      "2025-12-21 00:30:55.391314: val_loss -0.5199\n",
      "2025-12-21 00:30:55.393054: Pseudo dice [0.7659, 0.8293, 0.7228]\n",
      "2025-12-21 00:30:55.393054: Epoch time: 137.74 s\n",
      "2025-12-21 00:30:55.395056: Yayy! New best EMA pseudo Dice: 0.5435\n",
      "2025-12-21 00:30:56.454962: \n",
      "2025-12-21 00:30:56.454962: Epoch 10\n",
      "2025-12-21 00:30:56.456965: Current learning rate: 0.00991\n",
      "2025-12-21 00:33:14.256952: train_loss -0.5279\n",
      "2025-12-21 00:33:14.258955: val_loss -0.5469\n",
      "2025-12-21 00:33:14.259956: Pseudo dice [0.803, 0.8419, 0.7075]\n",
      "2025-12-21 00:33:14.259956: Epoch time: 137.8 s\n",
      "2025-12-21 00:33:14.259956: Yayy! New best EMA pseudo Dice: 0.5676\n",
      "2025-12-21 00:33:15.161855: \n",
      "2025-12-21 00:33:15.161855: Epoch 11\n",
      "2025-12-21 00:33:15.161855: Current learning rate: 0.0099\n",
      "2025-12-21 00:35:33.174048: train_loss -0.5248\n",
      "2025-12-21 00:35:33.174048: val_loss -0.5583\n",
      "2025-12-21 00:35:33.176051: Pseudo dice [0.7976, 0.8535, 0.7248]\n",
      "2025-12-21 00:35:33.176051: Epoch time: 138.01 s\n",
      "2025-12-21 00:35:33.178053: Yayy! New best EMA pseudo Dice: 0.59\n",
      "2025-12-21 00:35:34.050233: \n",
      "2025-12-21 00:35:34.050233: Epoch 12\n",
      "2025-12-21 00:35:34.050233: Current learning rate: 0.00989\n",
      "2025-12-21 00:37:51.947515: train_loss -0.5542\n",
      "2025-12-21 00:37:51.947515: val_loss -0.5922\n",
      "2025-12-21 00:37:51.949518: Pseudo dice [0.8112, 0.8583, 0.7538]\n",
      "2025-12-21 00:37:51.949518: Epoch time: 137.9 s\n",
      "2025-12-21 00:37:51.949518: Yayy! New best EMA pseudo Dice: 0.6118\n",
      "2025-12-21 00:37:53.191362: \n",
      "2025-12-21 00:37:53.191362: Epoch 13\n",
      "2025-12-21 00:37:53.191362: Current learning rate: 0.00988\n",
      "2025-12-21 00:40:11.132426: train_loss -0.5733\n",
      "2025-12-21 00:40:11.132426: val_loss -0.5835\n",
      "2025-12-21 00:40:11.134428: Pseudo dice [0.8002, 0.8538, 0.7789]\n",
      "2025-12-21 00:40:11.135431: Epoch time: 137.96 s\n",
      "2025-12-21 00:40:11.135431: Yayy! New best EMA pseudo Dice: 0.6317\n",
      "2025-12-21 00:40:12.045023: \n",
      "2025-12-21 00:40:12.045023: Epoch 14\n",
      "2025-12-21 00:40:12.047025: Current learning rate: 0.00987\n",
      "2025-12-21 00:42:29.912091: train_loss -0.6031\n",
      "2025-12-21 00:42:29.912091: val_loss -0.6008\n",
      "2025-12-21 00:42:29.914094: Pseudo dice [0.8088, 0.8699, 0.7756]\n",
      "2025-12-21 00:42:29.914094: Epoch time: 137.87 s\n",
      "2025-12-21 00:42:29.914094: Yayy! New best EMA pseudo Dice: 0.6503\n",
      "2025-12-21 00:42:30.832515: \n",
      "2025-12-21 00:42:30.832515: Epoch 15\n",
      "2025-12-21 00:42:30.834517: Current learning rate: 0.00986\n",
      "2025-12-21 00:44:48.858289: train_loss -0.6063\n",
      "2025-12-21 00:44:48.858289: val_loss -0.6228\n",
      "2025-12-21 00:44:48.860291: Pseudo dice [0.8232, 0.8727, 0.7982]\n",
      "2025-12-21 00:44:48.860291: Epoch time: 138.03 s\n",
      "2025-12-21 00:44:48.862294: Yayy! New best EMA pseudo Dice: 0.6684\n",
      "2025-12-21 00:44:49.915221: \n",
      "2025-12-21 00:44:49.915221: Epoch 16\n",
      "2025-12-21 00:44:49.915221: Current learning rate: 0.00986\n",
      "2025-12-21 00:47:07.754199: train_loss -0.6233\n",
      "2025-12-21 00:47:07.754199: val_loss -0.6225\n",
      "2025-12-21 00:47:07.756202: Pseudo dice [0.8295, 0.8729, 0.7675]\n",
      "2025-12-21 00:47:07.758204: Epoch time: 137.84 s\n",
      "2025-12-21 00:47:07.758204: Yayy! New best EMA pseudo Dice: 0.6839\n",
      "2025-12-21 00:47:08.658463: \n",
      "2025-12-21 00:47:08.658463: Epoch 17\n",
      "2025-12-21 00:47:08.658463: Current learning rate: 0.00985\n",
      "2025-12-21 00:49:26.444012: train_loss -0.6343\n",
      "2025-12-21 00:49:26.444012: val_loss -0.6364\n",
      "2025-12-21 00:49:26.444012: Pseudo dice [0.8339, 0.8803, 0.7975]\n",
      "2025-12-21 00:49:26.444012: Epoch time: 137.79 s\n",
      "2025-12-21 00:49:26.444012: Yayy! New best EMA pseudo Dice: 0.6993\n",
      "2025-12-21 00:49:27.357311: \n",
      "2025-12-21 00:49:27.359250: Epoch 18\n",
      "2025-12-21 00:49:27.359250: Current learning rate: 0.00984\n",
      "2025-12-21 00:51:45.215656: train_loss -0.6405\n",
      "2025-12-21 00:51:45.215656: val_loss -0.6461\n",
      "2025-12-21 00:51:45.215656: Pseudo dice [0.8356, 0.8798, 0.808]\n",
      "2025-12-21 00:51:45.215656: Epoch time: 137.86 s\n",
      "2025-12-21 00:51:45.215656: Yayy! New best EMA pseudo Dice: 0.7135\n",
      "2025-12-21 00:51:46.454872: \n",
      "2025-12-21 00:51:46.456875: Epoch 19\n",
      "2025-12-21 00:51:46.456875: Current learning rate: 0.00983\n",
      "2025-12-21 00:54:04.470508: train_loss -0.6545\n",
      "2025-12-21 00:54:04.470508: val_loss -0.6611\n",
      "2025-12-21 00:54:04.472511: Pseudo dice [0.8414, 0.8844, 0.8257]\n",
      "2025-12-21 00:54:04.472511: Epoch time: 138.02 s\n",
      "2025-12-21 00:54:04.474512: Yayy! New best EMA pseudo Dice: 0.7272\n",
      "2025-12-21 00:54:05.363715: \n",
      "2025-12-21 00:54:05.363715: Epoch 20\n",
      "2025-12-21 00:54:05.366158: Current learning rate: 0.00982\n",
      "2025-12-21 00:56:23.223639: train_loss -0.6639\n",
      "2025-12-21 00:56:23.223639: val_loss -0.6632\n",
      "2025-12-21 00:56:23.239681: Pseudo dice [0.8452, 0.8855, 0.8122]\n",
      "2025-12-21 00:56:23.239681: Epoch time: 137.86 s\n",
      "2025-12-21 00:56:23.239681: Yayy! New best EMA pseudo Dice: 0.7392\n",
      "2025-12-21 00:56:24.149620: \n",
      "2025-12-21 00:56:24.149620: Epoch 21\n",
      "2025-12-21 00:56:24.149620: Current learning rate: 0.00981\n",
      "2025-12-21 00:58:41.954263: train_loss -0.6725\n",
      "2025-12-21 00:58:41.956265: val_loss -0.6757\n",
      "2025-12-21 00:58:41.956265: Pseudo dice [0.8516, 0.8917, 0.797]\n",
      "2025-12-21 00:58:41.958268: Epoch time: 137.8 s\n",
      "2025-12-21 00:58:41.958268: Yayy! New best EMA pseudo Dice: 0.75\n",
      "2025-12-21 00:58:42.962697: \n",
      "2025-12-21 00:58:42.962697: Epoch 22\n",
      "2025-12-21 00:58:42.962697: Current learning rate: 0.0098\n",
      "2025-12-21 01:01:01.022724: train_loss -0.6775\n",
      "2025-12-21 01:01:01.024727: val_loss -0.677\n",
      "2025-12-21 01:01:01.024727: Pseudo dice [0.8476, 0.8919, 0.8321]\n",
      "2025-12-21 01:01:01.026730: Epoch time: 138.06 s\n",
      "2025-12-21 01:01:01.026730: Yayy! New best EMA pseudo Dice: 0.7607\n",
      "2025-12-21 01:01:01.910780: \n",
      "2025-12-21 01:01:01.910780: Epoch 23\n",
      "2025-12-21 01:01:01.910780: Current learning rate: 0.00979\n",
      "2025-12-21 01:03:19.862274: train_loss -0.6805\n",
      "2025-12-21 01:03:19.864277: val_loss -0.6954\n",
      "2025-12-21 01:03:19.864277: Pseudo dice [0.8591, 0.9, 0.8403]\n",
      "2025-12-21 01:03:19.866199: Epoch time: 137.95 s\n",
      "2025-12-21 01:03:19.866199: Yayy! New best EMA pseudo Dice: 0.7713\n",
      "2025-12-21 01:03:20.741822: \n",
      "2025-12-21 01:03:20.741822: Epoch 24\n",
      "2025-12-21 01:03:20.741822: Current learning rate: 0.00978\n",
      "2025-12-21 01:05:38.855836: train_loss -0.6896\n",
      "2025-12-21 01:05:38.855836: val_loss -0.677\n",
      "2025-12-21 01:05:38.857576: Pseudo dice [0.8469, 0.8899, 0.8314]\n",
      "2025-12-21 01:05:38.859579: Epoch time: 138.11 s\n",
      "2025-12-21 01:05:38.859579: Yayy! New best EMA pseudo Dice: 0.7797\n",
      "2025-12-21 01:05:40.086702: \n",
      "2025-12-21 01:05:40.086702: Epoch 25\n",
      "2025-12-21 01:05:40.086702: Current learning rate: 0.00977\n",
      "2025-12-21 01:07:57.976605: train_loss -0.6946\n",
      "2025-12-21 01:07:57.978607: val_loss -0.7113\n",
      "2025-12-21 01:07:57.980609: Pseudo dice [0.871, 0.9106, 0.8326]\n",
      "2025-12-21 01:07:57.980609: Epoch time: 137.89 s\n",
      "2025-12-21 01:07:57.980609: Yayy! New best EMA pseudo Dice: 0.7889\n",
      "2025-12-21 01:07:58.855897: \n",
      "2025-12-21 01:07:58.855897: Epoch 26\n",
      "2025-12-21 01:07:58.855897: Current learning rate: 0.00977\n",
      "2025-12-21 01:10:17.179848: train_loss -0.7004\n",
      "2025-12-21 01:10:17.179848: val_loss -0.7186\n",
      "2025-12-21 01:10:17.181853: Pseudo dice [0.8693, 0.9103, 0.8425]\n",
      "2025-12-21 01:10:17.181853: Epoch time: 138.33 s\n",
      "2025-12-21 01:10:17.183855: Yayy! New best EMA pseudo Dice: 0.7974\n",
      "2025-12-21 01:10:18.060783: \n",
      "2025-12-21 01:10:18.060783: Epoch 27\n",
      "2025-12-21 01:10:18.060783: Current learning rate: 0.00976\n",
      "2025-12-21 01:12:36.131841: train_loss -0.7075\n",
      "2025-12-21 01:12:36.131841: val_loss -0.7131\n",
      "2025-12-21 01:12:36.133843: Pseudo dice [0.8667, 0.9026, 0.8515]\n",
      "2025-12-21 01:12:36.133843: Epoch time: 138.07 s\n",
      "2025-12-21 01:12:36.135845: Yayy! New best EMA pseudo Dice: 0.805\n",
      "2025-12-21 01:12:37.183084: \n",
      "2025-12-21 01:12:37.183084: Epoch 28\n",
      "2025-12-21 01:12:37.183084: Current learning rate: 0.00975\n",
      "2025-12-21 01:14:55.188346: train_loss -0.7111\n",
      "2025-12-21 01:14:55.190348: val_loss -0.7345\n",
      "2025-12-21 01:14:55.192350: Pseudo dice [0.8713, 0.9154, 0.8565]\n",
      "2025-12-21 01:14:55.192350: Epoch time: 138.02 s\n",
      "2025-12-21 01:14:55.194352: Yayy! New best EMA pseudo Dice: 0.8126\n",
      "2025-12-21 01:14:56.080967: \n",
      "2025-12-21 01:14:56.080967: Epoch 29\n",
      "2025-12-21 01:14:56.080967: Current learning rate: 0.00974\n",
      "2025-12-21 01:17:13.959143: train_loss -0.7066\n",
      "2025-12-21 01:17:13.959143: val_loss -0.6734\n",
      "2025-12-21 01:17:13.959143: Pseudo dice [0.8367, 0.8814, 0.8365]\n",
      "2025-12-21 01:17:13.975061: Epoch time: 137.88 s\n",
      "2025-12-21 01:17:13.975061: Yayy! New best EMA pseudo Dice: 0.8165\n",
      "2025-12-21 01:17:14.885704: \n",
      "2025-12-21 01:17:14.885704: Epoch 30\n",
      "2025-12-21 01:17:14.885704: Current learning rate: 0.00973\n",
      "2025-12-21 01:19:32.661488: train_loss -0.724\n",
      "2025-12-21 01:19:32.661488: val_loss -0.703\n",
      "2025-12-21 01:19:32.677216: Pseudo dice [0.8452, 0.8917, 0.8614]\n",
      "2025-12-21 01:19:32.677216: Epoch time: 137.78 s\n",
      "2025-12-21 01:19:32.677216: Yayy! New best EMA pseudo Dice: 0.8215\n",
      "2025-12-21 01:19:33.752834: \n",
      "2025-12-21 01:19:33.752834: Epoch 31\n",
      "2025-12-21 01:19:33.753839: Current learning rate: 0.00972\n",
      "2025-12-21 01:21:51.529657: train_loss -0.7243\n",
      "2025-12-21 01:21:51.531660: val_loss -0.727\n",
      "2025-12-21 01:21:51.531660: Pseudo dice [0.8702, 0.9096, 0.8564]\n",
      "2025-12-21 01:21:51.531660: Epoch time: 137.79 s\n",
      "2025-12-21 01:21:51.531660: Yayy! New best EMA pseudo Dice: 0.8272\n",
      "2025-12-21 01:21:52.416843: \n",
      "2025-12-21 01:21:52.416843: Epoch 32\n",
      "2025-12-21 01:21:52.416843: Current learning rate: 0.00971\n",
      "2025-12-21 01:24:10.447987: train_loss -0.7258\n",
      "2025-12-21 01:24:10.447987: val_loss -0.7087\n",
      "2025-12-21 01:24:10.449989: Pseudo dice [0.8589, 0.8963, 0.8553]\n",
      "2025-12-21 01:24:10.450990: Epoch time: 138.03 s\n",
      "2025-12-21 01:24:10.450990: Yayy! New best EMA pseudo Dice: 0.8315\n",
      "2025-12-21 01:24:11.338925: \n",
      "2025-12-21 01:24:11.338925: Epoch 33\n",
      "2025-12-21 01:24:11.354705: Current learning rate: 0.0097\n",
      "2025-12-21 01:26:29.088120: train_loss -0.7352\n",
      "2025-12-21 01:26:29.088120: val_loss -0.7324\n",
      "2025-12-21 01:26:29.088120: Pseudo dice [0.8689, 0.9071, 0.8607]\n",
      "2025-12-21 01:26:29.088120: Epoch time: 137.75 s\n",
      "2025-12-21 01:26:29.088120: Yayy! New best EMA pseudo Dice: 0.8362\n",
      "2025-12-21 01:26:30.157952: \n",
      "2025-12-21 01:26:30.157952: Epoch 34\n",
      "2025-12-21 01:26:30.157952: Current learning rate: 0.00969\n",
      "2025-12-21 01:28:48.112607: train_loss -0.7344\n",
      "2025-12-21 01:28:48.114609: val_loss -0.7411\n",
      "2025-12-21 01:28:48.116611: Pseudo dice [0.8776, 0.9206, 0.8604]\n",
      "2025-12-21 01:28:48.116611: Epoch time: 137.96 s\n",
      "2025-12-21 01:28:48.116611: Yayy! New best EMA pseudo Dice: 0.8412\n",
      "2025-12-21 01:28:49.001624: \n",
      "2025-12-21 01:28:49.001624: Epoch 35\n",
      "2025-12-21 01:28:49.001624: Current learning rate: 0.00968\n",
      "2025-12-21 01:31:06.847506: train_loss -0.7359\n",
      "2025-12-21 01:31:06.849509: val_loss -0.7029\n",
      "2025-12-21 01:31:06.849509: Pseudo dice [0.8445, 0.8912, 0.8699]\n",
      "2025-12-21 01:31:06.851512: Epoch time: 137.85 s\n",
      "2025-12-21 01:31:06.851512: Yayy! New best EMA pseudo Dice: 0.844\n",
      "2025-12-21 01:31:07.922681: \n",
      "2025-12-21 01:31:07.922681: Epoch 36\n",
      "2025-12-21 01:31:07.922681: Current learning rate: 0.00968\n",
      "2025-12-21 01:33:25.747952: train_loss -0.7428\n",
      "2025-12-21 01:33:25.747952: val_loss -0.778\n",
      "2025-12-21 01:33:25.749955: Pseudo dice [0.9016, 0.931, 0.8774]\n",
      "2025-12-21 01:33:25.749955: Epoch time: 137.83 s\n",
      "2025-12-21 01:33:25.751958: Yayy! New best EMA pseudo Dice: 0.8499\n",
      "2025-12-21 01:33:26.815282: \n",
      "2025-12-21 01:33:26.815282: Epoch 37\n",
      "2025-12-21 01:33:26.817285: Current learning rate: 0.00967\n",
      "2025-12-21 01:35:44.730342: train_loss -0.7422\n",
      "2025-12-21 01:35:44.730342: val_loss -0.756\n",
      "2025-12-21 01:35:44.734085: Pseudo dice [0.887, 0.9181, 0.8661]\n",
      "2025-12-21 01:35:44.734085: Epoch time: 137.92 s\n",
      "2025-12-21 01:35:44.736087: Yayy! New best EMA pseudo Dice: 0.854\n",
      "2025-12-21 01:35:45.638263: \n",
      "2025-12-21 01:35:45.638263: Epoch 38\n",
      "2025-12-21 01:35:45.638263: Current learning rate: 0.00966\n",
      "2025-12-21 01:38:03.549651: train_loss -0.7439\n",
      "2025-12-21 01:38:03.549651: val_loss -0.7663\n",
      "2025-12-21 01:38:03.553246: Pseudo dice [0.8845, 0.9227, 0.8812]\n",
      "2025-12-21 01:38:03.555249: Epoch time: 137.91 s\n",
      "2025-12-21 01:38:03.555249: Yayy! New best EMA pseudo Dice: 0.8582\n",
      "2025-12-21 01:38:04.468801: \n",
      "2025-12-21 01:38:04.468801: Epoch 39\n",
      "2025-12-21 01:38:04.468801: Current learning rate: 0.00965\n",
      "2025-12-21 01:40:22.442265: train_loss -0.7395\n",
      "2025-12-21 01:40:22.442265: val_loss -0.7703\n",
      "2025-12-21 01:40:22.444268: Pseudo dice [0.8914, 0.9317, 0.8748]\n",
      "2025-12-21 01:40:22.446159: Epoch time: 137.97 s\n",
      "2025-12-21 01:40:22.446159: Yayy! New best EMA pseudo Dice: 0.8623\n",
      "2025-12-21 01:40:23.483113: \n",
      "2025-12-21 01:40:23.483113: Epoch 40\n",
      "2025-12-21 01:40:23.483113: Current learning rate: 0.00964\n",
      "2025-12-21 01:42:41.344076: train_loss -0.744\n",
      "2025-12-21 01:42:41.344076: val_loss -0.7451\n",
      "2025-12-21 01:42:41.344076: Pseudo dice [0.8735, 0.9116, 0.8738]\n",
      "2025-12-21 01:42:41.344076: Epoch time: 137.86 s\n",
      "2025-12-21 01:42:41.351377: Yayy! New best EMA pseudo Dice: 0.8647\n",
      "2025-12-21 01:42:42.275232: \n",
      "2025-12-21 01:42:42.275232: Epoch 41\n",
      "2025-12-21 01:42:42.275232: Current learning rate: 0.00963\n",
      "2025-12-21 01:45:00.174499: train_loss -0.7454\n",
      "2025-12-21 01:45:00.174499: val_loss -0.7635\n",
      "2025-12-21 01:45:00.176502: Pseudo dice [0.877, 0.9171, 0.8771]\n",
      "2025-12-21 01:45:00.176502: Epoch time: 137.9 s\n",
      "2025-12-21 01:45:00.178504: Yayy! New best EMA pseudo Dice: 0.8673\n",
      "2025-12-21 01:45:01.235835: \n",
      "2025-12-21 01:45:01.235835: Epoch 42\n",
      "2025-12-21 01:45:01.235835: Current learning rate: 0.00962\n",
      "2025-12-21 01:47:19.225559: train_loss -0.7512\n",
      "2025-12-21 01:47:19.227561: val_loss -0.76\n",
      "2025-12-21 01:47:19.227561: Pseudo dice [0.8814, 0.9202, 0.8696]\n",
      "2025-12-21 01:47:19.229563: Epoch time: 137.99 s\n",
      "2025-12-21 01:47:19.229563: Yayy! New best EMA pseudo Dice: 0.8696\n",
      "2025-12-21 01:47:20.255534: \n",
      "2025-12-21 01:47:20.255534: Epoch 43\n",
      "2025-12-21 01:47:20.268571: Current learning rate: 0.00961\n",
      "2025-12-21 01:49:38.131317: train_loss -0.7571\n",
      "2025-12-21 01:49:38.131317: val_loss -0.7853\n",
      "2025-12-21 01:49:38.131317: Pseudo dice [0.8982, 0.9307, 0.881]\n",
      "2025-12-21 01:49:38.131317: Epoch time: 137.88 s\n",
      "2025-12-21 01:49:38.131317: Yayy! New best EMA pseudo Dice: 0.8729\n",
      "2025-12-21 01:49:39.015544: \n",
      "2025-12-21 01:49:39.015544: Epoch 44\n",
      "2025-12-21 01:49:39.015544: Current learning rate: 0.0096\n",
      "2025-12-21 01:51:57.123634: train_loss -0.7545\n",
      "2025-12-21 01:51:57.123634: val_loss -0.7772\n",
      "2025-12-21 01:51:57.123634: Pseudo dice [0.8932, 0.9322, 0.8799]\n",
      "2025-12-21 01:51:57.123634: Epoch time: 138.11 s\n",
      "2025-12-21 01:51:57.123634: Yayy! New best EMA pseudo Dice: 0.8758\n",
      "2025-12-21 01:51:58.016456: \n",
      "2025-12-21 01:51:58.016456: Epoch 45\n",
      "2025-12-21 01:51:58.016456: Current learning rate: 0.00959\n",
      "2025-12-21 01:54:15.877929: train_loss -0.7661\n",
      "2025-12-21 01:54:15.877929: val_loss -0.78\n",
      "2025-12-21 01:54:15.879932: Pseudo dice [0.8975, 0.9246, 0.8791]\n",
      "2025-12-21 01:54:15.881935: Epoch time: 137.86 s\n",
      "2025-12-21 01:54:15.883938: Yayy! New best EMA pseudo Dice: 0.8783\n",
      "2025-12-21 01:54:16.945509: \n",
      "2025-12-21 01:54:16.945509: Epoch 46\n",
      "2025-12-21 01:54:16.945509: Current learning rate: 0.00959\n",
      "2025-12-21 01:56:34.868566: train_loss -0.7481\n",
      "2025-12-21 01:56:34.868566: val_loss -0.7323\n",
      "2025-12-21 01:56:34.868566: Pseudo dice [0.8719, 0.9033, 0.8542]\n",
      "2025-12-21 01:56:34.868566: Epoch time: 137.92 s\n",
      "2025-12-21 01:56:35.488670: \n",
      "2025-12-21 01:56:35.488670: Epoch 47\n",
      "2025-12-21 01:56:35.488670: Current learning rate: 0.00958\n",
      "2025-12-21 01:58:53.263906: train_loss -0.7135\n",
      "2025-12-21 01:58:53.263906: val_loss -0.7243\n",
      "2025-12-21 01:58:53.263906: Pseudo dice [0.8614, 0.8971, 0.8712]\n",
      "2025-12-21 01:58:53.263906: Epoch time: 137.78 s\n",
      "2025-12-21 01:58:54.040739: \n",
      "2025-12-21 01:58:54.040739: Epoch 48\n",
      "2025-12-21 01:58:54.040739: Current learning rate: 0.00957\n",
      "2025-12-21 02:01:11.860886: train_loss -0.7417\n",
      "2025-12-21 02:01:11.860886: val_loss -0.7724\n",
      "2025-12-21 02:01:11.860886: Pseudo dice [0.887, 0.9249, 0.8753]\n",
      "2025-12-21 02:01:11.860886: Epoch time: 137.82 s\n",
      "2025-12-21 02:01:11.860886: Yayy! New best EMA pseudo Dice: 0.8797\n",
      "2025-12-21 02:01:12.882561: \n",
      "2025-12-21 02:01:12.882561: Epoch 49\n",
      "2025-12-21 02:01:12.884564: Current learning rate: 0.00956\n",
      "2025-12-21 02:03:30.780850: train_loss -0.7576\n",
      "2025-12-21 02:03:30.780850: val_loss -0.7608\n",
      "2025-12-21 02:03:30.780850: Pseudo dice [0.8771, 0.9183, 0.8762]\n",
      "2025-12-21 02:03:30.780850: Epoch time: 137.9 s\n",
      "2025-12-21 02:03:31.016942: Yayy! New best EMA pseudo Dice: 0.8808\n",
      "2025-12-21 02:03:31.903753: \n",
      "2025-12-21 02:03:31.903753: Epoch 50\n",
      "2025-12-21 02:03:31.903753: Current learning rate: 0.00955\n",
      "2025-12-21 02:05:49.808812: train_loss -0.7593\n",
      "2025-12-21 02:05:49.810814: val_loss -0.7828\n",
      "2025-12-21 02:05:49.810814: Pseudo dice [0.8936, 0.9241, 0.8804]\n",
      "2025-12-21 02:05:49.813270: Epoch time: 137.91 s\n",
      "2025-12-21 02:05:49.813270: Yayy! New best EMA pseudo Dice: 0.8827\n",
      "2025-12-21 02:05:50.695833: \n",
      "2025-12-21 02:05:50.695833: Epoch 51\n",
      "2025-12-21 02:05:50.695833: Current learning rate: 0.00954\n",
      "2025-12-21 02:08:08.528747: train_loss -0.7653\n",
      "2025-12-21 02:08:08.528747: val_loss -0.7813\n",
      "2025-12-21 02:08:08.528747: Pseudo dice [0.884, 0.9208, 0.8987]\n",
      "2025-12-21 02:08:08.528747: Epoch time: 137.83 s\n",
      "2025-12-21 02:08:08.528747: Yayy! New best EMA pseudo Dice: 0.8845\n",
      "2025-12-21 02:08:09.543699: \n",
      "2025-12-21 02:08:09.543699: Epoch 52\n",
      "2025-12-21 02:08:09.543699: Current learning rate: 0.00953\n",
      "2025-12-21 02:10:27.850508: train_loss -0.7648\n",
      "2025-12-21 02:10:27.852510: val_loss -0.7643\n",
      "2025-12-21 02:10:27.854513: Pseudo dice [0.8765, 0.9183, 0.8793]\n",
      "2025-12-21 02:10:27.854513: Epoch time: 138.31 s\n",
      "2025-12-21 02:10:27.856514: Yayy! New best EMA pseudo Dice: 0.8852\n",
      "2025-12-21 02:10:28.912570: \n",
      "2025-12-21 02:10:28.912570: Epoch 53\n",
      "2025-12-21 02:10:28.912570: Current learning rate: 0.00952\n",
      "2025-12-21 02:12:46.870794: train_loss -0.7692\n",
      "2025-12-21 02:12:46.872535: val_loss -0.7657\n",
      "2025-12-21 02:12:46.872535: Pseudo dice [0.8737, 0.9135, 0.887]\n",
      "2025-12-21 02:12:46.872535: Epoch time: 137.96 s\n",
      "2025-12-21 02:12:46.872535: Yayy! New best EMA pseudo Dice: 0.8858\n",
      "2025-12-21 02:12:47.762710: \n",
      "2025-12-21 02:12:47.762710: Epoch 54\n",
      "2025-12-21 02:12:47.762710: Current learning rate: 0.00951\n",
      "2025-12-21 02:15:05.750652: train_loss -0.7657\n",
      "2025-12-21 02:15:05.752655: val_loss -0.79\n",
      "2025-12-21 02:15:05.754396: Pseudo dice [0.8979, 0.9243, 0.8844]\n",
      "2025-12-21 02:15:05.754396: Epoch time: 137.99 s\n",
      "2025-12-21 02:15:05.756399: Yayy! New best EMA pseudo Dice: 0.8875\n",
      "2025-12-21 02:15:06.814789: \n",
      "2025-12-21 02:15:06.814789: Epoch 55\n",
      "2025-12-21 02:15:06.814789: Current learning rate: 0.0095\n",
      "2025-12-21 02:17:24.775497: train_loss -0.7772\n",
      "2025-12-21 02:17:24.775497: val_loss -0.7635\n",
      "2025-12-21 02:17:24.777499: Pseudo dice [0.8771, 0.9142, 0.8946]\n",
      "2025-12-21 02:17:24.779501: Epoch time: 137.96 s\n",
      "2025-12-21 02:17:24.779501: Yayy! New best EMA pseudo Dice: 0.8882\n",
      "2025-12-21 02:17:25.673306: \n",
      "2025-12-21 02:17:25.673306: Epoch 56\n",
      "2025-12-21 02:17:25.673306: Current learning rate: 0.00949\n",
      "2025-12-21 02:19:43.630508: train_loss -0.7795\n",
      "2025-12-21 02:19:43.632511: val_loss -0.8013\n",
      "2025-12-21 02:19:43.632511: Pseudo dice [0.8981, 0.9337, 0.8963]\n",
      "2025-12-21 02:19:43.635660: Epoch time: 137.96 s\n",
      "2025-12-21 02:19:43.635660: Yayy! New best EMA pseudo Dice: 0.8904\n",
      "2025-12-21 02:19:44.510220: \n",
      "2025-12-21 02:19:44.510220: Epoch 57\n",
      "2025-12-21 02:19:44.510220: Current learning rate: 0.00949\n",
      "2025-12-21 02:22:02.338337: train_loss -0.7736\n",
      "2025-12-21 02:22:02.340340: val_loss -0.7405\n",
      "2025-12-21 02:22:02.341343: Pseudo dice [0.8671, 0.9034, 0.8746]\n",
      "2025-12-21 02:22:02.343216: Epoch time: 137.83 s\n",
      "2025-12-21 02:22:03.053729: \n",
      "2025-12-21 02:22:03.053729: Epoch 58\n",
      "2025-12-21 02:22:03.053729: Current learning rate: 0.00948\n",
      "2025-12-21 02:24:21.125225: train_loss -0.765\n",
      "2025-12-21 02:24:21.125225: val_loss -0.7994\n",
      "2025-12-21 02:24:21.125225: Pseudo dice [0.8986, 0.9261, 0.9065]\n",
      "2025-12-21 02:24:21.125225: Epoch time: 138.09 s\n",
      "2025-12-21 02:24:21.125225: Yayy! New best EMA pseudo Dice: 0.8916\n",
      "2025-12-21 02:24:22.011628: \n",
      "2025-12-21 02:24:22.011628: Epoch 59\n",
      "2025-12-21 02:24:22.011628: Current learning rate: 0.00947\n",
      "2025-12-21 02:26:39.749867: train_loss -0.7808\n",
      "2025-12-21 02:26:39.751871: val_loss -0.7843\n",
      "2025-12-21 02:26:39.753874: Pseudo dice [0.8875, 0.9265, 0.8868]\n",
      "2025-12-21 02:26:39.753874: Epoch time: 137.74 s\n",
      "2025-12-21 02:26:39.753874: Yayy! New best EMA pseudo Dice: 0.8925\n",
      "2025-12-21 02:26:40.821831: \n",
      "2025-12-21 02:26:40.821831: Epoch 60\n",
      "2025-12-21 02:26:40.821831: Current learning rate: 0.00946\n",
      "2025-12-21 02:28:58.771948: train_loss -0.776\n",
      "2025-12-21 02:28:58.775491: val_loss -0.813\n",
      "2025-12-21 02:28:58.775491: Pseudo dice [0.9052, 0.9423, 0.8953]\n",
      "2025-12-21 02:28:58.775491: Epoch time: 137.95 s\n",
      "2025-12-21 02:28:58.775491: Yayy! New best EMA pseudo Dice: 0.8946\n",
      "2025-12-21 02:28:59.696447: \n",
      "2025-12-21 02:28:59.696447: Epoch 61\n",
      "2025-12-21 02:28:59.696447: Current learning rate: 0.00945\n",
      "2025-12-21 02:31:17.494765: train_loss -0.7467\n",
      "2025-12-21 02:31:17.494765: val_loss -0.7767\n",
      "2025-12-21 02:31:17.494765: Pseudo dice [0.8854, 0.9218, 0.8922]\n",
      "2025-12-21 02:31:17.498557: Epoch time: 137.8 s\n",
      "2025-12-21 02:31:17.498557: Yayy! New best EMA pseudo Dice: 0.8951\n",
      "2025-12-21 02:31:18.411923: \n",
      "2025-12-21 02:31:18.411923: Epoch 62\n",
      "2025-12-21 02:31:18.411923: Current learning rate: 0.00944\n",
      "2025-12-21 02:33:36.306980: train_loss -0.7635\n",
      "2025-12-21 02:33:36.306980: val_loss -0.7777\n",
      "2025-12-21 02:33:36.308982: Pseudo dice [0.8863, 0.9212, 0.8843]\n",
      "2025-12-21 02:33:36.308982: Epoch time: 137.9 s\n",
      "2025-12-21 02:33:36.310984: Yayy! New best EMA pseudo Dice: 0.8954\n",
      "2025-12-21 02:33:37.205008: \n",
      "2025-12-21 02:33:37.205008: Epoch 63\n",
      "2025-12-21 02:33:37.205008: Current learning rate: 0.00943\n",
      "2025-12-21 02:35:55.048161: train_loss -0.7768\n",
      "2025-12-21 02:35:55.048161: val_loss -0.7893\n",
      "2025-12-21 02:35:55.048161: Pseudo dice [0.8928, 0.9325, 0.8852]\n",
      "2025-12-21 02:35:55.048161: Epoch time: 137.84 s\n",
      "2025-12-21 02:35:55.048161: Yayy! New best EMA pseudo Dice: 0.8962\n",
      "2025-12-21 02:35:55.944333: \n",
      "2025-12-21 02:35:55.944333: Epoch 64\n",
      "2025-12-21 02:35:55.944333: Current learning rate: 0.00942\n",
      "2025-12-21 02:38:13.904901: train_loss -0.777\n",
      "2025-12-21 02:38:13.906903: val_loss -0.7842\n",
      "2025-12-21 02:38:13.910909: Pseudo dice [0.8811, 0.925, 0.8991]\n",
      "2025-12-21 02:38:13.912914: Epoch time: 137.96 s\n",
      "2025-12-21 02:38:13.914659: Yayy! New best EMA pseudo Dice: 0.8967\n",
      "2025-12-21 02:38:14.967848: \n",
      "2025-12-21 02:38:14.967848: Epoch 65\n",
      "2025-12-21 02:38:14.967848: Current learning rate: 0.00941\n",
      "2025-12-21 02:40:32.846717: train_loss -0.7753\n",
      "2025-12-21 02:40:32.848720: val_loss -0.7763\n",
      "2025-12-21 02:40:32.848720: Pseudo dice [0.8837, 0.9189, 0.8912]\n",
      "2025-12-21 02:40:32.851017: Epoch time: 137.88 s\n",
      "2025-12-21 02:40:32.851017: Yayy! New best EMA pseudo Dice: 0.8968\n",
      "2025-12-21 02:40:33.756749: \n",
      "2025-12-21 02:40:33.756749: Epoch 66\n",
      "2025-12-21 02:40:33.756749: Current learning rate: 0.0094\n",
      "2025-12-21 02:42:51.668239: train_loss -0.7791\n",
      "2025-12-21 02:42:51.668239: val_loss -0.7766\n",
      "2025-12-21 02:42:51.668239: Pseudo dice [0.8783, 0.9171, 0.9127]\n",
      "2025-12-21 02:42:51.668239: Epoch time: 137.91 s\n",
      "2025-12-21 02:42:51.668239: Yayy! New best EMA pseudo Dice: 0.8974\n",
      "2025-12-21 02:42:52.575573: \n",
      "2025-12-21 02:42:52.575573: Epoch 67\n",
      "2025-12-21 02:42:52.575573: Current learning rate: 0.00939\n",
      "2025-12-21 02:45:10.601299: train_loss -0.7779\n",
      "2025-12-21 02:45:10.601299: val_loss -0.7806\n",
      "2025-12-21 02:45:10.603301: Pseudo dice [0.8907, 0.923, 0.8997]\n",
      "2025-12-21 02:45:10.605303: Epoch time: 138.03 s\n",
      "2025-12-21 02:45:10.605303: Yayy! New best EMA pseudo Dice: 0.8981\n",
      "2025-12-21 02:45:11.501077: \n",
      "2025-12-21 02:45:11.501077: Epoch 68\n",
      "2025-12-21 02:45:11.501077: Current learning rate: 0.00939\n",
      "2025-12-21 02:47:29.325450: train_loss -0.7873\n",
      "2025-12-21 02:47:29.325450: val_loss -0.7778\n",
      "2025-12-21 02:47:29.325450: Pseudo dice [0.8813, 0.9183, 0.8909]\n",
      "2025-12-21 02:47:29.341523: Epoch time: 137.82 s\n",
      "2025-12-21 02:47:29.976127: \n",
      "2025-12-21 02:47:29.976127: Epoch 69\n",
      "2025-12-21 02:47:29.976127: Current learning rate: 0.00938\n",
      "2025-12-21 02:49:47.979252: train_loss -0.7812\n",
      "2025-12-21 02:49:47.979252: val_loss -0.7786\n",
      "2025-12-21 02:49:47.979252: Pseudo dice [0.8848, 0.9176, 0.8872]\n",
      "2025-12-21 02:49:47.979252: Epoch time: 138.0 s\n",
      "2025-12-21 02:49:48.628468: \n",
      "2025-12-21 02:49:48.628468: Epoch 70\n",
      "2025-12-21 02:49:48.644234: Current learning rate: 0.00937\n",
      "2025-12-21 02:52:06.542110: train_loss -0.7616\n",
      "2025-12-21 02:52:06.542110: val_loss -0.7621\n",
      "2025-12-21 02:52:06.542110: Pseudo dice [0.877, 0.9127, 0.8825]\n",
      "2025-12-21 02:52:06.542110: Epoch time: 137.91 s\n",
      "2025-12-21 02:52:07.349376: \n",
      "2025-12-21 02:52:07.349376: Epoch 71\n",
      "2025-12-21 02:52:07.349376: Current learning rate: 0.00936\n",
      "2025-12-21 02:54:25.139472: train_loss -0.7771\n",
      "2025-12-21 02:54:25.139472: val_loss -0.8143\n",
      "2025-12-21 02:54:25.141474: Pseudo dice [0.9054, 0.9414, 0.8943]\n",
      "2025-12-21 02:54:25.141474: Epoch time: 137.79 s\n",
      "2025-12-21 02:54:25.143476: Yayy! New best EMA pseudo Dice: 0.8988\n",
      "2025-12-21 02:54:26.003169: \n",
      "2025-12-21 02:54:26.003169: Epoch 72\n",
      "2025-12-21 02:54:26.003169: Current learning rate: 0.00935\n",
      "2025-12-21 02:56:43.945709: train_loss -0.7852\n",
      "2025-12-21 02:56:43.947711: val_loss -0.7887\n",
      "2025-12-21 02:56:43.949713: Pseudo dice [0.8864, 0.9183, 0.8928]\n",
      "2025-12-21 02:56:43.951715: Epoch time: 137.94 s\n",
      "2025-12-21 02:56:43.953720: Yayy! New best EMA pseudo Dice: 0.8988\n",
      "2025-12-21 02:56:44.862936: \n",
      "2025-12-21 02:56:44.862936: Epoch 73\n",
      "2025-12-21 02:56:44.862936: Current learning rate: 0.00934\n",
      "2025-12-21 02:59:02.853680: train_loss -0.795\n",
      "2025-12-21 02:59:02.853680: val_loss -0.8269\n",
      "2025-12-21 02:59:02.857684: Pseudo dice [0.9189, 0.9437, 0.8939]\n",
      "2025-12-21 02:59:02.857684: Epoch time: 137.99 s\n",
      "2025-12-21 02:59:02.859686: Yayy! New best EMA pseudo Dice: 0.9008\n",
      "2025-12-21 02:59:03.759512: \n",
      "2025-12-21 02:59:03.759512: Epoch 74\n",
      "2025-12-21 02:59:03.759512: Current learning rate: 0.00933\n",
      "2025-12-21 03:01:21.580298: train_loss -0.7927\n",
      "2025-12-21 03:01:21.580298: val_loss -0.82\n",
      "2025-12-21 03:01:21.580298: Pseudo dice [0.9067, 0.9366, 0.9193]\n",
      "2025-12-21 03:01:21.580298: Epoch time: 137.82 s\n",
      "2025-12-21 03:01:21.580298: Yayy! New best EMA pseudo Dice: 0.9028\n",
      "2025-12-21 03:01:22.518624: \n",
      "2025-12-21 03:01:22.518624: Epoch 75\n",
      "2025-12-21 03:01:22.518624: Current learning rate: 0.00932\n",
      "2025-12-21 03:03:40.443808: train_loss -0.7891\n",
      "2025-12-21 03:03:40.445811: val_loss -0.8072\n",
      "2025-12-21 03:03:40.445811: Pseudo dice [0.9054, 0.9353, 0.8983]\n",
      "2025-12-21 03:03:40.448990: Epoch time: 137.93 s\n",
      "2025-12-21 03:03:40.448990: Yayy! New best EMA pseudo Dice: 0.9039\n",
      "2025-12-21 03:03:41.362157: \n",
      "2025-12-21 03:03:41.362157: Epoch 76\n",
      "2025-12-21 03:03:41.362157: Current learning rate: 0.00931\n",
      "2025-12-21 03:05:59.327783: train_loss -0.7925\n",
      "2025-12-21 03:05:59.329785: val_loss -0.7944\n",
      "2025-12-21 03:05:59.331787: Pseudo dice [0.8889, 0.9251, 0.9007]\n",
      "2025-12-21 03:05:59.331787: Epoch time: 137.97 s\n",
      "2025-12-21 03:05:59.333527: Yayy! New best EMA pseudo Dice: 0.904\n",
      "2025-12-21 03:06:00.434002: \n",
      "2025-12-21 03:06:00.434002: Epoch 77\n",
      "2025-12-21 03:06:00.434002: Current learning rate: 0.0093\n",
      "2025-12-21 03:08:18.370146: train_loss -0.7952\n",
      "2025-12-21 03:08:18.370146: val_loss -0.8167\n",
      "2025-12-21 03:08:18.385908: Pseudo dice [0.9058, 0.94, 0.8997]\n",
      "2025-12-21 03:08:18.385908: Epoch time: 137.94 s\n",
      "2025-12-21 03:08:18.385908: Yayy! New best EMA pseudo Dice: 0.9051\n",
      "2025-12-21 03:08:19.304371: \n",
      "2025-12-21 03:08:19.304371: Epoch 78\n",
      "2025-12-21 03:08:19.304371: Current learning rate: 0.0093\n",
      "2025-12-21 03:10:37.645746: train_loss -0.7944\n",
      "2025-12-21 03:10:37.645746: val_loss -0.8051\n",
      "2025-12-21 03:10:37.647748: Pseudo dice [0.8975, 0.932, 0.8979]\n",
      "2025-12-21 03:10:37.647748: Epoch time: 138.34 s\n",
      "2025-12-21 03:10:37.649750: Yayy! New best EMA pseudo Dice: 0.9055\n",
      "2025-12-21 03:10:38.558198: \n",
      "2025-12-21 03:10:38.558198: Epoch 79\n",
      "2025-12-21 03:10:38.558198: Current learning rate: 0.00929\n",
      "2025-12-21 03:12:56.500283: train_loss -0.7919\n",
      "2025-12-21 03:12:56.500283: val_loss -0.7972\n",
      "2025-12-21 03:12:56.516122: Pseudo dice [0.8933, 0.9266, 0.9013]\n",
      "2025-12-21 03:12:56.516122: Epoch time: 137.94 s\n",
      "2025-12-21 03:12:56.516122: Yayy! New best EMA pseudo Dice: 0.9056\n",
      "2025-12-21 03:12:57.440036: \n",
      "2025-12-21 03:12:57.440036: Epoch 80\n",
      "2025-12-21 03:12:57.440036: Current learning rate: 0.00928\n",
      "2025-12-21 03:15:15.479517: train_loss -0.793\n",
      "2025-12-21 03:15:15.479517: val_loss -0.7886\n",
      "2025-12-21 03:15:15.481523: Pseudo dice [0.8902, 0.9291, 0.8888]\n",
      "2025-12-21 03:15:15.483526: Epoch time: 138.04 s\n",
      "2025-12-21 03:15:16.146753: \n",
      "2025-12-21 03:15:16.146753: Epoch 81\n",
      "2025-12-21 03:15:16.146753: Current learning rate: 0.00927\n",
      "2025-12-21 03:17:34.016249: train_loss -0.7915\n",
      "2025-12-21 03:17:34.016249: val_loss -0.7863\n",
      "2025-12-21 03:17:34.018252: Pseudo dice [0.8909, 0.9264, 0.8891]\n",
      "2025-12-21 03:17:34.020829: Epoch time: 137.87 s\n",
      "2025-12-21 03:17:34.828668: \n",
      "2025-12-21 03:17:34.828668: Epoch 82\n",
      "2025-12-21 03:17:34.828668: Current learning rate: 0.00926\n",
      "2025-12-21 03:19:52.725755: train_loss -0.7929\n",
      "2025-12-21 03:19:52.725755: val_loss -0.8061\n",
      "2025-12-21 03:19:52.725755: Pseudo dice [0.9013, 0.934, 0.8939]\n",
      "2025-12-21 03:19:52.725755: Epoch time: 137.9 s\n",
      "2025-12-21 03:19:53.343986: \n",
      "2025-12-21 03:19:53.343986: Epoch 83\n",
      "2025-12-21 03:19:53.343986: Current learning rate: 0.00925\n",
      "2025-12-21 03:22:11.185674: train_loss -0.7995\n",
      "2025-12-21 03:22:11.187676: val_loss -0.8356\n",
      "2025-12-21 03:22:11.191683: Pseudo dice [0.9173, 0.9443, 0.9149]\n",
      "2025-12-21 03:22:11.191683: Epoch time: 137.84 s\n",
      "2025-12-21 03:22:11.193905: Yayy! New best EMA pseudo Dice: 0.9075\n",
      "2025-12-21 03:22:12.072384: \n",
      "2025-12-21 03:22:12.072384: Epoch 84\n",
      "2025-12-21 03:22:12.072384: Current learning rate: 0.00924\n",
      "2025-12-21 03:24:30.109237: train_loss -0.7975\n",
      "2025-12-21 03:24:30.109237: val_loss -0.7844\n",
      "2025-12-21 03:24:30.112982: Pseudo dice [0.8858, 0.9219, 0.8969]\n",
      "2025-12-21 03:24:30.114985: Epoch time: 138.04 s\n",
      "2025-12-21 03:24:30.742764: \n",
      "2025-12-21 03:24:30.742764: Epoch 85\n",
      "2025-12-21 03:24:30.758715: Current learning rate: 0.00923\n",
      "2025-12-21 03:26:48.742177: train_loss -0.8018\n",
      "2025-12-21 03:26:48.742177: val_loss -0.8075\n",
      "2025-12-21 03:26:48.742177: Pseudo dice [0.8954, 0.934, 0.909]\n",
      "2025-12-21 03:26:48.742177: Epoch time: 138.0 s\n",
      "2025-12-21 03:26:49.373533: \n",
      "2025-12-21 03:26:49.373533: Epoch 86\n",
      "2025-12-21 03:26:49.373533: Current learning rate: 0.00922\n",
      "2025-12-21 03:29:07.397904: train_loss -0.7847\n",
      "2025-12-21 03:29:07.399906: val_loss -0.7586\n",
      "2025-12-21 03:29:07.401909: Pseudo dice [0.8766, 0.9088, 0.9058]\n",
      "2025-12-21 03:29:07.401909: Epoch time: 138.02 s\n",
      "2025-12-21 03:29:08.177597: \n",
      "2025-12-21 03:29:08.177597: Epoch 87\n",
      "2025-12-21 03:29:08.177597: Current learning rate: 0.00921\n",
      "2025-12-21 03:31:26.013586: train_loss -0.7823\n",
      "2025-12-21 03:31:26.013586: val_loss -0.8061\n",
      "2025-12-21 03:31:26.017033: Pseudo dice [0.9057, 0.9317, 0.8941]\n",
      "2025-12-21 03:31:26.017033: Epoch time: 137.84 s\n",
      "2025-12-21 03:31:26.816898: \n",
      "2025-12-21 03:31:26.818900: Epoch 88\n",
      "2025-12-21 03:31:26.818900: Current learning rate: 0.0092\n",
      "2025-12-21 03:33:44.869175: train_loss -0.7954\n",
      "2025-12-21 03:33:44.871177: val_loss -0.8052\n",
      "2025-12-21 03:33:44.873179: Pseudo dice [0.9005, 0.935, 0.8989]\n",
      "2025-12-21 03:33:44.875181: Epoch time: 138.05 s\n",
      "2025-12-21 03:33:45.518675: \n",
      "2025-12-21 03:33:45.518675: Epoch 89\n",
      "2025-12-21 03:33:45.518675: Current learning rate: 0.0092\n",
      "2025-12-21 03:36:03.533588: train_loss -0.7959\n",
      "2025-12-21 03:36:03.535590: val_loss -0.8206\n",
      "2025-12-21 03:36:03.537592: Pseudo dice [0.905, 0.9338, 0.9131]\n",
      "2025-12-21 03:36:03.537592: Epoch time: 138.02 s\n",
      "2025-12-21 03:36:03.537592: Yayy! New best EMA pseudo Dice: 0.9083\n",
      "2025-12-21 03:36:04.592597: \n",
      "2025-12-21 03:36:04.592597: Epoch 90\n",
      "2025-12-21 03:36:04.592597: Current learning rate: 0.00919\n",
      "2025-12-21 03:38:22.412688: train_loss -0.8047\n",
      "2025-12-21 03:38:22.412688: val_loss -0.828\n",
      "2025-12-21 03:38:22.412688: Pseudo dice [0.915, 0.9448, 0.9065]\n",
      "2025-12-21 03:38:22.422260: Epoch time: 137.82 s\n",
      "2025-12-21 03:38:22.422260: Yayy! New best EMA pseudo Dice: 0.9097\n",
      "2025-12-21 03:38:23.317609: \n",
      "2025-12-21 03:38:23.317609: Epoch 91\n",
      "2025-12-21 03:38:23.317609: Current learning rate: 0.00918\n",
      "2025-12-21 03:40:41.208250: train_loss -0.8059\n",
      "2025-12-21 03:40:41.208250: val_loss -0.8335\n",
      "2025-12-21 03:40:41.208250: Pseudo dice [0.911, 0.9449, 0.9166]\n",
      "2025-12-21 03:40:41.208250: Epoch time: 137.89 s\n",
      "2025-12-21 03:40:41.224212: Yayy! New best EMA pseudo Dice: 0.9111\n",
      "2025-12-21 03:40:42.078810: \n",
      "2025-12-21 03:40:42.078810: Epoch 92\n",
      "2025-12-21 03:40:42.078810: Current learning rate: 0.00917\n",
      "2025-12-21 03:42:59.957395: train_loss -0.802\n",
      "2025-12-21 03:42:59.957395: val_loss -0.8071\n",
      "2025-12-21 03:42:59.959397: Pseudo dice [0.8939, 0.9293, 0.9074]\n",
      "2025-12-21 03:42:59.961400: Epoch time: 137.88 s\n",
      "2025-12-21 03:43:00.661609: \n",
      "2025-12-21 03:43:00.661609: Epoch 93\n",
      "2025-12-21 03:43:00.677723: Current learning rate: 0.00916\n",
      "2025-12-21 03:45:18.767899: train_loss -0.8002\n",
      "2025-12-21 03:45:18.767899: val_loss -0.8275\n",
      "2025-12-21 03:45:18.769902: Pseudo dice [0.9088, 0.9439, 0.909]\n",
      "2025-12-21 03:45:18.771905: Epoch time: 138.11 s\n",
      "2025-12-21 03:45:18.773907: Yayy! New best EMA pseudo Dice: 0.912\n",
      "2025-12-21 03:45:19.656358: \n",
      "2025-12-21 03:45:19.658360: Epoch 94\n",
      "2025-12-21 03:45:19.658360: Current learning rate: 0.00915\n",
      "2025-12-21 03:47:37.449721: train_loss -0.8071\n",
      "2025-12-21 03:47:37.449721: val_loss -0.7873\n",
      "2025-12-21 03:47:37.451724: Pseudo dice [0.8989, 0.9252, 0.883]\n",
      "2025-12-21 03:47:37.453727: Epoch time: 137.79 s\n",
      "2025-12-21 03:47:38.244441: \n",
      "2025-12-21 03:47:38.244441: Epoch 95\n",
      "2025-12-21 03:47:38.244441: Current learning rate: 0.00914\n",
      "2025-12-21 03:49:56.150432: train_loss -0.7791\n",
      "2025-12-21 03:49:56.150432: val_loss -0.8073\n",
      "2025-12-21 03:49:56.152434: Pseudo dice [0.8952, 0.9372, 0.9121]\n",
      "2025-12-21 03:49:56.152434: Epoch time: 137.91 s\n",
      "2025-12-21 03:49:56.867121: \n",
      "2025-12-21 03:49:56.867121: Epoch 96\n",
      "2025-12-21 03:49:56.872888: Current learning rate: 0.00913\n",
      "2025-12-21 03:52:14.765784: train_loss -0.7899\n",
      "2025-12-21 03:52:14.765784: val_loss -0.8068\n",
      "2025-12-21 03:52:14.767787: Pseudo dice [0.8992, 0.9315, 0.9018]\n",
      "2025-12-21 03:52:14.769789: Epoch time: 137.9 s\n",
      "2025-12-21 03:52:15.388520: \n",
      "2025-12-21 03:52:15.390260: Epoch 97\n",
      "2025-12-21 03:52:15.390260: Current learning rate: 0.00912\n",
      "2025-12-21 03:54:33.520335: train_loss -0.8028\n",
      "2025-12-21 03:54:33.520335: val_loss -0.813\n",
      "2025-12-21 03:54:33.524342: Pseudo dice [0.8982, 0.9402, 0.9063]\n",
      "2025-12-21 03:54:33.526345: Epoch time: 138.13 s\n",
      "2025-12-21 03:54:34.147421: \n",
      "2025-12-21 03:54:34.147421: Epoch 98\n",
      "2025-12-21 03:54:34.147421: Current learning rate: 0.00911\n",
      "2025-12-21 03:56:52.484117: train_loss -0.8032\n",
      "2025-12-21 03:56:52.484117: val_loss -0.8298\n",
      "2025-12-21 03:56:52.486120: Pseudo dice [0.9111, 0.9473, 0.9126]\n",
      "2025-12-21 03:56:52.488121: Epoch time: 138.34 s\n",
      "2025-12-21 03:56:52.490123: Yayy! New best EMA pseudo Dice: 0.9129\n",
      "2025-12-21 03:56:53.460493: \n",
      "2025-12-21 03:56:53.460493: Epoch 99\n",
      "2025-12-21 03:56:53.460493: Current learning rate: 0.0091\n",
      "2025-12-21 03:59:11.429938: train_loss -0.8081\n",
      "2025-12-21 03:59:11.429938: val_loss -0.8254\n",
      "2025-12-21 03:59:11.429938: Pseudo dice [0.9044, 0.9397, 0.9189]\n",
      "2025-12-21 03:59:11.429938: Epoch time: 137.99 s\n",
      "2025-12-21 03:59:11.667166: Yayy! New best EMA pseudo Dice: 0.9137\n",
      "2025-12-21 03:59:12.556192: \n",
      "2025-12-21 03:59:12.556192: Epoch 100\n",
      "2025-12-21 03:59:12.556192: Current learning rate: 0.0091\n",
      "2025-12-21 04:01:30.535384: train_loss -0.8066\n",
      "2025-12-21 04:01:30.535384: val_loss -0.7975\n",
      "2025-12-21 04:01:30.541391: Pseudo dice [0.8906, 0.9316, 0.8951]\n",
      "2025-12-21 04:01:30.541391: Epoch time: 137.98 s\n",
      "2025-12-21 04:01:31.339824: \n",
      "2025-12-21 04:01:31.341826: Epoch 101\n",
      "2025-12-21 04:01:31.341826: Current learning rate: 0.00909\n",
      "2025-12-21 04:03:49.430133: train_loss -0.8047\n",
      "2025-12-21 04:03:49.430133: val_loss -0.8345\n",
      "2025-12-21 04:03:49.430133: Pseudo dice [0.9133, 0.9425, 0.9144]\n",
      "2025-12-21 04:03:49.430133: Epoch time: 138.09 s\n",
      "2025-12-21 04:03:49.430133: Yayy! New best EMA pseudo Dice: 0.914\n",
      "2025-12-21 04:03:50.446555: \n",
      "2025-12-21 04:03:50.446555: Epoch 102\n",
      "2025-12-21 04:03:50.446555: Current learning rate: 0.00908\n",
      "2025-12-21 04:06:08.389098: train_loss -0.8069\n",
      "2025-12-21 04:06:08.391100: val_loss -0.7961\n",
      "2025-12-21 04:06:08.393103: Pseudo dice [0.8814, 0.9231, 0.9195]\n",
      "2025-12-21 04:06:08.395106: Epoch time: 137.94 s\n",
      "2025-12-21 04:06:09.019680: \n",
      "2025-12-21 04:06:09.021683: Epoch 103\n",
      "2025-12-21 04:06:09.021683: Current learning rate: 0.00907\n",
      "2025-12-21 04:08:27.003091: train_loss -0.8121\n",
      "2025-12-21 04:08:27.005093: val_loss -0.8235\n",
      "2025-12-21 04:08:27.007095: Pseudo dice [0.9015, 0.9416, 0.9148]\n",
      "2025-12-21 04:08:27.009098: Epoch time: 137.98 s\n",
      "2025-12-21 04:08:27.639632: \n",
      "2025-12-21 04:08:27.639632: Epoch 104\n",
      "2025-12-21 04:08:27.639632: Current learning rate: 0.00906\n",
      "2025-12-21 04:10:45.915492: train_loss -0.8114\n",
      "2025-12-21 04:10:45.915492: val_loss -0.8249\n",
      "2025-12-21 04:10:45.915492: Pseudo dice [0.9097, 0.9401, 0.916]\n",
      "2025-12-21 04:10:45.915492: Epoch time: 138.28 s\n",
      "2025-12-21 04:10:45.920084: Yayy! New best EMA pseudo Dice: 0.9148\n",
      "2025-12-21 04:10:46.949121: \n",
      "2025-12-21 04:10:46.952638: Epoch 105\n",
      "2025-12-21 04:10:46.952638: Current learning rate: 0.00905\n",
      "2025-12-21 04:13:04.879066: train_loss -0.8106\n",
      "2025-12-21 04:13:04.879066: val_loss -0.8336\n",
      "2025-12-21 04:13:04.879066: Pseudo dice [0.9105, 0.9464, 0.9165]\n",
      "2025-12-21 04:13:04.879066: Epoch time: 137.93 s\n",
      "2025-12-21 04:13:04.879066: Yayy! New best EMA pseudo Dice: 0.9157\n",
      "2025-12-21 04:13:05.780181: \n",
      "2025-12-21 04:13:05.780181: Epoch 106\n",
      "2025-12-21 04:13:05.780181: Current learning rate: 0.00904\n",
      "2025-12-21 04:15:23.745410: train_loss -0.8097\n",
      "2025-12-21 04:15:23.745410: val_loss -0.8276\n",
      "2025-12-21 04:15:23.747412: Pseudo dice [0.9037, 0.9397, 0.9161]\n",
      "2025-12-21 04:15:23.747412: Epoch time: 137.97 s\n",
      "2025-12-21 04:15:23.747412: Yayy! New best EMA pseudo Dice: 0.9161\n",
      "2025-12-21 04:15:24.816746: \n",
      "2025-12-21 04:15:24.816746: Epoch 107\n",
      "2025-12-21 04:15:24.816746: Current learning rate: 0.00903\n",
      "2025-12-21 04:17:42.746026: train_loss -0.8135\n",
      "2025-12-21 04:17:42.746026: val_loss -0.8308\n",
      "2025-12-21 04:17:42.746026: Pseudo dice [0.9075, 0.9431, 0.9218]\n",
      "2025-12-21 04:17:42.746026: Epoch time: 137.93 s\n",
      "2025-12-21 04:17:42.746026: Yayy! New best EMA pseudo Dice: 0.9169\n",
      "2025-12-21 04:17:43.741293: \n",
      "2025-12-21 04:17:43.741293: Epoch 108\n",
      "2025-12-21 04:17:43.741293: Current learning rate: 0.00902\n",
      "2025-12-21 04:20:01.589663: train_loss -0.8124\n",
      "2025-12-21 04:20:01.589663: val_loss -0.8324\n",
      "2025-12-21 04:20:01.589663: Pseudo dice [0.9119, 0.9489, 0.9132]\n",
      "2025-12-21 04:20:01.589663: Epoch time: 137.85 s\n",
      "2025-12-21 04:20:01.589663: Yayy! New best EMA pseudo Dice: 0.9177\n",
      "2025-12-21 04:20:02.481107: \n",
      "2025-12-21 04:20:02.481107: Epoch 109\n",
      "2025-12-21 04:20:02.481107: Current learning rate: 0.00901\n",
      "2025-12-21 04:22:20.378206: train_loss -0.813\n",
      "2025-12-21 04:22:20.378206: val_loss -0.834\n",
      "2025-12-21 04:22:20.380025: Pseudo dice [0.9138, 0.9446, 0.9137]\n",
      "2025-12-21 04:22:20.382027: Epoch time: 137.9 s\n",
      "2025-12-21 04:22:20.384029: Yayy! New best EMA pseudo Dice: 0.9183\n",
      "2025-12-21 04:22:21.279586: \n",
      "2025-12-21 04:22:21.279586: Epoch 110\n",
      "2025-12-21 04:22:21.279586: Current learning rate: 0.009\n",
      "2025-12-21 04:24:39.187354: train_loss -0.8143\n",
      "2025-12-21 04:24:39.187354: val_loss -0.8128\n",
      "2025-12-21 04:24:39.189356: Pseudo dice [0.8988, 0.9353, 0.9084]\n",
      "2025-12-21 04:24:39.191096: Epoch time: 137.91 s\n",
      "2025-12-21 04:24:39.929216: \n",
      "2025-12-21 04:24:39.929216: Epoch 111\n",
      "2025-12-21 04:24:39.932567: Current learning rate: 0.009\n",
      "2025-12-21 04:26:57.934510: train_loss -0.8103\n",
      "2025-12-21 04:26:57.934510: val_loss -0.8485\n",
      "2025-12-21 04:26:57.934510: Pseudo dice [0.9225, 0.9518, 0.9159]\n",
      "2025-12-21 04:26:57.934510: Epoch time: 138.02 s\n",
      "2025-12-21 04:26:57.934510: Yayy! New best EMA pseudo Dice: 0.9191\n",
      "2025-12-21 04:26:58.825694: \n",
      "2025-12-21 04:26:58.825694: Epoch 112\n",
      "2025-12-21 04:26:58.825694: Current learning rate: 0.00899\n",
      "2025-12-21 04:29:16.658491: train_loss -0.8109\n",
      "2025-12-21 04:29:16.658491: val_loss -0.8396\n",
      "2025-12-21 04:29:16.662456: Pseudo dice [0.9164, 0.9488, 0.9169]\n",
      "2025-12-21 04:29:16.664459: Epoch time: 137.83 s\n",
      "2025-12-21 04:29:16.666461: Yayy! New best EMA pseudo Dice: 0.92\n",
      "2025-12-21 04:29:17.746808: \n",
      "2025-12-21 04:29:17.746808: Epoch 113\n",
      "2025-12-21 04:29:17.746808: Current learning rate: 0.00898\n",
      "2025-12-21 04:31:35.581198: train_loss -0.8167\n",
      "2025-12-21 04:31:35.585485: val_loss -0.8185\n",
      "2025-12-21 04:31:35.585485: Pseudo dice [0.9022, 0.9365, 0.9086]\n",
      "2025-12-21 04:31:35.588383: Epoch time: 137.83 s\n",
      "2025-12-21 04:31:36.292577: \n",
      "2025-12-21 04:31:36.292577: Epoch 114\n",
      "2025-12-21 04:31:36.292577: Current learning rate: 0.00897\n",
      "2025-12-21 04:33:54.264463: train_loss -0.8147\n",
      "2025-12-21 04:33:54.264463: val_loss -0.836\n",
      "2025-12-21 04:33:54.266465: Pseudo dice [0.9152, 0.9492, 0.9062]\n",
      "2025-12-21 04:33:54.268468: Epoch time: 137.97 s\n",
      "2025-12-21 04:33:54.899418: \n",
      "2025-12-21 04:33:54.899418: Epoch 115\n",
      "2025-12-21 04:33:54.899418: Current learning rate: 0.00896\n",
      "2025-12-21 04:36:12.735974: train_loss -0.8093\n",
      "2025-12-21 04:36:12.735974: val_loss -0.8233\n",
      "2025-12-21 04:36:12.735974: Pseudo dice [0.9064, 0.939, 0.9153]\n",
      "2025-12-21 04:36:12.739747: Epoch time: 137.84 s\n",
      "2025-12-21 04:36:12.741750: Yayy! New best EMA pseudo Dice: 0.92\n",
      "2025-12-21 04:36:13.617772: \n",
      "2025-12-21 04:36:13.617772: Epoch 116\n",
      "2025-12-21 04:36:13.620305: Current learning rate: 0.00895\n",
      "2025-12-21 04:38:31.600079: train_loss -0.8071\n",
      "2025-12-21 04:38:31.602447: val_loss -0.8129\n",
      "2025-12-21 04:38:31.602447: Pseudo dice [0.902, 0.9324, 0.9125]\n",
      "2025-12-21 04:38:31.605637: Epoch time: 137.98 s\n",
      "2025-12-21 04:38:32.243199: \n",
      "2025-12-21 04:38:32.243199: Epoch 117\n",
      "2025-12-21 04:38:32.245542: Current learning rate: 0.00894\n",
      "2025-12-21 04:40:50.142964: train_loss -0.8106\n",
      "2025-12-21 04:40:50.142964: val_loss -0.8283\n",
      "2025-12-21 04:40:50.144966: Pseudo dice [0.9123, 0.9415, 0.9163]\n",
      "2025-12-21 04:40:50.146968: Epoch time: 137.9 s\n",
      "2025-12-21 04:40:50.803458: \n",
      "2025-12-21 04:40:50.803458: Epoch 118\n",
      "2025-12-21 04:40:50.803458: Current learning rate: 0.00893\n",
      "2025-12-21 04:43:08.739775: train_loss -0.807\n",
      "2025-12-21 04:43:08.741777: val_loss -0.8106\n",
      "2025-12-21 04:43:08.745525: Pseudo dice [0.9015, 0.9363, 0.9031]\n",
      "2025-12-21 04:43:08.747528: Epoch time: 137.94 s\n",
      "2025-12-21 04:43:09.584449: \n",
      "2025-12-21 04:43:09.584449: Epoch 119\n",
      "2025-12-21 04:43:09.584449: Current learning rate: 0.00892\n",
      "2025-12-21 04:45:27.663033: train_loss -0.811\n",
      "2025-12-21 04:45:27.665035: val_loss -0.8432\n",
      "2025-12-21 04:45:27.666775: Pseudo dice [0.9229, 0.9487, 0.9077]\n",
      "2025-12-21 04:45:27.666775: Epoch time: 138.08 s\n",
      "2025-12-21 04:45:27.669606: Yayy! New best EMA pseudo Dice: 0.92\n",
      "2025-12-21 04:45:28.537440: \n",
      "2025-12-21 04:45:28.539443: Epoch 120\n",
      "2025-12-21 04:45:28.539443: Current learning rate: 0.00891\n",
      "2025-12-21 04:47:46.483991: train_loss -0.8153\n",
      "2025-12-21 04:47:46.485993: val_loss -0.8306\n",
      "2025-12-21 04:47:46.487996: Pseudo dice [0.9108, 0.9413, 0.9084]\n",
      "2025-12-21 04:47:46.487996: Epoch time: 137.95 s\n",
      "2025-12-21 04:47:46.491084: Yayy! New best EMA pseudo Dice: 0.92\n",
      "2025-12-21 04:47:47.388817: \n",
      "2025-12-21 04:47:47.388817: Epoch 121\n",
      "2025-12-21 04:47:47.388817: Current learning rate: 0.0089\n",
      "2025-12-21 04:50:05.446152: train_loss -0.7911\n",
      "2025-12-21 04:50:05.448153: val_loss -0.7375\n",
      "2025-12-21 04:50:05.450156: Pseudo dice [0.8604, 0.9007, 0.8899]\n",
      "2025-12-21 04:50:05.450156: Epoch time: 138.06 s\n",
      "2025-12-21 04:50:06.093598: \n",
      "2025-12-21 04:50:06.093598: Epoch 122\n",
      "2025-12-21 04:50:06.093598: Current learning rate: 0.00889\n",
      "2025-12-21 04:52:23.997229: train_loss -0.7884\n",
      "2025-12-21 04:52:23.997229: val_loss -0.8\n",
      "2025-12-21 04:52:24.008891: Pseudo dice [0.8964, 0.9283, 0.9183]\n",
      "2025-12-21 04:52:24.010893: Epoch time: 137.9 s\n",
      "2025-12-21 04:52:24.645159: \n",
      "2025-12-21 04:52:24.645159: Epoch 123\n",
      "2025-12-21 04:52:24.646140: Current learning rate: 0.00889\n",
      "2025-12-21 04:54:42.552345: train_loss -0.8096\n",
      "2025-12-21 04:54:42.552345: val_loss -0.8155\n",
      "2025-12-21 04:54:42.556199: Pseudo dice [0.9031, 0.9352, 0.9039]\n",
      "2025-12-21 04:54:42.556199: Epoch time: 137.92 s\n",
      "2025-12-21 04:54:43.201458: \n",
      "2025-12-21 04:54:43.201458: Epoch 124\n",
      "2025-12-21 04:54:43.201458: Current learning rate: 0.00888\n",
      "2025-12-21 04:57:01.129638: train_loss -0.8057\n",
      "2025-12-21 04:57:01.129638: val_loss -0.8223\n",
      "2025-12-21 04:57:01.131642: Pseudo dice [0.9014, 0.9405, 0.9197]\n",
      "2025-12-21 04:57:01.133645: Epoch time: 137.93 s\n",
      "2025-12-21 04:57:01.990354: \n",
      "2025-12-21 04:57:01.990354: Epoch 125\n",
      "2025-12-21 04:57:01.990354: Current learning rate: 0.00887\n",
      "2025-12-21 04:59:19.847258: train_loss -0.8012\n",
      "2025-12-21 04:59:19.849260: val_loss -0.8374\n",
      "2025-12-21 04:59:19.851262: Pseudo dice [0.9111, 0.9497, 0.9103]\n",
      "2025-12-21 04:59:19.853264: Epoch time: 137.86 s\n",
      "2025-12-21 04:59:20.488953: \n",
      "2025-12-21 04:59:20.488953: Epoch 126\n",
      "2025-12-21 04:59:20.488953: Current learning rate: 0.00886\n",
      "2025-12-21 05:01:38.578087: train_loss -0.8084\n",
      "2025-12-21 05:01:38.578087: val_loss -0.8355\n",
      "2025-12-21 05:01:38.578087: Pseudo dice [0.9128, 0.9435, 0.9179]\n",
      "2025-12-21 05:01:38.578087: Epoch time: 138.09 s\n",
      "2025-12-21 05:01:39.212649: \n",
      "2025-12-21 05:01:39.212649: Epoch 127\n",
      "2025-12-21 05:01:39.212649: Current learning rate: 0.00885\n",
      "2025-12-21 05:03:56.949729: train_loss -0.8107\n",
      "2025-12-21 05:03:56.949729: val_loss -0.8337\n",
      "2025-12-21 05:03:56.953736: Pseudo dice [0.911, 0.9411, 0.914]\n",
      "2025-12-21 05:03:56.955738: Epoch time: 137.74 s\n",
      "2025-12-21 05:03:57.762934: \n",
      "2025-12-21 05:03:57.762934: Epoch 128\n",
      "2025-12-21 05:03:57.762934: Current learning rate: 0.00884\n",
      "2025-12-21 05:06:15.709572: train_loss -0.8168\n",
      "2025-12-21 05:06:15.709572: val_loss -0.835\n",
      "2025-12-21 05:06:15.709572: Pseudo dice [0.9156, 0.9467, 0.9043]\n",
      "2025-12-21 05:06:15.709572: Epoch time: 137.95 s\n",
      "2025-12-21 05:06:16.346319: \n",
      "2025-12-21 05:06:16.346319: Epoch 129\n",
      "2025-12-21 05:06:16.346319: Current learning rate: 0.00883\n",
      "2025-12-21 05:08:34.214648: train_loss -0.812\n",
      "2025-12-21 05:08:34.214648: val_loss -0.8244\n",
      "2025-12-21 05:08:34.216650: Pseudo dice [0.9014, 0.9369, 0.9273]\n",
      "2025-12-21 05:08:34.218651: Epoch time: 137.87 s\n",
      "2025-12-21 05:08:34.859930: \n",
      "2025-12-21 05:08:34.859930: Epoch 130\n",
      "2025-12-21 05:08:34.859930: Current learning rate: 0.00882\n",
      "2025-12-21 05:10:53.223937: train_loss -0.8134\n",
      "2025-12-21 05:10:53.223937: val_loss -0.8334\n",
      "2025-12-21 05:10:53.227679: Pseudo dice [0.9096, 0.9391, 0.9194]\n",
      "2025-12-21 05:10:53.229681: Epoch time: 138.36 s\n",
      "2025-12-21 05:10:54.180310: \n",
      "2025-12-21 05:10:54.180310: Epoch 131\n",
      "2025-12-21 05:10:54.180310: Current learning rate: 0.00881\n",
      "2025-12-21 05:13:12.252054: train_loss -0.814\n",
      "2025-12-21 05:13:12.252054: val_loss -0.8204\n",
      "2025-12-21 05:13:12.252054: Pseudo dice [0.9002, 0.9394, 0.906]\n",
      "2025-12-21 05:13:12.252054: Epoch time: 138.07 s\n",
      "2025-12-21 05:13:12.896940: \n",
      "2025-12-21 05:13:12.896940: Epoch 132\n",
      "2025-12-21 05:13:12.896940: Current learning rate: 0.0088\n",
      "2025-12-21 05:15:30.834666: train_loss -0.8083\n",
      "2025-12-21 05:15:30.834666: val_loss -0.8166\n",
      "2025-12-21 05:15:30.838343: Pseudo dice [0.9047, 0.937, 0.9147]\n",
      "2025-12-21 05:15:30.840345: Epoch time: 137.94 s\n",
      "2025-12-21 05:15:31.468567: \n",
      "2025-12-21 05:15:31.468567: Epoch 133\n",
      "2025-12-21 05:15:31.468567: Current learning rate: 0.00879\n",
      "2025-12-21 05:17:49.310548: train_loss -0.8166\n",
      "2025-12-21 05:17:49.310548: val_loss -0.8264\n",
      "2025-12-21 05:17:49.312550: Pseudo dice [0.9116, 0.9409, 0.8972]\n",
      "2025-12-21 05:17:49.314291: Epoch time: 137.84 s\n",
      "2025-12-21 05:17:50.074034: \n",
      "2025-12-21 05:17:50.074034: Epoch 134\n",
      "2025-12-21 05:17:50.075775: Current learning rate: 0.00879\n",
      "2025-12-21 05:20:07.942352: train_loss -0.8163\n",
      "2025-12-21 05:20:07.942352: val_loss -0.8289\n",
      "2025-12-21 05:20:07.942352: Pseudo dice [0.9066, 0.9388, 0.9222]\n",
      "2025-12-21 05:20:07.942352: Epoch time: 137.87 s\n",
      "2025-12-21 05:20:08.575224: \n",
      "2025-12-21 05:20:08.591036: Epoch 135\n",
      "2025-12-21 05:20:08.591036: Current learning rate: 0.00878\n",
      "2025-12-21 05:22:26.466424: train_loss -0.8098\n",
      "2025-12-21 05:22:26.466424: val_loss -0.8362\n",
      "2025-12-21 05:22:26.466424: Pseudo dice [0.9104, 0.9388, 0.9287]\n",
      "2025-12-21 05:22:26.482507: Epoch time: 137.89 s\n",
      "2025-12-21 05:22:27.131711: \n",
      "2025-12-21 05:22:27.131711: Epoch 136\n",
      "2025-12-21 05:22:27.131711: Current learning rate: 0.00877\n",
      "2025-12-21 05:24:44.987877: train_loss -0.8165\n",
      "2025-12-21 05:24:44.987877: val_loss -0.8481\n",
      "2025-12-21 05:24:44.993886: Pseudo dice [0.9217, 0.9523, 0.9192]\n",
      "2025-12-21 05:24:44.995888: Epoch time: 137.87 s\n",
      "2025-12-21 05:24:44.997893: Yayy! New best EMA pseudo Dice: 0.9209\n",
      "2025-12-21 05:24:45.873706: \n",
      "2025-12-21 05:24:45.873706: Epoch 137\n",
      "2025-12-21 05:24:45.873706: Current learning rate: 0.00876\n",
      "2025-12-21 05:27:03.884983: train_loss -0.8166\n",
      "2025-12-21 05:27:03.884983: val_loss -0.8321\n",
      "2025-12-21 05:27:03.884983: Pseudo dice [0.9059, 0.9406, 0.9188]\n",
      "2025-12-21 05:27:03.884983: Epoch time: 138.01 s\n",
      "2025-12-21 05:27:03.900783: Yayy! New best EMA pseudo Dice: 0.921\n",
      "2025-12-21 05:27:04.791445: \n",
      "2025-12-21 05:27:04.791445: Epoch 138\n",
      "2025-12-21 05:27:04.791445: Current learning rate: 0.00875\n",
      "2025-12-21 05:29:22.792122: train_loss -0.8181\n",
      "2025-12-21 05:29:22.792122: val_loss -0.8275\n",
      "2025-12-21 05:29:22.796127: Pseudo dice [0.9052, 0.9391, 0.9166]\n",
      "2025-12-21 05:29:22.796127: Epoch time: 138.0 s\n",
      "2025-12-21 05:29:23.431605: \n",
      "2025-12-21 05:29:23.447540: Epoch 139\n",
      "2025-12-21 05:29:23.447540: Current learning rate: 0.00874\n",
      "2025-12-21 05:31:41.376281: train_loss -0.8143\n",
      "2025-12-21 05:31:41.376281: val_loss -0.8207\n",
      "2025-12-21 05:31:41.378283: Pseudo dice [0.9018, 0.9358, 0.9175]\n",
      "2025-12-21 05:31:41.378283: Epoch time: 137.94 s\n",
      "2025-12-21 05:31:42.022801: \n",
      "2025-12-21 05:31:42.022801: Epoch 140\n",
      "2025-12-21 05:31:42.022801: Current learning rate: 0.00873\n",
      "2025-12-21 05:34:00.033511: train_loss -0.8196\n",
      "2025-12-21 05:34:00.033511: val_loss -0.8356\n",
      "2025-12-21 05:34:00.035514: Pseudo dice [0.9136, 0.9461, 0.9133]\n",
      "2025-12-21 05:34:00.037515: Epoch time: 138.01 s\n",
      "2025-12-21 05:34:00.037515: Yayy! New best EMA pseudo Dice: 0.921\n",
      "2025-12-21 05:34:00.897543: \n",
      "2025-12-21 05:34:00.897543: Epoch 141\n",
      "2025-12-21 05:34:00.913368: Current learning rate: 0.00872\n",
      "2025-12-21 05:36:18.782104: train_loss -0.8158\n",
      "2025-12-21 05:36:18.782104: val_loss -0.843\n",
      "2025-12-21 05:36:18.782104: Pseudo dice [0.9177, 0.9461, 0.9292]\n",
      "2025-12-21 05:36:18.782104: Epoch time: 137.88 s\n",
      "2025-12-21 05:36:18.782104: Yayy! New best EMA pseudo Dice: 0.922\n",
      "2025-12-21 05:36:19.852144: \n",
      "2025-12-21 05:36:19.852144: Epoch 142\n",
      "2025-12-21 05:36:19.867892: Current learning rate: 0.00871\n",
      "2025-12-21 05:38:37.764145: train_loss -0.8178\n",
      "2025-12-21 05:38:37.764145: val_loss -0.8459\n",
      "2025-12-21 05:38:37.776900: Pseudo dice [0.9193, 0.9502, 0.9167]\n",
      "2025-12-21 05:38:37.776900: Epoch time: 137.91 s\n",
      "2025-12-21 05:38:37.780199: Yayy! New best EMA pseudo Dice: 0.9227\n",
      "2025-12-21 05:38:38.668256: \n",
      "2025-12-21 05:38:38.668256: Epoch 143\n",
      "2025-12-21 05:38:38.668256: Current learning rate: 0.0087\n",
      "2025-12-21 05:40:56.444218: train_loss -0.818\n",
      "2025-12-21 05:40:56.446083: val_loss -0.8449\n",
      "2025-12-21 05:40:56.446083: Pseudo dice [0.9143, 0.9466, 0.9291]\n",
      "2025-12-21 05:40:56.446083: Epoch time: 137.78 s\n",
      "2025-12-21 05:40:56.446083: Yayy! New best EMA pseudo Dice: 0.9234\n",
      "2025-12-21 05:40:57.355216: \n",
      "2025-12-21 05:40:57.355216: Epoch 144\n",
      "2025-12-21 05:40:57.357219: Current learning rate: 0.00869\n",
      "2025-12-21 05:43:15.074841: train_loss -0.8203\n",
      "2025-12-21 05:43:15.074841: val_loss -0.8336\n",
      "2025-12-21 05:43:15.076583: Pseudo dice [0.9068, 0.9397, 0.9198]\n",
      "2025-12-21 05:43:15.076583: Epoch time: 137.72 s\n",
      "2025-12-21 05:43:15.727347: \n",
      "2025-12-21 05:43:15.727347: Epoch 145\n",
      "2025-12-21 05:43:15.727347: Current learning rate: 0.00868\n",
      "2025-12-21 05:45:33.698749: train_loss -0.8206\n",
      "2025-12-21 05:45:33.698749: val_loss -0.827\n",
      "2025-12-21 05:45:33.698749: Pseudo dice [0.9017, 0.9399, 0.9247]\n",
      "2025-12-21 05:45:33.698749: Epoch time: 137.97 s\n",
      "2025-12-21 05:45:34.354024: \n",
      "2025-12-21 05:45:34.354024: Epoch 146\n",
      "2025-12-21 05:45:34.354024: Current learning rate: 0.00868\n",
      "2025-12-21 05:47:52.255085: train_loss -0.8188\n",
      "2025-12-21 05:47:52.257087: val_loss -0.8409\n",
      "2025-12-21 05:47:52.259089: Pseudo dice [0.9206, 0.9479, 0.9123]\n",
      "2025-12-21 05:47:52.260830: Epoch time: 137.9 s\n",
      "2025-12-21 05:47:52.260830: Yayy! New best EMA pseudo Dice: 0.9236\n",
      "2025-12-21 05:47:53.132910: \n",
      "2025-12-21 05:47:53.132910: Epoch 147\n",
      "2025-12-21 05:47:53.132910: Current learning rate: 0.00867\n",
      "2025-12-21 05:50:11.063017: train_loss -0.8201\n",
      "2025-12-21 05:50:11.063017: val_loss -0.8461\n",
      "2025-12-21 05:50:11.067023: Pseudo dice [0.9185, 0.9514, 0.9177]\n",
      "2025-12-21 05:50:11.069026: Epoch time: 137.93 s\n",
      "2025-12-21 05:50:11.071029: Yayy! New best EMA pseudo Dice: 0.9241\n",
      "2025-12-21 05:50:12.328966: \n",
      "2025-12-21 05:50:12.328966: Epoch 148\n",
      "2025-12-21 05:50:12.328966: Current learning rate: 0.00866\n",
      "2025-12-21 05:52:30.118115: train_loss -0.8105\n",
      "2025-12-21 05:52:30.118115: val_loss -0.8411\n",
      "2025-12-21 05:52:30.118115: Pseudo dice [0.9122, 0.9454, 0.9208]\n",
      "2025-12-21 05:52:30.126763: Epoch time: 137.79 s\n",
      "2025-12-21 05:52:30.128766: Yayy! New best EMA pseudo Dice: 0.9243\n",
      "2025-12-21 05:52:31.057296: \n",
      "2025-12-21 05:52:31.057296: Epoch 149\n",
      "2025-12-21 05:52:31.057296: Current learning rate: 0.00865\n",
      "2025-12-21 05:54:49.083815: train_loss -0.8197\n",
      "2025-12-21 05:54:49.085817: val_loss -0.8389\n",
      "2025-12-21 05:54:49.087819: Pseudo dice [0.9155, 0.9476, 0.9089]\n",
      "2025-12-21 05:54:49.089821: Epoch time: 138.03 s\n",
      "2025-12-21 05:54:49.977719: \n",
      "2025-12-21 05:54:49.977719: Epoch 150\n",
      "2025-12-21 05:54:49.977719: Current learning rate: 0.00864\n",
      "2025-12-21 05:57:07.993537: train_loss -0.8256\n",
      "2025-12-21 05:57:07.993537: val_loss -0.8316\n",
      "2025-12-21 05:57:07.997541: Pseudo dice [0.9107, 0.9398, 0.9061]\n",
      "2025-12-21 05:57:07.999544: Epoch time: 138.02 s\n",
      "2025-12-21 05:57:08.767317: \n",
      "2025-12-21 05:57:08.767317: Epoch 151\n",
      "2025-12-21 05:57:08.767317: Current learning rate: 0.00863\n",
      "2025-12-21 05:59:26.497189: train_loss -0.8223\n",
      "2025-12-21 05:59:26.497189: val_loss -0.8389\n",
      "2025-12-21 05:59:26.512841: Pseudo dice [0.9114, 0.943, 0.9187]\n",
      "2025-12-21 05:59:26.512841: Epoch time: 137.73 s\n",
      "2025-12-21 05:59:27.162438: \n",
      "2025-12-21 05:59:27.162438: Epoch 152\n",
      "2025-12-21 05:59:27.162438: Current learning rate: 0.00862\n",
      "2025-12-21 06:01:44.972989: train_loss -0.8219\n",
      "2025-12-21 06:01:44.972989: val_loss -0.8506\n",
      "2025-12-21 06:01:44.974991: Pseudo dice [0.9219, 0.9472, 0.9251]\n",
      "2025-12-21 06:01:44.976993: Epoch time: 137.81 s\n",
      "2025-12-21 06:01:44.978995: Yayy! New best EMA pseudo Dice: 0.9246\n",
      "2025-12-21 06:01:45.862153: \n",
      "2025-12-21 06:01:45.864156: Epoch 153\n",
      "2025-12-21 06:01:45.864156: Current learning rate: 0.00861\n",
      "2025-12-21 06:04:03.873173: train_loss -0.8084\n",
      "2025-12-21 06:04:03.873173: val_loss -0.8097\n",
      "2025-12-21 06:04:03.875175: Pseudo dice [0.9013, 0.9307, 0.9098]\n",
      "2025-12-21 06:04:03.877177: Epoch time: 138.01 s\n",
      "2025-12-21 06:04:04.684340: \n",
      "2025-12-21 06:04:04.684340: Epoch 154\n",
      "2025-12-21 06:04:04.684340: Current learning rate: 0.0086\n",
      "2025-12-21 06:06:22.617383: train_loss -0.8123\n",
      "2025-12-21 06:06:22.617383: val_loss -0.8364\n",
      "2025-12-21 06:06:22.619385: Pseudo dice [0.9132, 0.9404, 0.9233]\n",
      "2025-12-21 06:06:22.621387: Epoch time: 137.93 s\n",
      "2025-12-21 06:06:23.260461: \n",
      "2025-12-21 06:06:23.260461: Epoch 155\n",
      "2025-12-21 06:06:23.260461: Current learning rate: 0.00859\n",
      "2025-12-21 06:08:41.341550: train_loss -0.8108\n",
      "2025-12-21 06:08:41.341550: val_loss -0.8235\n",
      "2025-12-21 06:08:41.345387: Pseudo dice [0.9047, 0.9416, 0.9195]\n",
      "2025-12-21 06:08:41.347390: Epoch time: 138.08 s\n",
      "2025-12-21 06:08:42.006307: \n",
      "2025-12-21 06:08:42.006307: Epoch 156\n",
      "2025-12-21 06:08:42.006307: Current learning rate: 0.00858\n",
      "2025-12-21 06:11:00.281232: train_loss -0.8242\n",
      "2025-12-21 06:11:00.281232: val_loss -0.8321\n",
      "2025-12-21 06:11:00.289949: Pseudo dice [0.9056, 0.9395, 0.9289]\n",
      "2025-12-21 06:11:00.289949: Epoch time: 138.27 s\n",
      "2025-12-21 06:11:01.053808: \n",
      "2025-12-21 06:11:01.053808: Epoch 157\n",
      "2025-12-21 06:11:01.053808: Current learning rate: 0.00858\n",
      "2025-12-21 06:13:18.820998: train_loss -0.8188\n",
      "2025-12-21 06:13:18.823002: val_loss -0.824\n",
      "2025-12-21 06:13:18.827007: Pseudo dice [0.9043, 0.935, 0.9128]\n",
      "2025-12-21 06:13:18.829010: Epoch time: 137.77 s\n",
      "2025-12-21 06:13:19.479234: \n",
      "2025-12-21 06:13:19.479234: Epoch 158\n",
      "2025-12-21 06:13:19.479234: Current learning rate: 0.00857\n",
      "2025-12-21 06:15:37.454806: train_loss -0.8169\n",
      "2025-12-21 06:15:37.454806: val_loss -0.811\n",
      "2025-12-21 06:15:37.456809: Pseudo dice [0.9031, 0.9389, 0.9167]\n",
      "2025-12-21 06:15:37.458811: Epoch time: 137.98 s\n",
      "2025-12-21 06:15:38.272460: \n",
      "2025-12-21 06:15:38.272460: Epoch 159\n",
      "2025-12-21 06:15:38.272460: Current learning rate: 0.00856\n",
      "2025-12-21 06:17:56.193266: train_loss -0.8154\n",
      "2025-12-21 06:17:56.193266: val_loss -0.8288\n",
      "2025-12-21 06:17:56.195007: Pseudo dice [0.9062, 0.9384, 0.9189]\n",
      "2025-12-21 06:17:56.195007: Epoch time: 137.92 s\n",
      "2025-12-21 06:17:56.949706: \n",
      "2025-12-21 06:17:56.949706: Epoch 160\n",
      "2025-12-21 06:17:56.949706: Current learning rate: 0.00855\n",
      "2025-12-21 06:20:14.792757: train_loss -0.8203\n",
      "2025-12-21 06:20:14.794760: val_loss -0.8327\n",
      "2025-12-21 06:20:14.797043: Pseudo dice [0.9125, 0.9426, 0.9189]\n",
      "2025-12-21 06:20:14.799934: Epoch time: 137.85 s\n",
      "2025-12-21 06:20:15.447983: \n",
      "2025-12-21 06:20:15.447983: Epoch 161\n",
      "2025-12-21 06:20:15.447983: Current learning rate: 0.00854\n",
      "2025-12-21 06:22:33.351009: train_loss -0.8177\n",
      "2025-12-21 06:22:33.351009: val_loss -0.8474\n",
      "2025-12-21 06:22:33.353011: Pseudo dice [0.9209, 0.9475, 0.9247]\n",
      "2025-12-21 06:22:33.357019: Epoch time: 137.9 s\n",
      "2025-12-21 06:22:34.005737: \n",
      "2025-12-21 06:22:34.005737: Epoch 162\n",
      "2025-12-21 06:22:34.005737: Current learning rate: 0.00853\n",
      "2025-12-21 06:24:51.948159: train_loss -0.8168\n",
      "2025-12-21 06:24:51.948159: val_loss -0.8025\n",
      "2025-12-21 06:24:51.948159: Pseudo dice [0.8927, 0.9264, 0.9148]\n",
      "2025-12-21 06:24:51.948159: Epoch time: 137.94 s\n",
      "2025-12-21 06:24:52.613366: \n",
      "2025-12-21 06:24:52.613366: Epoch 163\n",
      "2025-12-21 06:24:52.613366: Current learning rate: 0.00852\n",
      "2025-12-21 06:27:10.662506: train_loss -0.8163\n",
      "2025-12-21 06:27:10.662506: val_loss -0.8169\n",
      "2025-12-21 06:27:10.678465: Pseudo dice [0.9018, 0.9333, 0.9142]\n",
      "2025-12-21 06:27:10.678465: Epoch time: 138.05 s\n",
      "2025-12-21 06:27:11.314337: \n",
      "2025-12-21 06:27:11.314337: Epoch 164\n",
      "2025-12-21 06:27:11.314337: Current learning rate: 0.00851\n",
      "2025-12-21 06:29:29.291563: train_loss -0.8166\n",
      "2025-12-21 06:29:29.293566: val_loss -0.8452\n",
      "2025-12-21 06:29:29.295568: Pseudo dice [0.9245, 0.9495, 0.9064]\n",
      "2025-12-21 06:29:29.297571: Epoch time: 137.98 s\n",
      "2025-12-21 06:29:30.099766: \n",
      "2025-12-21 06:29:30.099766: Epoch 165\n",
      "2025-12-21 06:29:30.099766: Current learning rate: 0.0085\n",
      "2025-12-21 06:31:47.825740: train_loss -0.8209\n",
      "2025-12-21 06:31:47.826242: val_loss -0.8508\n",
      "2025-12-21 06:31:47.826745: Pseudo dice [0.9164, 0.952, 0.93]\n",
      "2025-12-21 06:31:47.826745: Epoch time: 137.73 s\n",
      "2025-12-21 06:31:48.466821: \n",
      "2025-12-21 06:31:48.466821: Epoch 166\n",
      "2025-12-21 06:31:48.466821: Current learning rate: 0.00849\n",
      "2025-12-21 06:34:06.178474: train_loss -0.8244\n",
      "2025-12-21 06:34:06.178474: val_loss -0.8428\n",
      "2025-12-21 06:34:06.178474: Pseudo dice [0.9176, 0.9465, 0.921]\n",
      "2025-12-21 06:34:06.178474: Epoch time: 137.71 s\n",
      "2025-12-21 06:34:06.827912: \n",
      "2025-12-21 06:34:06.827912: Epoch 167\n",
      "2025-12-21 06:34:06.827912: Current learning rate: 0.00848\n",
      "2025-12-21 06:36:24.708897: train_loss -0.8221\n",
      "2025-12-21 06:36:24.710900: val_loss -0.8249\n",
      "2025-12-21 06:36:24.710900: Pseudo dice [0.9028, 0.9337, 0.9156]\n",
      "2025-12-21 06:36:24.710900: Epoch time: 137.88 s\n",
      "2025-12-21 06:36:25.413543: \n",
      "2025-12-21 06:36:25.413543: Epoch 168\n",
      "2025-12-21 06:36:25.413543: Current learning rate: 0.00847\n",
      "2025-12-21 06:38:43.259479: train_loss -0.8269\n",
      "2025-12-21 06:38:43.261482: val_loss -0.8487\n",
      "2025-12-21 06:38:43.263485: Pseudo dice [0.9186, 0.9464, 0.9316]\n",
      "2025-12-21 06:38:43.265225: Epoch time: 137.85 s\n",
      "2025-12-21 06:38:43.914055: \n",
      "2025-12-21 06:38:43.914055: Epoch 169\n",
      "2025-12-21 06:38:43.914055: Current learning rate: 0.00847\n",
      "2025-12-21 06:41:02.064478: train_loss -0.824\n",
      "2025-12-21 06:41:02.066219: val_loss -0.8384\n",
      "2025-12-21 06:41:02.070224: Pseudo dice [0.9125, 0.9482, 0.9203]\n",
      "2025-12-21 06:41:02.072227: Epoch time: 138.15 s\n",
      "2025-12-21 06:41:02.731422: \n",
      "2025-12-21 06:41:02.731422: Epoch 170\n",
      "2025-12-21 06:41:02.731422: Current learning rate: 0.00846\n",
      "2025-12-21 06:43:20.594735: train_loss -0.8275\n",
      "2025-12-21 06:43:20.594735: val_loss -0.8486\n",
      "2025-12-21 06:43:20.603797: Pseudo dice [0.918, 0.9478, 0.9272]\n",
      "2025-12-21 06:43:20.603797: Epoch time: 137.88 s\n",
      "2025-12-21 06:43:20.603797: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2025-12-21 06:43:21.854597: \n",
      "2025-12-21 06:43:21.854597: Epoch 171\n",
      "2025-12-21 06:43:21.854597: Current learning rate: 0.00845\n",
      "2025-12-21 06:45:39.750698: train_loss -0.8208\n",
      "2025-12-21 06:45:39.750698: val_loss -0.8488\n",
      "2025-12-21 06:45:39.759250: Pseudo dice [0.9156, 0.9519, 0.9277]\n",
      "2025-12-21 06:45:39.759250: Epoch time: 137.9 s\n",
      "2025-12-21 06:45:39.759250: Yayy! New best EMA pseudo Dice: 0.9257\n",
      "2025-12-21 06:45:40.670433: \n",
      "2025-12-21 06:45:40.670433: Epoch 172\n",
      "2025-12-21 06:45:40.670433: Current learning rate: 0.00844\n",
      "2025-12-21 06:47:58.506942: train_loss -0.8249\n",
      "2025-12-21 06:47:58.508945: val_loss -0.8463\n",
      "2025-12-21 06:47:58.510949: Pseudo dice [0.9169, 0.9467, 0.9257]\n",
      "2025-12-21 06:47:58.512689: Epoch time: 137.84 s\n",
      "2025-12-21 06:47:58.514692: Yayy! New best EMA pseudo Dice: 0.9261\n",
      "2025-12-21 06:47:59.415754: \n",
      "2025-12-21 06:47:59.415754: Epoch 173\n",
      "2025-12-21 06:47:59.415754: Current learning rate: 0.00843\n",
      "2025-12-21 06:50:17.354433: train_loss -0.8234\n",
      "2025-12-21 06:50:17.354433: val_loss -0.8363\n",
      "2025-12-21 06:50:17.354433: Pseudo dice [0.9092, 0.9365, 0.928]\n",
      "2025-12-21 06:50:17.359597: Epoch time: 137.94 s\n",
      "2025-12-21 06:50:18.097091: \n",
      "2025-12-21 06:50:18.097091: Epoch 174\n",
      "2025-12-21 06:50:18.097091: Current learning rate: 0.00842\n",
      "2025-12-21 06:52:35.807996: train_loss -0.8228\n",
      "2025-12-21 06:52:35.807996: val_loss -0.8441\n",
      "2025-12-21 06:52:35.809998: Pseudo dice [0.9157, 0.9413, 0.9275]\n",
      "2025-12-21 06:52:35.812002: Epoch time: 137.71 s\n",
      "2025-12-21 06:52:35.814004: Yayy! New best EMA pseudo Dice: 0.9262\n",
      "2025-12-21 06:52:36.719626: \n",
      "2025-12-21 06:52:36.719626: Epoch 175\n",
      "2025-12-21 06:52:36.735327: Current learning rate: 0.00841\n",
      "2025-12-21 06:54:54.831028: train_loss -0.8235\n",
      "2025-12-21 06:54:54.833030: val_loss -0.8411\n",
      "2025-12-21 06:54:54.837035: Pseudo dice [0.916, 0.9459, 0.9192]\n",
      "2025-12-21 06:54:54.841040: Epoch time: 138.11 s\n",
      "2025-12-21 06:54:54.843043: Yayy! New best EMA pseudo Dice: 0.9263\n",
      "2025-12-21 06:54:55.941341: \n",
      "2025-12-21 06:54:55.941341: Epoch 176\n",
      "2025-12-21 06:54:55.941341: Current learning rate: 0.0084\n",
      "2025-12-21 06:57:13.602004: train_loss -0.8237\n",
      "2025-12-21 06:57:13.603745: val_loss -0.8597\n",
      "2025-12-21 06:57:13.605748: Pseudo dice [0.9239, 0.9535, 0.9385]\n",
      "2025-12-21 06:57:13.607750: Epoch time: 137.66 s\n",
      "2025-12-21 06:57:13.609752: Yayy! New best EMA pseudo Dice: 0.9275\n",
      "2025-12-21 06:57:14.710943: \n",
      "2025-12-21 06:57:14.712684: Epoch 177\n",
      "2025-12-21 06:57:14.712684: Current learning rate: 0.00839\n",
      "2025-12-21 06:59:32.587392: train_loss -0.8265\n",
      "2025-12-21 06:59:32.587392: val_loss -0.831\n",
      "2025-12-21 06:59:32.587392: Pseudo dice [0.9046, 0.9429, 0.9219]\n",
      "2025-12-21 06:59:32.587392: Epoch time: 137.88 s\n",
      "2025-12-21 06:59:33.233515: \n",
      "2025-12-21 06:59:33.233515: Epoch 178\n",
      "2025-12-21 06:59:33.233515: Current learning rate: 0.00838\n",
      "2025-12-21 07:01:51.237979: train_loss -0.8205\n",
      "2025-12-21 07:01:51.253945: val_loss -0.8113\n",
      "2025-12-21 07:01:51.253945: Pseudo dice [0.893, 0.9306, 0.9156]\n",
      "2025-12-21 07:01:51.253945: Epoch time: 138.0 s\n",
      "2025-12-21 07:01:51.902301: \n",
      "2025-12-21 07:01:51.902301: Epoch 179\n",
      "2025-12-21 07:01:51.902301: Current learning rate: 0.00837\n",
      "2025-12-21 07:04:09.757290: train_loss -0.8318\n",
      "2025-12-21 07:04:09.757290: val_loss -0.8585\n",
      "2025-12-21 07:04:09.757290: Pseudo dice [0.9247, 0.9534, 0.9239]\n",
      "2025-12-21 07:04:09.757290: Epoch time: 137.87 s\n",
      "2025-12-21 07:04:10.517078: \n",
      "2025-12-21 07:04:10.517078: Epoch 180\n",
      "2025-12-21 07:04:10.517078: Current learning rate: 0.00836\n",
      "2025-12-21 07:06:28.454338: train_loss -0.8319\n",
      "2025-12-21 07:06:28.454338: val_loss -0.8481\n",
      "2025-12-21 07:06:28.458081: Pseudo dice [0.9231, 0.9497, 0.9205]\n",
      "2025-12-21 07:06:28.458081: Epoch time: 137.94 s\n",
      "2025-12-21 07:06:29.107295: \n",
      "2025-12-21 07:06:29.107295: Epoch 181\n",
      "2025-12-21 07:06:29.123244: Current learning rate: 0.00836\n",
      "2025-12-21 07:08:47.215820: train_loss -0.8247\n",
      "2025-12-21 07:08:47.215820: val_loss -0.8525\n",
      "2025-12-21 07:08:47.215820: Pseudo dice [0.9236, 0.9516, 0.9153]\n",
      "2025-12-21 07:08:47.215820: Epoch time: 138.11 s\n",
      "2025-12-21 07:08:48.042814: \n",
      "2025-12-21 07:08:48.042814: Epoch 182\n",
      "2025-12-21 07:08:48.042814: Current learning rate: 0.00835\n",
      "2025-12-21 07:11:06.344431: train_loss -0.8257\n",
      "2025-12-21 07:11:06.344431: val_loss -0.8546\n",
      "2025-12-21 07:11:06.344431: Pseudo dice [0.9215, 0.946, 0.9258]\n",
      "2025-12-21 07:11:06.344431: Epoch time: 138.3 s\n",
      "2025-12-21 07:11:06.360432: Yayy! New best EMA pseudo Dice: 0.9277\n",
      "2025-12-21 07:11:07.325689: \n",
      "2025-12-21 07:11:07.325689: Epoch 183\n",
      "2025-12-21 07:11:07.325689: Current learning rate: 0.00834\n",
      "2025-12-21 07:13:25.301532: train_loss -0.8269\n",
      "2025-12-21 07:13:25.303537: val_loss -0.8463\n",
      "2025-12-21 07:13:25.307285: Pseudo dice [0.9184, 0.9465, 0.9281]\n",
      "2025-12-21 07:13:25.309287: Epoch time: 137.98 s\n",
      "2025-12-21 07:13:25.311289: Yayy! New best EMA pseudo Dice: 0.928\n",
      "2025-12-21 07:13:26.210828: \n",
      "2025-12-21 07:13:26.210828: Epoch 184\n",
      "2025-12-21 07:13:26.210828: Current learning rate: 0.00833\n",
      "2025-12-21 07:15:44.344748: train_loss -0.8234\n",
      "2025-12-21 07:15:44.344748: val_loss -0.8312\n",
      "2025-12-21 07:15:44.360442: Pseudo dice [0.9119, 0.9399, 0.9112]\n",
      "2025-12-21 07:15:44.360442: Epoch time: 138.13 s\n",
      "2025-12-21 07:15:45.007727: \n",
      "2025-12-21 07:15:45.007727: Epoch 185\n",
      "2025-12-21 07:15:45.007727: Current learning rate: 0.00832\n",
      "2025-12-21 07:18:03.063769: train_loss -0.8262\n",
      "2025-12-21 07:18:03.063769: val_loss -0.8563\n",
      "2025-12-21 07:18:03.063769: Pseudo dice [0.9233, 0.9511, 0.9298]\n",
      "2025-12-21 07:18:03.063769: Epoch time: 138.06 s\n",
      "2025-12-21 07:18:03.063769: Yayy! New best EMA pseudo Dice: 0.928\n",
      "2025-12-21 07:18:03.998459: \n",
      "2025-12-21 07:18:03.998459: Epoch 186\n",
      "2025-12-21 07:18:03.998459: Current learning rate: 0.00831\n",
      "2025-12-21 07:20:22.054327: train_loss -0.8255\n",
      "2025-12-21 07:20:22.054327: val_loss -0.8647\n",
      "2025-12-21 07:20:22.060335: Pseudo dice [0.9264, 0.9554, 0.9312]\n",
      "2025-12-21 07:20:22.062338: Epoch time: 138.06 s\n",
      "2025-12-21 07:20:22.062338: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2025-12-21 07:20:22.972230: \n",
      "2025-12-21 07:20:22.972230: Epoch 187\n",
      "2025-12-21 07:20:22.972230: Current learning rate: 0.0083\n",
      "2025-12-21 07:22:40.728629: train_loss -0.8289\n",
      "2025-12-21 07:22:40.728629: val_loss -0.8481\n",
      "2025-12-21 07:22:40.730631: Pseudo dice [0.9191, 0.9473, 0.9244]\n",
      "2025-12-21 07:22:40.730631: Epoch time: 137.76 s\n",
      "2025-12-21 07:22:40.735878: Yayy! New best EMA pseudo Dice: 0.9291\n",
      "2025-12-21 07:22:41.832029: \n",
      "2025-12-21 07:22:41.832029: Epoch 188\n",
      "2025-12-21 07:22:41.832029: Current learning rate: 0.00829\n",
      "2025-12-21 07:24:59.629142: train_loss -0.8336\n",
      "2025-12-21 07:24:59.629142: val_loss -0.8491\n",
      "2025-12-21 07:24:59.629142: Pseudo dice [0.9172, 0.9479, 0.9318]\n",
      "2025-12-21 07:24:59.629142: Epoch time: 137.8 s\n",
      "2025-12-21 07:24:59.629142: Yayy! New best EMA pseudo Dice: 0.9294\n",
      "2025-12-21 07:25:00.547119: \n",
      "2025-12-21 07:25:00.547119: Epoch 189\n",
      "2025-12-21 07:25:00.557455: Current learning rate: 0.00828\n",
      "2025-12-21 07:27:18.298160: train_loss -0.8303\n",
      "2025-12-21 07:27:18.298160: val_loss -0.845\n",
      "2025-12-21 07:27:18.300163: Pseudo dice [0.9179, 0.9482, 0.9119]\n",
      "2025-12-21 07:27:18.302165: Epoch time: 137.75 s\n",
      "2025-12-21 07:27:18.956692: \n",
      "2025-12-21 07:27:18.956692: Epoch 190\n",
      "2025-12-21 07:27:18.956692: Current learning rate: 0.00827\n",
      "2025-12-21 07:29:36.877980: train_loss -0.8282\n",
      "2025-12-21 07:29:36.879982: val_loss -0.8523\n",
      "2025-12-21 07:29:36.881825: Pseudo dice [0.9236, 0.9474, 0.9237]\n",
      "2025-12-21 07:29:36.883827: Epoch time: 137.92 s\n",
      "2025-12-21 07:29:37.529400: \n",
      "2025-12-21 07:29:37.529400: Epoch 191\n",
      "2025-12-21 07:29:37.529400: Current learning rate: 0.00826\n",
      "2025-12-21 07:31:55.360260: train_loss -0.8274\n",
      "2025-12-21 07:31:55.360260: val_loss -0.8391\n",
      "2025-12-21 07:31:55.363999: Pseudo dice [0.9089, 0.9379, 0.9321]\n",
      "2025-12-21 07:31:55.366001: Epoch time: 137.83 s\n",
      "2025-12-21 07:31:56.020464: \n",
      "2025-12-21 07:31:56.020464: Epoch 192\n",
      "2025-12-21 07:31:56.020464: Current learning rate: 0.00825\n",
      "2025-12-21 07:34:13.859871: train_loss -0.8339\n",
      "2025-12-21 07:34:13.861873: val_loss -0.854\n",
      "2025-12-21 07:34:13.863613: Pseudo dice [0.9239, 0.949, 0.9297]\n",
      "2025-12-21 07:34:13.865615: Epoch time: 137.84 s\n",
      "2025-12-21 07:34:13.867616: Yayy! New best EMA pseudo Dice: 0.9296\n",
      "2025-12-21 07:34:14.755046: \n",
      "2025-12-21 07:34:14.755046: Epoch 193\n",
      "2025-12-21 07:34:14.755046: Current learning rate: 0.00824\n",
      "2025-12-21 07:36:32.693829: train_loss -0.8225\n",
      "2025-12-21 07:36:32.693829: val_loss -0.8529\n",
      "2025-12-21 07:36:32.693829: Pseudo dice [0.922, 0.9525, 0.9229]\n",
      "2025-12-21 07:36:32.702103: Epoch time: 137.94 s\n",
      "2025-12-21 07:36:32.704106: Yayy! New best EMA pseudo Dice: 0.9298\n",
      "2025-12-21 07:36:33.840116: \n",
      "2025-12-21 07:36:33.840116: Epoch 194\n",
      "2025-12-21 07:36:33.840116: Current learning rate: 0.00824\n",
      "2025-12-21 07:38:51.799920: train_loss -0.8316\n",
      "2025-12-21 07:38:51.799920: val_loss -0.8494\n",
      "2025-12-21 07:38:51.799920: Pseudo dice [0.9187, 0.9472, 0.9282]\n",
      "2025-12-21 07:38:51.807178: Epoch time: 137.96 s\n",
      "2025-12-21 07:38:51.809180: Yayy! New best EMA pseudo Dice: 0.93\n",
      "2025-12-21 07:38:52.735621: \n",
      "2025-12-21 07:38:52.735621: Epoch 195\n",
      "2025-12-21 07:38:52.735621: Current learning rate: 0.00823\n",
      "2025-12-21 07:41:10.517476: train_loss -0.8355\n",
      "2025-12-21 07:41:10.517476: val_loss -0.8516\n",
      "2025-12-21 07:41:10.521481: Pseudo dice [0.9195, 0.9466, 0.928]\n",
      "2025-12-21 07:41:10.523482: Epoch time: 137.78 s\n",
      "2025-12-21 07:41:10.525484: Yayy! New best EMA pseudo Dice: 0.9301\n",
      "2025-12-21 07:41:11.452216: \n",
      "2025-12-21 07:41:11.452216: Epoch 196\n",
      "2025-12-21 07:41:11.452216: Current learning rate: 0.00822\n",
      "2025-12-21 07:43:29.249518: train_loss -0.8305\n",
      "2025-12-21 07:43:29.249518: val_loss -0.8372\n",
      "2025-12-21 07:43:29.252112: Pseudo dice [0.9102, 0.9404, 0.9245]\n",
      "2025-12-21 07:43:29.252112: Epoch time: 137.8 s\n",
      "2025-12-21 07:43:29.911006: \n",
      "2025-12-21 07:43:29.914516: Epoch 197\n",
      "2025-12-21 07:43:29.914516: Current learning rate: 0.00821\n",
      "2025-12-21 07:45:47.811336: train_loss -0.8281\n",
      "2025-12-21 07:45:47.813076: val_loss -0.8492\n",
      "2025-12-21 07:45:47.813076: Pseudo dice [0.9184, 0.9524, 0.9193]\n",
      "2025-12-21 07:45:47.817779: Epoch time: 137.9 s\n",
      "2025-12-21 07:45:48.486347: \n",
      "2025-12-21 07:45:48.486347: Epoch 198\n",
      "2025-12-21 07:45:48.488992: Current learning rate: 0.0082\n",
      "2025-12-21 07:48:06.369680: train_loss -0.8299\n",
      "2025-12-21 07:48:06.369680: val_loss -0.8229\n",
      "2025-12-21 07:48:06.385317: Pseudo dice [0.9014, 0.9299, 0.921]\n",
      "2025-12-21 07:48:06.385317: Epoch time: 137.89 s\n",
      "2025-12-21 07:48:07.044654: \n",
      "2025-12-21 07:48:07.046656: Epoch 199\n",
      "2025-12-21 07:48:07.046656: Current learning rate: 0.00819\n",
      "2025-12-21 07:50:24.784239: train_loss -0.8335\n",
      "2025-12-21 07:50:24.784239: val_loss -0.8372\n",
      "2025-12-21 07:50:24.787871: Pseudo dice [0.9119, 0.9415, 0.9165]\n",
      "2025-12-21 07:50:24.789875: Epoch time: 137.74 s\n",
      "2025-12-21 07:50:26.051174: \n",
      "2025-12-21 07:50:26.051174: Epoch 200\n",
      "2025-12-21 07:50:26.051174: Current learning rate: 0.00818\n",
      "2025-12-21 07:52:43.917813: train_loss -0.8279\n",
      "2025-12-21 07:52:43.917813: val_loss -0.8439\n",
      "2025-12-21 07:52:43.917813: Pseudo dice [0.9158, 0.9499, 0.9203]\n",
      "2025-12-21 07:52:43.930797: Epoch time: 137.87 s\n",
      "2025-12-21 07:52:44.599653: \n",
      "2025-12-21 07:52:44.599653: Epoch 201\n",
      "2025-12-21 07:52:44.599653: Current learning rate: 0.00817\n",
      "2025-12-21 07:55:02.583364: train_loss -0.836\n",
      "2025-12-21 07:55:02.585366: val_loss -0.8648\n",
      "2025-12-21 07:55:02.587368: Pseudo dice [0.924, 0.9521, 0.9354]\n",
      "2025-12-21 07:55:02.589369: Epoch time: 137.98 s\n",
      "2025-12-21 07:55:03.239238: \n",
      "2025-12-21 07:55:03.239238: Epoch 202\n",
      "2025-12-21 07:55:03.239238: Current learning rate: 0.00816\n",
      "2025-12-21 07:57:20.960049: train_loss -0.8347\n",
      "2025-12-21 07:57:20.960049: val_loss -0.8697\n",
      "2025-12-21 07:57:20.962052: Pseudo dice [0.9298, 0.9576, 0.9311]\n",
      "2025-12-21 07:57:20.966489: Epoch time: 137.72 s\n",
      "2025-12-21 07:57:21.748429: \n",
      "2025-12-21 07:57:21.750431: Epoch 203\n",
      "2025-12-21 07:57:21.750431: Current learning rate: 0.00815\n",
      "2025-12-21 07:59:39.632785: train_loss -0.8303\n",
      "2025-12-21 07:59:39.632785: val_loss -0.8329\n",
      "2025-12-21 07:59:39.636788: Pseudo dice [0.9061, 0.9385, 0.9234]\n",
      "2025-12-21 07:59:39.638529: Epoch time: 137.89 s\n",
      "2025-12-21 07:59:40.294236: \n",
      "2025-12-21 07:59:40.294236: Epoch 204\n",
      "2025-12-21 07:59:40.297241: Current learning rate: 0.00814\n",
      "2025-12-21 08:01:58.127683: train_loss -0.8318\n",
      "2025-12-21 08:01:58.127683: val_loss -0.8558\n",
      "2025-12-21 08:01:58.129685: Pseudo dice [0.9182, 0.9557, 0.9328]\n",
      "2025-12-21 08:01:58.131687: Epoch time: 137.83 s\n",
      "2025-12-21 08:01:58.960002: \n",
      "2025-12-21 08:01:58.960002: Epoch 205\n",
      "2025-12-21 08:01:58.960002: Current learning rate: 0.00813\n",
      "2025-12-21 08:04:16.869021: train_loss -0.8326\n",
      "2025-12-21 08:04:16.869021: val_loss -0.8429\n",
      "2025-12-21 08:04:16.873026: Pseudo dice [0.9139, 0.9437, 0.9309]\n",
      "2025-12-21 08:04:16.874768: Epoch time: 137.91 s\n",
      "2025-12-21 08:04:17.563689: \n",
      "2025-12-21 08:04:17.563689: Epoch 206\n",
      "2025-12-21 08:04:17.569855: Current learning rate: 0.00813\n",
      "2025-12-21 08:06:35.354836: train_loss -0.8347\n",
      "2025-12-21 08:06:35.354836: val_loss -0.8424\n",
      "2025-12-21 08:06:35.358261: Pseudo dice [0.9125, 0.9409, 0.9257]\n",
      "2025-12-21 08:06:35.360263: Epoch time: 137.79 s\n",
      "2025-12-21 08:06:35.981830: \n",
      "2025-12-21 08:06:35.981830: Epoch 207\n",
      "2025-12-21 08:06:35.981830: Current learning rate: 0.00812\n",
      "2025-12-21 08:08:54.040458: train_loss -0.8316\n",
      "2025-12-21 08:08:54.040458: val_loss -0.8573\n",
      "2025-12-21 08:08:54.044462: Pseudo dice [0.9234, 0.9532, 0.9295]\n",
      "2025-12-21 08:08:54.045464: Epoch time: 138.06 s\n",
      "2025-12-21 08:08:54.678358: \n",
      "2025-12-21 08:08:54.678358: Epoch 208\n",
      "2025-12-21 08:08:54.678358: Current learning rate: 0.00811\n",
      "2025-12-21 08:11:12.899852: train_loss -0.8239\n",
      "2025-12-21 08:11:12.901857: val_loss -0.8457\n",
      "2025-12-21 08:11:12.903860: Pseudo dice [0.9162, 0.9488, 0.9192]\n",
      "2025-12-21 08:11:12.907603: Epoch time: 138.22 s\n",
      "2025-12-21 08:11:13.567534: \n",
      "2025-12-21 08:11:13.567534: Epoch 209\n",
      "2025-12-21 08:11:13.571279: Current learning rate: 0.0081\n",
      "2025-12-21 08:13:31.588362: train_loss -0.823\n",
      "2025-12-21 08:13:31.588362: val_loss -0.8389\n",
      "2025-12-21 08:13:31.606660: Pseudo dice [0.9108, 0.9407, 0.9254]\n",
      "2025-12-21 08:13:31.608663: Epoch time: 138.02 s\n",
      "2025-12-21 08:13:32.238164: \n",
      "2025-12-21 08:13:32.238164: Epoch 210\n",
      "2025-12-21 08:13:32.238164: Current learning rate: 0.00809\n",
      "2025-12-21 08:15:50.079767: train_loss -0.8306\n",
      "2025-12-21 08:15:50.079767: val_loss -0.8467\n",
      "2025-12-21 08:15:50.079767: Pseudo dice [0.921, 0.9485, 0.9196]\n",
      "2025-12-21 08:15:50.085215: Epoch time: 137.84 s\n",
      "2025-12-21 08:15:50.720602: \n",
      "2025-12-21 08:15:50.720602: Epoch 211\n",
      "2025-12-21 08:15:50.722939: Current learning rate: 0.00808\n",
      "2025-12-21 08:18:08.602732: train_loss -0.834\n",
      "2025-12-21 08:18:08.602732: val_loss -0.837\n",
      "2025-12-21 08:18:08.604734: Pseudo dice [0.9147, 0.9476, 0.9147]\n",
      "2025-12-21 08:18:08.606736: Epoch time: 137.88 s\n",
      "2025-12-21 08:18:09.420748: \n",
      "2025-12-21 08:18:09.420748: Epoch 212\n",
      "2025-12-21 08:18:09.420748: Current learning rate: 0.00807\n",
      "2025-12-21 08:20:27.194003: train_loss -0.8262\n",
      "2025-12-21 08:20:27.194003: val_loss -0.8542\n",
      "2025-12-21 08:20:27.196006: Pseudo dice [0.9226, 0.948, 0.9315]\n",
      "2025-12-21 08:20:27.199508: Epoch time: 137.78 s\n",
      "2025-12-21 08:20:27.840547: \n",
      "2025-12-21 08:20:27.840547: Epoch 213\n",
      "2025-12-21 08:20:27.840547: Current learning rate: 0.00806\n",
      "2025-12-21 08:22:45.739163: train_loss -0.8365\n",
      "2025-12-21 08:22:45.741166: val_loss -0.8148\n",
      "2025-12-21 08:22:45.743170: Pseudo dice [0.8974, 0.9359, 0.9231]\n",
      "2025-12-21 08:22:45.746914: Epoch time: 137.9 s\n",
      "2025-12-21 08:22:46.376557: \n",
      "2025-12-21 08:22:46.392479: Epoch 214\n",
      "2025-12-21 08:22:46.392479: Current learning rate: 0.00805\n",
      "2025-12-21 08:25:04.358809: train_loss -0.828\n",
      "2025-12-21 08:25:04.358809: val_loss -0.8354\n",
      "2025-12-21 08:25:04.358809: Pseudo dice [0.9059, 0.9416, 0.9224]\n",
      "2025-12-21 08:25:04.368994: Epoch time: 137.98 s\n",
      "2025-12-21 08:25:05.007154: \n",
      "2025-12-21 08:25:05.007154: Epoch 215\n",
      "2025-12-21 08:25:05.008895: Current learning rate: 0.00804\n",
      "2025-12-21 08:27:22.718882: train_loss -0.8322\n",
      "2025-12-21 08:27:22.718882: val_loss -0.8502\n",
      "2025-12-21 08:27:22.720885: Pseudo dice [0.9209, 0.9493, 0.9229]\n",
      "2025-12-21 08:27:22.720885: Epoch time: 137.71 s\n",
      "2025-12-21 08:27:23.364853: \n",
      "2025-12-21 08:27:23.364853: Epoch 216\n",
      "2025-12-21 08:27:23.364853: Current learning rate: 0.00803\n",
      "2025-12-21 08:29:41.252512: train_loss -0.8265\n",
      "2025-12-21 08:29:41.252512: val_loss -0.8434\n",
      "2025-12-21 08:29:41.256254: Pseudo dice [0.9148, 0.9436, 0.9234]\n",
      "2025-12-21 08:29:41.260260: Epoch time: 137.89 s\n",
      "2025-12-21 08:29:41.905729: \n",
      "2025-12-21 08:29:41.905729: Epoch 217\n",
      "2025-12-21 08:29:41.905729: Current learning rate: 0.00802\n",
      "2025-12-21 08:31:59.917571: train_loss -0.828\n",
      "2025-12-21 08:31:59.917571: val_loss -0.8472\n",
      "2025-12-21 08:31:59.917571: Pseudo dice [0.9127, 0.9481, 0.9302]\n",
      "2025-12-21 08:31:59.921311: Epoch time: 138.01 s\n",
      "2025-12-21 08:32:00.732193: \n",
      "2025-12-21 08:32:00.732193: Epoch 218\n",
      "2025-12-21 08:32:00.732193: Current learning rate: 0.00801\n",
      "2025-12-21 08:34:18.667448: train_loss -0.8304\n",
      "2025-12-21 08:34:18.667448: val_loss -0.8432\n",
      "2025-12-21 08:34:18.669451: Pseudo dice [0.9136, 0.945, 0.9277]\n",
      "2025-12-21 08:34:18.673188: Epoch time: 137.94 s\n",
      "2025-12-21 08:34:19.302535: \n",
      "2025-12-21 08:34:19.302535: Epoch 219\n",
      "2025-12-21 08:34:19.302535: Current learning rate: 0.00801\n",
      "2025-12-21 08:36:37.233749: train_loss -0.8271\n",
      "2025-12-21 08:36:37.235752: val_loss -0.8521\n",
      "2025-12-21 08:36:37.237755: Pseudo dice [0.9165, 0.9475, 0.9199]\n",
      "2025-12-21 08:36:37.239757: Epoch time: 137.93 s\n",
      "2025-12-21 08:36:38.044648: \n",
      "2025-12-21 08:36:38.044648: Epoch 220\n",
      "2025-12-21 08:36:38.060475: Current learning rate: 0.008\n",
      "2025-12-21 08:38:55.959365: train_loss -0.8325\n",
      "2025-12-21 08:38:55.961367: val_loss -0.8553\n",
      "2025-12-21 08:38:55.965966: Pseudo dice [0.9222, 0.9521, 0.9277]\n",
      "2025-12-21 08:38:55.971476: Epoch time: 137.91 s\n",
      "2025-12-21 08:38:56.611327: \n",
      "2025-12-21 08:38:56.611327: Epoch 221\n",
      "2025-12-21 08:38:56.611327: Current learning rate: 0.00799\n",
      "2025-12-21 08:41:14.634765: train_loss -0.8308\n",
      "2025-12-21 08:41:14.634765: val_loss -0.8509\n",
      "2025-12-21 08:41:14.639771: Pseudo dice [0.9198, 0.9544, 0.916]\n",
      "2025-12-21 08:41:14.643776: Epoch time: 138.02 s\n",
      "2025-12-21 08:41:15.287461: \n",
      "2025-12-21 08:41:15.287461: Epoch 222\n",
      "2025-12-21 08:41:15.287461: Current learning rate: 0.00798\n",
      "2025-12-21 08:43:33.235998: train_loss -0.8353\n",
      "2025-12-21 08:43:33.235998: val_loss -0.8624\n",
      "2025-12-21 08:43:33.238000: Pseudo dice [0.9267, 0.9545, 0.9276]\n",
      "2025-12-21 08:43:33.239741: Epoch time: 137.95 s\n",
      "2025-12-21 08:43:33.981361: \n",
      "2025-12-21 08:43:33.981361: Epoch 223\n",
      "2025-12-21 08:43:33.983935: Current learning rate: 0.00797\n",
      "2025-12-21 08:45:51.684257: train_loss -0.8327\n",
      "2025-12-21 08:45:51.684257: val_loss -0.8412\n",
      "2025-12-21 08:45:51.684257: Pseudo dice [0.9104, 0.9415, 0.9228]\n",
      "2025-12-21 08:45:51.695400: Epoch time: 137.72 s\n",
      "2025-12-21 08:45:52.491538: \n",
      "2025-12-21 08:45:52.491538: Epoch 224\n",
      "2025-12-21 08:45:52.491538: Current learning rate: 0.00796\n",
      "2025-12-21 08:48:10.484014: train_loss -0.8256\n",
      "2025-12-21 08:48:10.486017: val_loss -0.857\n",
      "2025-12-21 08:48:10.488019: Pseudo dice [0.9239, 0.9514, 0.9215]\n",
      "2025-12-21 08:48:10.491769: Epoch time: 137.99 s\n",
      "2025-12-21 08:48:11.132849: \n",
      "2025-12-21 08:48:11.132849: Epoch 225\n",
      "2025-12-21 08:48:11.132849: Current learning rate: 0.00795\n",
      "2025-12-21 08:50:28.880582: train_loss -0.831\n",
      "2025-12-21 08:50:28.880582: val_loss -0.8562\n",
      "2025-12-21 08:50:28.880582: Pseudo dice [0.9252, 0.9562, 0.9129]\n",
      "2025-12-21 08:50:28.880582: Epoch time: 137.75 s\n",
      "2025-12-21 08:50:29.672829: \n",
      "2025-12-21 08:50:29.672829: Epoch 226\n",
      "2025-12-21 08:50:29.674832: Current learning rate: 0.00794\n",
      "2025-12-21 08:52:47.415727: train_loss -0.8378\n",
      "2025-12-21 08:52:47.415727: val_loss -0.8473\n",
      "2025-12-21 08:52:47.419732: Pseudo dice [0.9121, 0.9459, 0.9328]\n",
      "2025-12-21 08:52:47.421472: Epoch time: 137.74 s\n",
      "2025-12-21 08:52:48.044219: \n",
      "2025-12-21 08:52:48.044219: Epoch 227\n",
      "2025-12-21 08:52:48.059849: Current learning rate: 0.00793\n",
      "2025-12-21 08:55:06.047002: train_loss -0.834\n",
      "2025-12-21 08:55:06.047002: val_loss -0.8438\n",
      "2025-12-21 08:55:06.047002: Pseudo dice [0.9142, 0.9428, 0.934]\n",
      "2025-12-21 08:55:06.060507: Epoch time: 138.0 s\n",
      "2025-12-21 08:55:06.684528: \n",
      "2025-12-21 08:55:06.684528: Epoch 228\n",
      "2025-12-21 08:55:06.684528: Current learning rate: 0.00792\n",
      "2025-12-21 08:57:25.648463: train_loss -0.8383\n",
      "2025-12-21 08:57:25.664293: val_loss -0.8608\n",
      "2025-12-21 08:57:25.664293: Pseudo dice [0.9216, 0.9526, 0.9391]\n",
      "2025-12-21 08:57:25.667792: Epoch time: 138.97 s\n",
      "2025-12-21 08:57:25.669795: Yayy! New best EMA pseudo Dice: 0.9307\n",
      "2025-12-21 08:57:26.664636: \n",
      "2025-12-21 08:57:26.664636: Epoch 229\n",
      "2025-12-21 08:57:26.664636: Current learning rate: 0.00791\n",
      "2025-12-21 08:59:45.368671: train_loss -0.8298\n",
      "2025-12-21 08:59:45.368671: val_loss -0.853\n",
      "2025-12-21 08:59:45.368671: Pseudo dice [0.9193, 0.9469, 0.9222]\n",
      "2025-12-21 08:59:45.368671: Epoch time: 138.71 s\n",
      "2025-12-21 08:59:45.990458: \n",
      "2025-12-21 08:59:45.990458: Epoch 230\n",
      "2025-12-21 08:59:45.990458: Current learning rate: 0.0079\n",
      "2025-12-21 09:02:04.541201: train_loss -0.8339\n",
      "2025-12-21 09:02:04.541201: val_loss -0.8433\n",
      "2025-12-21 09:02:04.545205: Pseudo dice [0.9103, 0.948, 0.9254]\n",
      "2025-12-21 09:02:04.547045: Epoch time: 138.55 s\n",
      "2025-12-21 09:02:05.343411: \n",
      "2025-12-21 09:02:05.343411: Epoch 231\n",
      "2025-12-21 09:02:05.343411: Current learning rate: 0.00789\n",
      "2025-12-21 09:04:24.072331: train_loss -0.832\n",
      "2025-12-21 09:04:24.074335: val_loss -0.8431\n",
      "2025-12-21 09:04:24.076338: Pseudo dice [0.91, 0.945, 0.9272]\n",
      "2025-12-21 09:04:24.078340: Epoch time: 138.73 s\n",
      "2025-12-21 09:04:24.853197: \n",
      "2025-12-21 09:04:24.853197: Epoch 232\n",
      "2025-12-21 09:04:24.853197: Current learning rate: 0.00789\n",
      "2025-12-21 09:06:43.567759: train_loss -0.8303\n",
      "2025-12-21 09:06:43.567759: val_loss -0.8382\n",
      "2025-12-21 09:06:43.571763: Pseudo dice [0.9119, 0.9439, 0.9154]\n",
      "2025-12-21 09:06:43.573503: Epoch time: 138.71 s\n",
      "2025-12-21 09:06:44.200133: \n",
      "2025-12-21 09:06:44.200133: Epoch 233\n",
      "2025-12-21 09:06:44.200133: Current learning rate: 0.00788\n",
      "2025-12-21 09:09:03.120419: train_loss -0.8358\n",
      "2025-12-21 09:09:03.122161: val_loss -0.8487\n",
      "2025-12-21 09:09:03.124163: Pseudo dice [0.9137, 0.9466, 0.9317]\n",
      "2025-12-21 09:09:03.128168: Epoch time: 138.92 s\n",
      "2025-12-21 09:09:03.747493: \n",
      "2025-12-21 09:09:03.747493: Epoch 234\n",
      "2025-12-21 09:09:03.747493: Current learning rate: 0.00787\n",
      "2025-12-21 09:11:22.283614: train_loss -0.8361\n",
      "2025-12-21 09:11:22.283614: val_loss -0.8481\n",
      "2025-12-21 09:11:22.299592: Pseudo dice [0.9177, 0.9479, 0.9201]\n",
      "2025-12-21 09:11:22.299592: Epoch time: 138.54 s\n",
      "2025-12-21 09:11:22.949270: \n",
      "2025-12-21 09:11:22.949270: Epoch 235\n",
      "2025-12-21 09:11:22.949270: Current learning rate: 0.00786\n",
      "2025-12-21 09:13:41.252000: train_loss -0.8312\n",
      "2025-12-21 09:13:41.252000: val_loss -0.851\n",
      "2025-12-21 09:13:41.252000: Pseudo dice [0.9209, 0.9495, 0.9154]\n",
      "2025-12-21 09:13:41.258139: Epoch time: 138.3 s\n",
      "2025-12-21 09:13:41.870133: \n",
      "2025-12-21 09:13:41.870133: Epoch 236\n",
      "2025-12-21 09:13:41.870133: Current learning rate: 0.00785\n",
      "2025-12-21 09:16:00.265984: train_loss -0.8266\n",
      "2025-12-21 09:16:00.265984: val_loss -0.8499\n",
      "2025-12-21 09:16:00.269988: Pseudo dice [0.9182, 0.9487, 0.9248]\n",
      "2025-12-21 09:16:00.271990: Epoch time: 138.4 s\n",
      "2025-12-21 09:16:01.072122: \n",
      "2025-12-21 09:16:01.072122: Epoch 237\n",
      "2025-12-21 09:16:01.073125: Current learning rate: 0.00784\n",
      "2025-12-21 09:18:19.281341: train_loss -0.8292\n",
      "2025-12-21 09:18:19.281341: val_loss -0.857\n",
      "2025-12-21 09:18:19.286603: Pseudo dice [0.918, 0.9537, 0.9315]\n",
      "2025-12-21 09:18:19.290582: Epoch time: 138.21 s\n",
      "2025-12-21 09:18:19.939419: \n",
      "2025-12-21 09:18:19.939419: Epoch 238\n",
      "2025-12-21 09:18:19.939419: Current learning rate: 0.00783\n",
      "2025-12-21 09:20:37.837101: train_loss -0.8383\n",
      "2025-12-21 09:20:37.837101: val_loss -0.866\n",
      "2025-12-21 09:20:37.853060: Pseudo dice [0.9273, 0.956, 0.934]\n",
      "2025-12-21 09:20:37.853060: Epoch time: 137.9 s\n",
      "2025-12-21 09:20:37.853060: Yayy! New best EMA pseudo Dice: 0.9309\n",
      "2025-12-21 09:20:38.708763: \n",
      "2025-12-21 09:20:38.708763: Epoch 239\n",
      "2025-12-21 09:20:38.708763: Current learning rate: 0.00782\n",
      "2025-12-21 09:22:56.508838: train_loss -0.8426\n",
      "2025-12-21 09:22:56.508838: val_loss -0.861\n",
      "2025-12-21 09:22:56.512842: Pseudo dice [0.9219, 0.9516, 0.9366]\n",
      "2025-12-21 09:22:56.514344: Epoch time: 137.8 s\n",
      "2025-12-21 09:22:56.514344: Yayy! New best EMA pseudo Dice: 0.9315\n",
      "2025-12-21 09:22:57.416503: \n",
      "2025-12-21 09:22:57.416503: Epoch 240\n",
      "2025-12-21 09:22:57.416503: Current learning rate: 0.00781\n",
      "2025-12-21 09:25:15.366156: train_loss -0.8349\n",
      "2025-12-21 09:25:15.366156: val_loss -0.8508\n",
      "2025-12-21 09:25:15.366156: Pseudo dice [0.9166, 0.9513, 0.926]\n",
      "2025-12-21 09:25:15.366156: Epoch time: 137.95 s\n",
      "2025-12-21 09:25:16.115482: \n",
      "2025-12-21 09:25:16.115482: Epoch 241\n",
      "2025-12-21 09:25:16.115482: Current learning rate: 0.0078\n",
      "2025-12-21 09:27:34.086083: train_loss -0.8324\n",
      "2025-12-21 09:27:34.086083: val_loss -0.8526\n",
      "2025-12-21 09:27:34.096648: Pseudo dice [0.9159, 0.9468, 0.9283]\n",
      "2025-12-21 09:27:34.100508: Epoch time: 137.97 s\n",
      "2025-12-21 09:27:34.736386: \n",
      "2025-12-21 09:27:34.736386: Epoch 242\n",
      "2025-12-21 09:27:34.736386: Current learning rate: 0.00779\n",
      "2025-12-21 09:29:52.622767: train_loss -0.8258\n",
      "2025-12-21 09:29:52.622767: val_loss -0.8681\n",
      "2025-12-21 09:29:52.638876: Pseudo dice [0.9288, 0.9614, 0.9281]\n",
      "2025-12-21 09:29:52.638876: Epoch time: 137.89 s\n",
      "2025-12-21 09:29:52.638876: Yayy! New best EMA pseudo Dice: 0.9321\n",
      "2025-12-21 09:29:53.684799: \n",
      "2025-12-21 09:29:53.684799: Epoch 243\n",
      "2025-12-21 09:29:53.684799: Current learning rate: 0.00778\n",
      "2025-12-21 09:32:11.697230: train_loss -0.8272\n",
      "2025-12-21 09:32:11.697230: val_loss -0.848\n",
      "2025-12-21 09:32:11.701234: Pseudo dice [0.9182, 0.9481, 0.9171]\n",
      "2025-12-21 09:32:11.703236: Epoch time: 138.01 s\n",
      "2025-12-21 09:32:12.466113: \n",
      "2025-12-21 09:32:12.482197: Epoch 244\n",
      "2025-12-21 09:32:12.484435: Current learning rate: 0.00777\n",
      "2025-12-21 09:34:30.361091: train_loss -0.8262\n",
      "2025-12-21 09:34:30.362890: val_loss -0.8148\n",
      "2025-12-21 09:34:30.364893: Pseudo dice [0.8965, 0.9318, 0.9164]\n",
      "2025-12-21 09:34:30.366895: Epoch time: 137.89 s\n",
      "2025-12-21 09:34:31.002134: \n",
      "2025-12-21 09:34:31.002134: Epoch 245\n",
      "2025-12-21 09:34:31.004714: Current learning rate: 0.00777\n",
      "2025-12-21 09:36:48.772638: train_loss -0.8245\n",
      "2025-12-21 09:36:48.772638: val_loss -0.8447\n",
      "2025-12-21 09:36:48.772638: Pseudo dice [0.9159, 0.944, 0.9259]\n",
      "2025-12-21 09:36:48.779591: Epoch time: 137.77 s\n",
      "2025-12-21 09:36:49.408780: \n",
      "2025-12-21 09:36:49.408780: Epoch 246\n",
      "2025-12-21 09:36:49.408780: Current learning rate: 0.00776\n",
      "2025-12-21 09:39:07.296077: train_loss -0.8299\n",
      "2025-12-21 09:39:07.298085: val_loss -0.8569\n",
      "2025-12-21 09:39:07.300088: Pseudo dice [0.9237, 0.9483, 0.9381]\n",
      "2025-12-21 09:39:07.304092: Epoch time: 137.89 s\n",
      "2025-12-21 09:39:08.060960: \n",
      "2025-12-21 09:39:08.060960: Epoch 247\n",
      "2025-12-21 09:39:08.060960: Current learning rate: 0.00775\n",
      "2025-12-21 09:41:25.987833: train_loss -0.8375\n",
      "2025-12-21 09:41:25.987833: val_loss -0.8422\n",
      "2025-12-21 09:41:26.003601: Pseudo dice [0.9107, 0.9398, 0.9358]\n",
      "2025-12-21 09:41:26.007104: Epoch time: 137.94 s\n",
      "2025-12-21 09:41:26.636344: \n",
      "2025-12-21 09:41:26.636344: Epoch 248\n",
      "2025-12-21 09:41:26.636344: Current learning rate: 0.00774\n",
      "2025-12-21 09:43:44.345814: train_loss -0.8416\n",
      "2025-12-21 09:43:44.345814: val_loss -0.8225\n",
      "2025-12-21 09:43:44.361538: Pseudo dice [0.9046, 0.9379, 0.9068]\n",
      "2025-12-21 09:43:44.361538: Epoch time: 137.71 s\n",
      "2025-12-21 09:43:45.137590: \n",
      "2025-12-21 09:43:45.137590: Epoch 249\n",
      "2025-12-21 09:43:45.137590: Current learning rate: 0.00773\n",
      "2025-12-21 09:46:03.072028: train_loss -0.8388\n",
      "2025-12-21 09:46:03.072028: val_loss -0.8486\n",
      "2025-12-21 09:46:03.072028: Pseudo dice [0.9165, 0.9469, 0.9302]\n",
      "2025-12-21 09:46:03.072028: Epoch time: 137.93 s\n",
      "2025-12-21 09:46:04.021233: \n",
      "2025-12-21 09:46:04.021233: Epoch 250\n",
      "2025-12-21 09:46:04.021233: Current learning rate: 0.00772\n",
      "2025-12-21 09:48:21.973897: train_loss -0.8353\n",
      "2025-12-21 09:48:21.973897: val_loss -0.8547\n",
      "2025-12-21 09:48:21.977639: Pseudo dice [0.9212, 0.9482, 0.9368]\n",
      "2025-12-21 09:48:21.979494: Epoch time: 137.95 s\n",
      "2025-12-21 09:48:22.609727: \n",
      "2025-12-21 09:48:22.609727: Epoch 251\n",
      "2025-12-21 09:48:22.613111: Current learning rate: 0.00771\n",
      "2025-12-21 09:50:40.448169: train_loss -0.8301\n",
      "2025-12-21 09:50:40.450172: val_loss -0.8189\n",
      "2025-12-21 09:50:40.452175: Pseudo dice [0.8948, 0.9324, 0.9296]\n",
      "2025-12-21 09:50:40.454178: Epoch time: 137.84 s\n",
      "2025-12-21 09:50:41.087573: \n",
      "2025-12-21 09:50:41.087573: Epoch 252\n",
      "2025-12-21 09:50:41.089576: Current learning rate: 0.0077\n",
      "2025-12-21 09:52:59.088219: train_loss -0.827\n",
      "2025-12-21 09:52:59.088219: val_loss -0.8519\n",
      "2025-12-21 09:52:59.104255: Pseudo dice [0.9211, 0.95, 0.9328]\n",
      "2025-12-21 09:52:59.108785: Epoch time: 138.0 s\n",
      "2025-12-21 09:52:59.746727: \n",
      "2025-12-21 09:52:59.746727: Epoch 253\n",
      "2025-12-21 09:52:59.746727: Current learning rate: 0.00769\n",
      "2025-12-21 09:55:17.647071: train_loss -0.8234\n",
      "2025-12-21 09:55:17.647071: val_loss -0.8471\n",
      "2025-12-21 09:55:17.651016: Pseudo dice [0.9139, 0.9465, 0.9339]\n",
      "2025-12-21 09:55:17.653019: Epoch time: 137.9 s\n",
      "2025-12-21 09:55:18.318710: \n",
      "2025-12-21 09:55:18.320712: Epoch 254\n",
      "2025-12-21 09:55:18.320712: Current learning rate: 0.00768\n",
      "2025-12-21 09:57:36.389287: train_loss -0.8322\n",
      "2025-12-21 09:57:36.389287: val_loss -0.8159\n",
      "2025-12-21 09:57:36.393030: Pseudo dice [0.8986, 0.9295, 0.9195]\n",
      "2025-12-21 09:57:36.393030: Epoch time: 138.07 s\n",
      "2025-12-21 09:57:37.192507: \n",
      "2025-12-21 09:57:37.192507: Epoch 255\n",
      "2025-12-21 09:57:37.192507: Current learning rate: 0.00767\n",
      "2025-12-21 09:59:55.105769: train_loss -0.8306\n",
      "2025-12-21 09:59:55.105769: val_loss -0.8398\n",
      "2025-12-21 09:59:55.109776: Pseudo dice [0.9113, 0.9427, 0.9298]\n",
      "2025-12-21 09:59:55.111972: Epoch time: 137.91 s\n",
      "2025-12-21 09:59:55.741128: \n",
      "2025-12-21 09:59:55.741128: Epoch 256\n",
      "2025-12-21 09:59:55.741128: Current learning rate: 0.00766\n",
      "2025-12-21 10:02:13.642671: train_loss -0.8355\n",
      "2025-12-21 10:02:13.642671: val_loss -0.8314\n",
      "2025-12-21 10:02:13.646675: Pseudo dice [0.906, 0.9425, 0.9203]\n",
      "2025-12-21 10:02:13.648677: Epoch time: 137.9 s\n",
      "2025-12-21 10:02:14.268667: \n",
      "2025-12-21 10:02:14.268667: Epoch 257\n",
      "2025-12-21 10:02:14.284627: Current learning rate: 0.00765\n",
      "2025-12-21 10:04:32.320287: train_loss -0.8349\n",
      "2025-12-21 10:04:32.320287: val_loss -0.8568\n",
      "2025-12-21 10:04:32.322289: Pseudo dice [0.9237, 0.9508, 0.9269]\n",
      "2025-12-21 10:04:32.326293: Epoch time: 138.05 s\n",
      "2025-12-21 10:04:32.946814: \n",
      "2025-12-21 10:04:32.946814: Epoch 258\n",
      "2025-12-21 10:04:32.946814: Current learning rate: 0.00764\n",
      "2025-12-21 10:06:50.876733: train_loss -0.8362\n",
      "2025-12-21 10:06:50.876733: val_loss -0.8632\n",
      "2025-12-21 10:06:50.892422: Pseudo dice [0.9265, 0.9544, 0.9329]\n",
      "2025-12-21 10:06:50.892422: Epoch time: 137.93 s\n",
      "2025-12-21 10:06:51.509765: \n",
      "2025-12-21 10:06:51.509765: Epoch 259\n",
      "2025-12-21 10:06:51.509765: Current learning rate: 0.00764\n",
      "2025-12-21 10:09:09.769590: train_loss -0.8376\n",
      "2025-12-21 10:09:09.769590: val_loss -0.8144\n",
      "2025-12-21 10:09:09.769590: Pseudo dice [0.8919, 0.9261, 0.9253]\n",
      "2025-12-21 10:09:09.769590: Epoch time: 138.26 s\n",
      "2025-12-21 10:09:10.402131: \n",
      "2025-12-21 10:09:10.402131: Epoch 260\n",
      "2025-12-21 10:09:10.402131: Current learning rate: 0.00763\n",
      "2025-12-21 10:11:28.529488: train_loss -0.8298\n",
      "2025-12-21 10:11:28.529488: val_loss -0.8393\n",
      "2025-12-21 10:11:28.531490: Pseudo dice [0.9127, 0.9446, 0.9274]\n",
      "2025-12-21 10:11:28.535494: Epoch time: 138.13 s\n",
      "2025-12-21 10:11:29.153255: \n",
      "2025-12-21 10:11:29.153255: Epoch 261\n",
      "2025-12-21 10:11:29.153255: Current learning rate: 0.00762\n",
      "2025-12-21 10:13:47.139173: train_loss -0.8219\n",
      "2025-12-21 10:13:47.139173: val_loss -0.8215\n",
      "2025-12-21 10:13:47.139173: Pseudo dice [0.9036, 0.935, 0.9241]\n",
      "2025-12-21 10:13:47.139173: Epoch time: 137.99 s\n",
      "2025-12-21 10:13:47.953237: \n",
      "2025-12-21 10:13:47.953237: Epoch 262\n",
      "2025-12-21 10:13:47.953237: Current learning rate: 0.00761\n",
      "2025-12-21 10:16:05.963736: train_loss -0.8087\n",
      "2025-12-21 10:16:05.963736: val_loss -0.8276\n",
      "2025-12-21 10:16:05.967525: Pseudo dice [0.9045, 0.9383, 0.9254]\n",
      "2025-12-21 10:16:05.969527: Epoch time: 138.01 s\n",
      "2025-12-21 10:16:06.594291: \n",
      "2025-12-21 10:16:06.594291: Epoch 263\n",
      "2025-12-21 10:16:06.594291: Current learning rate: 0.0076\n",
      "2025-12-21 10:18:24.349025: train_loss -0.8272\n",
      "2025-12-21 10:18:24.351027: val_loss -0.8516\n",
      "2025-12-21 10:18:24.355030: Pseudo dice [0.9237, 0.9496, 0.9179]\n",
      "2025-12-21 10:18:24.357032: Epoch time: 137.75 s\n",
      "2025-12-21 10:18:25.117146: \n",
      "2025-12-21 10:18:25.117146: Epoch 264\n",
      "2025-12-21 10:18:25.127830: Current learning rate: 0.00759\n",
      "2025-12-21 10:20:43.218935: train_loss -0.8288\n",
      "2025-12-21 10:20:43.218935: val_loss -0.8459\n",
      "2025-12-21 10:20:43.230047: Pseudo dice [0.9161, 0.9438, 0.9288]\n",
      "2025-12-21 10:20:43.232049: Epoch time: 138.1 s\n",
      "2025-12-21 10:20:43.892794: \n",
      "2025-12-21 10:20:43.892794: Epoch 265\n",
      "2025-12-21 10:20:43.902926: Current learning rate: 0.00758\n",
      "2025-12-21 10:23:01.580369: train_loss -0.8333\n",
      "2025-12-21 10:23:01.580369: val_loss -0.8405\n",
      "2025-12-21 10:23:01.596047: Pseudo dice [0.9133, 0.9478, 0.9155]\n",
      "2025-12-21 10:23:01.596047: Epoch time: 137.69 s\n",
      "2025-12-21 10:23:02.228738: \n",
      "2025-12-21 10:23:02.230741: Epoch 266\n",
      "2025-12-21 10:23:02.230741: Current learning rate: 0.00757\n",
      "2025-12-21 10:25:20.172001: train_loss -0.8241\n",
      "2025-12-21 10:25:20.172001: val_loss -0.8216\n",
      "2025-12-21 10:25:20.175746: Pseudo dice [0.8979, 0.9381, 0.9293]\n",
      "2025-12-21 10:25:20.179751: Epoch time: 137.94 s\n",
      "2025-12-21 10:25:20.948397: \n",
      "2025-12-21 10:25:20.948397: Epoch 267\n",
      "2025-12-21 10:25:20.948397: Current learning rate: 0.00756\n",
      "2025-12-21 10:27:38.928850: train_loss -0.8268\n",
      "2025-12-21 10:27:38.928850: val_loss -0.8526\n",
      "2025-12-21 10:27:38.932854: Pseudo dice [0.9181, 0.9494, 0.9243]\n",
      "2025-12-21 10:27:38.932854: Epoch time: 137.98 s\n",
      "2025-12-21 10:27:39.740571: \n",
      "2025-12-21 10:27:39.740571: Epoch 268\n",
      "2025-12-21 10:27:39.740571: Current learning rate: 0.00755\n",
      "2025-12-21 10:29:57.678244: train_loss -0.8295\n",
      "2025-12-21 10:29:57.678244: val_loss -0.8556\n",
      "2025-12-21 10:29:57.681702: Pseudo dice [0.9203, 0.9505, 0.9296]\n",
      "2025-12-21 10:29:57.685706: Epoch time: 137.94 s\n",
      "2025-12-21 10:29:58.309647: \n",
      "2025-12-21 10:29:58.309647: Epoch 269\n",
      "2025-12-21 10:29:58.309647: Current learning rate: 0.00754\n",
      "2025-12-21 10:32:16.309625: train_loss -0.8377\n",
      "2025-12-21 10:32:16.309625: val_loss -0.8669\n",
      "2025-12-21 10:32:16.311626: Pseudo dice [0.9332, 0.956, 0.9258]\n",
      "2025-12-21 10:32:16.316042: Epoch time: 138.0 s\n",
      "2025-12-21 10:32:17.045491: \n",
      "2025-12-21 10:32:17.045491: Epoch 270\n",
      "2025-12-21 10:32:17.047494: Current learning rate: 0.00753\n",
      "2025-12-21 10:34:35.050798: train_loss -0.8343\n",
      "2025-12-21 10:34:35.050798: val_loss -0.873\n",
      "2025-12-21 10:34:35.054803: Pseudo dice [0.9333, 0.957, 0.9333]\n",
      "2025-12-21 10:34:35.058807: Epoch time: 138.01 s\n",
      "2025-12-21 10:34:35.702698: \n",
      "2025-12-21 10:34:35.702698: Epoch 271\n",
      "2025-12-21 10:34:35.702698: Current learning rate: 0.00752\n",
      "2025-12-21 10:36:53.666497: train_loss -0.8329\n",
      "2025-12-21 10:36:53.666497: val_loss -0.8662\n",
      "2025-12-21 10:36:53.674513: Pseudo dice [0.9279, 0.9552, 0.9311]\n",
      "2025-12-21 10:36:53.676253: Epoch time: 137.97 s\n",
      "2025-12-21 10:36:54.311651: \n",
      "2025-12-21 10:36:54.311651: Epoch 272\n",
      "2025-12-21 10:36:54.311651: Current learning rate: 0.00751\n",
      "2025-12-21 10:39:12.281512: train_loss -0.8344\n",
      "2025-12-21 10:39:12.281512: val_loss -0.8483\n",
      "2025-12-21 10:39:12.285516: Pseudo dice [0.9157, 0.9428, 0.9278]\n",
      "2025-12-21 10:39:12.288913: Epoch time: 137.97 s\n",
      "2025-12-21 10:39:13.064185: \n",
      "2025-12-21 10:39:13.064185: Epoch 273\n",
      "2025-12-21 10:39:13.064185: Current learning rate: 0.00751\n",
      "2025-12-21 10:41:30.974445: train_loss -0.8378\n",
      "2025-12-21 10:41:30.977948: val_loss -0.8457\n",
      "2025-12-21 10:41:30.979950: Pseudo dice [0.9132, 0.9439, 0.9342]\n",
      "2025-12-21 10:41:30.981952: Epoch time: 137.91 s\n",
      "2025-12-21 10:41:31.779516: \n",
      "2025-12-21 10:41:31.779516: Epoch 274\n",
      "2025-12-21 10:41:31.779516: Current learning rate: 0.0075\n",
      "2025-12-21 10:43:49.648578: train_loss -0.84\n",
      "2025-12-21 10:43:49.648578: val_loss -0.8535\n",
      "2025-12-21 10:43:49.652582: Pseudo dice [0.9149, 0.9495, 0.9275]\n",
      "2025-12-21 10:43:49.654583: Epoch time: 137.87 s\n",
      "2025-12-21 10:43:50.275941: \n",
      "2025-12-21 10:43:50.275941: Epoch 275\n",
      "2025-12-21 10:43:50.275941: Current learning rate: 0.00749\n",
      "2025-12-21 10:46:08.218065: train_loss -0.8339\n",
      "2025-12-21 10:46:08.221951: val_loss -0.8704\n",
      "2025-12-21 10:46:08.225957: Pseudo dice [0.9297, 0.953, 0.9395]\n",
      "2025-12-21 10:46:08.229961: Epoch time: 137.94 s\n",
      "2025-12-21 10:46:08.855467: \n",
      "2025-12-21 10:46:08.855467: Epoch 276\n",
      "2025-12-21 10:46:08.855467: Current learning rate: 0.00748\n",
      "2025-12-21 10:48:26.841901: train_loss -0.8346\n",
      "2025-12-21 10:48:26.841901: val_loss -0.86\n",
      "2025-12-21 10:48:26.841901: Pseudo dice [0.9241, 0.9497, 0.9353]\n",
      "2025-12-21 10:48:26.841901: Epoch time: 137.99 s\n",
      "2025-12-21 10:48:27.474857: \n",
      "2025-12-21 10:48:27.474857: Epoch 277\n",
      "2025-12-21 10:48:27.490786: Current learning rate: 0.00747\n",
      "2025-12-21 10:50:45.374282: train_loss -0.8347\n",
      "2025-12-21 10:50:45.374282: val_loss -0.8534\n",
      "2025-12-21 10:50:45.378026: Pseudo dice [0.9165, 0.9448, 0.9353]\n",
      "2025-12-21 10:50:45.378026: Epoch time: 137.9 s\n",
      "2025-12-21 10:50:46.008204: \n",
      "2025-12-21 10:50:46.008204: Epoch 278\n",
      "2025-12-21 10:50:46.008204: Current learning rate: 0.00746\n",
      "2025-12-21 10:53:04.104450: train_loss -0.8361\n",
      "2025-12-21 10:53:04.104450: val_loss -0.8647\n",
      "2025-12-21 10:53:04.104450: Pseudo dice [0.9285, 0.9539, 0.9209]\n",
      "2025-12-21 10:53:04.104450: Epoch time: 138.1 s\n",
      "2025-12-21 10:53:04.104450: Yayy! New best EMA pseudo Dice: 0.9323\n",
      "2025-12-21 10:53:04.977254: \n",
      "2025-12-21 10:53:04.977254: Epoch 279\n",
      "2025-12-21 10:53:04.977254: Current learning rate: 0.00745\n",
      "2025-12-21 10:55:23.021196: train_loss -0.8383\n",
      "2025-12-21 10:55:23.023199: val_loss -0.8589\n",
      "2025-12-21 10:55:23.024940: Pseudo dice [0.922, 0.9474, 0.9319]\n",
      "2025-12-21 10:55:23.028438: Epoch time: 138.05 s\n",
      "2025-12-21 10:55:23.032442: Yayy! New best EMA pseudo Dice: 0.9324\n",
      "2025-12-21 10:55:24.106347: \n",
      "2025-12-21 10:55:24.106347: Epoch 280\n",
      "2025-12-21 10:55:24.109350: Current learning rate: 0.00744\n",
      "2025-12-21 10:57:42.043749: train_loss -0.8389\n",
      "2025-12-21 10:57:42.045751: val_loss -0.86\n",
      "2025-12-21 10:57:42.049755: Pseudo dice [0.9241, 0.9506, 0.9256]\n",
      "2025-12-21 10:57:42.051758: Epoch time: 137.94 s\n",
      "2025-12-21 10:57:42.053761: Yayy! New best EMA pseudo Dice: 0.9325\n",
      "2025-12-21 10:57:42.965699: \n",
      "2025-12-21 10:57:42.965699: Epoch 281\n",
      "2025-12-21 10:57:42.965699: Current learning rate: 0.00743\n",
      "2025-12-21 11:00:01.057251: train_loss -0.8375\n",
      "2025-12-21 11:00:01.059254: val_loss -0.8493\n",
      "2025-12-21 11:00:01.059254: Pseudo dice [0.9131, 0.9444, 0.934]\n",
      "2025-12-21 11:00:01.059254: Epoch time: 138.09 s\n",
      "2025-12-21 11:00:01.692494: \n",
      "2025-12-21 11:00:01.694496: Epoch 282\n",
      "2025-12-21 11:00:01.694496: Current learning rate: 0.00742\n",
      "2025-12-21 11:02:19.639783: train_loss -0.8424\n",
      "2025-12-21 11:02:19.639783: val_loss -0.838\n",
      "2025-12-21 11:02:19.639783: Pseudo dice [0.9047, 0.9444, 0.9323]\n",
      "2025-12-21 11:02:19.652086: Epoch time: 137.95 s\n",
      "2025-12-21 11:02:20.290174: \n",
      "2025-12-21 11:02:20.290174: Epoch 283\n",
      "2025-12-21 11:02:20.290174: Current learning rate: 0.00741\n",
      "2025-12-21 11:04:38.424343: train_loss -0.8397\n",
      "2025-12-21 11:04:38.424343: val_loss -0.8491\n",
      "2025-12-21 11:04:38.428347: Pseudo dice [0.9164, 0.9452, 0.9327]\n",
      "2025-12-21 11:04:38.430349: Epoch time: 138.14 s\n",
      "2025-12-21 11:04:39.097104: \n",
      "2025-12-21 11:04:39.097104: Epoch 284\n",
      "2025-12-21 11:04:39.097104: Current learning rate: 0.0074\n",
      "2025-12-21 11:06:57.184355: train_loss -0.8372\n",
      "2025-12-21 11:06:57.184355: val_loss -0.8529\n",
      "2025-12-21 11:06:57.195925: Pseudo dice [0.9192, 0.9496, 0.9308]\n",
      "2025-12-21 11:06:57.197928: Epoch time: 138.09 s\n",
      "2025-12-21 11:06:57.834678: \n",
      "2025-12-21 11:06:57.834678: Epoch 285\n",
      "2025-12-21 11:06:57.834678: Current learning rate: 0.00739\n",
      "2025-12-21 11:09:15.992819: train_loss -0.836\n",
      "2025-12-21 11:09:15.992819: val_loss -0.8679\n",
      "2025-12-21 11:09:16.000162: Pseudo dice [0.9303, 0.9561, 0.9321]\n",
      "2025-12-21 11:09:16.002164: Epoch time: 138.16 s\n",
      "2025-12-21 11:09:16.004166: Yayy! New best EMA pseudo Dice: 0.9327\n",
      "2025-12-21 11:09:17.071867: \n",
      "2025-12-21 11:09:17.071867: Epoch 286\n",
      "2025-12-21 11:09:17.071867: Current learning rate: 0.00738\n",
      "2025-12-21 11:11:35.074668: train_loss -0.8367\n",
      "2025-12-21 11:11:35.074668: val_loss -0.8395\n",
      "2025-12-21 11:11:35.074668: Pseudo dice [0.9069, 0.9394, 0.9346]\n",
      "2025-12-21 11:11:35.074668: Epoch time: 138.0 s\n",
      "2025-12-21 11:11:35.713383: \n",
      "2025-12-21 11:11:35.713383: Epoch 287\n",
      "2025-12-21 11:11:35.713383: Current learning rate: 0.00738\n",
      "2025-12-21 11:13:53.682053: train_loss -0.8298\n",
      "2025-12-21 11:13:53.682053: val_loss -0.8698\n",
      "2025-12-21 11:13:53.686058: Pseudo dice [0.9301, 0.9554, 0.9338]\n",
      "2025-12-21 11:13:53.686058: Epoch time: 137.97 s\n",
      "2025-12-21 11:13:53.686058: Yayy! New best EMA pseudo Dice: 0.9329\n",
      "2025-12-21 11:13:54.610805: \n",
      "2025-12-21 11:13:54.610805: Epoch 288\n",
      "2025-12-21 11:13:54.610805: Current learning rate: 0.00737\n",
      "2025-12-21 11:16:12.544002: train_loss -0.8357\n",
      "2025-12-21 11:16:12.544002: val_loss -0.8661\n",
      "2025-12-21 11:16:12.548006: Pseudo dice [0.928, 0.9536, 0.9369]\n",
      "2025-12-21 11:16:12.550009: Epoch time: 137.93 s\n",
      "2025-12-21 11:16:12.552259: Yayy! New best EMA pseudo Dice: 0.9335\n",
      "2025-12-21 11:16:13.458719: \n",
      "2025-12-21 11:16:13.458719: Epoch 289\n",
      "2025-12-21 11:16:13.458719: Current learning rate: 0.00736\n",
      "2025-12-21 11:18:31.362436: train_loss -0.8311\n",
      "2025-12-21 11:18:31.362436: val_loss -0.8565\n",
      "2025-12-21 11:18:31.367981: Pseudo dice [0.9201, 0.9514, 0.9344]\n",
      "2025-12-21 11:18:31.369983: Epoch time: 137.91 s\n",
      "2025-12-21 11:18:31.371985: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-21 11:18:32.280037: \n",
      "2025-12-21 11:18:32.280037: Epoch 290\n",
      "2025-12-21 11:18:32.296097: Current learning rate: 0.00735\n",
      "2025-12-21 11:20:50.312850: train_loss -0.8381\n",
      "2025-12-21 11:20:50.312850: val_loss -0.8529\n",
      "2025-12-21 11:20:50.316854: Pseudo dice [0.919, 0.9533, 0.9229]\n",
      "2025-12-21 11:20:50.318856: Epoch time: 138.03 s\n",
      "2025-12-21 11:20:50.953070: \n",
      "2025-12-21 11:20:50.953070: Epoch 291\n",
      "2025-12-21 11:20:50.953070: Current learning rate: 0.00734\n",
      "2025-12-21 11:23:08.981228: train_loss -0.8363\n",
      "2025-12-21 11:23:08.981228: val_loss -0.8455\n",
      "2025-12-21 11:23:08.983231: Pseudo dice [0.9173, 0.9382, 0.9246]\n",
      "2025-12-21 11:23:08.989792: Epoch time: 138.03 s\n",
      "2025-12-21 11:23:09.835256: \n",
      "2025-12-21 11:23:09.835256: Epoch 292\n",
      "2025-12-21 11:23:09.835256: Current learning rate: 0.00733\n",
      "2025-12-21 11:25:27.897825: train_loss -0.8417\n",
      "2025-12-21 11:25:27.897825: val_loss -0.8722\n",
      "2025-12-21 11:25:27.902008: Pseudo dice [0.9323, 0.9557, 0.9315]\n",
      "2025-12-21 11:25:27.902008: Epoch time: 138.08 s\n",
      "2025-12-21 11:25:28.536037: \n",
      "2025-12-21 11:25:28.536037: Epoch 293\n",
      "2025-12-21 11:25:28.536037: Current learning rate: 0.00732\n",
      "2025-12-21 11:27:46.364721: train_loss -0.8423\n",
      "2025-12-21 11:27:46.380410: val_loss -0.8684\n",
      "2025-12-21 11:27:46.382412: Pseudo dice [0.9287, 0.959, 0.9294]\n",
      "2025-12-21 11:27:46.384415: Epoch time: 137.83 s\n",
      "2025-12-21 11:27:46.388419: Yayy! New best EMA pseudo Dice: 0.9341\n",
      "2025-12-21 11:27:47.251224: \n",
      "2025-12-21 11:27:47.251224: Epoch 294\n",
      "2025-12-21 11:27:47.266925: Current learning rate: 0.00731\n",
      "2025-12-21 11:30:05.054093: train_loss -0.8408\n",
      "2025-12-21 11:30:05.054093: val_loss -0.8464\n",
      "2025-12-21 11:30:05.060107: Pseudo dice [0.916, 0.9466, 0.9228]\n",
      "2025-12-21 11:30:05.063851: Epoch time: 137.8 s\n",
      "2025-12-21 11:30:05.697790: \n",
      "2025-12-21 11:30:05.697790: Epoch 295\n",
      "2025-12-21 11:30:05.697790: Current learning rate: 0.0073\n",
      "2025-12-21 11:32:23.733169: train_loss -0.839\n",
      "2025-12-21 11:32:23.733169: val_loss -0.8524\n",
      "2025-12-21 11:32:23.733169: Pseudo dice [0.9194, 0.9526, 0.9268]\n",
      "2025-12-21 11:32:23.733169: Epoch time: 138.04 s\n",
      "2025-12-21 11:32:24.376707: \n",
      "2025-12-21 11:32:24.376707: Epoch 296\n",
      "2025-12-21 11:32:24.376707: Current learning rate: 0.00729\n",
      "2025-12-21 11:34:42.243343: train_loss -0.8377\n",
      "2025-12-21 11:34:42.245345: val_loss -0.8494\n",
      "2025-12-21 11:34:42.249351: Pseudo dice [0.9156, 0.9451, 0.9293]\n",
      "2025-12-21 11:34:42.251354: Epoch time: 137.87 s\n",
      "2025-12-21 11:34:42.925571: \n",
      "2025-12-21 11:34:42.925571: Epoch 297\n",
      "2025-12-21 11:34:42.925571: Current learning rate: 0.00728\n",
      "2025-12-21 11:37:00.979109: train_loss -0.8332\n",
      "2025-12-21 11:37:00.980849: val_loss -0.85\n",
      "2025-12-21 11:37:00.984853: Pseudo dice [0.916, 0.9477, 0.927]\n",
      "2025-12-21 11:37:00.986855: Epoch time: 138.05 s\n",
      "2025-12-21 11:37:01.789858: \n",
      "2025-12-21 11:37:01.789858: Epoch 298\n",
      "2025-12-21 11:37:01.789858: Current learning rate: 0.00727\n",
      "2025-12-21 11:39:19.670851: train_loss -0.8304\n",
      "2025-12-21 11:39:19.670851: val_loss -0.8656\n",
      "2025-12-21 11:39:19.674855: Pseudo dice [0.9287, 0.9599, 0.9199]\n",
      "2025-12-21 11:39:19.678596: Epoch time: 137.88 s\n",
      "2025-12-21 11:39:20.504602: \n",
      "2025-12-21 11:39:20.504602: Epoch 299\n",
      "2025-12-21 11:39:20.504602: Current learning rate: 0.00726\n",
      "2025-12-21 11:41:38.389230: train_loss -0.8327\n",
      "2025-12-21 11:41:38.389230: val_loss -0.8712\n",
      "2025-12-21 11:41:38.404960: Pseudo dice [0.9277, 0.9573, 0.9378]\n",
      "2025-12-21 11:41:38.404960: Epoch time: 137.88 s\n",
      "2025-12-21 11:41:39.277612: \n",
      "2025-12-21 11:41:39.277612: Epoch 300\n",
      "2025-12-21 11:41:39.277612: Current learning rate: 0.00725\n",
      "2025-12-21 11:43:57.202331: train_loss -0.8407\n",
      "2025-12-21 11:43:57.202331: val_loss -0.8613\n",
      "2025-12-21 11:43:57.204337: Pseudo dice [0.922, 0.9537, 0.9352]\n",
      "2025-12-21 11:43:57.204337: Epoch time: 137.92 s\n",
      "2025-12-21 11:43:57.204337: Yayy! New best EMA pseudo Dice: 0.9342\n",
      "2025-12-21 11:43:58.124349: \n",
      "2025-12-21 11:43:58.124349: Epoch 301\n",
      "2025-12-21 11:43:58.124349: Current learning rate: 0.00724\n",
      "2025-12-21 11:46:16.037428: train_loss -0.8396\n",
      "2025-12-21 11:46:16.037428: val_loss -0.8626\n",
      "2025-12-21 11:46:16.041709: Pseudo dice [0.9225, 0.9511, 0.9336]\n",
      "2025-12-21 11:46:16.043712: Epoch time: 137.91 s\n",
      "2025-12-21 11:46:16.045715: Yayy! New best EMA pseudo Dice: 0.9344\n",
      "2025-12-21 11:46:17.149917: \n",
      "2025-12-21 11:46:17.151920: Epoch 302\n",
      "2025-12-21 11:46:17.151920: Current learning rate: 0.00724\n",
      "2025-12-21 11:48:35.156805: train_loss -0.8395\n",
      "2025-12-21 11:48:35.156805: val_loss -0.8644\n",
      "2025-12-21 11:48:35.156805: Pseudo dice [0.9229, 0.9487, 0.9446]\n",
      "2025-12-21 11:48:35.172839: Epoch time: 138.01 s\n",
      "2025-12-21 11:48:35.172839: Yayy! New best EMA pseudo Dice: 0.9348\n",
      "2025-12-21 11:48:36.077414: \n",
      "2025-12-21 11:48:36.077414: Epoch 303\n",
      "2025-12-21 11:48:36.077414: Current learning rate: 0.00723\n",
      "2025-12-21 11:50:54.140265: train_loss -0.8438\n",
      "2025-12-21 11:50:54.140265: val_loss -0.8428\n",
      "2025-12-21 11:50:54.144269: Pseudo dice [0.9043, 0.9396, 0.936]\n",
      "2025-12-21 11:50:54.148273: Epoch time: 138.06 s\n",
      "2025-12-21 11:50:54.943008: \n",
      "2025-12-21 11:50:54.943008: Epoch 304\n",
      "2025-12-21 11:50:54.958963: Current learning rate: 0.00722\n",
      "2025-12-21 11:53:13.012535: train_loss -0.8402\n",
      "2025-12-21 11:53:13.014275: val_loss -0.865\n",
      "2025-12-21 11:53:13.020296: Pseudo dice [0.923, 0.9533, 0.9365]\n",
      "2025-12-21 11:53:13.024310: Epoch time: 138.07 s\n",
      "2025-12-21 11:53:13.788510: \n",
      "2025-12-21 11:53:13.788510: Epoch 305\n",
      "2025-12-21 11:53:13.792170: Current learning rate: 0.00721\n",
      "2025-12-21 11:55:32.017223: train_loss -0.8354\n",
      "2025-12-21 11:55:32.017223: val_loss -0.8697\n",
      "2025-12-21 11:55:32.033205: Pseudo dice [0.9276, 0.9557, 0.937]\n",
      "2025-12-21 11:55:32.033205: Epoch time: 138.24 s\n",
      "2025-12-21 11:55:32.033205: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2025-12-21 11:55:32.909351: \n",
      "2025-12-21 11:55:32.909351: Epoch 306\n",
      "2025-12-21 11:55:32.911647: Current learning rate: 0.0072\n",
      "2025-12-21 11:57:50.798608: train_loss -0.8419\n",
      "2025-12-21 11:57:50.798608: val_loss -0.8657\n",
      "2025-12-21 11:57:50.802522: Pseudo dice [0.9306, 0.956, 0.9245]\n",
      "2025-12-21 11:57:50.802522: Epoch time: 137.89 s\n",
      "2025-12-21 11:57:50.802522: Yayy! New best EMA pseudo Dice: 0.9351\n",
      "2025-12-21 11:57:51.701800: \n",
      "2025-12-21 11:57:51.701800: Epoch 307\n",
      "2025-12-21 11:57:51.701800: Current learning rate: 0.00719\n",
      "2025-12-21 12:00:09.686876: train_loss -0.8376\n",
      "2025-12-21 12:00:09.686876: val_loss -0.8698\n",
      "2025-12-21 12:00:09.690880: Pseudo dice [0.9288, 0.9581, 0.9321]\n",
      "2025-12-21 12:00:09.692882: Epoch time: 137.99 s\n",
      "2025-12-21 12:00:09.696886: Yayy! New best EMA pseudo Dice: 0.9356\n",
      "2025-12-21 12:00:10.753808: \n",
      "2025-12-21 12:00:10.753808: Epoch 308\n",
      "2025-12-21 12:00:10.758512: Current learning rate: 0.00718\n",
      "2025-12-21 12:02:28.752738: train_loss -0.8425\n",
      "2025-12-21 12:02:28.752738: val_loss -0.8663\n",
      "2025-12-21 12:02:28.756742: Pseudo dice [0.9234, 0.953, 0.936]\n",
      "2025-12-21 12:02:28.758744: Epoch time: 138.0 s\n",
      "2025-12-21 12:02:28.760485: Yayy! New best EMA pseudo Dice: 0.9358\n",
      "2025-12-21 12:02:29.831937: \n",
      "2025-12-21 12:02:29.831937: Epoch 309\n",
      "2025-12-21 12:02:29.831937: Current learning rate: 0.00717\n",
      "2025-12-21 12:04:47.846361: train_loss -0.839\n",
      "2025-12-21 12:04:47.848364: val_loss -0.8671\n",
      "2025-12-21 12:04:47.852369: Pseudo dice [0.9289, 0.9525, 0.9303]\n",
      "2025-12-21 12:04:47.854371: Epoch time: 138.01 s\n",
      "2025-12-21 12:04:47.858375: Yayy! New best EMA pseudo Dice: 0.9359\n",
      "2025-12-21 12:04:48.745526: \n",
      "2025-12-21 12:04:48.745526: Epoch 310\n",
      "2025-12-21 12:04:48.761561: Current learning rate: 0.00716\n",
      "2025-12-21 12:07:06.692546: train_loss -0.8399\n",
      "2025-12-21 12:07:06.692546: val_loss -0.8513\n",
      "2025-12-21 12:07:06.696550: Pseudo dice [0.9145, 0.9474, 0.934]\n",
      "2025-12-21 12:07:06.698551: Epoch time: 137.95 s\n",
      "2025-12-21 12:07:07.467482: \n",
      "2025-12-21 12:07:07.469486: Epoch 311\n",
      "2025-12-21 12:07:07.469486: Current learning rate: 0.00715\n",
      "2025-12-21 12:09:25.767579: train_loss -0.8399\n",
      "2025-12-21 12:09:25.767579: val_loss -0.868\n",
      "2025-12-21 12:09:25.773123: Pseudo dice [0.9241, 0.9564, 0.9358]\n",
      "2025-12-21 12:09:25.775125: Epoch time: 138.3 s\n",
      "2025-12-21 12:09:26.428420: \n",
      "2025-12-21 12:09:26.428420: Epoch 312\n",
      "2025-12-21 12:09:26.428420: Current learning rate: 0.00714\n",
      "2025-12-21 12:11:44.407958: train_loss -0.8343\n",
      "2025-12-21 12:11:44.407958: val_loss -0.8541\n",
      "2025-12-21 12:11:44.412229: Pseudo dice [0.9151, 0.9477, 0.9337]\n",
      "2025-12-21 12:11:44.415233: Epoch time: 137.98 s\n",
      "2025-12-21 12:11:45.062668: \n",
      "2025-12-21 12:11:45.062668: Epoch 313\n",
      "2025-12-21 12:11:45.064671: Current learning rate: 0.00713\n",
      "2025-12-21 12:14:03.119748: train_loss -0.8384\n",
      "2025-12-21 12:14:03.121751: val_loss -0.8641\n",
      "2025-12-21 12:14:03.121751: Pseudo dice [0.9208, 0.9515, 0.9364]\n",
      "2025-12-21 12:14:03.121751: Epoch time: 138.06 s\n",
      "2025-12-21 12:14:03.938896: \n",
      "2025-12-21 12:14:03.938896: Epoch 314\n",
      "2025-12-21 12:14:03.946270: Current learning rate: 0.00712\n",
      "2025-12-21 12:16:21.774854: train_loss -0.844\n",
      "2025-12-21 12:16:21.776856: val_loss -0.8615\n",
      "2025-12-21 12:16:21.778858: Pseudo dice [0.9239, 0.9516, 0.9271]\n",
      "2025-12-21 12:16:21.782358: Epoch time: 137.84 s\n",
      "2025-12-21 12:16:22.589432: \n",
      "2025-12-21 12:16:22.589432: Epoch 315\n",
      "2025-12-21 12:16:22.589432: Current learning rate: 0.00711\n",
      "2025-12-21 12:18:40.589475: train_loss -0.8288\n",
      "2025-12-21 12:18:40.591478: val_loss -0.8609\n",
      "2025-12-21 12:18:40.591478: Pseudo dice [0.9213, 0.9528, 0.9288]\n",
      "2025-12-21 12:18:40.591478: Epoch time: 138.0 s\n",
      "2025-12-21 12:18:41.243063: \n",
      "2025-12-21 12:18:41.243063: Epoch 316\n",
      "2025-12-21 12:18:41.243063: Current learning rate: 0.0071\n",
      "2025-12-21 12:20:59.231233: train_loss -0.8321\n",
      "2025-12-21 12:20:59.231233: val_loss -0.8457\n",
      "2025-12-21 12:20:59.236981: Pseudo dice [0.9111, 0.9421, 0.9322]\n",
      "2025-12-21 12:20:59.240991: Epoch time: 137.99 s\n",
      "2025-12-21 12:20:59.994403: \n",
      "2025-12-21 12:20:59.994403: Epoch 317\n",
      "2025-12-21 12:20:59.994403: Current learning rate: 0.0071\n",
      "2025-12-21 12:23:18.044915: train_loss -0.8398\n",
      "2025-12-21 12:23:18.044915: val_loss -0.8691\n",
      "2025-12-21 12:23:18.048919: Pseudo dice [0.927, 0.9577, 0.9277]\n",
      "2025-12-21 12:23:18.051924: Epoch time: 138.05 s\n",
      "2025-12-21 12:23:18.684139: \n",
      "2025-12-21 12:23:18.684139: Epoch 318\n",
      "2025-12-21 12:23:18.684139: Current learning rate: 0.00709\n",
      "2025-12-21 12:25:36.631607: train_loss -0.8422\n",
      "2025-12-21 12:25:36.631607: val_loss -0.8717\n",
      "2025-12-21 12:25:36.631607: Pseudo dice [0.9295, 0.9575, 0.932]\n",
      "2025-12-21 12:25:36.631607: Epoch time: 137.95 s\n",
      "2025-12-21 12:25:37.274118: \n",
      "2025-12-21 12:25:37.274118: Epoch 319\n",
      "2025-12-21 12:25:37.274118: Current learning rate: 0.00708\n",
      "2025-12-21 12:27:55.307791: train_loss -0.8319\n",
      "2025-12-21 12:27:55.307791: val_loss -0.844\n",
      "2025-12-21 12:27:55.313345: Pseudo dice [0.9143, 0.9449, 0.9301]\n",
      "2025-12-21 12:27:55.315348: Epoch time: 138.03 s\n",
      "2025-12-21 12:27:56.076955: \n",
      "2025-12-21 12:27:56.076955: Epoch 320\n",
      "2025-12-21 12:27:56.087807: Current learning rate: 0.00707\n",
      "2025-12-21 12:30:13.979008: train_loss -0.8365\n",
      "2025-12-21 12:30:13.979008: val_loss -0.8491\n",
      "2025-12-21 12:30:13.983176: Pseudo dice [0.9206, 0.9504, 0.9167]\n",
      "2025-12-21 12:30:13.984992: Epoch time: 137.9 s\n",
      "2025-12-21 12:30:14.784549: \n",
      "2025-12-21 12:30:14.784549: Epoch 321\n",
      "2025-12-21 12:30:14.784549: Current learning rate: 0.00706\n",
      "2025-12-21 12:32:32.472912: train_loss -0.8487\n",
      "2025-12-21 12:32:32.472912: val_loss -0.8637\n",
      "2025-12-21 12:32:32.472912: Pseudo dice [0.9222, 0.9516, 0.9369]\n",
      "2025-12-21 12:32:32.472912: Epoch time: 137.69 s\n",
      "2025-12-21 12:32:33.125555: \n",
      "2025-12-21 12:32:33.125555: Epoch 322\n",
      "2025-12-21 12:32:33.125555: Current learning rate: 0.00705\n",
      "2025-12-21 12:34:50.879588: train_loss -0.8451\n",
      "2025-12-21 12:34:50.879588: val_loss -0.8349\n",
      "2025-12-21 12:34:50.879588: Pseudo dice [0.9063, 0.9431, 0.926]\n",
      "2025-12-21 12:34:50.886549: Epoch time: 137.76 s\n",
      "2025-12-21 12:34:51.605283: \n",
      "2025-12-21 12:34:51.605283: Epoch 323\n",
      "2025-12-21 12:34:51.619588: Current learning rate: 0.00704\n",
      "2025-12-21 12:37:09.480507: train_loss -0.8404\n",
      "2025-12-21 12:37:09.480507: val_loss -0.8485\n",
      "2025-12-21 12:37:09.484511: Pseudo dice [0.9102, 0.9398, 0.9394]\n",
      "2025-12-21 12:37:09.488015: Epoch time: 137.88 s\n",
      "2025-12-21 12:37:10.122117: \n",
      "2025-12-21 12:37:10.137760: Epoch 324\n",
      "2025-12-21 12:37:10.137760: Current learning rate: 0.00703\n",
      "2025-12-21 12:39:28.149970: train_loss -0.8405\n",
      "2025-12-21 12:39:28.149970: val_loss -0.8663\n",
      "2025-12-21 12:39:28.149970: Pseudo dice [0.9286, 0.9544, 0.931]\n",
      "2025-12-21 12:39:28.149970: Epoch time: 138.03 s\n",
      "2025-12-21 12:39:28.783436: \n",
      "2025-12-21 12:39:28.783436: Epoch 325\n",
      "2025-12-21 12:39:28.783436: Current learning rate: 0.00702\n",
      "2025-12-21 12:41:46.799082: train_loss -0.8409\n",
      "2025-12-21 12:41:46.799082: val_loss -0.8547\n",
      "2025-12-21 12:41:46.804829: Pseudo dice [0.9143, 0.9486, 0.9389]\n",
      "2025-12-21 12:41:46.808835: Epoch time: 138.02 s\n",
      "2025-12-21 12:41:47.434681: \n",
      "2025-12-21 12:41:47.434681: Epoch 326\n",
      "2025-12-21 12:41:47.450480: Current learning rate: 0.00701\n",
      "2025-12-21 12:44:05.235622: train_loss -0.842\n",
      "2025-12-21 12:44:05.237487: val_loss -0.8569\n",
      "2025-12-21 12:44:05.241124: Pseudo dice [0.9168, 0.9502, 0.9364]\n",
      "2025-12-21 12:44:05.243127: Epoch time: 137.8 s\n",
      "2025-12-21 12:44:06.057292: \n",
      "2025-12-21 12:44:06.057292: Epoch 327\n",
      "2025-12-21 12:44:06.057292: Current learning rate: 0.007\n",
      "2025-12-21 12:46:24.092143: train_loss -0.8387\n",
      "2025-12-21 12:46:24.092143: val_loss -0.8508\n",
      "2025-12-21 12:46:24.108178: Pseudo dice [0.9168, 0.9475, 0.9241]\n",
      "2025-12-21 12:46:24.108178: Epoch time: 138.03 s\n",
      "2025-12-21 12:46:24.759569: \n",
      "2025-12-21 12:46:24.761309: Epoch 328\n",
      "2025-12-21 12:46:24.761309: Current learning rate: 0.00699\n",
      "2025-12-21 12:48:42.675581: train_loss -0.844\n",
      "2025-12-21 12:48:42.675581: val_loss -0.8626\n",
      "2025-12-21 12:48:42.677584: Pseudo dice [0.9176, 0.9507, 0.9407]\n",
      "2025-12-21 12:48:42.682876: Epoch time: 137.92 s\n",
      "2025-12-21 12:48:43.332799: \n",
      "2025-12-21 12:48:43.332799: Epoch 329\n",
      "2025-12-21 12:48:43.335289: Current learning rate: 0.00698\n",
      "2025-12-21 12:51:01.370615: train_loss -0.8436\n",
      "2025-12-21 12:51:01.372617: val_loss -0.8652\n",
      "2025-12-21 12:51:01.378364: Pseudo dice [0.9271, 0.9556, 0.9215]\n",
      "2025-12-21 12:51:01.380366: Epoch time: 138.04 s\n",
      "2025-12-21 12:51:02.017025: \n",
      "2025-12-21 12:51:02.017025: Epoch 330\n",
      "2025-12-21 12:51:02.033027: Current learning rate: 0.00697\n",
      "2025-12-21 12:53:20.144976: train_loss -0.8424\n",
      "2025-12-21 12:53:20.144976: val_loss -0.8667\n",
      "2025-12-21 12:53:20.148461: Pseudo dice [0.9236, 0.9499, 0.9426]\n",
      "2025-12-21 12:53:20.152465: Epoch time: 138.13 s\n",
      "2025-12-21 12:53:20.800309: \n",
      "2025-12-21 12:53:20.800309: Epoch 331\n",
      "2025-12-21 12:53:20.800309: Current learning rate: 0.00696\n",
      "2025-12-21 12:55:39.021054: train_loss -0.8451\n",
      "2025-12-21 12:55:39.021054: val_loss -0.858\n",
      "2025-12-21 12:55:39.025058: Pseudo dice [0.9161, 0.946, 0.9403]\n",
      "2025-12-21 12:55:39.029012: Epoch time: 138.22 s\n",
      "2025-12-21 12:55:39.678469: \n",
      "2025-12-21 12:55:39.678469: Epoch 332\n",
      "2025-12-21 12:55:39.678469: Current learning rate: 0.00696\n",
      "2025-12-21 12:57:57.753935: train_loss -0.8411\n",
      "2025-12-21 12:57:57.753935: val_loss -0.8617\n",
      "2025-12-21 12:57:57.761764: Pseudo dice [0.9245, 0.9531, 0.9321]\n",
      "2025-12-21 12:57:57.763767: Epoch time: 138.08 s\n",
      "2025-12-21 12:57:58.403671: \n",
      "2025-12-21 12:57:58.403671: Epoch 333\n",
      "2025-12-21 12:57:58.403671: Current learning rate: 0.00695\n",
      "2025-12-21 13:00:16.454846: train_loss -0.8376\n",
      "2025-12-21 13:00:16.454846: val_loss -0.861\n",
      "2025-12-21 13:00:16.458850: Pseudo dice [0.9194, 0.948, 0.9412]\n",
      "2025-12-21 13:00:16.460852: Epoch time: 138.05 s\n",
      "2025-12-21 13:00:17.278310: \n",
      "2025-12-21 13:00:17.278310: Epoch 334\n",
      "2025-12-21 13:00:17.278310: Current learning rate: 0.00694\n",
      "2025-12-21 13:02:35.263957: train_loss -0.8419\n",
      "2025-12-21 13:02:35.265960: val_loss -0.8431\n",
      "2025-12-21 13:02:35.269467: Pseudo dice [0.9077, 0.9411, 0.9319]\n",
      "2025-12-21 13:02:35.271470: Epoch time: 137.99 s\n",
      "2025-12-21 13:02:35.916353: \n",
      "2025-12-21 13:02:35.916353: Epoch 335\n",
      "2025-12-21 13:02:35.916353: Current learning rate: 0.00693\n",
      "2025-12-21 13:04:53.836101: train_loss -0.8435\n",
      "2025-12-21 13:04:53.836101: val_loss -0.8586\n",
      "2025-12-21 13:04:53.836101: Pseudo dice [0.9191, 0.9499, 0.935]\n",
      "2025-12-21 13:04:53.836101: Epoch time: 137.92 s\n",
      "2025-12-21 13:04:54.486542: \n",
      "2025-12-21 13:04:54.486542: Epoch 336\n",
      "2025-12-21 13:04:54.486542: Current learning rate: 0.00692\n",
      "2025-12-21 13:07:12.397713: train_loss -0.8383\n",
      "2025-12-21 13:07:12.397713: val_loss -0.8641\n",
      "2025-12-21 13:07:12.401717: Pseudo dice [0.9229, 0.9503, 0.9359]\n",
      "2025-12-21 13:07:12.403719: Epoch time: 137.91 s\n",
      "2025-12-21 13:07:13.126886: \n",
      "2025-12-21 13:07:13.126886: Epoch 337\n",
      "2025-12-21 13:07:13.126886: Current learning rate: 0.00691\n",
      "2025-12-21 13:09:31.377246: train_loss -0.8418\n",
      "2025-12-21 13:09:31.377246: val_loss -0.8535\n",
      "2025-12-21 13:09:31.381251: Pseudo dice [0.913, 0.9451, 0.9342]\n",
      "2025-12-21 13:09:31.381251: Epoch time: 138.25 s\n",
      "2025-12-21 13:09:32.047284: \n",
      "2025-12-21 13:09:32.047284: Epoch 338\n",
      "2025-12-21 13:09:32.047284: Current learning rate: 0.0069\n",
      "2025-12-21 13:11:50.263608: train_loss -0.8394\n",
      "2025-12-21 13:11:50.263608: val_loss -0.8747\n",
      "2025-12-21 13:11:50.267612: Pseudo dice [0.9298, 0.9538, 0.9394]\n",
      "2025-12-21 13:11:50.269614: Epoch time: 138.22 s\n",
      "2025-12-21 13:11:51.091604: \n",
      "2025-12-21 13:11:51.091604: Epoch 339\n",
      "2025-12-21 13:11:51.091604: Current learning rate: 0.00689\n",
      "2025-12-21 13:14:09.047779: train_loss -0.8384\n",
      "2025-12-21 13:14:09.047779: val_loss -0.8422\n",
      "2025-12-21 13:14:09.047779: Pseudo dice [0.9033, 0.9414, 0.9369]\n",
      "2025-12-21 13:14:09.063734: Epoch time: 137.96 s\n",
      "2025-12-21 13:14:09.754899: \n",
      "2025-12-21 13:14:09.754899: Epoch 340\n",
      "2025-12-21 13:14:09.754899: Current learning rate: 0.00688\n",
      "2025-12-21 13:16:27.538400: train_loss -0.8448\n",
      "2025-12-21 13:16:27.538400: val_loss -0.8501\n",
      "2025-12-21 13:16:27.538400: Pseudo dice [0.9163, 0.948, 0.922]\n",
      "2025-12-21 13:16:27.538400: Epoch time: 137.78 s\n",
      "2025-12-21 13:16:28.193322: \n",
      "2025-12-21 13:16:28.195325: Epoch 341\n",
      "2025-12-21 13:16:28.195325: Current learning rate: 0.00687\n",
      "2025-12-21 13:18:46.114049: train_loss -0.8336\n",
      "2025-12-21 13:18:46.114049: val_loss -0.869\n",
      "2025-12-21 13:18:46.114049: Pseudo dice [0.9302, 0.9583, 0.9264]\n",
      "2025-12-21 13:18:46.114049: Epoch time: 137.92 s\n",
      "2025-12-21 13:18:46.769969: \n",
      "2025-12-21 13:18:46.769969: Epoch 342\n",
      "2025-12-21 13:18:46.769969: Current learning rate: 0.00686\n",
      "2025-12-21 13:21:04.533012: train_loss -0.822\n",
      "2025-12-21 13:21:04.533012: val_loss -0.8325\n",
      "2025-12-21 13:21:04.538018: Pseudo dice [0.9085, 0.9409, 0.9197]\n",
      "2025-12-21 13:21:04.542023: Epoch time: 137.77 s\n",
      "2025-12-21 13:21:05.355807: \n",
      "2025-12-21 13:21:05.355807: Epoch 343\n",
      "2025-12-21 13:21:05.355807: Current learning rate: 0.00685\n",
      "2025-12-21 13:23:23.236966: train_loss -0.8245\n",
      "2025-12-21 13:23:23.236966: val_loss -0.8602\n",
      "2025-12-21 13:23:23.240970: Pseudo dice [0.9281, 0.9554, 0.9298]\n",
      "2025-12-21 13:23:23.242973: Epoch time: 137.88 s\n",
      "2025-12-21 13:23:23.890097: \n",
      "2025-12-21 13:23:23.890097: Epoch 344\n",
      "2025-12-21 13:23:23.890097: Current learning rate: 0.00684\n",
      "2025-12-21 13:25:41.810021: train_loss -0.832\n",
      "2025-12-21 13:25:41.812024: val_loss -0.8612\n",
      "2025-12-21 13:25:41.816029: Pseudo dice [0.9289, 0.9537, 0.9235]\n",
      "2025-12-21 13:25:41.818031: Epoch time: 137.92 s\n",
      "2025-12-21 13:25:42.469700: \n",
      "2025-12-21 13:25:42.469700: Epoch 345\n",
      "2025-12-21 13:25:42.469700: Current learning rate: 0.00683\n",
      "2025-12-21 13:28:00.514692: train_loss -0.8329\n",
      "2025-12-21 13:28:00.514692: val_loss -0.8579\n",
      "2025-12-21 13:28:00.518697: Pseudo dice [0.9213, 0.9525, 0.9282]\n",
      "2025-12-21 13:28:00.520698: Epoch time: 138.04 s\n",
      "2025-12-21 13:28:01.265973: \n",
      "2025-12-21 13:28:01.265973: Epoch 346\n",
      "2025-12-21 13:28:01.270220: Current learning rate: 0.00682\n",
      "2025-12-21 13:30:19.185736: train_loss -0.8407\n",
      "2025-12-21 13:30:19.185736: val_loss -0.8668\n",
      "2025-12-21 13:30:19.185736: Pseudo dice [0.9267, 0.9549, 0.9345]\n",
      "2025-12-21 13:30:19.201084: Epoch time: 137.92 s\n",
      "2025-12-21 13:30:19.853747: \n",
      "2025-12-21 13:30:19.853747: Epoch 347\n",
      "2025-12-21 13:30:19.853747: Current learning rate: 0.00681\n",
      "2025-12-21 13:32:37.781008: train_loss -0.8374\n",
      "2025-12-21 13:32:37.783010: val_loss -0.8567\n",
      "2025-12-21 13:32:37.787014: Pseudo dice [0.92, 0.9471, 0.9347]\n",
      "2025-12-21 13:32:37.790760: Epoch time: 137.93 s\n",
      "2025-12-21 13:32:38.442642: \n",
      "2025-12-21 13:32:38.444644: Epoch 348\n",
      "2025-12-21 13:32:38.444644: Current learning rate: 0.0068\n",
      "2025-12-21 13:34:56.488838: train_loss -0.8281\n",
      "2025-12-21 13:34:56.488838: val_loss -0.8248\n",
      "2025-12-21 13:34:56.502342: Pseudo dice [0.9059, 0.941, 0.9162]\n",
      "2025-12-21 13:34:56.504344: Epoch time: 138.05 s\n",
      "2025-12-21 13:34:57.230565: \n",
      "2025-12-21 13:34:57.230565: Epoch 349\n",
      "2025-12-21 13:34:57.248888: Current learning rate: 0.0068\n",
      "2025-12-21 13:37:15.159390: train_loss -0.7939\n",
      "2025-12-21 13:37:15.159390: val_loss -0.8148\n",
      "2025-12-21 13:37:15.165006: Pseudo dice [0.9078, 0.9366, 0.9074]\n",
      "2025-12-21 13:37:15.169011: Epoch time: 137.93 s\n",
      "2025-12-21 13:37:16.070189: \n",
      "2025-12-21 13:37:16.070189: Epoch 350\n",
      "2025-12-21 13:37:16.070189: Current learning rate: 0.00679\n",
      "2025-12-21 13:39:34.107594: train_loss -0.8025\n",
      "2025-12-21 13:39:34.107594: val_loss -0.845\n",
      "2025-12-21 13:39:34.125111: Pseudo dice [0.9216, 0.9506, 0.9256]\n",
      "2025-12-21 13:39:34.129116: Epoch time: 138.04 s\n",
      "2025-12-21 13:39:34.930887: \n",
      "2025-12-21 13:39:34.930887: Epoch 351\n",
      "2025-12-21 13:39:34.930887: Current learning rate: 0.00678\n",
      "2025-12-21 13:41:52.695105: train_loss -0.8172\n",
      "2025-12-21 13:41:52.695105: val_loss -0.8374\n",
      "2025-12-21 13:41:52.701041: Pseudo dice [0.9137, 0.9471, 0.9131]\n",
      "2025-12-21 13:41:52.703044: Epoch time: 137.76 s\n",
      "2025-12-21 13:41:53.345185: \n",
      "2025-12-21 13:41:53.345185: Epoch 352\n",
      "2025-12-21 13:41:53.345185: Current learning rate: 0.00677\n",
      "2025-12-21 13:44:11.347115: train_loss -0.8191\n",
      "2025-12-21 13:44:11.347115: val_loss -0.8463\n",
      "2025-12-21 13:44:11.358127: Pseudo dice [0.9123, 0.9483, 0.9325]\n",
      "2025-12-21 13:44:11.362980: Epoch time: 138.0 s\n",
      "2025-12-21 13:44:12.013527: \n",
      "2025-12-21 13:44:12.013527: Epoch 353\n",
      "2025-12-21 13:44:12.013527: Current learning rate: 0.00676\n",
      "2025-12-21 13:46:29.920510: train_loss -0.8304\n",
      "2025-12-21 13:46:29.920510: val_loss -0.8358\n",
      "2025-12-21 13:46:29.927196: Pseudo dice [0.9084, 0.9368, 0.9237]\n",
      "2025-12-21 13:46:29.931201: Epoch time: 137.91 s\n",
      "2025-12-21 13:46:30.583578: \n",
      "2025-12-21 13:46:30.583578: Epoch 354\n",
      "2025-12-21 13:46:30.583578: Current learning rate: 0.00675\n",
      "2025-12-21 13:48:48.427278: train_loss -0.8368\n",
      "2025-12-21 13:48:48.429280: val_loss -0.8332\n",
      "2025-12-21 13:48:48.433285: Pseudo dice [0.9031, 0.9346, 0.9292]\n",
      "2025-12-21 13:48:48.439293: Epoch time: 137.84 s\n",
      "2025-12-21 13:48:49.086511: \n",
      "2025-12-21 13:48:49.086511: Epoch 355\n",
      "2025-12-21 13:48:49.102144: Current learning rate: 0.00674\n",
      "2025-12-21 13:51:07.309236: train_loss -0.835\n",
      "2025-12-21 13:51:07.309236: val_loss -0.8747\n",
      "2025-12-21 13:51:07.314755: Pseudo dice [0.9322, 0.9603, 0.9349]\n",
      "2025-12-21 13:51:07.318759: Epoch time: 138.22 s\n",
      "2025-12-21 13:51:07.966918: \n",
      "2025-12-21 13:51:07.966918: Epoch 356\n",
      "2025-12-21 13:51:07.969185: Current learning rate: 0.00673\n",
      "2025-12-21 13:53:25.851294: train_loss -0.8296\n",
      "2025-12-21 13:53:25.854795: val_loss -0.8688\n",
      "2025-12-21 13:53:25.854795: Pseudo dice [0.927, 0.9538, 0.9367]\n",
      "2025-12-21 13:53:25.854795: Epoch time: 137.88 s\n",
      "2025-12-21 13:53:26.670112: \n",
      "2025-12-21 13:53:26.670112: Epoch 357\n",
      "2025-12-21 13:53:26.670112: Current learning rate: 0.00672\n",
      "2025-12-21 13:55:44.506145: train_loss -0.8309\n",
      "2025-12-21 13:55:44.508148: val_loss -0.8506\n",
      "2025-12-21 13:55:44.508148: Pseudo dice [0.9154, 0.9457, 0.9304]\n",
      "2025-12-21 13:55:44.508148: Epoch time: 137.84 s\n",
      "2025-12-21 13:55:45.157415: \n",
      "2025-12-21 13:55:45.157415: Epoch 358\n",
      "2025-12-21 13:55:45.159922: Current learning rate: 0.00671\n",
      "2025-12-21 13:58:03.128656: train_loss -0.8326\n",
      "2025-12-21 13:58:03.128656: val_loss -0.8684\n",
      "2025-12-21 13:58:03.136405: Pseudo dice [0.9282, 0.9546, 0.9332]\n",
      "2025-12-21 13:58:03.140411: Epoch time: 137.97 s\n",
      "2025-12-21 13:58:03.778409: \n",
      "2025-12-21 13:58:03.794164: Epoch 359\n",
      "2025-12-21 13:58:03.796943: Current learning rate: 0.0067\n",
      "2025-12-21 14:00:21.701547: train_loss -0.8471\n",
      "2025-12-21 14:00:21.701547: val_loss -0.8655\n",
      "2025-12-21 14:00:21.701547: Pseudo dice [0.9246, 0.9572, 0.9311]\n",
      "2025-12-21 14:00:21.710918: Epoch time: 137.92 s\n",
      "2025-12-21 14:00:22.416197: \n",
      "2025-12-21 14:00:22.416197: Epoch 360\n",
      "2025-12-21 14:00:22.430000: Current learning rate: 0.00669\n",
      "2025-12-21 14:02:40.287225: train_loss -0.8408\n",
      "2025-12-21 14:02:40.287225: val_loss -0.867\n",
      "2025-12-21 14:02:40.295236: Pseudo dice [0.924, 0.9546, 0.9367]\n",
      "2025-12-21 14:02:40.300986: Epoch time: 137.87 s\n",
      "2025-12-21 14:02:40.949672: \n",
      "2025-12-21 14:02:40.949672: Epoch 361\n",
      "2025-12-21 14:02:40.949672: Current learning rate: 0.00668\n",
      "2025-12-21 14:04:58.936064: train_loss -0.8399\n",
      "2025-12-21 14:04:58.936064: val_loss -0.8542\n",
      "2025-12-21 14:04:58.936064: Pseudo dice [0.9187, 0.9477, 0.9323]\n",
      "2025-12-21 14:04:58.950687: Epoch time: 137.99 s\n",
      "2025-12-21 14:04:59.585669: \n",
      "2025-12-21 14:04:59.585669: Epoch 362\n",
      "2025-12-21 14:04:59.585669: Current learning rate: 0.00667\n",
      "2025-12-21 14:07:17.543704: train_loss -0.8381\n",
      "2025-12-21 14:07:17.543704: val_loss -0.8614\n",
      "2025-12-21 14:07:17.547446: Pseudo dice [0.9249, 0.954, 0.916]\n",
      "2025-12-21 14:07:17.549449: Epoch time: 137.96 s\n",
      "2025-12-21 14:07:18.327044: \n",
      "2025-12-21 14:07:18.327044: Epoch 363\n",
      "2025-12-21 14:07:18.336464: Current learning rate: 0.00666\n",
      "2025-12-21 14:09:36.665069: train_loss -0.8434\n",
      "2025-12-21 14:09:36.667072: val_loss -0.8698\n",
      "2025-12-21 14:09:36.668813: Pseudo dice [0.9283, 0.9532, 0.9356]\n",
      "2025-12-21 14:09:36.668813: Epoch time: 138.34 s\n",
      "2025-12-21 14:09:37.509807: \n",
      "2025-12-21 14:09:37.509807: Epoch 364\n",
      "2025-12-21 14:09:37.525662: Current learning rate: 0.00665\n",
      "2025-12-21 14:11:55.688962: train_loss -0.8448\n",
      "2025-12-21 14:11:55.688962: val_loss -0.861\n",
      "2025-12-21 14:11:55.690965: Pseudo dice [0.9194, 0.951, 0.93]\n",
      "2025-12-21 14:11:55.690965: Epoch time: 138.18 s\n",
      "2025-12-21 14:11:56.347743: \n",
      "2025-12-21 14:11:56.347743: Epoch 365\n",
      "2025-12-21 14:11:56.347743: Current learning rate: 0.00665\n",
      "2025-12-21 14:14:14.295190: train_loss -0.8466\n",
      "2025-12-21 14:14:14.295190: val_loss -0.8632\n",
      "2025-12-21 14:14:14.295190: Pseudo dice [0.9247, 0.9558, 0.9357]\n",
      "2025-12-21 14:14:14.295190: Epoch time: 137.96 s\n",
      "2025-12-21 14:14:15.134621: \n",
      "2025-12-21 14:14:15.134621: Epoch 366\n",
      "2025-12-21 14:14:15.134621: Current learning rate: 0.00664\n",
      "2025-12-21 14:16:33.177858: train_loss -0.8428\n",
      "2025-12-21 14:16:33.177858: val_loss -0.856\n",
      "2025-12-21 14:16:33.181863: Pseudo dice [0.9162, 0.9503, 0.9327]\n",
      "2025-12-21 14:16:33.185442: Epoch time: 138.04 s\n",
      "2025-12-21 14:16:33.833100: \n",
      "2025-12-21 14:16:33.833100: Epoch 367\n",
      "2025-12-21 14:16:33.848305: Current learning rate: 0.00663\n",
      "2025-12-21 14:18:51.810994: train_loss -0.8412\n",
      "2025-12-21 14:18:51.810994: val_loss -0.8647\n",
      "2025-12-21 14:18:51.814058: Pseudo dice [0.9231, 0.9538, 0.9345]\n",
      "2025-12-21 14:18:51.818495: Epoch time: 137.98 s\n",
      "2025-12-21 14:18:52.474286: \n",
      "2025-12-21 14:18:52.474286: Epoch 368\n",
      "2025-12-21 14:18:52.474286: Current learning rate: 0.00662\n",
      "2025-12-21 14:21:10.435001: train_loss -0.8414\n",
      "2025-12-21 14:21:10.435001: val_loss -0.8453\n",
      "2025-12-21 14:21:10.439563: Pseudo dice [0.9082, 0.9437, 0.935]\n",
      "2025-12-21 14:21:10.441566: Epoch time: 137.96 s\n",
      "2025-12-21 14:21:11.356695: \n",
      "2025-12-21 14:21:11.356695: Epoch 369\n",
      "2025-12-21 14:21:11.372677: Current learning rate: 0.00661\n",
      "2025-12-21 14:23:29.315326: train_loss -0.8399\n",
      "2025-12-21 14:23:29.315326: val_loss -0.8407\n",
      "2025-12-21 14:23:29.330626: Pseudo dice [0.9078, 0.9437, 0.9309]\n",
      "2025-12-21 14:23:29.330626: Epoch time: 137.96 s\n",
      "2025-12-21 14:23:29.979430: \n",
      "2025-12-21 14:23:29.979430: Epoch 370\n",
      "2025-12-21 14:23:29.979430: Current learning rate: 0.0066\n",
      "2025-12-21 14:25:48.012082: train_loss -0.8402\n",
      "2025-12-21 14:25:48.012082: val_loss -0.8436\n",
      "2025-12-21 14:25:48.017036: Pseudo dice [0.9059, 0.9428, 0.9357]\n",
      "2025-12-21 14:25:48.017036: Epoch time: 138.03 s\n",
      "2025-12-21 14:25:48.672363: \n",
      "2025-12-21 14:25:48.672363: Epoch 371\n",
      "2025-12-21 14:25:48.672363: Current learning rate: 0.00659\n",
      "2025-12-21 14:28:06.599633: train_loss -0.8467\n",
      "2025-12-21 14:28:06.599633: val_loss -0.8649\n",
      "2025-12-21 14:28:06.601635: Pseudo dice [0.9221, 0.9539, 0.9357]\n",
      "2025-12-21 14:28:06.601635: Epoch time: 137.93 s\n",
      "2025-12-21 14:28:07.354017: \n",
      "2025-12-21 14:28:07.354017: Epoch 372\n",
      "2025-12-21 14:28:07.354017: Current learning rate: 0.00658\n",
      "2025-12-21 14:30:25.398865: train_loss -0.8348\n",
      "2025-12-21 14:30:25.414638: val_loss -0.8516\n",
      "2025-12-21 14:30:25.418545: Pseudo dice [0.9203, 0.9495, 0.9274]\n",
      "2025-12-21 14:30:25.418545: Epoch time: 138.04 s\n",
      "2025-12-21 14:30:26.064887: \n",
      "2025-12-21 14:30:26.064887: Epoch 373\n",
      "2025-12-21 14:30:26.064887: Current learning rate: 0.00657\n",
      "2025-12-21 14:32:43.839560: train_loss -0.8342\n",
      "2025-12-21 14:32:43.839560: val_loss -0.8634\n",
      "2025-12-21 14:32:43.843304: Pseudo dice [0.9248, 0.9533, 0.9307]\n",
      "2025-12-21 14:32:43.843304: Epoch time: 137.79 s\n",
      "2025-12-21 14:32:44.508963: \n",
      "2025-12-21 14:32:44.508963: Epoch 374\n",
      "2025-12-21 14:32:44.524766: Current learning rate: 0.00656\n",
      "2025-12-21 14:35:02.571709: train_loss -0.8408\n",
      "2025-12-21 14:35:02.571709: val_loss -0.8613\n",
      "2025-12-21 14:35:02.576716: Pseudo dice [0.9204, 0.9493, 0.9327]\n",
      "2025-12-21 14:35:02.580722: Epoch time: 138.06 s\n",
      "2025-12-21 14:35:03.525366: \n",
      "2025-12-21 14:35:03.525366: Epoch 375\n",
      "2025-12-21 14:35:03.525366: Current learning rate: 0.00655\n",
      "2025-12-21 14:37:21.390001: train_loss -0.8449\n",
      "2025-12-21 14:37:21.392004: val_loss -0.8588\n",
      "2025-12-21 14:37:21.396009: Pseudo dice [0.9172, 0.9474, 0.9382]\n",
      "2025-12-21 14:37:21.399752: Epoch time: 137.86 s\n",
      "2025-12-21 14:37:22.048140: \n",
      "2025-12-21 14:37:22.048140: Epoch 376\n",
      "2025-12-21 14:37:22.064097: Current learning rate: 0.00654\n",
      "2025-12-21 14:39:39.979552: train_loss -0.8433\n",
      "2025-12-21 14:39:39.979552: val_loss -0.8656\n",
      "2025-12-21 14:39:39.988016: Pseudo dice [0.9263, 0.9526, 0.9315]\n",
      "2025-12-21 14:39:39.991860: Epoch time: 137.93 s\n",
      "2025-12-21 14:39:40.676471: \n",
      "2025-12-21 14:39:40.676471: Epoch 377\n",
      "2025-12-21 14:39:40.692139: Current learning rate: 0.00653\n",
      "2025-12-21 14:41:58.521606: train_loss -0.8451\n",
      "2025-12-21 14:41:58.521606: val_loss -0.8525\n",
      "2025-12-21 14:41:58.528360: Pseudo dice [0.9141, 0.9452, 0.9345]\n",
      "2025-12-21 14:41:58.528360: Epoch time: 137.85 s\n",
      "2025-12-21 14:41:59.187874: \n",
      "2025-12-21 14:41:59.187874: Epoch 378\n",
      "2025-12-21 14:41:59.187874: Current learning rate: 0.00652\n",
      "2025-12-21 14:44:17.431822: train_loss -0.8407\n",
      "2025-12-21 14:44:17.431822: val_loss -0.8721\n",
      "2025-12-21 14:44:17.449549: Pseudo dice [0.9291, 0.9562, 0.9356]\n",
      "2025-12-21 14:44:17.449549: Epoch time: 138.24 s\n",
      "2025-12-21 14:44:18.112329: \n",
      "2025-12-21 14:44:18.112329: Epoch 379\n",
      "2025-12-21 14:44:18.112329: Current learning rate: 0.00651\n",
      "2025-12-21 14:46:36.182077: train_loss -0.8416\n",
      "2025-12-21 14:46:36.182077: val_loss -0.8615\n",
      "2025-12-21 14:46:36.197473: Pseudo dice [0.9228, 0.9494, 0.936]\n",
      "2025-12-21 14:46:36.199476: Epoch time: 138.07 s\n",
      "2025-12-21 14:46:36.843419: \n",
      "2025-12-21 14:46:36.843419: Epoch 380\n",
      "2025-12-21 14:46:36.843419: Current learning rate: 0.0065\n",
      "2025-12-21 14:48:54.895124: train_loss -0.8351\n",
      "2025-12-21 14:48:54.895124: val_loss -0.8531\n",
      "2025-12-21 14:48:54.911141: Pseudo dice [0.9223, 0.9479, 0.9199]\n",
      "2025-12-21 14:48:54.911141: Epoch time: 138.05 s\n",
      "2025-12-21 14:48:55.560033: \n",
      "2025-12-21 14:48:55.560033: Epoch 381\n",
      "2025-12-21 14:48:55.560033: Current learning rate: 0.00649\n",
      "2025-12-21 14:51:13.541375: train_loss -0.8435\n",
      "2025-12-21 14:51:13.541375: val_loss -0.86\n",
      "2025-12-21 14:51:13.543116: Pseudo dice [0.9209, 0.9497, 0.9295]\n",
      "2025-12-21 14:51:13.543116: Epoch time: 137.98 s\n",
      "2025-12-21 14:51:14.208935: \n",
      "2025-12-21 14:51:14.208935: Epoch 382\n",
      "2025-12-21 14:51:14.208935: Current learning rate: 0.00648\n",
      "2025-12-21 14:53:32.195480: train_loss -0.8452\n",
      "2025-12-21 14:53:32.195480: val_loss -0.8592\n",
      "2025-12-21 14:53:32.195480: Pseudo dice [0.9185, 0.9531, 0.9313]\n",
      "2025-12-21 14:53:32.211479: Epoch time: 137.99 s\n",
      "2025-12-21 14:53:32.863412: \n",
      "2025-12-21 14:53:32.863412: Epoch 383\n",
      "2025-12-21 14:53:32.863412: Current learning rate: 0.00648\n",
      "2025-12-21 14:55:50.840130: train_loss -0.8461\n",
      "2025-12-21 14:55:50.840130: val_loss -0.8661\n",
      "2025-12-21 14:55:50.856196: Pseudo dice [0.9242, 0.9548, 0.9373]\n",
      "2025-12-21 14:55:50.856196: Epoch time: 137.98 s\n",
      "2025-12-21 14:55:51.507067: \n",
      "2025-12-21 14:55:51.507067: Epoch 384\n",
      "2025-12-21 14:55:51.507067: Current learning rate: 0.00647\n",
      "2025-12-21 14:58:09.453808: train_loss -0.8389\n",
      "2025-12-21 14:58:09.453808: val_loss -0.8654\n",
      "2025-12-21 14:58:09.457813: Pseudo dice [0.924, 0.9475, 0.9382]\n",
      "2025-12-21 14:58:09.461818: Epoch time: 137.95 s\n",
      "2025-12-21 14:58:10.129791: \n",
      "2025-12-21 14:58:10.129791: Epoch 385\n",
      "2025-12-21 14:58:10.129791: Current learning rate: 0.00646\n",
      "2025-12-21 15:00:27.953935: train_loss -0.849\n",
      "2025-12-21 15:00:27.953935: val_loss -0.8462\n",
      "2025-12-21 15:00:27.957939: Pseudo dice [0.9103, 0.9412, 0.9296]\n",
      "2025-12-21 15:00:27.961682: Epoch time: 137.82 s\n",
      "2025-12-21 15:00:28.753853: \n",
      "2025-12-21 15:00:28.753853: Epoch 386\n",
      "2025-12-21 15:00:28.753853: Current learning rate: 0.00645\n",
      "2025-12-21 15:02:46.637960: train_loss -0.8455\n",
      "2025-12-21 15:02:46.637960: val_loss -0.8804\n",
      "2025-12-21 15:02:46.637960: Pseudo dice [0.9358, 0.9602, 0.9377]\n",
      "2025-12-21 15:02:46.653893: Epoch time: 137.88 s\n",
      "2025-12-21 15:02:47.445433: \n",
      "2025-12-21 15:02:47.445433: Epoch 387\n",
      "2025-12-21 15:02:47.461435: Current learning rate: 0.00644\n",
      "2025-12-21 15:05:05.484916: train_loss -0.8403\n",
      "2025-12-21 15:05:05.484916: val_loss -0.8663\n",
      "2025-12-21 15:05:05.486919: Pseudo dice [0.9231, 0.9532, 0.9361]\n",
      "2025-12-21 15:05:05.486919: Epoch time: 138.04 s\n",
      "2025-12-21 15:05:06.144847: \n",
      "2025-12-21 15:05:06.144847: Epoch 388\n",
      "2025-12-21 15:05:06.144847: Current learning rate: 0.00643\n",
      "2025-12-21 15:07:24.081120: train_loss -0.8446\n",
      "2025-12-21 15:07:24.081120: val_loss -0.873\n",
      "2025-12-21 15:07:24.086126: Pseudo dice [0.9293, 0.9563, 0.9357]\n",
      "2025-12-21 15:07:24.086126: Epoch time: 137.94 s\n",
      "2025-12-21 15:07:24.879305: \n",
      "2025-12-21 15:07:24.879305: Epoch 389\n",
      "2025-12-21 15:07:24.895334: Current learning rate: 0.00642\n",
      "2025-12-21 15:09:43.027495: train_loss -0.8434\n",
      "2025-12-21 15:09:43.027495: val_loss -0.8561\n",
      "2025-12-21 15:09:43.036739: Pseudo dice [0.9156, 0.9452, 0.9344]\n",
      "2025-12-21 15:09:43.040743: Epoch time: 138.15 s\n",
      "2025-12-21 15:09:43.692179: \n",
      "2025-12-21 15:09:43.692179: Epoch 390\n",
      "2025-12-21 15:09:43.707810: Current learning rate: 0.00641\n",
      "2025-12-21 15:12:01.720296: train_loss -0.8454\n",
      "2025-12-21 15:12:01.720296: val_loss -0.8605\n",
      "2025-12-21 15:12:01.726043: Pseudo dice [0.9197, 0.9515, 0.9318]\n",
      "2025-12-21 15:12:01.730049: Epoch time: 138.03 s\n",
      "2025-12-21 15:12:02.390773: \n",
      "2025-12-21 15:12:02.390773: Epoch 391\n",
      "2025-12-21 15:12:02.390773: Current learning rate: 0.0064\n",
      "2025-12-21 15:14:20.298078: train_loss -0.8391\n",
      "2025-12-21 15:14:20.298078: val_loss -0.8522\n",
      "2025-12-21 15:14:20.303826: Pseudo dice [0.9188, 0.9464, 0.9233]\n",
      "2025-12-21 15:14:20.305829: Epoch time: 137.91 s\n",
      "2025-12-21 15:14:21.111259: \n",
      "2025-12-21 15:14:21.111259: Epoch 392\n",
      "2025-12-21 15:14:21.111259: Current learning rate: 0.00639\n",
      "2025-12-21 15:16:39.031919: train_loss -0.8436\n",
      "2025-12-21 15:16:39.031919: val_loss -0.8704\n",
      "2025-12-21 15:16:39.047900: Pseudo dice [0.9239, 0.9541, 0.9364]\n",
      "2025-12-21 15:16:39.047900: Epoch time: 137.92 s\n",
      "2025-12-21 15:16:39.872922: \n",
      "2025-12-21 15:16:39.872922: Epoch 393\n",
      "2025-12-21 15:16:39.872922: Current learning rate: 0.00638\n",
      "2025-12-21 15:18:57.749968: train_loss -0.849\n",
      "2025-12-21 15:18:57.749968: val_loss -0.8707\n",
      "2025-12-21 15:18:57.749968: Pseudo dice [0.927, 0.9554, 0.9332]\n",
      "2025-12-21 15:18:57.749968: Epoch time: 137.88 s\n",
      "2025-12-21 15:18:58.413545: \n",
      "2025-12-21 15:18:58.413545: Epoch 394\n",
      "2025-12-21 15:18:58.413545: Current learning rate: 0.00637\n",
      "2025-12-21 15:21:16.479795: train_loss -0.8404\n",
      "2025-12-21 15:21:16.479795: val_loss -0.8704\n",
      "2025-12-21 15:21:16.481797: Pseudo dice [0.9265, 0.9548, 0.936]\n",
      "2025-12-21 15:21:16.481797: Epoch time: 138.07 s\n",
      "2025-12-21 15:21:17.210413: \n",
      "2025-12-21 15:21:17.210413: Epoch 395\n",
      "2025-12-21 15:21:17.228276: Current learning rate: 0.00636\n",
      "2025-12-21 15:23:35.387284: train_loss -0.8465\n",
      "2025-12-21 15:23:35.387284: val_loss -0.8676\n",
      "2025-12-21 15:23:35.387284: Pseudo dice [0.9228, 0.9535, 0.9349]\n",
      "2025-12-21 15:23:35.401099: Epoch time: 138.18 s\n",
      "2025-12-21 15:23:36.052310: \n",
      "2025-12-21 15:23:36.052310: Epoch 396\n",
      "2025-12-21 15:23:36.052310: Current learning rate: 0.00635\n",
      "2025-12-21 15:25:53.801772: train_loss -0.8461\n",
      "2025-12-21 15:25:53.803775: val_loss -0.8655\n",
      "2025-12-21 15:25:53.803775: Pseudo dice [0.928, 0.9525, 0.9322]\n",
      "2025-12-21 15:25:53.803775: Epoch time: 137.75 s\n",
      "2025-12-21 15:25:53.803775: Yayy! New best EMA pseudo Dice: 0.936\n",
      "2025-12-21 15:25:54.702931: \n",
      "2025-12-21 15:25:54.702931: Epoch 397\n",
      "2025-12-21 15:25:54.702931: Current learning rate: 0.00634\n",
      "2025-12-21 15:28:12.588122: train_loss -0.8485\n",
      "2025-12-21 15:28:12.588122: val_loss -0.8704\n",
      "2025-12-21 15:28:12.592125: Pseudo dice [0.9243, 0.9507, 0.9425]\n",
      "2025-12-21 15:28:12.596129: Epoch time: 137.89 s\n",
      "2025-12-21 15:28:12.599960: Yayy! New best EMA pseudo Dice: 0.9363\n",
      "2025-12-21 15:28:13.575854: \n",
      "2025-12-21 15:28:13.575854: Epoch 398\n",
      "2025-12-21 15:28:13.592741: Current learning rate: 0.00633\n",
      "2025-12-21 15:30:31.757443: train_loss -0.8425\n",
      "2025-12-21 15:30:31.759445: val_loss -0.853\n",
      "2025-12-21 15:30:31.763450: Pseudo dice [0.9152, 0.9472, 0.9377]\n",
      "2025-12-21 15:30:31.767307: Epoch time: 138.18 s\n",
      "2025-12-21 15:30:32.605681: \n",
      "2025-12-21 15:30:32.607422: Epoch 399\n",
      "2025-12-21 15:30:32.607422: Current learning rate: 0.00632\n",
      "2025-12-21 15:32:50.612806: train_loss -0.8457\n",
      "2025-12-21 15:32:50.614546: val_loss -0.8622\n",
      "2025-12-21 15:32:50.614546: Pseudo dice [0.9235, 0.9482, 0.9347]\n",
      "2025-12-21 15:32:50.614546: Epoch time: 138.01 s\n",
      "2025-12-21 15:32:51.528563: \n",
      "2025-12-21 15:32:51.528563: Epoch 400\n",
      "2025-12-21 15:32:51.528563: Current learning rate: 0.00631\n",
      "2025-12-21 15:35:09.466793: train_loss -0.8487\n",
      "2025-12-21 15:35:09.466793: val_loss -0.8762\n",
      "2025-12-21 15:35:09.470609: Pseudo dice [0.9322, 0.9585, 0.9342]\n",
      "2025-12-21 15:35:09.474613: Epoch time: 137.94 s\n",
      "2025-12-21 15:35:09.478617: Yayy! New best EMA pseudo Dice: 0.9366\n",
      "2025-12-21 15:35:10.568059: \n",
      "2025-12-21 15:35:10.568059: Epoch 401\n",
      "2025-12-21 15:35:10.583829: Current learning rate: 0.0063\n",
      "2025-12-21 15:37:28.391340: train_loss -0.8488\n",
      "2025-12-21 15:37:28.393144: val_loss -0.8578\n",
      "2025-12-21 15:37:28.397148: Pseudo dice [0.9189, 0.9542, 0.9369]\n",
      "2025-12-21 15:37:28.399150: Epoch time: 137.82 s\n",
      "2025-12-21 15:37:28.403154: Yayy! New best EMA pseudo Dice: 0.9366\n",
      "2025-12-21 15:37:29.345723: \n",
      "2025-12-21 15:37:29.347725: Epoch 402\n",
      "2025-12-21 15:37:29.349467: Current learning rate: 0.0063\n",
      "2025-12-21 15:39:47.125787: train_loss -0.8413\n",
      "2025-12-21 15:39:47.127791: val_loss -0.8612\n",
      "2025-12-21 15:39:47.134236: Pseudo dice [0.918, 0.9532, 0.9349]\n",
      "2025-12-21 15:39:47.136239: Epoch time: 137.78 s\n",
      "2025-12-21 15:39:47.801086: \n",
      "2025-12-21 15:39:47.801086: Epoch 403\n",
      "2025-12-21 15:39:47.801086: Current learning rate: 0.00629\n",
      "2025-12-21 15:42:05.785554: train_loss -0.8408\n",
      "2025-12-21 15:42:05.785554: val_loss -0.8771\n",
      "2025-12-21 15:42:05.798621: Pseudo dice [0.93, 0.9573, 0.9379]\n",
      "2025-12-21 15:42:05.801625: Epoch time: 137.98 s\n",
      "2025-12-21 15:42:05.805442: Yayy! New best EMA pseudo Dice: 0.937\n",
      "2025-12-21 15:42:07.036550: \n",
      "2025-12-21 15:42:07.036550: Epoch 404\n",
      "2025-12-21 15:42:07.052513: Current learning rate: 0.00628\n",
      "2025-12-21 15:44:24.955930: train_loss -0.844\n",
      "2025-12-21 15:44:24.955930: val_loss -0.855\n",
      "2025-12-21 15:44:24.971730: Pseudo dice [0.9203, 0.9437, 0.9333]\n",
      "2025-12-21 15:44:24.971730: Epoch time: 137.92 s\n",
      "2025-12-21 15:44:25.620685: \n",
      "2025-12-21 15:44:25.620685: Epoch 405\n",
      "2025-12-21 15:44:25.620685: Current learning rate: 0.00627\n",
      "2025-12-21 15:46:43.539892: train_loss -0.8412\n",
      "2025-12-21 15:46:43.539892: val_loss -0.8625\n",
      "2025-12-21 15:46:43.543897: Pseudo dice [0.9251, 0.9549, 0.9326]\n",
      "2025-12-21 15:46:43.546901: Epoch time: 137.92 s\n",
      "2025-12-21 15:46:44.200879: \n",
      "2025-12-21 15:46:44.200879: Epoch 406\n",
      "2025-12-21 15:46:44.200879: Current learning rate: 0.00626\n",
      "2025-12-21 15:49:02.127982: train_loss -0.8417\n",
      "2025-12-21 15:49:02.127982: val_loss -0.8672\n",
      "2025-12-21 15:49:02.127982: Pseudo dice [0.9289, 0.9555, 0.9312]\n",
      "2025-12-21 15:49:02.139487: Epoch time: 137.93 s\n",
      "2025-12-21 15:49:02.797175: \n",
      "2025-12-21 15:49:02.797175: Epoch 407\n",
      "2025-12-21 15:49:02.797175: Current learning rate: 0.00625\n",
      "2025-12-21 15:51:20.640376: train_loss -0.8481\n",
      "2025-12-21 15:51:20.642378: val_loss -0.8529\n",
      "2025-12-21 15:51:20.648473: Pseudo dice [0.9147, 0.9471, 0.9335]\n",
      "2025-12-21 15:51:20.652478: Epoch time: 137.84 s\n",
      "2025-12-21 15:51:21.321679: \n",
      "2025-12-21 15:51:21.321679: Epoch 408\n",
      "2025-12-21 15:51:21.321679: Current learning rate: 0.00624\n",
      "2025-12-21 15:53:39.260115: train_loss -0.8429\n",
      "2025-12-21 15:53:39.262118: val_loss -0.8588\n",
      "2025-12-21 15:53:39.263861: Pseudo dice [0.9177, 0.9512, 0.931]\n",
      "2025-12-21 15:53:39.263861: Epoch time: 137.94 s\n",
      "2025-12-21 15:53:39.928653: \n",
      "2025-12-21 15:53:39.928653: Epoch 409\n",
      "2025-12-21 15:53:39.928653: Current learning rate: 0.00623\n",
      "2025-12-21 15:55:57.803555: train_loss -0.8364\n",
      "2025-12-21 15:55:57.803555: val_loss -0.8592\n",
      "2025-12-21 15:55:57.814829: Pseudo dice [0.9228, 0.9471, 0.9331]\n",
      "2025-12-21 15:55:57.818834: Epoch time: 137.87 s\n",
      "2025-12-21 15:55:58.648856: \n",
      "2025-12-21 15:55:58.648856: Epoch 410\n",
      "2025-12-21 15:55:58.648856: Current learning rate: 0.00622\n",
      "2025-12-21 15:58:16.671811: train_loss -0.834\n",
      "2025-12-21 15:58:16.671811: val_loss -0.8622\n",
      "2025-12-21 15:58:16.673814: Pseudo dice [0.9238, 0.9526, 0.9363]\n",
      "2025-12-21 15:58:16.680775: Epoch time: 138.02 s\n",
      "2025-12-21 15:58:17.309235: \n",
      "2025-12-21 15:58:17.311237: Epoch 411\n",
      "2025-12-21 15:58:17.311237: Current learning rate: 0.00621\n",
      "2025-12-21 16:00:35.269405: train_loss -0.8429\n",
      "2025-12-21 16:00:35.269405: val_loss -0.8714\n",
      "2025-12-21 16:00:35.269405: Pseudo dice [0.9284, 0.9556, 0.9391]\n",
      "2025-12-21 16:00:35.285069: Epoch time: 137.96 s\n",
      "2025-12-21 16:00:35.904889: \n",
      "2025-12-21 16:00:35.904889: Epoch 412\n",
      "2025-12-21 16:00:35.904889: Current learning rate: 0.0062\n",
      "2025-12-21 16:02:53.737102: train_loss -0.8441\n",
      "2025-12-21 16:02:53.737102: val_loss -0.8711\n",
      "2025-12-21 16:02:53.737102: Pseudo dice [0.9286, 0.9582, 0.9404]\n",
      "2025-12-21 16:02:53.752901: Epoch time: 137.83 s\n",
      "2025-12-21 16:02:53.752901: Yayy! New best EMA pseudo Dice: 0.9371\n",
      "2025-12-21 16:02:54.610949: \n",
      "2025-12-21 16:02:54.610949: Epoch 413\n",
      "2025-12-21 16:02:54.619354: Current learning rate: 0.00619\n",
      "2025-12-21 16:05:12.590559: train_loss -0.8498\n",
      "2025-12-21 16:05:12.590559: val_loss -0.8701\n",
      "2025-12-21 16:05:12.596565: Pseudo dice [0.9294, 0.9568, 0.933]\n",
      "2025-12-21 16:05:12.600569: Epoch time: 137.98 s\n",
      "2025-12-21 16:05:12.604311: Yayy! New best EMA pseudo Dice: 0.9374\n",
      "2025-12-21 16:05:13.523066: \n",
      "2025-12-21 16:05:13.523066: Epoch 414\n",
      "2025-12-21 16:05:13.523066: Current learning rate: 0.00618\n",
      "2025-12-21 16:07:31.428684: train_loss -0.8433\n",
      "2025-12-21 16:07:31.428684: val_loss -0.8695\n",
      "2025-12-21 16:07:31.432688: Pseudo dice [0.9246, 0.9541, 0.9431]\n",
      "2025-12-21 16:07:31.434428: Epoch time: 137.91 s\n",
      "2025-12-21 16:07:31.439397: Yayy! New best EMA pseudo Dice: 0.9377\n",
      "2025-12-21 16:07:32.355482: \n",
      "2025-12-21 16:07:32.355482: Epoch 415\n",
      "2025-12-21 16:07:32.355482: Current learning rate: 0.00617\n",
      "2025-12-21 16:09:50.907614: train_loss -0.8421\n",
      "2025-12-21 16:09:50.909616: val_loss -0.8545\n",
      "2025-12-21 16:09:50.915626: Pseudo dice [0.9148, 0.9461, 0.9389]\n",
      "2025-12-21 16:09:50.919370: Epoch time: 138.55 s\n",
      "2025-12-21 16:09:51.751378: \n",
      "2025-12-21 16:09:51.751378: Epoch 416\n",
      "2025-12-21 16:09:51.751378: Current learning rate: 0.00616\n",
      "2025-12-21 16:12:09.757672: train_loss -0.8401\n",
      "2025-12-21 16:12:09.757672: val_loss -0.8669\n",
      "2025-12-21 16:12:09.763678: Pseudo dice [0.9233, 0.9574, 0.9323]\n",
      "2025-12-21 16:12:09.767683: Epoch time: 138.01 s\n",
      "2025-12-21 16:12:10.400550: \n",
      "2025-12-21 16:12:10.400550: Epoch 417\n",
      "2025-12-21 16:12:10.400550: Current learning rate: 0.00615\n",
      "2025-12-21 16:14:28.262204: train_loss -0.8447\n",
      "2025-12-21 16:14:28.262204: val_loss -0.8604\n",
      "2025-12-21 16:14:28.267658: Pseudo dice [0.9165, 0.9446, 0.9461]\n",
      "2025-12-21 16:14:28.271663: Epoch time: 137.86 s\n",
      "2025-12-21 16:14:28.894776: \n",
      "2025-12-21 16:14:28.894776: Epoch 418\n",
      "2025-12-21 16:14:28.910429: Current learning rate: 0.00614\n",
      "2025-12-21 16:16:46.799559: train_loss -0.8384\n",
      "2025-12-21 16:16:46.799559: val_loss -0.8624\n",
      "2025-12-21 16:16:46.805168: Pseudo dice [0.9235, 0.9501, 0.9393]\n",
      "2025-12-21 16:16:46.807169: Epoch time: 137.9 s\n",
      "2025-12-21 16:16:47.433215: \n",
      "2025-12-21 16:16:47.433215: Epoch 419\n",
      "2025-12-21 16:16:47.433215: Current learning rate: 0.00613\n",
      "2025-12-21 16:19:05.662814: train_loss -0.8353\n",
      "2025-12-21 16:19:05.662814: val_loss -0.8533\n",
      "2025-12-21 16:19:05.662814: Pseudo dice [0.9198, 0.9486, 0.9237]\n",
      "2025-12-21 16:19:05.662814: Epoch time: 138.23 s\n",
      "2025-12-21 16:19:06.302748: \n",
      "2025-12-21 16:19:06.306576: Epoch 420\n",
      "2025-12-21 16:19:06.309595: Current learning rate: 0.00612\n",
      "2025-12-21 16:21:24.889766: train_loss -0.8432\n",
      "2025-12-21 16:21:24.889766: val_loss -0.8647\n",
      "2025-12-21 16:21:24.893772: Pseudo dice [0.9283, 0.9556, 0.9198]\n",
      "2025-12-21 16:21:24.898216: Epoch time: 138.59 s\n",
      "2025-12-21 16:21:25.529736: \n",
      "2025-12-21 16:21:25.529736: Epoch 421\n",
      "2025-12-21 16:21:25.529736: Current learning rate: 0.00612\n",
      "2025-12-21 16:23:43.932185: train_loss -0.8409\n",
      "2025-12-21 16:23:43.932185: val_loss -0.8704\n",
      "2025-12-21 16:23:43.940632: Pseudo dice [0.9262, 0.9557, 0.9315]\n",
      "2025-12-21 16:23:43.940632: Epoch time: 138.4 s\n",
      "2025-12-21 16:23:44.733822: \n",
      "2025-12-21 16:23:44.733822: Epoch 422\n",
      "2025-12-21 16:23:44.733822: Current learning rate: 0.00611\n",
      "2025-12-21 16:26:03.187827: train_loss -0.8447\n",
      "2025-12-21 16:26:03.187827: val_loss -0.8723\n",
      "2025-12-21 16:26:03.187827: Pseudo dice [0.929, 0.9589, 0.9353]\n",
      "2025-12-21 16:26:03.187827: Epoch time: 138.45 s\n",
      "2025-12-21 16:26:03.836648: \n",
      "2025-12-21 16:26:03.836648: Epoch 423\n",
      "2025-12-21 16:26:03.836648: Current learning rate: 0.0061\n",
      "2025-12-21 16:28:22.249602: train_loss -0.8394\n",
      "2025-12-21 16:28:22.249602: val_loss -0.8696\n",
      "2025-12-21 16:28:22.256894: Pseudo dice [0.9273, 0.9548, 0.937]\n",
      "2025-12-21 16:28:22.256894: Epoch time: 138.43 s\n",
      "2025-12-21 16:28:22.885140: \n",
      "2025-12-21 16:28:22.885140: Epoch 424\n",
      "2025-12-21 16:28:22.885140: Current learning rate: 0.00609\n",
      "2025-12-21 16:30:41.052885: train_loss -0.8481\n",
      "2025-12-21 16:30:41.052885: val_loss -0.8779\n",
      "2025-12-21 16:30:41.058891: Pseudo dice [0.9326, 0.959, 0.9316]\n",
      "2025-12-21 16:30:41.062895: Epoch time: 138.17 s\n",
      "2025-12-21 16:30:41.685546: \n",
      "2025-12-21 16:30:41.685546: Epoch 425\n",
      "2025-12-21 16:30:41.701299: Current learning rate: 0.00608\n",
      "2025-12-21 16:32:59.643138: train_loss -0.8461\n",
      "2025-12-21 16:32:59.643138: val_loss -0.8709\n",
      "2025-12-21 16:32:59.643138: Pseudo dice [0.9284, 0.9573, 0.9321]\n",
      "2025-12-21 16:32:59.651153: Epoch time: 137.96 s\n",
      "2025-12-21 16:32:59.651153: Yayy! New best EMA pseudo Dice: 0.9378\n",
      "2025-12-21 16:33:00.514623: \n",
      "2025-12-21 16:33:00.514623: Epoch 426\n",
      "2025-12-21 16:33:00.514623: Current learning rate: 0.00607\n",
      "2025-12-21 16:35:18.562937: train_loss -0.8479\n",
      "2025-12-21 16:35:18.564940: val_loss -0.8698\n",
      "2025-12-21 16:35:18.568944: Pseudo dice [0.9281, 0.9574, 0.9311]\n",
      "2025-12-21 16:35:18.571687: Epoch time: 138.05 s\n",
      "2025-12-21 16:35:18.575691: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2025-12-21 16:35:19.490209: \n",
      "2025-12-21 16:35:19.490209: Epoch 427\n",
      "2025-12-21 16:35:19.490209: Current learning rate: 0.00606\n",
      "2025-12-21 16:37:37.287493: train_loss -0.8461\n",
      "2025-12-21 16:37:37.287493: val_loss -0.8742\n",
      "2025-12-21 16:37:37.303380: Pseudo dice [0.9272, 0.9549, 0.9434]\n",
      "2025-12-21 16:37:37.307941: Epoch time: 137.8 s\n",
      "2025-12-21 16:37:37.307941: Yayy! New best EMA pseudo Dice: 0.9383\n",
      "2025-12-21 16:37:38.368821: \n",
      "2025-12-21 16:37:38.368821: Epoch 428\n",
      "2025-12-21 16:37:38.376814: Current learning rate: 0.00605\n",
      "2025-12-21 16:39:56.273439: train_loss -0.8418\n",
      "2025-12-21 16:39:56.273439: val_loss -0.873\n",
      "2025-12-21 16:39:56.291200: Pseudo dice [0.9289, 0.9568, 0.9346]\n",
      "2025-12-21 16:39:56.291200: Epoch time: 137.9 s\n",
      "2025-12-21 16:39:56.297669: Yayy! New best EMA pseudo Dice: 0.9385\n",
      "2025-12-21 16:39:57.211799: \n",
      "2025-12-21 16:39:57.211799: Epoch 429\n",
      "2025-12-21 16:39:57.216365: Current learning rate: 0.00604\n",
      "2025-12-21 16:42:15.289882: train_loss -0.8377\n",
      "2025-12-21 16:42:15.289882: val_loss -0.8645\n",
      "2025-12-21 16:42:15.300363: Pseudo dice [0.9225, 0.9534, 0.9353]\n",
      "2025-12-21 16:42:15.300363: Epoch time: 138.08 s\n",
      "2025-12-21 16:42:15.955909: \n",
      "2025-12-21 16:42:15.955909: Epoch 430\n",
      "2025-12-21 16:42:15.958565: Current learning rate: 0.00603\n",
      "2025-12-21 16:44:34.113009: train_loss -0.8442\n",
      "2025-12-21 16:44:34.113009: val_loss -0.8654\n",
      "2025-12-21 16:44:34.118515: Pseudo dice [0.9255, 0.9543, 0.9329]\n",
      "2025-12-21 16:44:34.122354: Epoch time: 138.16 s\n",
      "2025-12-21 16:44:34.761312: \n",
      "2025-12-21 16:44:34.761312: Epoch 431\n",
      "2025-12-21 16:44:34.761312: Current learning rate: 0.00602\n",
      "2025-12-21 16:46:52.791536: train_loss -0.8406\n",
      "2025-12-21 16:46:52.793539: val_loss -0.8744\n",
      "2025-12-21 16:46:52.793539: Pseudo dice [0.9331, 0.9569, 0.939]\n",
      "2025-12-21 16:46:52.793539: Epoch time: 138.03 s\n",
      "2025-12-21 16:46:52.793539: Yayy! New best EMA pseudo Dice: 0.9387\n",
      "2025-12-21 16:46:53.678686: \n",
      "2025-12-21 16:46:53.678686: Epoch 432\n",
      "2025-12-21 16:46:53.678686: Current learning rate: 0.00601\n",
      "2025-12-21 16:49:11.745090: train_loss -0.8456\n",
      "2025-12-21 16:49:11.745090: val_loss -0.8804\n",
      "2025-12-21 16:49:11.745090: Pseudo dice [0.9333, 0.957, 0.9374]\n",
      "2025-12-21 16:49:11.758940: Epoch time: 138.07 s\n",
      "2025-12-21 16:49:11.760942: Yayy! New best EMA pseudo Dice: 0.9391\n",
      "2025-12-21 16:49:12.726409: \n",
      "2025-12-21 16:49:12.726409: Epoch 433\n",
      "2025-12-21 16:49:12.742410: Current learning rate: 0.006\n",
      "2025-12-21 16:51:30.708327: train_loss -0.8456\n",
      "2025-12-21 16:51:30.708327: val_loss -0.8744\n",
      "2025-12-21 16:51:30.708327: Pseudo dice [0.9287, 0.9617, 0.9336]\n",
      "2025-12-21 16:51:30.722055: Epoch time: 137.98 s\n",
      "2025-12-21 16:51:30.722055: Yayy! New best EMA pseudo Dice: 0.9393\n",
      "2025-12-21 16:51:31.819169: \n",
      "2025-12-21 16:51:31.819169: Epoch 434\n",
      "2025-12-21 16:51:31.835169: Current learning rate: 0.00599\n",
      "2025-12-21 16:53:50.033618: train_loss -0.8472\n",
      "2025-12-21 16:53:50.041617: val_loss -0.8632\n",
      "2025-12-21 16:53:50.047489: Pseudo dice [0.9209, 0.9499, 0.9411]\n",
      "2025-12-21 16:53:50.049491: Epoch time: 138.21 s\n",
      "2025-12-21 16:53:50.676337: \n",
      "2025-12-21 16:53:50.676337: Epoch 435\n",
      "2025-12-21 16:53:50.676337: Current learning rate: 0.00598\n",
      "2025-12-21 16:56:08.937778: train_loss -0.8412\n",
      "2025-12-21 16:56:08.939780: val_loss -0.8744\n",
      "2025-12-21 16:56:08.943254: Pseudo dice [0.9316, 0.9571, 0.9309]\n",
      "2025-12-21 16:56:08.943254: Epoch time: 138.26 s\n",
      "2025-12-21 16:56:09.613075: \n",
      "2025-12-21 16:56:09.613075: Epoch 436\n",
      "2025-12-21 16:56:09.613075: Current learning rate: 0.00597\n",
      "2025-12-21 16:58:27.678575: train_loss -0.8452\n",
      "2025-12-21 16:58:27.678575: val_loss -0.8842\n",
      "2025-12-21 16:58:27.685649: Pseudo dice [0.9345, 0.9618, 0.9425]\n",
      "2025-12-21 16:58:27.689890: Epoch time: 138.07 s\n",
      "2025-12-21 16:58:27.692408: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-21 16:58:28.582648: \n",
      "2025-12-21 16:58:28.582648: Epoch 437\n",
      "2025-12-21 16:58:28.586634: Current learning rate: 0.00596\n",
      "2025-12-21 17:00:46.560299: train_loss -0.8499\n",
      "2025-12-21 17:00:46.560299: val_loss -0.8604\n",
      "2025-12-21 17:00:46.560299: Pseudo dice [0.922, 0.951, 0.9323]\n",
      "2025-12-21 17:00:46.560299: Epoch time: 137.98 s\n",
      "2025-12-21 17:00:47.238801: \n",
      "2025-12-21 17:00:47.238801: Epoch 438\n",
      "2025-12-21 17:00:47.238801: Current learning rate: 0.00595\n",
      "2025-12-21 17:03:05.350413: train_loss -0.8442\n",
      "2025-12-21 17:03:05.350413: val_loss -0.8613\n",
      "2025-12-21 17:03:05.366394: Pseudo dice [0.9201, 0.9531, 0.9363]\n",
      "2025-12-21 17:03:05.366394: Epoch time: 138.11 s\n",
      "2025-12-21 17:03:06.034987: \n",
      "2025-12-21 17:03:06.034987: Epoch 439\n",
      "2025-12-21 17:03:06.034987: Current learning rate: 0.00594\n",
      "2025-12-21 17:05:24.135991: train_loss -0.8492\n",
      "2025-12-21 17:05:24.135991: val_loss -0.878\n",
      "2025-12-21 17:05:24.141437: Pseudo dice [0.9304, 0.9615, 0.9388]\n",
      "2025-12-21 17:05:24.141437: Epoch time: 138.1 s\n",
      "2025-12-21 17:05:24.990745: \n",
      "2025-12-21 17:05:24.990745: Epoch 440\n",
      "2025-12-21 17:05:24.990745: Current learning rate: 0.00593\n",
      "2025-12-21 17:07:43.047531: train_loss -0.8328\n",
      "2025-12-21 17:07:43.049534: val_loss -0.8292\n",
      "2025-12-21 17:07:43.051275: Pseudo dice [0.9094, 0.941, 0.9173]\n",
      "2025-12-21 17:07:43.051275: Epoch time: 138.06 s\n",
      "2025-12-21 17:07:43.698964: \n",
      "2025-12-21 17:07:43.698964: Epoch 441\n",
      "2025-12-21 17:07:43.698964: Current learning rate: 0.00592\n",
      "2025-12-21 17:10:01.945314: train_loss -0.8272\n",
      "2025-12-21 17:10:01.945314: val_loss -0.8444\n",
      "2025-12-21 17:10:01.951060: Pseudo dice [0.9158, 0.9435, 0.9152]\n",
      "2025-12-21 17:10:01.957067: Epoch time: 138.25 s\n",
      "2025-12-21 17:10:02.692215: \n",
      "2025-12-21 17:10:02.692215: Epoch 442\n",
      "2025-12-21 17:10:02.692215: Current learning rate: 0.00592\n",
      "2025-12-21 17:12:20.579865: train_loss -0.8381\n",
      "2025-12-21 17:12:20.579865: val_loss -0.8547\n",
      "2025-12-21 17:12:20.587878: Pseudo dice [0.9186, 0.9501, 0.9295]\n",
      "2025-12-21 17:12:20.591885: Epoch time: 137.89 s\n",
      "2025-12-21 17:12:21.213911: \n",
      "2025-12-21 17:12:21.213911: Epoch 443\n",
      "2025-12-21 17:12:21.228014: Current learning rate: 0.00591\n",
      "2025-12-21 17:14:39.295206: train_loss -0.8444\n",
      "2025-12-21 17:14:39.295206: val_loss -0.8316\n",
      "2025-12-21 17:14:39.299210: Pseudo dice [0.8978, 0.9339, 0.9334]\n",
      "2025-12-21 17:14:39.303214: Epoch time: 138.08 s\n",
      "2025-12-21 17:14:39.918160: \n",
      "2025-12-21 17:14:39.918160: Epoch 444\n",
      "2025-12-21 17:14:39.918160: Current learning rate: 0.0059\n",
      "2025-12-21 17:16:57.980472: train_loss -0.8416\n",
      "2025-12-21 17:16:57.982474: val_loss -0.8433\n",
      "2025-12-21 17:16:57.986217: Pseudo dice [0.9116, 0.9436, 0.9294]\n",
      "2025-12-21 17:16:57.986217: Epoch time: 138.06 s\n",
      "2025-12-21 17:16:58.712103: \n",
      "2025-12-21 17:16:58.712103: Epoch 445\n",
      "2025-12-21 17:16:58.712103: Current learning rate: 0.00589\n",
      "2025-12-21 17:19:16.520526: train_loss -0.8389\n",
      "2025-12-21 17:19:16.522528: val_loss -0.8458\n",
      "2025-12-21 17:19:16.526533: Pseudo dice [0.9112, 0.9416, 0.9296]\n",
      "2025-12-21 17:19:16.532280: Epoch time: 137.81 s\n",
      "2025-12-21 17:19:17.327348: \n",
      "2025-12-21 17:19:17.327348: Epoch 446\n",
      "2025-12-21 17:19:17.327348: Current learning rate: 0.00588\n",
      "2025-12-21 17:21:35.213169: train_loss -0.8415\n",
      "2025-12-21 17:21:35.215171: val_loss -0.8703\n",
      "2025-12-21 17:21:35.218676: Pseudo dice [0.9259, 0.9583, 0.9361]\n",
      "2025-12-21 17:21:35.218676: Epoch time: 137.89 s\n",
      "2025-12-21 17:21:35.833565: \n",
      "2025-12-21 17:21:35.833565: Epoch 447\n",
      "2025-12-21 17:21:35.849505: Current learning rate: 0.00587\n",
      "2025-12-21 17:23:53.792251: train_loss -0.8453\n",
      "2025-12-21 17:23:53.794253: val_loss -0.8707\n",
      "2025-12-21 17:23:53.795254: Pseudo dice [0.9264, 0.9517, 0.94]\n",
      "2025-12-21 17:23:53.795254: Epoch time: 137.96 s\n",
      "2025-12-21 17:23:54.537977: \n",
      "2025-12-21 17:23:54.537977: Epoch 448\n",
      "2025-12-21 17:23:54.537977: Current learning rate: 0.00586\n",
      "2025-12-21 17:26:12.478262: train_loss -0.8378\n",
      "2025-12-21 17:26:12.478262: val_loss -0.8525\n",
      "2025-12-21 17:26:12.478262: Pseudo dice [0.9191, 0.947, 0.9271]\n",
      "2025-12-21 17:26:12.478262: Epoch time: 137.94 s\n",
      "2025-12-21 17:26:13.096400: \n",
      "2025-12-21 17:26:13.096400: Epoch 449\n",
      "2025-12-21 17:26:13.096400: Current learning rate: 0.00585\n",
      "2025-12-21 17:28:30.962226: train_loss -0.8403\n",
      "2025-12-21 17:28:30.962226: val_loss -0.8671\n",
      "2025-12-21 17:28:30.977965: Pseudo dice [0.9262, 0.9553, 0.9314]\n",
      "2025-12-21 17:28:30.979968: Epoch time: 137.87 s\n",
      "2025-12-21 17:28:31.832421: \n",
      "2025-12-21 17:28:31.832421: Epoch 450\n",
      "2025-12-21 17:28:31.848136: Current learning rate: 0.00584\n",
      "2025-12-21 17:30:49.759735: train_loss -0.8417\n",
      "2025-12-21 17:30:49.759735: val_loss -0.8623\n",
      "2025-12-21 17:30:49.765742: Pseudo dice [0.9224, 0.9539, 0.9284]\n",
      "2025-12-21 17:30:49.771748: Epoch time: 137.93 s\n",
      "2025-12-21 17:30:50.422149: \n",
      "2025-12-21 17:30:50.422149: Epoch 451\n",
      "2025-12-21 17:30:50.422149: Current learning rate: 0.00583\n",
      "2025-12-21 17:33:08.583190: train_loss -0.8433\n",
      "2025-12-21 17:33:08.585193: val_loss -0.8563\n",
      "2025-12-21 17:33:08.585193: Pseudo dice [0.9141, 0.9481, 0.9341]\n",
      "2025-12-21 17:33:08.585193: Epoch time: 138.16 s\n",
      "2025-12-21 17:33:09.434560: \n",
      "2025-12-21 17:33:09.436563: Epoch 452\n",
      "2025-12-21 17:33:09.436563: Current learning rate: 0.00582\n",
      "2025-12-21 17:35:27.472801: train_loss -0.8404\n",
      "2025-12-21 17:35:27.474803: val_loss -0.8528\n",
      "2025-12-21 17:35:27.478545: Pseudo dice [0.9117, 0.9435, 0.9412]\n",
      "2025-12-21 17:35:27.480547: Epoch time: 138.04 s\n",
      "2025-12-21 17:35:28.128006: \n",
      "2025-12-21 17:35:28.128006: Epoch 453\n",
      "2025-12-21 17:35:28.143899: Current learning rate: 0.00581\n",
      "2025-12-21 17:37:45.925364: train_loss -0.8435\n",
      "2025-12-21 17:37:45.941359: val_loss -0.8672\n",
      "2025-12-21 17:37:45.941359: Pseudo dice [0.9226, 0.9527, 0.9397]\n",
      "2025-12-21 17:37:45.941359: Epoch time: 137.8 s\n",
      "2025-12-21 17:37:46.558682: \n",
      "2025-12-21 17:37:46.558682: Epoch 454\n",
      "2025-12-21 17:37:46.558682: Current learning rate: 0.0058\n",
      "2025-12-21 17:40:04.556309: train_loss -0.8471\n",
      "2025-12-21 17:40:04.556309: val_loss -0.8657\n",
      "2025-12-21 17:40:04.572120: Pseudo dice [0.9245, 0.9543, 0.937]\n",
      "2025-12-21 17:40:04.574122: Epoch time: 138.0 s\n",
      "2025-12-21 17:40:05.187382: \n",
      "2025-12-21 17:40:05.187382: Epoch 455\n",
      "2025-12-21 17:40:05.187382: Current learning rate: 0.00579\n",
      "2025-12-21 17:42:23.168500: train_loss -0.8476\n",
      "2025-12-21 17:42:23.168500: val_loss -0.8752\n",
      "2025-12-21 17:42:23.168500: Pseudo dice [0.928, 0.9562, 0.9406]\n",
      "2025-12-21 17:42:23.184234: Epoch time: 137.98 s\n",
      "2025-12-21 17:42:23.788372: \n",
      "2025-12-21 17:42:23.804317: Epoch 456\n",
      "2025-12-21 17:42:23.804317: Current learning rate: 0.00578\n",
      "2025-12-21 17:44:41.810499: train_loss -0.8406\n",
      "2025-12-21 17:44:41.821528: val_loss -0.8657\n",
      "2025-12-21 17:44:41.824477: Pseudo dice [0.9282, 0.9523, 0.9331]\n",
      "2025-12-21 17:44:41.824477: Epoch time: 138.02 s\n",
      "2025-12-21 17:44:42.444052: \n",
      "2025-12-21 17:44:42.444052: Epoch 457\n",
      "2025-12-21 17:44:42.444052: Current learning rate: 0.00577\n",
      "2025-12-21 17:47:00.380553: train_loss -0.8453\n",
      "2025-12-21 17:47:00.382556: val_loss -0.8758\n",
      "2025-12-21 17:47:00.384559: Pseudo dice [0.9272, 0.955, 0.9455]\n",
      "2025-12-21 17:47:00.384559: Epoch time: 137.94 s\n",
      "2025-12-21 17:47:01.012000: \n",
      "2025-12-21 17:47:01.012000: Epoch 458\n",
      "2025-12-21 17:47:01.012000: Current learning rate: 0.00576\n",
      "2025-12-21 17:49:19.010282: train_loss -0.8468\n",
      "2025-12-21 17:49:19.010282: val_loss -0.8683\n",
      "2025-12-21 17:49:19.026108: Pseudo dice [0.9233, 0.9536, 0.9394]\n",
      "2025-12-21 17:49:19.026108: Epoch time: 138.0 s\n",
      "2025-12-21 17:49:19.866271: \n",
      "2025-12-21 17:49:19.866271: Epoch 459\n",
      "2025-12-21 17:49:19.882055: Current learning rate: 0.00575\n",
      "2025-12-21 17:51:37.808821: train_loss -0.8479\n",
      "2025-12-21 17:51:37.810824: val_loss -0.8547\n",
      "2025-12-21 17:51:37.814828: Pseudo dice [0.9183, 0.9517, 0.9329]\n",
      "2025-12-21 17:51:37.820837: Epoch time: 137.94 s\n",
      "2025-12-21 17:51:38.444862: \n",
      "2025-12-21 17:51:38.444862: Epoch 460\n",
      "2025-12-21 17:51:38.444862: Current learning rate: 0.00574\n",
      "2025-12-21 17:53:56.344759: train_loss -0.8461\n",
      "2025-12-21 17:53:56.344759: val_loss -0.8653\n",
      "2025-12-21 17:53:56.344759: Pseudo dice [0.9242, 0.9502, 0.9368]\n",
      "2025-12-21 17:53:56.360559: Epoch time: 137.9 s\n",
      "2025-12-21 17:53:56.964010: \n",
      "2025-12-21 17:53:56.964010: Epoch 461\n",
      "2025-12-21 17:53:56.964010: Current learning rate: 0.00573\n",
      "2025-12-21 17:56:14.976835: train_loss -0.841\n",
      "2025-12-21 17:56:14.976835: val_loss -0.8643\n",
      "2025-12-21 17:56:14.982842: Pseudo dice [0.9289, 0.9535, 0.9204]\n",
      "2025-12-21 17:56:14.986847: Epoch time: 138.01 s\n",
      "2025-12-21 17:56:15.754397: \n",
      "2025-12-21 17:56:15.754397: Epoch 462\n",
      "2025-12-21 17:56:15.756399: Current learning rate: 0.00572\n",
      "2025-12-21 17:58:33.713530: train_loss -0.8399\n",
      "2025-12-21 17:58:33.713530: val_loss -0.8635\n",
      "2025-12-21 17:58:33.713530: Pseudo dice [0.9237, 0.9569, 0.9366]\n",
      "2025-12-21 17:58:33.713530: Epoch time: 137.96 s\n",
      "2025-12-21 17:58:34.348802: \n",
      "2025-12-21 17:58:34.348802: Epoch 463\n",
      "2025-12-21 17:58:34.358692: Current learning rate: 0.00571\n",
      "2025-12-21 18:00:52.187365: train_loss -0.8371\n",
      "2025-12-21 18:00:52.187365: val_loss -0.8491\n",
      "2025-12-21 18:00:52.194997: Pseudo dice [0.915, 0.9452, 0.9324]\n",
      "2025-12-21 18:00:52.194997: Epoch time: 137.84 s\n",
      "2025-12-21 18:00:52.823951: \n",
      "2025-12-21 18:00:52.823951: Epoch 464\n",
      "2025-12-21 18:00:52.823951: Current learning rate: 0.0057\n",
      "2025-12-21 18:03:10.608262: train_loss -0.8436\n",
      "2025-12-21 18:03:10.608262: val_loss -0.8665\n",
      "2025-12-21 18:03:10.610264: Pseudo dice [0.9228, 0.9499, 0.9402]\n",
      "2025-12-21 18:03:10.616846: Epoch time: 137.78 s\n",
      "2025-12-21 18:03:11.565795: \n",
      "2025-12-21 18:03:11.567700: Epoch 465\n",
      "2025-12-21 18:03:11.567700: Current learning rate: 0.0057\n",
      "2025-12-21 18:05:29.466781: train_loss -0.8452\n",
      "2025-12-21 18:05:29.466781: val_loss -0.8581\n",
      "2025-12-21 18:05:29.470524: Pseudo dice [0.9197, 0.9464, 0.9341]\n",
      "2025-12-21 18:05:29.475809: Epoch time: 137.9 s\n",
      "2025-12-21 18:05:30.090944: \n",
      "2025-12-21 18:05:30.090944: Epoch 466\n",
      "2025-12-21 18:05:30.104807: Current learning rate: 0.00569\n",
      "2025-12-21 18:07:48.043084: train_loss -0.8432\n",
      "2025-12-21 18:07:48.043084: val_loss -0.8717\n",
      "2025-12-21 18:07:48.054040: Pseudo dice [0.9256, 0.9495, 0.9431]\n",
      "2025-12-21 18:07:48.058948: Epoch time: 137.95 s\n",
      "2025-12-21 18:07:48.680265: \n",
      "2025-12-21 18:07:48.680265: Epoch 467\n",
      "2025-12-21 18:07:48.680265: Current learning rate: 0.00568\n",
      "2025-12-21 18:10:06.892935: train_loss -0.8404\n",
      "2025-12-21 18:10:06.892935: val_loss -0.8716\n",
      "2025-12-21 18:10:06.898181: Pseudo dice [0.9301, 0.9601, 0.929]\n",
      "2025-12-21 18:10:06.898181: Epoch time: 138.21 s\n",
      "2025-12-21 18:10:07.590682: \n",
      "2025-12-21 18:10:07.590682: Epoch 468\n",
      "2025-12-21 18:10:07.600730: Current learning rate: 0.00567\n",
      "2025-12-21 18:12:25.491661: train_loss -0.839\n",
      "2025-12-21 18:12:25.491661: val_loss -0.8471\n",
      "2025-12-21 18:12:25.509406: Pseudo dice [0.9161, 0.9457, 0.9322]\n",
      "2025-12-21 18:12:25.511408: Epoch time: 137.9 s\n",
      "2025-12-21 18:12:26.140476: \n",
      "2025-12-21 18:12:26.140476: Epoch 469\n",
      "2025-12-21 18:12:26.140476: Current learning rate: 0.00566\n",
      "2025-12-21 18:14:43.996490: train_loss -0.8461\n",
      "2025-12-21 18:14:44.003994: val_loss -0.8597\n",
      "2025-12-21 18:14:44.005996: Pseudo dice [0.9178, 0.9497, 0.9394]\n",
      "2025-12-21 18:14:44.005996: Epoch time: 137.86 s\n",
      "2025-12-21 18:14:44.644870: \n",
      "2025-12-21 18:14:44.644870: Epoch 470\n",
      "2025-12-21 18:14:44.654569: Current learning rate: 0.00565\n",
      "2025-12-21 18:17:02.446092: train_loss -0.8451\n",
      "2025-12-21 18:17:02.446092: val_loss -0.8706\n",
      "2025-12-21 18:17:02.446092: Pseudo dice [0.9262, 0.9562, 0.9353]\n",
      "2025-12-21 18:17:02.459973: Epoch time: 137.8 s\n",
      "2025-12-21 18:17:03.063231: \n",
      "2025-12-21 18:17:03.063231: Epoch 471\n",
      "2025-12-21 18:17:03.078947: Current learning rate: 0.00564\n",
      "2025-12-21 18:19:21.028422: train_loss -0.8372\n",
      "2025-12-21 18:19:21.028422: val_loss -0.8599\n",
      "2025-12-21 18:19:21.030424: Pseudo dice [0.9179, 0.9492, 0.9386]\n",
      "2025-12-21 18:19:21.030424: Epoch time: 137.97 s\n",
      "2025-12-21 18:19:21.838402: \n",
      "2025-12-21 18:19:21.838402: Epoch 472\n",
      "2025-12-21 18:19:21.838402: Current learning rate: 0.00563\n",
      "2025-12-21 18:21:39.714804: train_loss -0.846\n",
      "2025-12-21 18:21:39.714804: val_loss -0.8783\n",
      "2025-12-21 18:21:39.730665: Pseudo dice [0.9315, 0.9561, 0.9421]\n",
      "2025-12-21 18:21:39.730665: Epoch time: 137.88 s\n",
      "2025-12-21 18:21:40.349216: \n",
      "2025-12-21 18:21:40.349216: Epoch 473\n",
      "2025-12-21 18:21:40.349216: Current learning rate: 0.00562\n",
      "2025-12-21 18:23:58.550512: train_loss -0.8414\n",
      "2025-12-21 18:23:58.550512: val_loss -0.874\n",
      "2025-12-21 18:23:58.556395: Pseudo dice [0.9281, 0.9598, 0.9409]\n",
      "2025-12-21 18:23:58.561812: Epoch time: 138.2 s\n",
      "2025-12-21 18:23:59.185949: \n",
      "2025-12-21 18:23:59.185949: Epoch 474\n",
      "2025-12-21 18:23:59.185949: Current learning rate: 0.00561\n",
      "2025-12-21 18:26:17.149112: train_loss -0.8437\n",
      "2025-12-21 18:26:17.149112: val_loss -0.8793\n",
      "2025-12-21 18:26:17.149112: Pseudo dice [0.9338, 0.959, 0.9388]\n",
      "2025-12-21 18:26:17.160421: Epoch time: 137.98 s\n",
      "2025-12-21 18:26:17.783610: \n",
      "2025-12-21 18:26:17.783610: Epoch 475\n",
      "2025-12-21 18:26:17.783610: Current learning rate: 0.0056\n",
      "2025-12-21 18:28:35.535673: train_loss -0.8495\n",
      "2025-12-21 18:28:35.535673: val_loss -0.8594\n",
      "2025-12-21 18:28:35.535673: Pseudo dice [0.9186, 0.9504, 0.9353]\n",
      "2025-12-21 18:28:35.535673: Epoch time: 137.75 s\n",
      "2025-12-21 18:28:36.263492: \n",
      "2025-12-21 18:28:36.263492: Epoch 476\n",
      "2025-12-21 18:28:36.269510: Current learning rate: 0.00559\n",
      "2025-12-21 18:30:54.209459: train_loss -0.842\n",
      "2025-12-21 18:30:54.225122: val_loss -0.8586\n",
      "2025-12-21 18:30:54.225122: Pseudo dice [0.9182, 0.9482, 0.9399]\n",
      "2025-12-21 18:30:54.225122: Epoch time: 137.95 s\n",
      "2025-12-21 18:30:54.875868: \n",
      "2025-12-21 18:30:54.875868: Epoch 477\n",
      "2025-12-21 18:30:54.875868: Current learning rate: 0.00558\n",
      "2025-12-21 18:33:12.876846: train_loss -0.8435\n",
      "2025-12-21 18:33:12.876846: val_loss -0.8631\n",
      "2025-12-21 18:33:12.886588: Pseudo dice [0.9246, 0.9478, 0.9276]\n",
      "2025-12-21 18:33:12.888847: Epoch time: 138.0 s\n",
      "2025-12-21 18:33:13.672573: \n",
      "2025-12-21 18:33:13.672573: Epoch 478\n",
      "2025-12-21 18:33:13.688595: Current learning rate: 0.00557\n",
      "2025-12-21 18:35:31.652564: train_loss -0.8452\n",
      "2025-12-21 18:35:31.652564: val_loss -0.8663\n",
      "2025-12-21 18:35:31.654305: Pseudo dice [0.9222, 0.9512, 0.9396]\n",
      "2025-12-21 18:35:31.654305: Epoch time: 137.98 s\n",
      "2025-12-21 18:35:32.420847: \n",
      "2025-12-21 18:35:32.420847: Epoch 479\n",
      "2025-12-21 18:35:32.420847: Current learning rate: 0.00556\n",
      "2025-12-21 18:37:50.327494: train_loss -0.8485\n",
      "2025-12-21 18:37:50.327494: val_loss -0.8632\n",
      "2025-12-21 18:37:50.327494: Pseudo dice [0.9208, 0.9513, 0.9311]\n",
      "2025-12-21 18:37:50.327494: Epoch time: 137.92 s\n",
      "2025-12-21 18:37:50.960160: \n",
      "2025-12-21 18:37:50.960160: Epoch 480\n",
      "2025-12-21 18:37:50.960160: Current learning rate: 0.00555\n",
      "2025-12-21 18:40:08.899325: train_loss -0.8449\n",
      "2025-12-21 18:40:08.899325: val_loss -0.8621\n",
      "2025-12-21 18:40:08.910656: Pseudo dice [0.9194, 0.9477, 0.9425]\n",
      "2025-12-21 18:40:08.912745: Epoch time: 137.94 s\n",
      "2025-12-21 18:40:09.532169: \n",
      "2025-12-21 18:40:09.532169: Epoch 481\n",
      "2025-12-21 18:40:09.548145: Current learning rate: 0.00554\n",
      "2025-12-21 18:42:27.587736: train_loss -0.8486\n",
      "2025-12-21 18:42:27.587736: val_loss -0.8612\n",
      "2025-12-21 18:42:27.587736: Pseudo dice [0.9219, 0.9475, 0.9411]\n",
      "2025-12-21 18:42:27.587736: Epoch time: 138.06 s\n",
      "2025-12-21 18:42:28.326010: \n",
      "2025-12-21 18:42:28.326010: Epoch 482\n",
      "2025-12-21 18:42:28.341783: Current learning rate: 0.00553\n",
      "2025-12-21 18:44:46.310994: train_loss -0.8498\n",
      "2025-12-21 18:44:46.310994: val_loss -0.862\n",
      "2025-12-21 18:44:46.310994: Pseudo dice [0.9218, 0.9518, 0.9377]\n",
      "2025-12-21 18:44:46.326885: Epoch time: 137.98 s\n",
      "2025-12-21 18:44:46.945850: \n",
      "2025-12-21 18:44:46.945850: Epoch 483\n",
      "2025-12-21 18:44:46.961544: Current learning rate: 0.00552\n",
      "2025-12-21 18:47:04.999736: train_loss -0.8495\n",
      "2025-12-21 18:47:04.999736: val_loss -0.8847\n",
      "2025-12-21 18:47:05.004126: Pseudo dice [0.9319, 0.9637, 0.9446]\n",
      "2025-12-21 18:47:05.008130: Epoch time: 138.05 s\n",
      "2025-12-21 18:47:05.629202: \n",
      "2025-12-21 18:47:05.629202: Epoch 484\n",
      "2025-12-21 18:47:05.629202: Current learning rate: 0.00551\n",
      "2025-12-21 18:49:23.605129: train_loss -0.8468\n",
      "2025-12-21 18:49:23.605129: val_loss -0.863\n",
      "2025-12-21 18:49:23.609133: Pseudo dice [0.9229, 0.9507, 0.9413]\n",
      "2025-12-21 18:49:23.611136: Epoch time: 137.98 s\n",
      "2025-12-21 18:49:24.422494: \n",
      "2025-12-21 18:49:24.422494: Epoch 485\n",
      "2025-12-21 18:49:24.435110: Current learning rate: 0.0055\n",
      "2025-12-21 18:51:42.195941: train_loss -0.8431\n",
      "2025-12-21 18:51:42.195941: val_loss -0.8724\n",
      "2025-12-21 18:51:42.201685: Pseudo dice [0.9266, 0.955, 0.9402]\n",
      "2025-12-21 18:51:42.205691: Epoch time: 137.77 s\n",
      "2025-12-21 18:51:42.834655: \n",
      "2025-12-21 18:51:42.834655: Epoch 486\n",
      "2025-12-21 18:51:42.834655: Current learning rate: 0.00549\n",
      "2025-12-21 18:54:00.810832: train_loss -0.8498\n",
      "2025-12-21 18:54:00.810832: val_loss -0.8535\n",
      "2025-12-21 18:54:00.823138: Pseudo dice [0.9172, 0.9488, 0.9319]\n",
      "2025-12-21 18:54:00.827024: Epoch time: 137.98 s\n",
      "2025-12-21 18:54:01.440313: \n",
      "2025-12-21 18:54:01.440313: Epoch 487\n",
      "2025-12-21 18:54:01.459121: Current learning rate: 0.00548\n",
      "2025-12-21 18:56:19.259943: train_loss -0.8431\n",
      "2025-12-21 18:56:19.259943: val_loss -0.8677\n",
      "2025-12-21 18:56:19.265744: Pseudo dice [0.924, 0.9543, 0.9367]\n",
      "2025-12-21 18:56:19.269720: Epoch time: 137.82 s\n",
      "2025-12-21 18:56:19.901832: \n",
      "2025-12-21 18:56:19.901832: Epoch 488\n",
      "2025-12-21 18:56:19.910823: Current learning rate: 0.00547\n",
      "2025-12-21 18:58:38.055633: train_loss -0.8452\n",
      "2025-12-21 18:58:38.057636: val_loss -0.8777\n",
      "2025-12-21 18:58:38.061378: Pseudo dice [0.932, 0.9557, 0.9379]\n",
      "2025-12-21 18:58:38.064177: Epoch time: 138.15 s\n",
      "2025-12-21 18:58:38.696863: \n",
      "2025-12-21 18:58:38.696863: Epoch 489\n",
      "2025-12-21 18:58:38.703851: Current learning rate: 0.00546\n",
      "2025-12-21 19:00:56.706471: train_loss -0.8454\n",
      "2025-12-21 19:00:56.706471: val_loss -0.8601\n",
      "2025-12-21 19:00:56.708473: Pseudo dice [0.9202, 0.9482, 0.9313]\n",
      "2025-12-21 19:00:56.708473: Epoch time: 138.01 s\n",
      "2025-12-21 19:00:57.352653: \n",
      "2025-12-21 19:00:57.352653: Epoch 490\n",
      "2025-12-21 19:00:57.360843: Current learning rate: 0.00546\n",
      "2025-12-21 19:03:15.116842: train_loss -0.8459\n",
      "2025-12-21 19:03:15.116842: val_loss -0.8642\n",
      "2025-12-21 19:03:15.134555: Pseudo dice [0.9199, 0.9476, 0.9438]\n",
      "2025-12-21 19:03:15.134555: Epoch time: 137.76 s\n",
      "2025-12-21 19:03:15.828331: \n",
      "2025-12-21 19:03:15.828331: Epoch 491\n",
      "2025-12-21 19:03:15.828331: Current learning rate: 0.00545\n",
      "2025-12-21 19:05:33.786060: train_loss -0.8482\n",
      "2025-12-21 19:05:33.786060: val_loss -0.8734\n",
      "2025-12-21 19:05:33.786060: Pseudo dice [0.9271, 0.9535, 0.9473]\n",
      "2025-12-21 19:05:33.801781: Epoch time: 137.96 s\n",
      "2025-12-21 19:05:34.595025: \n",
      "2025-12-21 19:05:34.595025: Epoch 492\n",
      "2025-12-21 19:05:34.611090: Current learning rate: 0.00544\n",
      "2025-12-21 19:07:52.538801: train_loss -0.8499\n",
      "2025-12-21 19:07:52.538801: val_loss -0.8775\n",
      "2025-12-21 19:07:52.538801: Pseudo dice [0.9312, 0.9586, 0.938]\n",
      "2025-12-21 19:07:52.556839: Epoch time: 137.94 s\n",
      "2025-12-21 19:07:53.239245: \n",
      "2025-12-21 19:07:53.239245: Epoch 493\n",
      "2025-12-21 19:07:53.244276: Current learning rate: 0.00543\n",
      "2025-12-21 19:10:11.697570: train_loss -0.8511\n",
      "2025-12-21 19:10:11.697570: val_loss -0.865\n",
      "2025-12-21 19:10:11.704795: Pseudo dice [0.9206, 0.9514, 0.9357]\n",
      "2025-12-21 19:10:11.704795: Epoch time: 138.46 s\n",
      "2025-12-21 19:10:12.347598: \n",
      "2025-12-21 19:10:12.347598: Epoch 494\n",
      "2025-12-21 19:10:12.350790: Current learning rate: 0.00542\n",
      "2025-12-21 19:12:30.370530: train_loss -0.8436\n",
      "2025-12-21 19:12:30.370530: val_loss -0.8633\n",
      "2025-12-21 19:12:30.389870: Pseudo dice [0.922, 0.9517, 0.9328]\n",
      "2025-12-21 19:12:30.389870: Epoch time: 138.04 s\n",
      "2025-12-21 19:12:31.035158: \n",
      "2025-12-21 19:12:31.035158: Epoch 495\n",
      "2025-12-21 19:12:31.035158: Current learning rate: 0.00541\n",
      "2025-12-21 19:14:49.187761: train_loss -0.8444\n",
      "2025-12-21 19:14:49.187761: val_loss -0.87\n",
      "2025-12-21 19:14:49.193508: Pseudo dice [0.9304, 0.9538, 0.9251]\n",
      "2025-12-21 19:14:49.201535: Epoch time: 138.15 s\n",
      "2025-12-21 19:14:49.917659: \n",
      "2025-12-21 19:14:49.917659: Epoch 496\n",
      "2025-12-21 19:14:49.924165: Current learning rate: 0.0054\n",
      "2025-12-21 19:17:07.976200: train_loss -0.8471\n",
      "2025-12-21 19:17:07.976200: val_loss -0.8651\n",
      "2025-12-21 19:17:07.980204: Pseudo dice [0.9257, 0.9518, 0.9367]\n",
      "2025-12-21 19:17:07.985246: Epoch time: 138.06 s\n",
      "2025-12-21 19:17:08.604195: \n",
      "2025-12-21 19:17:08.604195: Epoch 497\n",
      "2025-12-21 19:17:08.604195: Current learning rate: 0.00539\n",
      "2025-12-21 19:19:26.602060: train_loss -0.849\n",
      "2025-12-21 19:19:26.602060: val_loss -0.8654\n",
      "2025-12-21 19:19:26.609817: Pseudo dice [0.924, 0.9515, 0.9291]\n",
      "2025-12-21 19:19:26.613822: Epoch time: 138.0 s\n",
      "2025-12-21 19:19:27.404233: \n",
      "2025-12-21 19:19:27.404233: Epoch 498\n",
      "2025-12-21 19:19:27.420227: Current learning rate: 0.00538\n",
      "2025-12-21 19:21:45.307192: train_loss -0.8455\n",
      "2025-12-21 19:21:45.307192: val_loss -0.8694\n",
      "2025-12-21 19:21:45.317070: Pseudo dice [0.9238, 0.9541, 0.9413]\n",
      "2025-12-21 19:21:45.317070: Epoch time: 137.9 s\n",
      "2025-12-21 19:21:46.110558: \n",
      "2025-12-21 19:21:46.110558: Epoch 499\n",
      "2025-12-21 19:21:46.112980: Current learning rate: 0.00537\n",
      "2025-12-21 19:24:04.002120: train_loss -0.8484\n",
      "2025-12-21 19:24:04.002120: val_loss -0.8735\n",
      "2025-12-21 19:24:04.015824: Pseudo dice [0.9254, 0.9566, 0.9456]\n",
      "2025-12-21 19:24:04.020849: Epoch time: 137.89 s\n",
      "2025-12-21 19:24:04.902832: \n",
      "2025-12-21 19:24:04.902832: Epoch 500\n",
      "2025-12-21 19:24:04.909556: Current learning rate: 0.00536\n",
      "2025-12-21 19:26:22.917700: train_loss -0.8466\n",
      "2025-12-21 19:26:22.917700: val_loss -0.8735\n",
      "2025-12-21 19:26:22.923408: Pseudo dice [0.9301, 0.9553, 0.9382]\n",
      "2025-12-21 19:26:22.923408: Epoch time: 138.01 s\n",
      "2025-12-21 19:26:23.536784: \n",
      "2025-12-21 19:26:23.552716: Epoch 501\n",
      "2025-12-21 19:26:23.554719: Current learning rate: 0.00535\n",
      "2025-12-21 19:28:41.295825: train_loss -0.8479\n",
      "2025-12-21 19:28:41.295825: val_loss -0.8726\n",
      "2025-12-21 19:28:41.311088: Pseudo dice [0.9265, 0.9506, 0.9451]\n",
      "2025-12-21 19:28:41.313092: Epoch time: 137.76 s\n",
      "2025-12-21 19:28:42.043206: \n",
      "2025-12-21 19:28:42.045209: Epoch 502\n",
      "2025-12-21 19:28:42.049216: Current learning rate: 0.00534\n",
      "2025-12-21 19:31:00.218142: train_loss -0.846\n",
      "2025-12-21 19:31:00.220143: val_loss -0.8835\n",
      "2025-12-21 19:31:00.224147: Pseudo dice [0.934, 0.9603, 0.9432]\n",
      "2025-12-21 19:31:00.227890: Epoch time: 138.17 s\n",
      "2025-12-21 19:31:00.861997: \n",
      "2025-12-21 19:31:00.861997: Epoch 503\n",
      "2025-12-21 19:31:00.861997: Current learning rate: 0.00533\n",
      "2025-12-21 19:33:18.853796: train_loss -0.8438\n",
      "2025-12-21 19:33:18.853796: val_loss -0.8724\n",
      "2025-12-21 19:33:18.858843: Pseudo dice [0.9278, 0.9556, 0.9348]\n",
      "2025-12-21 19:33:18.858843: Epoch time: 137.99 s\n",
      "2025-12-21 19:33:19.631265: \n",
      "2025-12-21 19:33:19.631265: Epoch 504\n",
      "2025-12-21 19:33:19.647347: Current learning rate: 0.00532\n",
      "2025-12-21 19:35:37.519110: train_loss -0.8498\n",
      "2025-12-21 19:35:37.519110: val_loss -0.8694\n",
      "2025-12-21 19:35:37.525119: Pseudo dice [0.9229, 0.9504, 0.9413]\n",
      "2025-12-21 19:35:37.527122: Epoch time: 137.89 s\n",
      "2025-12-21 19:35:38.257518: \n",
      "2025-12-21 19:35:38.257518: Epoch 505\n",
      "2025-12-21 19:35:38.257518: Current learning rate: 0.00531\n",
      "2025-12-21 19:37:56.119464: train_loss -0.8421\n",
      "2025-12-21 19:37:56.119464: val_loss -0.8672\n",
      "2025-12-21 19:37:56.135250: Pseudo dice [0.9234, 0.9524, 0.9378]\n",
      "2025-12-21 19:37:56.135250: Epoch time: 137.86 s\n",
      "2025-12-21 19:37:56.769526: \n",
      "2025-12-21 19:37:56.769526: Epoch 506\n",
      "2025-12-21 19:37:56.769526: Current learning rate: 0.0053\n",
      "2025-12-21 19:40:14.735069: train_loss -0.8502\n",
      "2025-12-21 19:40:14.737071: val_loss -0.8766\n",
      "2025-12-21 19:40:14.740075: Pseudo dice [0.9311, 0.9584, 0.9402]\n",
      "2025-12-21 19:40:14.740075: Epoch time: 137.97 s\n",
      "2025-12-21 19:40:15.373381: \n",
      "2025-12-21 19:40:15.373381: Epoch 507\n",
      "2025-12-21 19:40:15.373381: Current learning rate: 0.00529\n",
      "2025-12-21 19:42:33.603330: train_loss -0.8434\n",
      "2025-12-21 19:42:33.605333: val_loss -0.8588\n",
      "2025-12-21 19:42:33.611340: Pseudo dice [0.917, 0.9478, 0.9359]\n",
      "2025-12-21 19:42:33.615344: Epoch time: 138.25 s\n",
      "2025-12-21 19:42:34.299100: \n",
      "2025-12-21 19:42:34.300842: Epoch 508\n",
      "2025-12-21 19:42:34.302845: Current learning rate: 0.00528\n",
      "2025-12-21 19:44:52.295666: train_loss -0.8482\n",
      "2025-12-21 19:44:52.297668: val_loss -0.8768\n",
      "2025-12-21 19:44:52.301677: Pseudo dice [0.9328, 0.9586, 0.9371]\n",
      "2025-12-21 19:44:52.303679: Epoch time: 138.0 s\n",
      "2025-12-21 19:44:52.981806: \n",
      "2025-12-21 19:44:52.981806: Epoch 509\n",
      "2025-12-21 19:44:52.996292: Current learning rate: 0.00527\n",
      "2025-12-21 19:47:10.812122: train_loss -0.8517\n",
      "2025-12-21 19:47:10.812122: val_loss -0.8694\n",
      "2025-12-21 19:47:10.812122: Pseudo dice [0.9256, 0.9503, 0.94]\n",
      "2025-12-21 19:47:10.812122: Epoch time: 137.83 s\n",
      "2025-12-21 19:47:11.619761: \n",
      "2025-12-21 19:47:11.619761: Epoch 510\n",
      "2025-12-21 19:47:11.635600: Current learning rate: 0.00526\n",
      "2025-12-21 19:49:29.494465: train_loss -0.8525\n",
      "2025-12-21 19:49:29.494465: val_loss -0.8735\n",
      "2025-12-21 19:49:29.510290: Pseudo dice [0.9275, 0.9559, 0.94]\n",
      "2025-12-21 19:49:29.514294: Epoch time: 137.87 s\n",
      "2025-12-21 19:49:30.190650: \n",
      "2025-12-21 19:49:30.190650: Epoch 511\n",
      "2025-12-21 19:49:30.204733: Current learning rate: 0.00525\n",
      "2025-12-21 19:51:48.345147: train_loss -0.846\n",
      "2025-12-21 19:51:48.345147: val_loss -0.8727\n",
      "2025-12-21 19:51:48.345147: Pseudo dice [0.9265, 0.9552, 0.9432]\n",
      "2025-12-21 19:51:48.345147: Epoch time: 138.15 s\n",
      "2025-12-21 19:51:49.009405: \n",
      "2025-12-21 19:51:49.009405: Epoch 512\n",
      "2025-12-21 19:51:49.009405: Current learning rate: 0.00524\n",
      "2025-12-21 19:54:06.880843: train_loss -0.8502\n",
      "2025-12-21 19:54:06.880843: val_loss -0.8637\n",
      "2025-12-21 19:54:06.886196: Pseudo dice [0.9245, 0.9504, 0.9316]\n",
      "2025-12-21 19:54:06.890882: Epoch time: 137.87 s\n",
      "2025-12-21 19:54:07.522665: \n",
      "2025-12-21 19:54:07.522665: Epoch 513\n",
      "2025-12-21 19:54:07.538751: Current learning rate: 0.00523\n",
      "2025-12-21 19:56:25.397543: train_loss -0.8504\n",
      "2025-12-21 19:56:25.397543: val_loss -0.8718\n",
      "2025-12-21 19:56:25.399546: Pseudo dice [0.9275, 0.9559, 0.9395]\n",
      "2025-12-21 19:56:25.405559: Epoch time: 137.87 s\n",
      "2025-12-21 19:56:26.036894: \n",
      "2025-12-21 19:56:26.036894: Epoch 514\n",
      "2025-12-21 19:56:26.036894: Current learning rate: 0.00522\n",
      "2025-12-21 19:58:43.943009: train_loss -0.8467\n",
      "2025-12-21 19:58:43.943009: val_loss -0.8564\n",
      "2025-12-21 19:58:43.943009: Pseudo dice [0.9173, 0.9468, 0.9447]\n",
      "2025-12-21 19:58:43.954525: Epoch time: 137.91 s\n",
      "2025-12-21 19:58:44.596233: \n",
      "2025-12-21 19:58:44.596233: Epoch 515\n",
      "2025-12-21 19:58:44.596233: Current learning rate: 0.00521\n",
      "2025-12-21 20:01:02.710811: train_loss -0.8478\n",
      "2025-12-21 20:01:02.712814: val_loss -0.8768\n",
      "2025-12-21 20:01:02.716820: Pseudo dice [0.9286, 0.9555, 0.9392]\n",
      "2025-12-21 20:01:02.720564: Epoch time: 138.11 s\n",
      "2025-12-21 20:01:03.547452: \n",
      "2025-12-21 20:01:03.547452: Epoch 516\n",
      "2025-12-21 20:01:03.561425: Current learning rate: 0.0052\n",
      "2025-12-21 20:03:21.340612: train_loss -0.8441\n",
      "2025-12-21 20:03:21.340612: val_loss -0.865\n",
      "2025-12-21 20:03:21.347842: Pseudo dice [0.919, 0.9513, 0.9386]\n",
      "2025-12-21 20:03:21.351069: Epoch time: 137.79 s\n",
      "2025-12-21 20:03:21.979260: \n",
      "2025-12-21 20:03:21.979260: Epoch 517\n",
      "2025-12-21 20:03:21.979260: Current learning rate: 0.00519\n",
      "2025-12-21 20:05:39.965264: train_loss -0.8484\n",
      "2025-12-21 20:05:39.965264: val_loss -0.8743\n",
      "2025-12-21 20:05:39.983076: Pseudo dice [0.9279, 0.9574, 0.9434]\n",
      "2025-12-21 20:05:39.983076: Epoch time: 137.99 s\n",
      "2025-12-21 20:05:40.616415: \n",
      "2025-12-21 20:05:40.616415: Epoch 518\n",
      "2025-12-21 20:05:40.616415: Current learning rate: 0.00518\n",
      "2025-12-21 20:07:58.566864: train_loss -0.8485\n",
      "2025-12-21 20:07:58.568866: val_loss -0.8678\n",
      "2025-12-21 20:07:58.569869: Pseudo dice [0.9231, 0.9538, 0.9379]\n",
      "2025-12-21 20:07:58.569869: Epoch time: 137.95 s\n",
      "2025-12-21 20:07:59.360729: \n",
      "2025-12-21 20:07:59.360729: Epoch 519\n",
      "2025-12-21 20:07:59.360729: Current learning rate: 0.00518\n",
      "2025-12-21 20:10:17.668943: train_loss -0.8553\n",
      "2025-12-21 20:10:17.668943: val_loss -0.8561\n",
      "2025-12-21 20:10:17.687065: Pseudo dice [0.9164, 0.946, 0.9357]\n",
      "2025-12-21 20:10:17.691069: Epoch time: 138.32 s\n",
      "2025-12-21 20:10:18.335953: \n",
      "2025-12-21 20:10:18.335953: Epoch 520\n",
      "2025-12-21 20:10:18.335953: Current learning rate: 0.00517\n",
      "2025-12-21 20:12:36.415351: train_loss -0.8524\n",
      "2025-12-21 20:12:36.417354: val_loss -0.8563\n",
      "2025-12-21 20:12:36.421097: Pseudo dice [0.9161, 0.9459, 0.9401]\n",
      "2025-12-21 20:12:36.421097: Epoch time: 138.08 s\n",
      "2025-12-21 20:12:37.055733: \n",
      "2025-12-21 20:12:37.055733: Epoch 521\n",
      "2025-12-21 20:12:37.066449: Current learning rate: 0.00516\n",
      "2025-12-21 20:14:55.070854: train_loss -0.8495\n",
      "2025-12-21 20:14:55.070854: val_loss -0.8703\n",
      "2025-12-21 20:14:55.070854: Pseudo dice [0.9258, 0.9512, 0.9416]\n",
      "2025-12-21 20:14:55.080219: Epoch time: 138.02 s\n",
      "2025-12-21 20:14:55.870238: \n",
      "2025-12-21 20:14:55.872241: Epoch 522\n",
      "2025-12-21 20:14:55.874243: Current learning rate: 0.00515\n",
      "2025-12-21 20:17:13.928667: train_loss -0.8453\n",
      "2025-12-21 20:17:13.930670: val_loss -0.8602\n",
      "2025-12-21 20:17:13.936678: Pseudo dice [0.9201, 0.9515, 0.9345]\n",
      "2025-12-21 20:17:13.940611: Epoch time: 138.06 s\n",
      "2025-12-21 20:17:14.753087: \n",
      "2025-12-21 20:17:14.753087: Epoch 523\n",
      "2025-12-21 20:17:14.753087: Current learning rate: 0.00514\n",
      "2025-12-21 20:19:32.521169: train_loss -0.8497\n",
      "2025-12-21 20:19:32.521169: val_loss -0.8627\n",
      "2025-12-21 20:19:32.525173: Pseudo dice [0.9189, 0.9528, 0.9353]\n",
      "2025-12-21 20:19:32.525173: Epoch time: 137.77 s\n",
      "2025-12-21 20:19:33.154100: \n",
      "2025-12-21 20:19:33.154100: Epoch 524\n",
      "2025-12-21 20:19:33.154100: Current learning rate: 0.00513\n",
      "2025-12-21 20:21:51.099535: train_loss -0.8443\n",
      "2025-12-21 20:21:51.099535: val_loss -0.8687\n",
      "2025-12-21 20:21:51.103539: Pseudo dice [0.9296, 0.9519, 0.9356]\n",
      "2025-12-21 20:21:51.107281: Epoch time: 137.95 s\n",
      "2025-12-21 20:21:51.831023: \n",
      "2025-12-21 20:21:51.831023: Epoch 525\n",
      "2025-12-21 20:21:51.833536: Current learning rate: 0.00512\n",
      "2025-12-21 20:24:09.958301: train_loss -0.8489\n",
      "2025-12-21 20:24:09.958301: val_loss -0.8707\n",
      "2025-12-21 20:24:09.958301: Pseudo dice [0.9253, 0.9578, 0.9379]\n",
      "2025-12-21 20:24:09.958301: Epoch time: 138.13 s\n",
      "2025-12-21 20:24:10.621032: \n",
      "2025-12-21 20:24:10.623034: Epoch 526\n",
      "2025-12-21 20:24:10.623034: Current learning rate: 0.00511\n",
      "2025-12-21 20:26:28.577146: train_loss -0.8463\n",
      "2025-12-21 20:26:28.578887: val_loss -0.8637\n",
      "2025-12-21 20:26:28.578887: Pseudo dice [0.9237, 0.9513, 0.9353]\n",
      "2025-12-21 20:26:28.588529: Epoch time: 137.96 s\n",
      "2025-12-21 20:26:29.228552: \n",
      "2025-12-21 20:26:29.228552: Epoch 527\n",
      "2025-12-21 20:26:29.228552: Current learning rate: 0.0051\n",
      "2025-12-21 20:28:47.328184: train_loss -0.8443\n",
      "2025-12-21 20:28:47.329925: val_loss -0.8595\n",
      "2025-12-21 20:28:47.335931: Pseudo dice [0.9175, 0.9485, 0.94]\n",
      "2025-12-21 20:28:47.339935: Epoch time: 138.1 s\n",
      "2025-12-21 20:28:48.071698: \n",
      "2025-12-21 20:28:48.071698: Epoch 528\n",
      "2025-12-21 20:28:48.073442: Current learning rate: 0.00509\n",
      "2025-12-21 20:31:06.185351: train_loss -0.8427\n",
      "2025-12-21 20:31:06.185351: val_loss -0.8609\n",
      "2025-12-21 20:31:06.197235: Pseudo dice [0.926, 0.9486, 0.9338]\n",
      "2025-12-21 20:31:06.203242: Epoch time: 138.12 s\n",
      "2025-12-21 20:31:06.996201: \n",
      "2025-12-21 20:31:06.996201: Epoch 529\n",
      "2025-12-21 20:31:07.012291: Current learning rate: 0.00508\n",
      "2025-12-21 20:33:25.014418: train_loss -0.8425\n",
      "2025-12-21 20:33:25.014418: val_loss -0.8707\n",
      "2025-12-21 20:33:25.019429: Pseudo dice [0.9266, 0.9538, 0.9375]\n",
      "2025-12-21 20:33:25.024133: Epoch time: 138.02 s\n",
      "2025-12-21 20:33:25.654975: \n",
      "2025-12-21 20:33:25.654975: Epoch 530\n",
      "2025-12-21 20:33:25.672845: Current learning rate: 0.00507\n",
      "2025-12-21 20:35:43.571288: train_loss -0.8514\n",
      "2025-12-21 20:35:43.571288: val_loss -0.8587\n",
      "2025-12-21 20:35:43.571288: Pseudo dice [0.9186, 0.9521, 0.9284]\n",
      "2025-12-21 20:35:43.581763: Epoch time: 137.92 s\n",
      "2025-12-21 20:35:44.204473: \n",
      "2025-12-21 20:35:44.204473: Epoch 531\n",
      "2025-12-21 20:35:44.215289: Current learning rate: 0.00506\n",
      "2025-12-21 20:38:02.089998: train_loss -0.8471\n",
      "2025-12-21 20:38:02.089998: val_loss -0.8652\n",
      "2025-12-21 20:38:02.105917: Pseudo dice [0.9223, 0.9519, 0.9426]\n",
      "2025-12-21 20:38:02.105917: Epoch time: 137.89 s\n",
      "2025-12-21 20:38:02.739623: \n",
      "2025-12-21 20:38:02.739623: Epoch 532\n",
      "2025-12-21 20:38:02.750457: Current learning rate: 0.00505\n",
      "2025-12-21 20:40:20.589997: train_loss -0.8483\n",
      "2025-12-21 20:40:20.592000: val_loss -0.875\n",
      "2025-12-21 20:40:20.596005: Pseudo dice [0.9317, 0.9589, 0.9352]\n",
      "2025-12-21 20:40:20.600835: Epoch time: 137.85 s\n",
      "2025-12-21 20:40:21.215432: \n",
      "2025-12-21 20:40:21.215432: Epoch 533\n",
      "2025-12-21 20:40:21.231215: Current learning rate: 0.00504\n",
      "2025-12-21 20:42:39.002256: train_loss -0.845\n",
      "2025-12-21 20:42:39.002256: val_loss -0.8755\n",
      "2025-12-21 20:42:39.008084: Pseudo dice [0.929, 0.9578, 0.9419]\n",
      "2025-12-21 20:42:39.008084: Epoch time: 137.79 s\n",
      "2025-12-21 20:42:39.643233: \n",
      "2025-12-21 20:42:39.643233: Epoch 534\n",
      "2025-12-21 20:42:39.643233: Current learning rate: 0.00503\n",
      "2025-12-21 20:44:57.641513: train_loss -0.8427\n",
      "2025-12-21 20:44:57.641513: val_loss -0.8709\n",
      "2025-12-21 20:44:57.657324: Pseudo dice [0.9242, 0.9517, 0.9427]\n",
      "2025-12-21 20:44:57.657324: Epoch time: 138.0 s\n",
      "2025-12-21 20:44:58.448621: \n",
      "2025-12-21 20:44:58.448621: Epoch 535\n",
      "2025-12-21 20:44:58.464263: Current learning rate: 0.00502\n",
      "2025-12-21 20:47:16.296263: train_loss -0.85\n",
      "2025-12-21 20:47:16.296263: val_loss -0.8666\n",
      "2025-12-21 20:47:16.302270: Pseudo dice [0.9237, 0.9526, 0.9403]\n",
      "2025-12-21 20:47:16.306274: Epoch time: 137.85 s\n",
      "2025-12-21 20:47:17.055880: \n",
      "2025-12-21 20:47:17.055880: Epoch 536\n",
      "2025-12-21 20:47:17.055880: Current learning rate: 0.00501\n",
      "2025-12-21 20:49:34.822687: train_loss -0.8505\n",
      "2025-12-21 20:49:34.822687: val_loss -0.8738\n",
      "2025-12-21 20:49:34.838762: Pseudo dice [0.9289, 0.9591, 0.9356]\n",
      "2025-12-21 20:49:34.838762: Epoch time: 137.77 s\n",
      "2025-12-21 20:49:35.459290: \n",
      "2025-12-21 20:49:35.459290: Epoch 537\n",
      "2025-12-21 20:49:35.475387: Current learning rate: 0.005\n",
      "2025-12-21 20:51:53.609541: train_loss -0.8465\n",
      "2025-12-21 20:51:53.609541: val_loss -0.8735\n",
      "2025-12-21 20:51:53.613990: Pseudo dice [0.9258, 0.9563, 0.9349]\n",
      "2025-12-21 20:51:53.613990: Epoch time: 138.15 s\n",
      "2025-12-21 20:51:54.228967: \n",
      "2025-12-21 20:51:54.228967: Epoch 538\n",
      "2025-12-21 20:51:54.244939: Current learning rate: 0.00499\n",
      "2025-12-21 20:54:12.193887: train_loss -0.8419\n",
      "2025-12-21 20:54:12.193887: val_loss -0.8621\n",
      "2025-12-21 20:54:12.209624: Pseudo dice [0.9208, 0.9518, 0.9367]\n",
      "2025-12-21 20:54:12.213630: Epoch time: 137.96 s\n",
      "2025-12-21 20:54:12.984100: \n",
      "2025-12-21 20:54:12.984100: Epoch 539\n",
      "2025-12-21 20:54:12.984100: Current learning rate: 0.00498\n",
      "2025-12-21 20:56:30.760816: train_loss -0.8532\n",
      "2025-12-21 20:56:30.760816: val_loss -0.8582\n",
      "2025-12-21 20:56:30.770108: Pseudo dice [0.9159, 0.9487, 0.9354]\n",
      "2025-12-21 20:56:30.770108: Epoch time: 137.79 s\n",
      "2025-12-21 20:56:31.413767: \n",
      "2025-12-21 20:56:31.413767: Epoch 540\n",
      "2025-12-21 20:56:31.413767: Current learning rate: 0.00497\n",
      "2025-12-21 20:58:49.345990: train_loss -0.8497\n",
      "2025-12-21 20:58:49.361954: val_loss -0.8704\n",
      "2025-12-21 20:58:49.365960: Pseudo dice [0.9249, 0.9521, 0.938]\n",
      "2025-12-21 20:58:49.369964: Epoch time: 137.93 s\n",
      "2025-12-21 20:58:50.168633: \n",
      "2025-12-21 20:58:50.168633: Epoch 541\n",
      "2025-12-21 20:58:50.168633: Current learning rate: 0.00496\n",
      "2025-12-21 21:01:08.193594: train_loss -0.8518\n",
      "2025-12-21 21:01:08.193594: val_loss -0.8569\n",
      "2025-12-21 21:01:08.199010: Pseudo dice [0.9174, 0.9462, 0.9314]\n",
      "2025-12-21 21:01:08.203016: Epoch time: 138.02 s\n",
      "2025-12-21 21:01:08.942215: \n",
      "2025-12-21 21:01:08.942215: Epoch 542\n",
      "2025-12-21 21:01:08.944681: Current learning rate: 0.00495\n",
      "2025-12-21 21:03:26.951297: train_loss -0.8518\n",
      "2025-12-21 21:03:26.951297: val_loss -0.8749\n",
      "2025-12-21 21:03:26.957869: Pseudo dice [0.9255, 0.9525, 0.9481]\n",
      "2025-12-21 21:03:26.960975: Epoch time: 138.01 s\n",
      "2025-12-21 21:03:27.584915: \n",
      "2025-12-21 21:03:27.584915: Epoch 543\n",
      "2025-12-21 21:03:27.602921: Current learning rate: 0.00494\n",
      "2025-12-21 21:05:45.414918: train_loss -0.8482\n",
      "2025-12-21 21:05:45.414918: val_loss -0.8639\n",
      "2025-12-21 21:05:45.430914: Pseudo dice [0.9251, 0.9507, 0.9323]\n",
      "2025-12-21 21:05:45.434472: Epoch time: 137.83 s\n",
      "2025-12-21 21:05:46.049823: \n",
      "2025-12-21 21:05:46.049823: Epoch 544\n",
      "2025-12-21 21:05:46.062364: Current learning rate: 0.00493\n",
      "2025-12-21 21:08:03.959628: train_loss -0.8537\n",
      "2025-12-21 21:08:03.959628: val_loss -0.8534\n",
      "2025-12-21 21:08:03.965636: Pseudo dice [0.9144, 0.9454, 0.9392]\n",
      "2025-12-21 21:08:03.971643: Epoch time: 137.91 s\n",
      "2025-12-21 21:08:04.606823: \n",
      "2025-12-21 21:08:04.606823: Epoch 545\n",
      "2025-12-21 21:08:04.622922: Current learning rate: 0.00492\n",
      "2025-12-21 21:10:22.879071: train_loss -0.8488\n",
      "2025-12-21 21:10:22.879071: val_loss -0.8752\n",
      "2025-12-21 21:10:22.879071: Pseudo dice [0.9274, 0.9576, 0.9384]\n",
      "2025-12-21 21:10:22.895110: Epoch time: 138.27 s\n",
      "2025-12-21 21:10:23.531291: \n",
      "2025-12-21 21:10:23.531291: Epoch 546\n",
      "2025-12-21 21:10:23.542674: Current learning rate: 0.00491\n",
      "2025-12-21 21:12:41.634819: train_loss -0.8493\n",
      "2025-12-21 21:12:41.634819: val_loss -0.8674\n",
      "2025-12-21 21:12:41.634819: Pseudo dice [0.9227, 0.9521, 0.9447]\n",
      "2025-12-21 21:12:41.650808: Epoch time: 138.1 s\n",
      "2025-12-21 21:12:42.446126: \n",
      "2025-12-21 21:12:42.446126: Epoch 547\n",
      "2025-12-21 21:12:42.449361: Current learning rate: 0.0049\n",
      "2025-12-21 21:15:00.524512: train_loss -0.8487\n",
      "2025-12-21 21:15:00.526252: val_loss -0.8811\n",
      "2025-12-21 21:15:00.526252: Pseudo dice [0.9305, 0.9646, 0.9401]\n",
      "2025-12-21 21:15:00.535000: Epoch time: 138.08 s\n",
      "2025-12-21 21:15:01.207430: \n",
      "2025-12-21 21:15:01.207430: Epoch 548\n",
      "2025-12-21 21:15:01.220481: Current learning rate: 0.00489\n",
      "2025-12-21 21:17:19.264665: train_loss -0.8473\n",
      "2025-12-21 21:17:19.264665: val_loss -0.8768\n",
      "2025-12-21 21:17:19.280351: Pseudo dice [0.9297, 0.9565, 0.9392]\n",
      "2025-12-21 21:17:19.283939: Epoch time: 138.06 s\n",
      "2025-12-21 21:17:19.915437: \n",
      "2025-12-21 21:17:19.915437: Epoch 549\n",
      "2025-12-21 21:17:19.920557: Current learning rate: 0.00488\n",
      "2025-12-21 21:19:37.814982: train_loss -0.8467\n",
      "2025-12-21 21:19:37.814982: val_loss -0.8714\n",
      "2025-12-21 21:19:37.820730: Pseudo dice [0.9299, 0.9532, 0.939]\n",
      "2025-12-21 21:19:37.824735: Epoch time: 137.9 s\n",
      "2025-12-21 21:19:38.710516: \n",
      "2025-12-21 21:19:38.710516: Epoch 550\n",
      "2025-12-21 21:19:38.714953: Current learning rate: 0.00487\n",
      "2025-12-21 21:21:56.588192: train_loss -0.8497\n",
      "2025-12-21 21:21:56.590197: val_loss -0.8493\n",
      "2025-12-21 21:21:56.600223: Pseudo dice [0.9093, 0.94, 0.9414]\n",
      "2025-12-21 21:21:56.603967: Epoch time: 137.88 s\n",
      "2025-12-21 21:21:57.236357: \n",
      "2025-12-21 21:21:57.236357: Epoch 551\n",
      "2025-12-21 21:21:57.236357: Current learning rate: 0.00486\n",
      "2025-12-21 21:24:16.715372: train_loss -0.851\n",
      "2025-12-21 21:24:16.715372: val_loss -0.8882\n",
      "2025-12-21 21:24:16.715372: Pseudo dice [0.9383, 0.9629, 0.939]\n",
      "2025-12-21 21:24:16.729282: Epoch time: 139.48 s\n",
      "2025-12-21 21:24:17.387920: \n",
      "2025-12-21 21:24:17.387920: Epoch 552\n",
      "2025-12-21 21:24:17.387920: Current learning rate: 0.00485\n",
      "2025-12-21 21:26:35.790158: train_loss -0.8392\n",
      "2025-12-21 21:26:35.790158: val_loss -0.8656\n",
      "2025-12-21 21:26:35.796163: Pseudo dice [0.9249, 0.9559, 0.9351]\n",
      "2025-12-21 21:26:35.799906: Epoch time: 138.4 s\n",
      "2025-12-21 21:26:36.602530: \n",
      "2025-12-21 21:26:36.602530: Epoch 553\n",
      "2025-12-21 21:26:36.602530: Current learning rate: 0.00484\n",
      "2025-12-21 21:28:55.122247: train_loss -0.8449\n",
      "2025-12-21 21:28:55.122247: val_loss -0.8728\n",
      "2025-12-21 21:28:55.122247: Pseudo dice [0.9316, 0.9567, 0.929]\n",
      "2025-12-21 21:28:55.122247: Epoch time: 138.52 s\n",
      "2025-12-21 21:28:55.847997: \n",
      "2025-12-21 21:28:55.847997: Epoch 554\n",
      "2025-12-21 21:28:55.847997: Current learning rate: 0.00484\n",
      "2025-12-21 21:31:14.344447: train_loss -0.8535\n",
      "2025-12-21 21:31:14.344447: val_loss -0.8577\n",
      "2025-12-21 21:31:14.344447: Pseudo dice [0.9149, 0.9456, 0.938]\n",
      "2025-12-21 21:31:14.360414: Epoch time: 138.5 s\n",
      "2025-12-21 21:31:14.988052: \n",
      "2025-12-21 21:31:14.988052: Epoch 555\n",
      "2025-12-21 21:31:14.988052: Current learning rate: 0.00483\n",
      "2025-12-21 21:33:33.276003: train_loss -0.8523\n",
      "2025-12-21 21:33:33.276003: val_loss -0.884\n",
      "2025-12-21 21:33:33.280008: Pseudo dice [0.935, 0.9621, 0.9401]\n",
      "2025-12-21 21:33:33.284012: Epoch time: 138.29 s\n",
      "2025-12-21 21:33:33.910098: \n",
      "2025-12-21 21:33:33.910098: Epoch 556\n",
      "2025-12-21 21:33:33.924117: Current learning rate: 0.00482\n",
      "2025-12-21 21:35:51.822572: train_loss -0.853\n",
      "2025-12-21 21:35:51.822572: val_loss -0.8486\n",
      "2025-12-21 21:35:51.822572: Pseudo dice [0.9091, 0.9411, 0.9313]\n",
      "2025-12-21 21:35:51.822572: Epoch time: 137.91 s\n",
      "2025-12-21 21:35:52.577723: \n",
      "2025-12-21 21:35:52.577723: Epoch 557\n",
      "2025-12-21 21:35:52.577723: Current learning rate: 0.00481\n",
      "2025-12-21 21:38:10.426920: train_loss -0.8534\n",
      "2025-12-21 21:38:10.426920: val_loss -0.8685\n",
      "2025-12-21 21:38:10.428923: Pseudo dice [0.9213, 0.954, 0.9415]\n",
      "2025-12-21 21:38:10.428923: Epoch time: 137.85 s\n",
      "2025-12-21 21:38:11.074062: \n",
      "2025-12-21 21:38:11.074062: Epoch 558\n",
      "2025-12-21 21:38:11.074062: Current learning rate: 0.0048\n",
      "2025-12-21 21:40:28.964802: train_loss -0.852\n",
      "2025-12-21 21:40:28.964802: val_loss -0.8554\n",
      "2025-12-21 21:40:28.968806: Pseudo dice [0.9184, 0.9453, 0.9312]\n",
      "2025-12-21 21:40:28.968806: Epoch time: 137.89 s\n",
      "2025-12-21 21:40:29.817305: \n",
      "2025-12-21 21:40:29.817305: Epoch 559\n",
      "2025-12-21 21:40:29.817305: Current learning rate: 0.00479\n",
      "2025-12-21 21:42:47.705788: train_loss -0.8512\n",
      "2025-12-21 21:42:47.705788: val_loss -0.8697\n",
      "2025-12-21 21:42:47.709529: Pseudo dice [0.923, 0.9567, 0.9395]\n",
      "2025-12-21 21:42:47.709529: Epoch time: 137.89 s\n",
      "2025-12-21 21:42:48.406761: \n",
      "2025-12-21 21:42:48.406761: Epoch 560\n",
      "2025-12-21 21:42:48.406761: Current learning rate: 0.00478\n",
      "2025-12-21 21:45:06.395784: train_loss -0.8535\n",
      "2025-12-21 21:45:06.395784: val_loss -0.8846\n",
      "2025-12-21 21:45:06.411886: Pseudo dice [0.9345, 0.96, 0.9459]\n",
      "2025-12-21 21:45:06.415716: Epoch time: 137.99 s\n",
      "2025-12-21 21:45:07.059797: \n",
      "2025-12-21 21:45:07.059797: Epoch 561\n",
      "2025-12-21 21:45:07.059797: Current learning rate: 0.00477\n",
      "2025-12-21 21:47:25.041074: train_loss -0.8524\n",
      "2025-12-21 21:47:25.043076: val_loss -0.8659\n",
      "2025-12-21 21:47:25.045082: Pseudo dice [0.919, 0.9507, 0.9407]\n",
      "2025-12-21 21:47:25.045082: Epoch time: 137.98 s\n",
      "2025-12-21 21:47:25.692043: \n",
      "2025-12-21 21:47:25.692043: Epoch 562\n",
      "2025-12-21 21:47:25.692043: Current learning rate: 0.00476\n",
      "2025-12-21 21:49:43.715281: train_loss -0.8448\n",
      "2025-12-21 21:49:43.715281: val_loss -0.8785\n",
      "2025-12-21 21:49:43.723028: Pseudo dice [0.931, 0.9583, 0.9374]\n",
      "2025-12-21 21:49:43.727032: Epoch time: 138.02 s\n",
      "2025-12-21 21:49:44.352322: \n",
      "2025-12-21 21:49:44.352322: Epoch 563\n",
      "2025-12-21 21:49:44.352322: Current learning rate: 0.00475\n",
      "2025-12-21 21:52:02.206766: train_loss -0.8524\n",
      "2025-12-21 21:52:02.208769: val_loss -0.8662\n",
      "2025-12-21 21:52:02.214276: Pseudo dice [0.9226, 0.9526, 0.9408]\n",
      "2025-12-21 21:52:02.214276: Epoch time: 137.85 s\n",
      "2025-12-21 21:52:02.845132: \n",
      "2025-12-21 21:52:02.845132: Epoch 564\n",
      "2025-12-21 21:52:02.845132: Current learning rate: 0.00474\n",
      "2025-12-21 21:54:20.961734: train_loss -0.8527\n",
      "2025-12-21 21:54:20.961734: val_loss -0.8627\n",
      "2025-12-21 21:54:20.968713: Pseudo dice [0.9169, 0.9517, 0.9415]\n",
      "2025-12-21 21:54:20.972717: Epoch time: 138.12 s\n",
      "2025-12-21 21:54:21.610301: \n",
      "2025-12-21 21:54:21.610301: Epoch 565\n",
      "2025-12-21 21:54:21.610301: Current learning rate: 0.00473\n",
      "2025-12-21 21:56:39.432834: train_loss -0.8579\n",
      "2025-12-21 21:56:39.432834: val_loss -0.8402\n",
      "2025-12-21 21:56:39.438842: Pseudo dice [0.9095, 0.9414, 0.925]\n",
      "2025-12-21 21:56:39.440844: Epoch time: 137.82 s\n",
      "2025-12-21 21:56:40.299476: \n",
      "2025-12-21 21:56:40.299476: Epoch 566\n",
      "2025-12-21 21:56:40.299476: Current learning rate: 0.00472\n",
      "2025-12-21 21:58:58.287178: train_loss -0.8516\n",
      "2025-12-21 21:58:58.289179: val_loss -0.8739\n",
      "2025-12-21 21:58:58.292921: Pseudo dice [0.928, 0.9544, 0.9416]\n",
      "2025-12-21 21:58:58.296925: Epoch time: 137.99 s\n",
      "2025-12-21 21:58:58.937608: \n",
      "2025-12-21 21:58:58.937608: Epoch 567\n",
      "2025-12-21 21:58:58.937608: Current learning rate: 0.00471\n",
      "2025-12-21 22:01:17.122481: train_loss -0.8471\n",
      "2025-12-21 22:01:17.122481: val_loss -0.8796\n",
      "2025-12-21 22:01:17.122481: Pseudo dice [0.9303, 0.9579, 0.9432]\n",
      "2025-12-21 22:01:17.122481: Epoch time: 138.18 s\n",
      "2025-12-21 22:01:17.757240: \n",
      "2025-12-21 22:01:17.757240: Epoch 568\n",
      "2025-12-21 22:01:17.773128: Current learning rate: 0.0047\n",
      "2025-12-21 22:03:35.720850: train_loss -0.8537\n",
      "2025-12-21 22:03:35.722852: val_loss -0.8857\n",
      "2025-12-21 22:03:35.728596: Pseudo dice [0.9372, 0.963, 0.9438]\n",
      "2025-12-21 22:03:35.732601: Epoch time: 137.96 s\n",
      "2025-12-21 22:03:36.357991: \n",
      "2025-12-21 22:03:36.357991: Epoch 569\n",
      "2025-12-21 22:03:36.357991: Current learning rate: 0.00469\n",
      "2025-12-21 22:05:54.253614: train_loss -0.8497\n",
      "2025-12-21 22:05:54.253614: val_loss -0.8635\n",
      "2025-12-21 22:05:54.253614: Pseudo dice [0.9251, 0.9514, 0.9292]\n",
      "2025-12-21 22:05:54.271556: Epoch time: 137.9 s\n",
      "2025-12-21 22:05:54.902629: \n",
      "2025-12-21 22:05:54.902629: Epoch 570\n",
      "2025-12-21 22:05:54.902629: Current learning rate: 0.00468\n",
      "2025-12-21 22:08:12.874455: train_loss -0.8522\n",
      "2025-12-21 22:08:12.874455: val_loss -0.8807\n",
      "2025-12-21 22:08:12.878458: Pseudo dice [0.9315, 0.9603, 0.9396]\n",
      "2025-12-21 22:08:12.882200: Epoch time: 137.97 s\n",
      "2025-12-21 22:08:13.694672: \n",
      "2025-12-21 22:08:13.694672: Epoch 571\n",
      "2025-12-21 22:08:13.694672: Current learning rate: 0.00467\n",
      "2025-12-21 22:10:32.217488: train_loss -0.8525\n",
      "2025-12-21 22:10:32.217488: val_loss -0.8624\n",
      "2025-12-21 22:10:32.217488: Pseudo dice [0.9157, 0.9524, 0.9358]\n",
      "2025-12-21 22:10:32.233314: Epoch time: 138.52 s\n",
      "2025-12-21 22:10:33.037305: \n",
      "2025-12-21 22:10:33.037305: Epoch 572\n",
      "2025-12-21 22:10:33.037305: Current learning rate: 0.00466\n",
      "2025-12-21 22:12:50.927425: train_loss -0.8535\n",
      "2025-12-21 22:12:50.927425: val_loss -0.8856\n",
      "2025-12-21 22:12:50.927425: Pseudo dice [0.9348, 0.9622, 0.9411]\n",
      "2025-12-21 22:12:50.927425: Epoch time: 137.89 s\n",
      "2025-12-21 22:12:51.624269: \n",
      "2025-12-21 22:12:51.624269: Epoch 573\n",
      "2025-12-21 22:12:51.624269: Current learning rate: 0.00465\n",
      "2025-12-21 22:15:09.608680: train_loss -0.856\n",
      "2025-12-21 22:15:09.608680: val_loss -0.8463\n",
      "2025-12-21 22:15:09.614688: Pseudo dice [0.9111, 0.9429, 0.9245]\n",
      "2025-12-21 22:15:09.618692: Epoch time: 137.98 s\n",
      "2025-12-21 22:15:10.401536: \n",
      "2025-12-21 22:15:10.401536: Epoch 574\n",
      "2025-12-21 22:15:10.401536: Current learning rate: 0.00464\n",
      "2025-12-21 22:17:28.394682: train_loss -0.8504\n",
      "2025-12-21 22:17:28.394682: val_loss -0.8741\n",
      "2025-12-21 22:17:28.410440: Pseudo dice [0.926, 0.9536, 0.9432]\n",
      "2025-12-21 22:17:28.410440: Epoch time: 137.99 s\n",
      "2025-12-21 22:17:29.076231: \n",
      "2025-12-21 22:17:29.076231: Epoch 575\n",
      "2025-12-21 22:17:29.076231: Current learning rate: 0.00463\n",
      "2025-12-21 22:19:46.963966: train_loss -0.8547\n",
      "2025-12-21 22:19:46.963966: val_loss -0.8686\n",
      "2025-12-21 22:19:46.979674: Pseudo dice [0.9239, 0.9545, 0.9351]\n",
      "2025-12-21 22:19:46.981985: Epoch time: 137.89 s\n",
      "2025-12-21 22:19:47.610470: \n",
      "2025-12-21 22:19:47.610470: Epoch 576\n",
      "2025-12-21 22:19:47.626293: Current learning rate: 0.00462\n",
      "2025-12-21 22:22:05.586119: train_loss -0.8469\n",
      "2025-12-21 22:22:05.588121: val_loss -0.87\n",
      "2025-12-21 22:22:05.592125: Pseudo dice [0.9268, 0.9532, 0.933]\n",
      "2025-12-21 22:22:05.593865: Epoch time: 137.98 s\n",
      "2025-12-21 22:22:06.344269: \n",
      "2025-12-21 22:22:06.344269: Epoch 577\n",
      "2025-12-21 22:22:06.358281: Current learning rate: 0.00461\n",
      "2025-12-21 22:24:24.458627: train_loss -0.8544\n",
      "2025-12-21 22:24:24.460630: val_loss -0.8654\n",
      "2025-12-21 22:24:24.468380: Pseudo dice [0.9174, 0.95, 0.942]\n",
      "2025-12-21 22:24:24.472384: Epoch time: 138.11 s\n",
      "2025-12-21 22:24:25.270778: \n",
      "2025-12-21 22:24:25.270778: Epoch 578\n",
      "2025-12-21 22:24:25.286697: Current learning rate: 0.0046\n",
      "2025-12-21 22:26:43.143774: train_loss -0.8527\n",
      "2025-12-21 22:26:43.143774: val_loss -0.8632\n",
      "2025-12-21 22:26:43.149534: Pseudo dice [0.9203, 0.9507, 0.9324]\n",
      "2025-12-21 22:26:43.155541: Epoch time: 137.87 s\n",
      "2025-12-21 22:26:43.781801: \n",
      "2025-12-21 22:26:43.781801: Epoch 579\n",
      "2025-12-21 22:26:43.797702: Current learning rate: 0.00459\n",
      "2025-12-21 22:29:01.693473: train_loss -0.8461\n",
      "2025-12-21 22:29:01.693473: val_loss -0.8714\n",
      "2025-12-21 22:29:01.699479: Pseudo dice [0.9267, 0.953, 0.9407]\n",
      "2025-12-21 22:29:01.703483: Epoch time: 137.91 s\n",
      "2025-12-21 22:29:02.429766: \n",
      "2025-12-21 22:29:02.429766: Epoch 580\n",
      "2025-12-21 22:29:02.429766: Current learning rate: 0.00458\n",
      "2025-12-21 22:31:20.402569: train_loss -0.8447\n",
      "2025-12-21 22:31:20.402569: val_loss -0.862\n",
      "2025-12-21 22:31:20.418482: Pseudo dice [0.9213, 0.9485, 0.9356]\n",
      "2025-12-21 22:31:20.420484: Epoch time: 137.97 s\n",
      "2025-12-21 22:31:21.066219: \n",
      "2025-12-21 22:31:21.066219: Epoch 581\n",
      "2025-12-21 22:31:21.066219: Current learning rate: 0.00457\n",
      "2025-12-21 22:33:39.219863: train_loss -0.8471\n",
      "2025-12-21 22:33:39.221865: val_loss -0.8692\n",
      "2025-12-21 22:33:39.231884: Pseudo dice [0.9229, 0.9539, 0.9371]\n",
      "2025-12-21 22:33:39.235531: Epoch time: 138.15 s\n",
      "2025-12-21 22:33:39.914549: \n",
      "2025-12-21 22:33:39.914549: Epoch 582\n",
      "2025-12-21 22:33:39.914549: Current learning rate: 0.00456\n",
      "2025-12-21 22:35:58.146109: train_loss -0.8469\n",
      "2025-12-21 22:35:58.146109: val_loss -0.857\n",
      "2025-12-21 22:35:58.152116: Pseudo dice [0.9223, 0.955, 0.9236]\n",
      "2025-12-21 22:35:58.158122: Epoch time: 138.23 s\n",
      "2025-12-21 22:35:58.840559: \n",
      "2025-12-21 22:35:58.840559: Epoch 583\n",
      "2025-12-21 22:35:58.840559: Current learning rate: 0.00455\n",
      "2025-12-21 22:38:16.786911: train_loss -0.8446\n",
      "2025-12-21 22:38:16.786911: val_loss -0.8763\n",
      "2025-12-21 22:38:16.792655: Pseudo dice [0.9276, 0.9546, 0.945]\n",
      "2025-12-21 22:38:16.796397: Epoch time: 137.95 s\n",
      "2025-12-21 22:38:17.590168: \n",
      "2025-12-21 22:38:17.605979: Epoch 584\n",
      "2025-12-21 22:38:17.605979: Current learning rate: 0.00454\n",
      "2025-12-21 22:40:35.695489: train_loss -0.8505\n",
      "2025-12-21 22:40:35.695489: val_loss -0.8877\n",
      "2025-12-21 22:40:35.700494: Pseudo dice [0.9359, 0.9617, 0.9408]\n",
      "2025-12-21 22:40:35.706302: Epoch time: 138.11 s\n",
      "2025-12-21 22:40:36.351284: \n",
      "2025-12-21 22:40:36.351284: Epoch 585\n",
      "2025-12-21 22:40:36.351284: Current learning rate: 0.00453\n",
      "2025-12-21 22:42:54.098349: train_loss -0.8571\n",
      "2025-12-21 22:42:54.100352: val_loss -0.8672\n",
      "2025-12-21 22:42:54.104359: Pseudo dice [0.9243, 0.951, 0.9401]\n",
      "2025-12-21 22:42:54.104359: Epoch time: 137.75 s\n",
      "2025-12-21 22:42:54.750718: \n",
      "2025-12-21 22:42:54.750718: Epoch 586\n",
      "2025-12-21 22:42:54.766516: Current learning rate: 0.00452\n",
      "2025-12-21 22:45:12.976638: train_loss -0.8448\n",
      "2025-12-21 22:45:12.976638: val_loss -0.8879\n",
      "2025-12-21 22:45:12.976638: Pseudo dice [0.9359, 0.9623, 0.9479]\n",
      "2025-12-21 22:45:12.976638: Epoch time: 138.23 s\n",
      "2025-12-21 22:45:13.624954: \n",
      "2025-12-21 22:45:13.624954: Epoch 587\n",
      "2025-12-21 22:45:13.624954: Current learning rate: 0.00451\n",
      "2025-12-21 22:47:31.545012: train_loss -0.8508\n",
      "2025-12-21 22:47:31.545012: val_loss -0.8609\n",
      "2025-12-21 22:47:31.560790: Pseudo dice [0.9164, 0.9442, 0.9473]\n",
      "2025-12-21 22:47:31.560790: Epoch time: 137.92 s\n",
      "2025-12-21 22:47:32.209847: \n",
      "2025-12-21 22:47:32.209847: Epoch 588\n",
      "2025-12-21 22:47:32.209847: Current learning rate: 0.0045\n",
      "2025-12-21 22:49:50.175599: train_loss -0.851\n",
      "2025-12-21 22:49:50.175599: val_loss -0.872\n",
      "2025-12-21 22:49:50.191334: Pseudo dice [0.9304, 0.9576, 0.9316]\n",
      "2025-12-21 22:49:50.191334: Epoch time: 137.97 s\n",
      "2025-12-21 22:49:50.886894: \n",
      "2025-12-21 22:49:50.886894: Epoch 589\n",
      "2025-12-21 22:49:50.891621: Current learning rate: 0.00449\n",
      "2025-12-21 22:52:08.969062: train_loss -0.8529\n",
      "2025-12-21 22:52:08.971065: val_loss -0.8898\n",
      "2025-12-21 22:52:08.977075: Pseudo dice [0.9385, 0.9615, 0.9444]\n",
      "2025-12-21 22:52:08.981084: Epoch time: 138.1 s\n",
      "2025-12-21 22:52:08.985089: Yayy! New best EMA pseudo Dice: 0.9402\n",
      "2025-12-21 22:52:10.069064: \n",
      "2025-12-21 22:52:10.069064: Epoch 590\n",
      "2025-12-21 22:52:10.069064: Current learning rate: 0.00448\n",
      "2025-12-21 22:54:28.232607: train_loss -0.8473\n",
      "2025-12-21 22:54:28.234609: val_loss -0.8766\n",
      "2025-12-21 22:54:28.238615: Pseudo dice [0.9283, 0.9574, 0.9423]\n",
      "2025-12-21 22:54:28.242619: Epoch time: 138.17 s\n",
      "2025-12-21 22:54:28.246361: Yayy! New best EMA pseudo Dice: 0.9405\n",
      "2025-12-21 22:54:29.213981: \n",
      "2025-12-21 22:54:29.213981: Epoch 591\n",
      "2025-12-21 22:54:29.213981: Current learning rate: 0.00447\n",
      "2025-12-21 22:56:47.181273: train_loss -0.8562\n",
      "2025-12-21 22:56:47.189854: val_loss -0.8613\n",
      "2025-12-21 22:56:47.194515: Pseudo dice [0.9204, 0.9485, 0.9396]\n",
      "2025-12-21 22:56:47.200098: Epoch time: 137.97 s\n",
      "2025-12-21 22:56:47.841745: \n",
      "2025-12-21 22:56:47.841745: Epoch 592\n",
      "2025-12-21 22:56:47.841745: Current learning rate: 0.00446\n",
      "2025-12-21 22:59:05.744145: train_loss -0.8449\n",
      "2025-12-21 22:59:05.744145: val_loss -0.8614\n",
      "2025-12-21 22:59:05.744145: Pseudo dice [0.918, 0.9527, 0.9309]\n",
      "2025-12-21 22:59:05.760034: Epoch time: 137.9 s\n",
      "2025-12-21 22:59:06.377620: \n",
      "2025-12-21 22:59:06.377620: Epoch 593\n",
      "2025-12-21 22:59:06.393537: Current learning rate: 0.00445\n",
      "2025-12-21 23:01:24.392865: train_loss -0.8504\n",
      "2025-12-21 23:01:24.392865: val_loss -0.8638\n",
      "2025-12-21 23:01:24.398610: Pseudo dice [0.922, 0.9455, 0.9439]\n",
      "2025-12-21 23:01:24.404617: Epoch time: 138.02 s\n",
      "2025-12-21 23:01:25.061818: \n",
      "2025-12-21 23:01:25.061818: Epoch 594\n",
      "2025-12-21 23:01:25.061818: Current learning rate: 0.00444\n",
      "2025-12-21 23:03:42.797209: train_loss -0.8566\n",
      "2025-12-21 23:03:42.799211: val_loss -0.8738\n",
      "2025-12-21 23:03:42.805217: Pseudo dice [0.9283, 0.9563, 0.9397]\n",
      "2025-12-21 23:03:42.810224: Epoch time: 137.74 s\n",
      "2025-12-21 23:03:43.601839: \n",
      "2025-12-21 23:03:43.601839: Epoch 595\n",
      "2025-12-21 23:03:43.617709: Current learning rate: 0.00443\n",
      "2025-12-21 23:06:01.628841: train_loss -0.8512\n",
      "2025-12-21 23:06:01.630843: val_loss -0.8591\n",
      "2025-12-21 23:06:01.634847: Pseudo dice [0.9146, 0.9482, 0.9376]\n",
      "2025-12-21 23:06:01.634847: Epoch time: 138.03 s\n",
      "2025-12-21 23:06:02.267718: \n",
      "2025-12-21 23:06:02.267718: Epoch 596\n",
      "2025-12-21 23:06:02.283405: Current learning rate: 0.00442\n",
      "2025-12-21 23:08:20.046977: train_loss -0.8527\n",
      "2025-12-21 23:08:20.046977: val_loss -0.873\n",
      "2025-12-21 23:08:20.056236: Pseudo dice [0.9258, 0.9534, 0.9416]\n",
      "2025-12-21 23:08:20.060242: Epoch time: 137.78 s\n",
      "2025-12-21 23:08:20.865720: \n",
      "2025-12-21 23:08:20.865720: Epoch 597\n",
      "2025-12-21 23:08:20.877923: Current learning rate: 0.00441\n",
      "2025-12-21 23:10:39.188118: train_loss -0.8541\n",
      "2025-12-21 23:10:39.188118: val_loss -0.8766\n",
      "2025-12-21 23:10:39.193124: Pseudo dice [0.926, 0.9558, 0.9438]\n",
      "2025-12-21 23:10:39.197504: Epoch time: 138.32 s\n",
      "2025-12-21 23:10:39.851227: \n",
      "2025-12-21 23:10:39.851227: Epoch 598\n",
      "2025-12-21 23:10:39.853230: Current learning rate: 0.0044\n",
      "2025-12-21 23:12:57.960452: train_loss -0.8535\n",
      "2025-12-21 23:12:57.960452: val_loss -0.8758\n",
      "2025-12-21 23:12:57.966458: Pseudo dice [0.9273, 0.9542, 0.946]\n",
      "2025-12-21 23:12:57.970463: Epoch time: 138.11 s\n",
      "2025-12-21 23:12:58.619008: \n",
      "2025-12-21 23:12:58.619008: Epoch 599\n",
      "2025-12-21 23:12:58.621793: Current learning rate: 0.00439\n",
      "2025-12-21 23:15:16.480674: train_loss -0.8521\n",
      "2025-12-21 23:15:16.482676: val_loss -0.8879\n",
      "2025-12-21 23:15:16.486680: Pseudo dice [0.9373, 0.9631, 0.9405]\n",
      "2025-12-21 23:15:16.492335: Epoch time: 137.86 s\n",
      "2025-12-21 23:15:17.548673: \n",
      "2025-12-21 23:15:17.548673: Epoch 600\n",
      "2025-12-21 23:15:17.548673: Current learning rate: 0.00438\n",
      "2025-12-21 23:17:35.592357: train_loss -0.8507\n",
      "2025-12-21 23:17:35.592357: val_loss -0.8482\n",
      "2025-12-21 23:17:35.597363: Pseudo dice [0.9095, 0.9415, 0.9398]\n",
      "2025-12-21 23:17:35.603370: Epoch time: 138.04 s\n",
      "2025-12-21 23:17:36.425077: \n",
      "2025-12-21 23:17:36.426817: Epoch 601\n",
      "2025-12-21 23:17:36.426817: Current learning rate: 0.00437\n",
      "2025-12-21 23:19:54.298667: train_loss -0.8532\n",
      "2025-12-21 23:19:54.298667: val_loss -0.8644\n",
      "2025-12-21 23:19:54.298667: Pseudo dice [0.9199, 0.9517, 0.9395]\n",
      "2025-12-21 23:19:54.314737: Epoch time: 137.87 s\n",
      "2025-12-21 23:19:54.943568: \n",
      "2025-12-21 23:19:54.943568: Epoch 602\n",
      "2025-12-21 23:19:54.943568: Current learning rate: 0.00436\n",
      "2025-12-21 23:22:12.978637: train_loss -0.8515\n",
      "2025-12-21 23:22:12.978637: val_loss -0.8755\n",
      "2025-12-21 23:22:12.984946: Pseudo dice [0.9294, 0.954, 0.9401]\n",
      "2025-12-21 23:22:12.988953: Epoch time: 138.04 s\n",
      "2025-12-21 23:22:13.802471: \n",
      "2025-12-21 23:22:13.802471: Epoch 603\n",
      "2025-12-21 23:22:13.807977: Current learning rate: 0.00435\n",
      "2025-12-21 23:24:31.835455: train_loss -0.8512\n",
      "2025-12-21 23:24:31.837459: val_loss -0.8568\n",
      "2025-12-21 23:24:31.841020: Pseudo dice [0.919, 0.9495, 0.9303]\n",
      "2025-12-21 23:24:31.845025: Epoch time: 138.05 s\n",
      "2025-12-21 23:24:32.483381: \n",
      "2025-12-21 23:24:32.483381: Epoch 604\n",
      "2025-12-21 23:24:32.489319: Current learning rate: 0.00434\n",
      "2025-12-21 23:26:50.354169: train_loss -0.8493\n",
      "2025-12-21 23:26:50.354169: val_loss -0.8708\n",
      "2025-12-21 23:26:50.362176: Pseudo dice [0.9262, 0.9544, 0.9347]\n",
      "2025-12-21 23:26:50.365919: Epoch time: 137.87 s\n",
      "2025-12-21 23:26:50.999802: \n",
      "2025-12-21 23:26:50.999802: Epoch 605\n",
      "2025-12-21 23:26:51.015907: Current learning rate: 0.00433\n",
      "2025-12-21 23:29:08.970231: train_loss -0.8597\n",
      "2025-12-21 23:29:08.970231: val_loss -0.868\n",
      "2025-12-21 23:29:08.978239: Pseudo dice [0.9288, 0.9521, 0.9314]\n",
      "2025-12-21 23:29:08.983984: Epoch time: 137.97 s\n",
      "2025-12-21 23:29:09.787384: \n",
      "2025-12-21 23:29:09.787384: Epoch 606\n",
      "2025-12-21 23:29:09.794592: Current learning rate: 0.00432\n",
      "2025-12-21 23:31:27.771431: train_loss -0.8496\n",
      "2025-12-21 23:31:27.771431: val_loss -0.8676\n",
      "2025-12-21 23:31:27.782415: Pseudo dice [0.9225, 0.9484, 0.9357]\n",
      "2025-12-21 23:31:27.787422: Epoch time: 137.98 s\n",
      "2025-12-21 23:31:28.426098: \n",
      "2025-12-21 23:31:28.426098: Epoch 607\n",
      "2025-12-21 23:31:28.429694: Current learning rate: 0.00431\n",
      "2025-12-21 23:33:46.461154: train_loss -0.8311\n",
      "2025-12-21 23:33:46.463157: val_loss -0.8288\n",
      "2025-12-21 23:33:46.465685: Pseudo dice [0.9087, 0.9398, 0.917]\n",
      "2025-12-21 23:33:46.465685: Epoch time: 138.04 s\n",
      "2025-12-21 23:33:47.299014: \n",
      "2025-12-21 23:33:47.299014: Epoch 608\n",
      "2025-12-21 23:33:47.314773: Current learning rate: 0.0043\n",
      "2025-12-21 23:36:05.285922: train_loss -0.8236\n",
      "2025-12-21 23:36:05.285922: val_loss -0.8612\n",
      "2025-12-21 23:36:05.285922: Pseudo dice [0.9236, 0.9555, 0.9279]\n",
      "2025-12-21 23:36:05.297530: Epoch time: 137.99 s\n",
      "2025-12-21 23:36:05.994189: \n",
      "2025-12-21 23:36:05.994189: Epoch 609\n",
      "2025-12-21 23:36:06.012498: Current learning rate: 0.00429\n",
      "2025-12-21 23:38:23.971449: train_loss -0.8287\n",
      "2025-12-21 23:38:23.973452: val_loss -0.8505\n",
      "2025-12-21 23:38:23.973452: Pseudo dice [0.9166, 0.945, 0.9335]\n",
      "2025-12-21 23:38:23.979141: Epoch time: 137.98 s\n",
      "2025-12-21 23:38:24.656105: \n",
      "2025-12-21 23:38:24.656105: Epoch 610\n",
      "2025-12-21 23:38:24.656105: Current learning rate: 0.00429\n",
      "2025-12-21 23:40:42.641584: train_loss -0.8389\n",
      "2025-12-21 23:40:42.643587: val_loss -0.86\n",
      "2025-12-21 23:40:42.645589: Pseudo dice [0.9213, 0.9499, 0.9353]\n",
      "2025-12-21 23:40:42.652487: Epoch time: 137.99 s\n",
      "2025-12-21 23:40:43.297920: \n",
      "2025-12-21 23:40:43.299923: Epoch 611\n",
      "2025-12-21 23:40:43.299923: Current learning rate: 0.00428\n",
      "2025-12-21 23:43:01.205904: train_loss -0.8458\n",
      "2025-12-21 23:43:01.207906: val_loss -0.871\n",
      "2025-12-21 23:43:01.213650: Pseudo dice [0.9328, 0.9547, 0.9291]\n",
      "2025-12-21 23:43:01.219656: Epoch time: 137.91 s\n",
      "2025-12-21 23:43:01.971716: \n",
      "2025-12-21 23:43:01.971716: Epoch 612\n",
      "2025-12-21 23:43:01.977899: Current learning rate: 0.00427\n",
      "2025-12-21 23:45:19.890964: train_loss -0.8379\n",
      "2025-12-21 23:45:19.890964: val_loss -0.8735\n",
      "2025-12-21 23:45:19.896970: Pseudo dice [0.9308, 0.9561, 0.9373]\n",
      "2025-12-21 23:45:19.902976: Epoch time: 137.92 s\n",
      "2025-12-21 23:45:20.589212: \n",
      "2025-12-21 23:45:20.589212: Epoch 613\n",
      "2025-12-21 23:45:20.589212: Current learning rate: 0.00426\n",
      "2025-12-21 23:47:38.436436: train_loss -0.8337\n",
      "2025-12-21 23:47:38.436436: val_loss -0.8651\n",
      "2025-12-21 23:47:38.444445: Pseudo dice [0.9256, 0.9545, 0.9353]\n",
      "2025-12-21 23:47:38.448448: Epoch time: 137.85 s\n",
      "2025-12-21 23:47:39.085239: \n",
      "2025-12-21 23:47:39.085239: Epoch 614\n",
      "2025-12-21 23:47:39.101187: Current learning rate: 0.00425\n",
      "2025-12-21 23:49:57.139794: train_loss -0.8419\n",
      "2025-12-21 23:49:57.139794: val_loss -0.864\n",
      "2025-12-21 23:49:57.147541: Pseudo dice [0.9222, 0.9541, 0.9305]\n",
      "2025-12-21 23:49:57.151545: Epoch time: 138.05 s\n",
      "2025-12-21 23:49:57.841863: \n",
      "2025-12-21 23:49:57.841863: Epoch 615\n",
      "2025-12-21 23:49:57.841863: Current learning rate: 0.00424\n",
      "2025-12-21 23:52:16.017584: train_loss -0.8424\n",
      "2025-12-21 23:52:16.017584: val_loss -0.8712\n",
      "2025-12-21 23:52:16.022927: Pseudo dice [0.9275, 0.9576, 0.928]\n",
      "2025-12-21 23:52:16.026932: Epoch time: 138.17 s\n",
      "2025-12-21 23:52:16.676574: \n",
      "2025-12-21 23:52:16.676574: Epoch 616\n",
      "2025-12-21 23:52:16.676574: Current learning rate: 0.00423\n",
      "2025-12-21 23:54:34.554220: train_loss -0.844\n",
      "2025-12-21 23:54:34.556222: val_loss -0.8632\n",
      "2025-12-21 23:54:34.560226: Pseudo dice [0.9195, 0.9465, 0.9422]\n",
      "2025-12-21 23:54:34.564230: Epoch time: 137.88 s\n",
      "2025-12-21 23:54:35.204910: \n",
      "2025-12-21 23:54:35.204910: Epoch 617\n",
      "2025-12-21 23:54:35.204910: Current learning rate: 0.00422\n",
      "2025-12-21 23:56:53.236970: train_loss -0.8452\n",
      "2025-12-21 23:56:53.236970: val_loss -0.8658\n",
      "2025-12-21 23:56:53.245193: Pseudo dice [0.9244, 0.9515, 0.9327]\n",
      "2025-12-21 23:56:53.249197: Epoch time: 138.03 s\n",
      "2025-12-21 23:56:53.894566: \n",
      "2025-12-21 23:56:53.896569: Epoch 618\n",
      "2025-12-21 23:56:53.899604: Current learning rate: 0.00421\n",
      "2025-12-21 23:59:11.792309: train_loss -0.8472\n",
      "2025-12-21 23:59:11.794050: val_loss -0.8866\n",
      "2025-12-21 23:59:11.794050: Pseudo dice [0.9378, 0.9616, 0.9409]\n",
      "2025-12-21 23:59:11.794050: Epoch time: 137.9 s\n",
      "2025-12-21 23:59:12.603105: \n",
      "2025-12-21 23:59:12.603105: Epoch 619\n",
      "2025-12-21 23:59:12.618882: Current learning rate: 0.0042\n",
      "2025-12-22 00:01:30.527371: train_loss -0.847\n",
      "2025-12-22 00:01:30.527371: val_loss -0.8801\n",
      "2025-12-22 00:01:30.543088: Pseudo dice [0.9331, 0.9595, 0.9414]\n",
      "2025-12-22 00:01:30.543088: Epoch time: 137.92 s\n",
      "2025-12-22 00:01:31.177625: \n",
      "2025-12-22 00:01:31.177625: Epoch 620\n",
      "2025-12-22 00:01:31.177625: Current learning rate: 0.00419\n",
      "2025-12-22 00:03:49.110970: train_loss -0.8562\n",
      "2025-12-22 00:03:49.110970: val_loss -0.8596\n",
      "2025-12-22 00:03:49.110970: Pseudo dice [0.9176, 0.9484, 0.9404]\n",
      "2025-12-22 00:03:49.110970: Epoch time: 137.93 s\n",
      "2025-12-22 00:03:49.764163: \n",
      "2025-12-22 00:03:49.764163: Epoch 621\n",
      "2025-12-22 00:03:49.764163: Current learning rate: 0.00418\n",
      "2025-12-22 00:06:07.604903: train_loss -0.8476\n",
      "2025-12-22 00:06:07.604903: val_loss -0.8766\n",
      "2025-12-22 00:06:07.612652: Pseudo dice [0.9264, 0.9564, 0.943]\n",
      "2025-12-22 00:06:07.616394: Epoch time: 137.84 s\n",
      "2025-12-22 00:06:08.308259: \n",
      "2025-12-22 00:06:08.308259: Epoch 622\n",
      "2025-12-22 00:06:08.308259: Current learning rate: 0.00417\n",
      "2025-12-22 00:08:26.225407: train_loss -0.8516\n",
      "2025-12-22 00:08:26.225407: val_loss -0.8809\n",
      "2025-12-22 00:08:26.232219: Pseudo dice [0.931, 0.9562, 0.9452]\n",
      "2025-12-22 00:08:26.236226: Epoch time: 137.92 s\n",
      "2025-12-22 00:08:27.049422: \n",
      "2025-12-22 00:08:27.049422: Epoch 623\n",
      "2025-12-22 00:08:27.065305: Current learning rate: 0.00416\n",
      "2025-12-22 00:10:45.353837: train_loss -0.8495\n",
      "2025-12-22 00:10:45.353837: val_loss -0.8663\n",
      "2025-12-22 00:10:45.353837: Pseudo dice [0.9238, 0.9497, 0.9324]\n",
      "2025-12-22 00:10:45.369625: Epoch time: 138.3 s\n",
      "2025-12-22 00:10:46.002098: \n",
      "2025-12-22 00:10:46.002098: Epoch 624\n",
      "2025-12-22 00:10:46.002098: Current learning rate: 0.00415\n",
      "2025-12-22 00:13:03.761261: train_loss -0.8537\n",
      "2025-12-22 00:13:03.761261: val_loss -0.8777\n",
      "2025-12-22 00:13:03.765265: Pseudo dice [0.9322, 0.9559, 0.9403]\n",
      "2025-12-22 00:13:03.765265: Epoch time: 137.76 s\n",
      "2025-12-22 00:13:04.408172: \n",
      "2025-12-22 00:13:04.408172: Epoch 625\n",
      "2025-12-22 00:13:04.408172: Current learning rate: 0.00414\n",
      "2025-12-22 00:15:22.497521: train_loss -0.8475\n",
      "2025-12-22 00:15:22.497521: val_loss -0.8707\n",
      "2025-12-22 00:15:22.504528: Pseudo dice [0.9244, 0.9529, 0.9386]\n",
      "2025-12-22 00:15:22.508533: Epoch time: 138.09 s\n",
      "2025-12-22 00:15:23.474338: \n",
      "2025-12-22 00:15:23.474338: Epoch 626\n",
      "2025-12-22 00:15:23.476341: Current learning rate: 0.00413\n",
      "2025-12-22 00:17:41.419582: train_loss -0.8471\n",
      "2025-12-22 00:17:41.419582: val_loss -0.8767\n",
      "2025-12-22 00:17:41.419582: Pseudo dice [0.9314, 0.9585, 0.9369]\n",
      "2025-12-22 00:17:41.419582: Epoch time: 137.95 s\n",
      "2025-12-22 00:17:42.068901: \n",
      "2025-12-22 00:17:42.068901: Epoch 627\n",
      "2025-12-22 00:17:42.084588: Current learning rate: 0.00412\n",
      "2025-12-22 00:20:00.097589: train_loss -0.8493\n",
      "2025-12-22 00:20:00.097589: val_loss -0.852\n",
      "2025-12-22 00:20:00.103333: Pseudo dice [0.9106, 0.9437, 0.9428]\n",
      "2025-12-22 00:20:00.105335: Epoch time: 138.03 s\n",
      "2025-12-22 00:20:00.739634: \n",
      "2025-12-22 00:20:00.739634: Epoch 628\n",
      "2025-12-22 00:20:00.739634: Current learning rate: 0.00411\n",
      "2025-12-22 00:22:18.580738: train_loss -0.8441\n",
      "2025-12-22 00:22:18.596458: val_loss -0.8731\n",
      "2025-12-22 00:22:18.596458: Pseudo dice [0.9264, 0.9522, 0.9352]\n",
      "2025-12-22 00:22:18.596458: Epoch time: 137.84 s\n",
      "2025-12-22 00:22:19.342026: \n",
      "2025-12-22 00:22:19.342026: Epoch 629\n",
      "2025-12-22 00:22:19.357831: Current learning rate: 0.0041\n",
      "2025-12-22 00:24:37.448271: train_loss -0.8496\n",
      "2025-12-22 00:24:37.448271: val_loss -0.8822\n",
      "2025-12-22 00:24:37.453277: Pseudo dice [0.9282, 0.9621, 0.9448]\n",
      "2025-12-22 00:24:37.457280: Epoch time: 138.11 s\n",
      "2025-12-22 00:24:38.094280: \n",
      "2025-12-22 00:24:38.094280: Epoch 630\n",
      "2025-12-22 00:24:38.094280: Current learning rate: 0.00409\n",
      "2025-12-22 00:26:56.040912: train_loss -0.8481\n",
      "2025-12-22 00:26:56.040912: val_loss -0.8765\n",
      "2025-12-22 00:26:56.046917: Pseudo dice [0.9268, 0.9572, 0.9425]\n",
      "2025-12-22 00:26:56.049922: Epoch time: 137.95 s\n",
      "2025-12-22 00:26:56.683184: \n",
      "2025-12-22 00:26:56.683184: Epoch 631\n",
      "2025-12-22 00:26:56.699095: Current learning rate: 0.00408\n",
      "2025-12-22 00:29:14.745888: train_loss -0.8522\n",
      "2025-12-22 00:29:14.745888: val_loss -0.8751\n",
      "2025-12-22 00:29:14.745888: Pseudo dice [0.9255, 0.9558, 0.9447]\n",
      "2025-12-22 00:29:14.761722: Epoch time: 138.06 s\n",
      "2025-12-22 00:29:15.647236: \n",
      "2025-12-22 00:29:15.647236: Epoch 632\n",
      "2025-12-22 00:29:15.663125: Current learning rate: 0.00407\n",
      "2025-12-22 00:31:33.642545: train_loss -0.8527\n",
      "2025-12-22 00:31:33.642545: val_loss -0.8702\n",
      "2025-12-22 00:31:33.650544: Pseudo dice [0.9228, 0.9544, 0.9373]\n",
      "2025-12-22 00:31:33.654548: Epoch time: 138.0 s\n",
      "2025-12-22 00:31:34.296431: \n",
      "2025-12-22 00:31:34.296431: Epoch 633\n",
      "2025-12-22 00:31:34.296431: Current learning rate: 0.00406\n",
      "2025-12-22 00:33:52.292734: train_loss -0.8518\n",
      "2025-12-22 00:33:52.292734: val_loss -0.8779\n",
      "2025-12-22 00:33:52.308502: Pseudo dice [0.9302, 0.9617, 0.9407]\n",
      "2025-12-22 00:33:52.308502: Epoch time: 138.0 s\n",
      "2025-12-22 00:33:52.958699: \n",
      "2025-12-22 00:33:52.958699: Epoch 634\n",
      "2025-12-22 00:33:52.958699: Current learning rate: 0.00405\n",
      "2025-12-22 00:36:11.012021: train_loss -0.8508\n",
      "2025-12-22 00:36:11.027751: val_loss -0.8772\n",
      "2025-12-22 00:36:11.027751: Pseudo dice [0.9308, 0.9549, 0.9392]\n",
      "2025-12-22 00:36:11.027751: Epoch time: 138.07 s\n",
      "2025-12-22 00:36:11.699985: \n",
      "2025-12-22 00:36:11.699985: Epoch 635\n",
      "2025-12-22 00:36:11.699985: Current learning rate: 0.00404\n",
      "2025-12-22 00:38:29.714067: train_loss -0.8521\n",
      "2025-12-22 00:38:29.716069: val_loss -0.8857\n",
      "2025-12-22 00:38:29.722076: Pseudo dice [0.9398, 0.9589, 0.9368]\n",
      "2025-12-22 00:38:29.728084: Epoch time: 138.02 s\n",
      "2025-12-22 00:38:29.731826: Yayy! New best EMA pseudo Dice: 0.9407\n",
      "2025-12-22 00:38:30.679440: \n",
      "2025-12-22 00:38:30.679440: Epoch 636\n",
      "2025-12-22 00:38:30.679440: Current learning rate: 0.00403\n",
      "2025-12-22 00:40:48.673887: train_loss -0.8533\n",
      "2025-12-22 00:40:48.673887: val_loss -0.8719\n",
      "2025-12-22 00:40:48.683097: Pseudo dice [0.9258, 0.952, 0.9347]\n",
      "2025-12-22 00:40:48.689103: Epoch time: 137.99 s\n",
      "2025-12-22 00:40:49.491175: \n",
      "2025-12-22 00:40:49.491175: Epoch 637\n",
      "2025-12-22 00:40:49.491175: Current learning rate: 0.00402\n",
      "2025-12-22 00:43:07.389519: train_loss -0.853\n",
      "2025-12-22 00:43:07.389519: val_loss -0.878\n",
      "2025-12-22 00:43:07.397026: Pseudo dice [0.9299, 0.9591, 0.9384]\n",
      "2025-12-22 00:43:07.401030: Epoch time: 137.9 s\n",
      "2025-12-22 00:43:08.054157: \n",
      "2025-12-22 00:43:08.054157: Epoch 638\n",
      "2025-12-22 00:43:08.070079: Current learning rate: 0.00401\n",
      "2025-12-22 00:45:26.130820: train_loss -0.8541\n",
      "2025-12-22 00:45:26.130820: val_loss -0.8664\n",
      "2025-12-22 00:45:26.136825: Pseudo dice [0.9241, 0.9529, 0.9374]\n",
      "2025-12-22 00:45:26.140829: Epoch time: 138.08 s\n",
      "2025-12-22 00:45:26.842404: \n",
      "2025-12-22 00:45:26.842404: Epoch 639\n",
      "2025-12-22 00:45:26.842404: Current learning rate: 0.004\n",
      "2025-12-22 00:47:44.770080: train_loss -0.8528\n",
      "2025-12-22 00:47:44.770080: val_loss -0.8673\n",
      "2025-12-22 00:47:44.770080: Pseudo dice [0.9249, 0.9541, 0.9372]\n",
      "2025-12-22 00:47:44.770080: Epoch time: 137.93 s\n",
      "2025-12-22 00:47:45.419432: \n",
      "2025-12-22 00:47:45.419432: Epoch 640\n",
      "2025-12-22 00:47:45.419432: Current learning rate: 0.00399\n",
      "2025-12-22 00:50:03.189957: train_loss -0.8532\n",
      "2025-12-22 00:50:03.189957: val_loss -0.879\n",
      "2025-12-22 00:50:03.189957: Pseudo dice [0.9317, 0.9576, 0.94]\n",
      "2025-12-22 00:50:03.207837: Epoch time: 137.77 s\n",
      "2025-12-22 00:50:03.855048: \n",
      "2025-12-22 00:50:03.855048: Epoch 641\n",
      "2025-12-22 00:50:03.855048: Current learning rate: 0.00398\n",
      "2025-12-22 00:52:21.614201: train_loss -0.8594\n",
      "2025-12-22 00:52:21.614201: val_loss -0.8676\n",
      "2025-12-22 00:52:21.631900: Pseudo dice [0.923, 0.9529, 0.9377]\n",
      "2025-12-22 00:52:21.631900: Epoch time: 137.76 s\n",
      "2025-12-22 00:52:22.279191: \n",
      "2025-12-22 00:52:22.279191: Epoch 642\n",
      "2025-12-22 00:52:22.279191: Current learning rate: 0.00397\n",
      "2025-12-22 00:54:40.244226: train_loss -0.8558\n",
      "2025-12-22 00:54:40.244226: val_loss -0.8863\n",
      "2025-12-22 00:54:40.246228: Pseudo dice [0.9345, 0.9616, 0.9455]\n",
      "2025-12-22 00:54:40.254086: Epoch time: 137.97 s\n",
      "2025-12-22 00:54:40.259487: Yayy! New best EMA pseudo Dice: 0.9409\n",
      "2025-12-22 00:54:41.185595: \n",
      "2025-12-22 00:54:41.185595: Epoch 643\n",
      "2025-12-22 00:54:41.185595: Current learning rate: 0.00396\n",
      "2025-12-22 00:56:59.025584: train_loss -0.8559\n",
      "2025-12-22 00:56:59.025584: val_loss -0.8815\n",
      "2025-12-22 00:56:59.029588: Pseudo dice [0.9319, 0.9583, 0.9412]\n",
      "2025-12-22 00:56:59.033592: Epoch time: 137.84 s\n",
      "2025-12-22 00:56:59.039336: Yayy! New best EMA pseudo Dice: 0.9412\n",
      "2025-12-22 00:57:00.173004: \n",
      "2025-12-22 00:57:00.173004: Epoch 644\n",
      "2025-12-22 00:57:00.188993: Current learning rate: 0.00395\n",
      "2025-12-22 00:59:18.110029: train_loss -0.8524\n",
      "2025-12-22 00:59:18.110029: val_loss -0.875\n",
      "2025-12-22 00:59:18.118387: Pseudo dice [0.9272, 0.9551, 0.943]\n",
      "2025-12-22 00:59:18.126399: Epoch time: 137.94 s\n",
      "2025-12-22 00:59:18.132149: Yayy! New best EMA pseudo Dice: 0.9413\n",
      "2025-12-22 00:59:19.068963: \n",
      "2025-12-22 00:59:19.068963: Epoch 645\n",
      "2025-12-22 00:59:19.075168: Current learning rate: 0.00394\n",
      "2025-12-22 01:01:36.949436: train_loss -0.8548\n",
      "2025-12-22 01:01:36.949436: val_loss -0.8879\n",
      "2025-12-22 01:01:36.949436: Pseudo dice [0.9381, 0.9619, 0.9451]\n",
      "2025-12-22 01:01:36.964698: Epoch time: 137.88 s\n",
      "2025-12-22 01:01:36.967611: Yayy! New best EMA pseudo Dice: 0.942\n",
      "2025-12-22 01:01:37.897147: \n",
      "2025-12-22 01:01:37.897147: Epoch 646\n",
      "2025-12-22 01:01:37.897147: Current learning rate: 0.00393\n",
      "2025-12-22 01:03:55.988930: train_loss -0.8503\n",
      "2025-12-22 01:03:55.988930: val_loss -0.8755\n",
      "2025-12-22 01:03:55.994427: Pseudo dice [0.9278, 0.9554, 0.94]\n",
      "2025-12-22 01:03:55.998431: Epoch time: 138.1 s\n",
      "2025-12-22 01:03:56.645748: \n",
      "2025-12-22 01:03:56.645748: Epoch 647\n",
      "2025-12-22 01:03:56.650462: Current learning rate: 0.00392\n",
      "2025-12-22 01:06:14.783987: train_loss -0.8566\n",
      "2025-12-22 01:06:14.785989: val_loss -0.8757\n",
      "2025-12-22 01:06:14.792173: Pseudo dice [0.928, 0.9561, 0.941]\n",
      "2025-12-22 01:06:14.798179: Epoch time: 138.14 s\n",
      "2025-12-22 01:06:15.434215: \n",
      "2025-12-22 01:06:15.449980: Epoch 648\n",
      "2025-12-22 01:06:15.449980: Current learning rate: 0.00391\n",
      "2025-12-22 01:08:33.404295: train_loss -0.8558\n",
      "2025-12-22 01:08:33.404295: val_loss -0.8841\n",
      "2025-12-22 01:08:33.411803: Pseudo dice [0.9345, 0.9612, 0.9422]\n",
      "2025-12-22 01:08:33.415806: Epoch time: 137.97 s\n",
      "2025-12-22 01:08:33.419810: Yayy! New best EMA pseudo Dice: 0.9423\n",
      "2025-12-22 01:08:34.563265: \n",
      "2025-12-22 01:08:34.563265: Epoch 649\n",
      "2025-12-22 01:08:34.563265: Current learning rate: 0.0039\n",
      "2025-12-22 01:10:52.937339: train_loss -0.8537\n",
      "2025-12-22 01:10:52.937339: val_loss -0.8841\n",
      "2025-12-22 01:10:52.941344: Pseudo dice [0.9335, 0.9584, 0.9444]\n",
      "2025-12-22 01:10:52.947350: Epoch time: 138.37 s\n",
      "2025-12-22 01:10:53.216866: Yayy! New best EMA pseudo Dice: 0.9426\n",
      "2025-12-22 01:10:54.155871: \n",
      "2025-12-22 01:10:54.155871: Epoch 650\n",
      "2025-12-22 01:10:54.157875: Current learning rate: 0.00389\n",
      "2025-12-22 01:13:12.213471: train_loss -0.8491\n",
      "2025-12-22 01:13:12.213471: val_loss -0.8748\n",
      "2025-12-22 01:13:12.221481: Pseudo dice [0.928, 0.9568, 0.9416]\n",
      "2025-12-22 01:13:12.225486: Epoch time: 138.06 s\n",
      "2025-12-22 01:13:12.918700: \n",
      "2025-12-22 01:13:12.918700: Epoch 651\n",
      "2025-12-22 01:13:12.923413: Current learning rate: 0.00388\n",
      "2025-12-22 01:15:31.072450: train_loss -0.8467\n",
      "2025-12-22 01:15:31.072450: val_loss -0.8681\n",
      "2025-12-22 01:15:31.072450: Pseudo dice [0.9279, 0.9544, 0.9303]\n",
      "2025-12-22 01:15:31.091624: Epoch time: 138.17 s\n",
      "2025-12-22 01:15:31.736824: \n",
      "2025-12-22 01:15:31.736824: Epoch 652\n",
      "2025-12-22 01:15:31.744268: Current learning rate: 0.00387\n",
      "2025-12-22 01:17:49.765259: train_loss -0.8398\n",
      "2025-12-22 01:17:49.767262: val_loss -0.8605\n",
      "2025-12-22 01:17:49.773008: Pseudo dice [0.9206, 0.9526, 0.9384]\n",
      "2025-12-22 01:17:49.779014: Epoch time: 138.03 s\n",
      "2025-12-22 01:17:50.420517: \n",
      "2025-12-22 01:17:50.420517: Epoch 653\n",
      "2025-12-22 01:17:50.432381: Current learning rate: 0.00386\n",
      "2025-12-22 01:20:08.470048: train_loss -0.84\n",
      "2025-12-22 01:20:08.470048: val_loss -0.8564\n",
      "2025-12-22 01:20:08.489588: Pseudo dice [0.9235, 0.9481, 0.9336]\n",
      "2025-12-22 01:20:08.494686: Epoch time: 138.05 s\n",
      "2025-12-22 01:20:09.153012: \n",
      "2025-12-22 01:20:09.153012: Epoch 654\n",
      "2025-12-22 01:20:09.169035: Current learning rate: 0.00385\n",
      "2025-12-22 01:22:26.965451: train_loss -0.8417\n",
      "2025-12-22 01:22:26.965451: val_loss -0.8547\n",
      "2025-12-22 01:22:26.981488: Pseudo dice [0.9197, 0.9452, 0.9379]\n",
      "2025-12-22 01:22:26.981488: Epoch time: 137.81 s\n",
      "2025-12-22 01:22:27.847070: \n",
      "2025-12-22 01:22:27.847070: Epoch 655\n",
      "2025-12-22 01:22:27.853427: Current learning rate: 0.00384\n",
      "2025-12-22 01:24:45.782434: train_loss -0.8439\n",
      "2025-12-22 01:24:45.782434: val_loss -0.8705\n",
      "2025-12-22 01:24:45.798138: Pseudo dice [0.9279, 0.9553, 0.9376]\n",
      "2025-12-22 01:24:45.807019: Epoch time: 137.94 s\n",
      "2025-12-22 01:24:46.498997: \n",
      "2025-12-22 01:24:46.498997: Epoch 656\n",
      "2025-12-22 01:24:46.498997: Current learning rate: 0.00383\n",
      "2025-12-22 01:27:04.449296: train_loss -0.8414\n",
      "2025-12-22 01:27:04.449296: val_loss -0.8648\n",
      "2025-12-22 01:27:04.463745: Pseudo dice [0.9255, 0.9526, 0.935]\n",
      "2025-12-22 01:27:04.467095: Epoch time: 137.95 s\n",
      "2025-12-22 01:27:05.114453: \n",
      "2025-12-22 01:27:05.114453: Epoch 657\n",
      "2025-12-22 01:27:05.114453: Current learning rate: 0.00382\n",
      "2025-12-22 01:29:22.881408: train_loss -0.8462\n",
      "2025-12-22 01:29:22.881408: val_loss -0.8645\n",
      "2025-12-22 01:29:22.886913: Pseudo dice [0.9227, 0.9519, 0.9308]\n",
      "2025-12-22 01:29:22.890920: Epoch time: 137.77 s\n",
      "2025-12-22 01:29:23.728678: \n",
      "2025-12-22 01:29:23.728678: Epoch 658\n",
      "2025-12-22 01:29:23.738187: Current learning rate: 0.00381\n",
      "2025-12-22 01:31:41.486641: train_loss -0.8449\n",
      "2025-12-22 01:31:41.486641: val_loss -0.8695\n",
      "2025-12-22 01:31:41.492647: Pseudo dice [0.9264, 0.954, 0.9394]\n",
      "2025-12-22 01:31:41.498393: Epoch time: 137.76 s\n",
      "2025-12-22 01:31:42.127048: \n",
      "2025-12-22 01:31:42.127048: Epoch 659\n",
      "2025-12-22 01:31:42.143099: Current learning rate: 0.0038\n",
      "2025-12-22 01:34:00.037049: train_loss -0.8535\n",
      "2025-12-22 01:34:00.037049: val_loss -0.8638\n",
      "2025-12-22 01:34:00.037049: Pseudo dice [0.9208, 0.9496, 0.9372]\n",
      "2025-12-22 01:34:00.037049: Epoch time: 137.91 s\n",
      "2025-12-22 01:34:00.682864: \n",
      "2025-12-22 01:34:00.682864: Epoch 660\n",
      "2025-12-22 01:34:00.686894: Current learning rate: 0.00379\n",
      "2025-12-22 01:36:18.562509: train_loss -0.847\n",
      "2025-12-22 01:36:18.562509: val_loss -0.8786\n",
      "2025-12-22 01:36:18.567703: Pseudo dice [0.9315, 0.9601, 0.9392]\n",
      "2025-12-22 01:36:18.571445: Epoch time: 137.88 s\n",
      "2025-12-22 01:36:19.515524: \n",
      "2025-12-22 01:36:19.517527: Epoch 661\n",
      "2025-12-22 01:36:19.521534: Current learning rate: 0.00378\n",
      "2025-12-22 01:38:37.322269: train_loss -0.8508\n",
      "2025-12-22 01:38:37.322269: val_loss -0.8792\n",
      "2025-12-22 01:38:37.328201: Pseudo dice [0.9329, 0.9571, 0.9374]\n",
      "2025-12-22 01:38:37.331751: Epoch time: 137.81 s\n",
      "2025-12-22 01:38:37.977601: \n",
      "2025-12-22 01:38:37.977601: Epoch 662\n",
      "2025-12-22 01:38:37.977601: Current learning rate: 0.00377\n",
      "2025-12-22 01:40:56.033101: train_loss -0.8481\n",
      "2025-12-22 01:40:56.033101: val_loss -0.8702\n",
      "2025-12-22 01:40:56.048309: Pseudo dice [0.9252, 0.9567, 0.9416]\n",
      "2025-12-22 01:40:56.048309: Epoch time: 138.06 s\n",
      "2025-12-22 01:40:56.698267: \n",
      "2025-12-22 01:40:56.698267: Epoch 663\n",
      "2025-12-22 01:40:56.698267: Current learning rate: 0.00376\n",
      "2025-12-22 01:43:14.741373: train_loss -0.8479\n",
      "2025-12-22 01:43:14.741373: val_loss -0.872\n",
      "2025-12-22 01:43:14.741373: Pseudo dice [0.9255, 0.9567, 0.9418]\n",
      "2025-12-22 01:43:14.750607: Epoch time: 138.05 s\n",
      "2025-12-22 01:43:15.576918: \n",
      "2025-12-22 01:43:15.576918: Epoch 664\n",
      "2025-12-22 01:43:15.576918: Current learning rate: 0.00375\n",
      "2025-12-22 01:45:33.540583: train_loss -0.8503\n",
      "2025-12-22 01:45:33.540583: val_loss -0.872\n",
      "2025-12-22 01:45:33.546589: Pseudo dice [0.9243, 0.9565, 0.9404]\n",
      "2025-12-22 01:45:33.549593: Epoch time: 137.96 s\n",
      "2025-12-22 01:45:34.199438: \n",
      "2025-12-22 01:45:34.199438: Epoch 665\n",
      "2025-12-22 01:45:34.199438: Current learning rate: 0.00374\n",
      "2025-12-22 01:47:52.158336: train_loss -0.8516\n",
      "2025-12-22 01:47:52.158336: val_loss -0.8829\n",
      "2025-12-22 01:47:52.166619: Pseudo dice [0.9361, 0.9586, 0.9392]\n",
      "2025-12-22 01:47:52.170623: Epoch time: 137.96 s\n",
      "2025-12-22 01:47:52.821842: \n",
      "2025-12-22 01:47:52.823582: Epoch 666\n",
      "2025-12-22 01:47:52.823582: Current learning rate: 0.00373\n",
      "2025-12-22 01:50:10.642043: train_loss -0.8548\n",
      "2025-12-22 01:50:10.642043: val_loss -0.8826\n",
      "2025-12-22 01:50:10.642043: Pseudo dice [0.9321, 0.9588, 0.9482]\n",
      "2025-12-22 01:50:10.642043: Epoch time: 137.82 s\n",
      "2025-12-22 01:50:11.461840: \n",
      "2025-12-22 01:50:11.461840: Epoch 667\n",
      "2025-12-22 01:50:11.465584: Current learning rate: 0.00372\n",
      "2025-12-22 01:52:29.280492: train_loss -0.8513\n",
      "2025-12-22 01:52:29.282232: val_loss -0.8533\n",
      "2025-12-22 01:52:29.288239: Pseudo dice [0.9135, 0.9437, 0.9406]\n",
      "2025-12-22 01:52:29.293983: Epoch time: 137.82 s\n",
      "2025-12-22 01:52:29.932043: \n",
      "2025-12-22 01:52:29.932043: Epoch 668\n",
      "2025-12-22 01:52:29.932043: Current learning rate: 0.00371\n",
      "2025-12-22 01:54:48.070639: train_loss -0.8511\n",
      "2025-12-22 01:54:48.070639: val_loss -0.8732\n",
      "2025-12-22 01:54:48.076383: Pseudo dice [0.926, 0.955, 0.939]\n",
      "2025-12-22 01:54:48.080502: Epoch time: 138.14 s\n",
      "2025-12-22 01:54:48.729387: \n",
      "2025-12-22 01:54:48.729387: Epoch 669\n",
      "2025-12-22 01:54:48.729387: Current learning rate: 0.0037\n",
      "2025-12-22 01:57:06.609147: train_loss -0.8547\n",
      "2025-12-22 01:57:06.609147: val_loss -0.8675\n",
      "2025-12-22 01:57:06.616655: Pseudo dice [0.926, 0.9534, 0.9347]\n",
      "2025-12-22 01:57:06.620659: Epoch time: 137.88 s\n",
      "2025-12-22 01:57:07.276175: \n",
      "2025-12-22 01:57:07.276175: Epoch 670\n",
      "2025-12-22 01:57:07.276175: Current learning rate: 0.00369\n",
      "2025-12-22 01:59:25.167491: train_loss -0.8492\n",
      "2025-12-22 01:59:25.167491: val_loss -0.8619\n",
      "2025-12-22 01:59:25.173497: Pseudo dice [0.9203, 0.9508, 0.9404]\n",
      "2025-12-22 01:59:25.177501: Epoch time: 137.89 s\n",
      "2025-12-22 01:59:25.832327: \n",
      "2025-12-22 01:59:25.832327: Epoch 671\n",
      "2025-12-22 01:59:25.832327: Current learning rate: 0.00368\n",
      "2025-12-22 02:01:43.955209: train_loss -0.8463\n",
      "2025-12-22 02:01:43.955209: val_loss -0.8771\n",
      "2025-12-22 02:01:43.970881: Pseudo dice [0.9292, 0.9595, 0.9368]\n",
      "2025-12-22 02:01:43.970881: Epoch time: 138.12 s\n",
      "2025-12-22 02:01:44.764149: \n",
      "2025-12-22 02:01:44.764149: Epoch 672\n",
      "2025-12-22 02:01:44.780056: Current learning rate: 0.00367\n",
      "2025-12-22 02:04:02.693042: train_loss -0.8524\n",
      "2025-12-22 02:04:02.693042: val_loss -0.8701\n",
      "2025-12-22 02:04:02.699049: Pseudo dice [0.9257, 0.9521, 0.9406]\n",
      "2025-12-22 02:04:02.703053: Epoch time: 137.93 s\n",
      "2025-12-22 02:04:03.357351: \n",
      "2025-12-22 02:04:03.357351: Epoch 673\n",
      "2025-12-22 02:04:03.357351: Current learning rate: 0.00366\n",
      "2025-12-22 02:06:21.252969: train_loss -0.8515\n",
      "2025-12-22 02:06:21.252969: val_loss -0.8839\n",
      "2025-12-22 02:06:21.262044: Pseudo dice [0.9325, 0.9541, 0.9437]\n",
      "2025-12-22 02:06:21.266788: Epoch time: 137.9 s\n",
      "2025-12-22 02:06:21.914562: \n",
      "2025-12-22 02:06:21.914562: Epoch 674\n",
      "2025-12-22 02:06:21.914562: Current learning rate: 0.00365\n",
      "2025-12-22 02:08:40.034693: train_loss -0.8554\n",
      "2025-12-22 02:08:40.036695: val_loss -0.8467\n",
      "2025-12-22 02:08:40.044441: Pseudo dice [0.9088, 0.9423, 0.9344]\n",
      "2025-12-22 02:08:40.048446: Epoch time: 138.12 s\n",
      "2025-12-22 02:08:40.830337: \n",
      "2025-12-22 02:08:40.830337: Epoch 675\n",
      "2025-12-22 02:08:40.830337: Current learning rate: 0.00364\n",
      "2025-12-22 02:10:59.149126: train_loss -0.85\n",
      "2025-12-22 02:10:59.149126: val_loss -0.8727\n",
      "2025-12-22 02:10:59.153130: Pseudo dice [0.9267, 0.9587, 0.9402]\n",
      "2025-12-22 02:10:59.162231: Epoch time: 138.32 s\n",
      "2025-12-22 02:10:59.798774: \n",
      "2025-12-22 02:10:59.798774: Epoch 676\n",
      "2025-12-22 02:10:59.814688: Current learning rate: 0.00363\n",
      "2025-12-22 02:13:17.828670: train_loss -0.8546\n",
      "2025-12-22 02:13:17.830673: val_loss -0.8775\n",
      "2025-12-22 02:13:17.838680: Pseudo dice [0.9304, 0.9564, 0.9396]\n",
      "2025-12-22 02:13:17.846431: Epoch time: 138.03 s\n",
      "2025-12-22 02:13:18.521738: \n",
      "2025-12-22 02:13:18.521738: Epoch 677\n",
      "2025-12-22 02:13:18.521738: Current learning rate: 0.00362\n",
      "2025-12-22 02:15:36.469383: train_loss -0.8532\n",
      "2025-12-22 02:15:36.471387: val_loss -0.8758\n",
      "2025-12-22 02:15:36.476640: Pseudo dice [0.9303, 0.9548, 0.9397]\n",
      "2025-12-22 02:15:36.476640: Epoch time: 137.95 s\n",
      "2025-12-22 02:15:37.218224: \n",
      "2025-12-22 02:15:37.218224: Epoch 678\n",
      "2025-12-22 02:15:37.234091: Current learning rate: 0.00361\n",
      "2025-12-22 02:17:55.304005: train_loss -0.8545\n",
      "2025-12-22 02:17:55.304005: val_loss -0.8785\n",
      "2025-12-22 02:17:55.304005: Pseudo dice [0.9313, 0.9569, 0.9423]\n",
      "2025-12-22 02:17:55.304005: Epoch time: 138.09 s\n",
      "2025-12-22 02:17:55.954843: \n",
      "2025-12-22 02:17:55.954843: Epoch 679\n",
      "2025-12-22 02:17:55.954843: Current learning rate: 0.0036\n",
      "2025-12-22 02:20:13.717736: train_loss -0.8595\n",
      "2025-12-22 02:20:13.717736: val_loss -0.8586\n",
      "2025-12-22 02:20:13.717736: Pseudo dice [0.9157, 0.949, 0.9393]\n",
      "2025-12-22 02:20:13.733543: Epoch time: 137.76 s\n",
      "2025-12-22 02:20:14.370083: \n",
      "2025-12-22 02:20:14.370083: Epoch 680\n",
      "2025-12-22 02:20:14.370083: Current learning rate: 0.00359\n",
      "2025-12-22 02:22:32.234276: train_loss -0.8542\n",
      "2025-12-22 02:22:32.236279: val_loss -0.8764\n",
      "2025-12-22 02:22:32.241523: Pseudo dice [0.9259, 0.9552, 0.9482]\n",
      "2025-12-22 02:22:32.245527: Epoch time: 137.86 s\n",
      "2025-12-22 02:22:32.973195: \n",
      "2025-12-22 02:22:32.973195: Epoch 681\n",
      "2025-12-22 02:22:32.973195: Current learning rate: 0.00358\n",
      "2025-12-22 02:24:51.229970: train_loss -0.8579\n",
      "2025-12-22 02:24:51.229970: val_loss -0.8625\n",
      "2025-12-22 02:24:51.233712: Pseudo dice [0.9212, 0.9535, 0.9323]\n",
      "2025-12-22 02:24:51.233712: Epoch time: 138.25 s\n",
      "2025-12-22 02:24:51.882830: \n",
      "2025-12-22 02:24:51.882830: Epoch 682\n",
      "2025-12-22 02:24:51.898611: Current learning rate: 0.00357\n",
      "2025-12-22 02:27:09.956164: train_loss -0.8548\n",
      "2025-12-22 02:27:09.956164: val_loss -0.8717\n",
      "2025-12-22 02:27:09.962169: Pseudo dice [0.9247, 0.9543, 0.9441]\n",
      "2025-12-22 02:27:09.967913: Epoch time: 138.07 s\n",
      "2025-12-22 02:27:10.622712: \n",
      "2025-12-22 02:27:10.622712: Epoch 683\n",
      "2025-12-22 02:27:10.622712: Current learning rate: 0.00356\n",
      "2025-12-22 02:29:28.594120: train_loss -0.8551\n",
      "2025-12-22 02:29:28.594120: val_loss -0.8645\n",
      "2025-12-22 02:29:28.596122: Pseudo dice [0.9218, 0.9477, 0.9461]\n",
      "2025-12-22 02:29:28.603924: Epoch time: 137.97 s\n",
      "2025-12-22 02:29:29.554317: \n",
      "2025-12-22 02:29:29.554317: Epoch 684\n",
      "2025-12-22 02:29:29.554317: Current learning rate: 0.00355\n",
      "2025-12-22 02:31:47.541542: train_loss -0.852\n",
      "2025-12-22 02:31:47.543544: val_loss -0.8722\n",
      "2025-12-22 02:31:47.545547: Pseudo dice [0.9274, 0.9533, 0.9409]\n",
      "2025-12-22 02:31:47.553714: Epoch time: 138.0 s\n",
      "2025-12-22 02:31:48.202958: \n",
      "2025-12-22 02:31:48.202958: Epoch 685\n",
      "2025-12-22 02:31:48.202958: Current learning rate: 0.00354\n",
      "2025-12-22 02:34:06.131332: train_loss -0.8563\n",
      "2025-12-22 02:34:06.131332: val_loss -0.87\n",
      "2025-12-22 02:34:06.139827: Pseudo dice [0.921, 0.9532, 0.9455]\n",
      "2025-12-22 02:34:06.147115: Epoch time: 137.93 s\n",
      "2025-12-22 02:34:06.794333: \n",
      "2025-12-22 02:34:06.794333: Epoch 686\n",
      "2025-12-22 02:34:06.810205: Current learning rate: 0.00353\n",
      "2025-12-22 02:36:24.751802: train_loss -0.8555\n",
      "2025-12-22 02:36:24.751802: val_loss -0.8757\n",
      "2025-12-22 02:36:24.767532: Pseudo dice [0.9268, 0.9543, 0.9454]\n",
      "2025-12-22 02:36:24.767532: Epoch time: 137.96 s\n",
      "2025-12-22 02:36:25.463464: \n",
      "2025-12-22 02:36:25.463464: Epoch 687\n",
      "2025-12-22 02:36:25.463464: Current learning rate: 0.00352\n",
      "2025-12-22 02:38:43.389579: train_loss -0.8546\n",
      "2025-12-22 02:38:43.391581: val_loss -0.8869\n",
      "2025-12-22 02:38:43.403368: Pseudo dice [0.9348, 0.9606, 0.9396]\n",
      "2025-12-22 02:38:43.407775: Epoch time: 137.93 s\n",
      "2025-12-22 02:38:44.067321: \n",
      "2025-12-22 02:38:44.067321: Epoch 688\n",
      "2025-12-22 02:38:44.067321: Current learning rate: 0.00351\n",
      "2025-12-22 02:41:02.096036: train_loss -0.8551\n",
      "2025-12-22 02:41:02.096036: val_loss -0.8705\n",
      "2025-12-22 02:41:02.102043: Pseudo dice [0.9225, 0.9556, 0.9395]\n",
      "2025-12-22 02:41:02.107650: Epoch time: 138.04 s\n",
      "2025-12-22 02:41:02.808928: \n",
      "2025-12-22 02:41:02.808928: Epoch 689\n",
      "2025-12-22 02:41:02.808928: Current learning rate: 0.0035\n",
      "2025-12-22 02:43:20.886425: train_loss -0.852\n",
      "2025-12-22 02:43:20.886425: val_loss -0.8611\n",
      "2025-12-22 02:43:20.892431: Pseudo dice [0.919, 0.949, 0.9343]\n",
      "2025-12-22 02:43:20.898438: Epoch time: 138.09 s\n",
      "2025-12-22 02:43:21.713375: \n",
      "2025-12-22 02:43:21.713375: Epoch 690\n",
      "2025-12-22 02:43:21.713375: Current learning rate: 0.00349\n",
      "2025-12-22 02:45:39.548953: train_loss -0.8525\n",
      "2025-12-22 02:45:39.548953: val_loss -0.8617\n",
      "2025-12-22 02:45:39.564702: Pseudo dice [0.9174, 0.9498, 0.9377]\n",
      "2025-12-22 02:45:39.564702: Epoch time: 137.84 s\n",
      "2025-12-22 02:45:40.230623: \n",
      "2025-12-22 02:45:40.230623: Epoch 691\n",
      "2025-12-22 02:45:40.230623: Current learning rate: 0.00348\n",
      "2025-12-22 02:47:57.925928: train_loss -0.8599\n",
      "2025-12-22 02:47:57.925928: val_loss -0.8795\n",
      "2025-12-22 02:47:57.925928: Pseudo dice [0.9305, 0.9605, 0.9407]\n",
      "2025-12-22 02:47:57.941649: Epoch time: 137.71 s\n",
      "2025-12-22 02:47:58.575801: \n",
      "2025-12-22 02:47:58.575801: Epoch 692\n",
      "2025-12-22 02:47:58.586810: Current learning rate: 0.00346\n",
      "2025-12-22 02:50:16.447983: train_loss -0.8531\n",
      "2025-12-22 02:50:16.447983: val_loss -0.8673\n",
      "2025-12-22 02:50:16.447983: Pseudo dice [0.9236, 0.9501, 0.9371]\n",
      "2025-12-22 02:50:16.465778: Epoch time: 137.87 s\n",
      "2025-12-22 02:50:17.112059: \n",
      "2025-12-22 02:50:17.112059: Epoch 693\n",
      "2025-12-22 02:50:17.112059: Current learning rate: 0.00345\n",
      "2025-12-22 02:52:34.985271: train_loss -0.8507\n",
      "2025-12-22 02:52:34.985271: val_loss -0.8809\n",
      "2025-12-22 02:52:34.985271: Pseudo dice [0.9319, 0.9555, 0.941]\n",
      "2025-12-22 02:52:34.985271: Epoch time: 137.87 s\n",
      "2025-12-22 02:52:35.634682: \n",
      "2025-12-22 02:52:35.634682: Epoch 694\n",
      "2025-12-22 02:52:35.634682: Current learning rate: 0.00344\n",
      "2025-12-22 02:54:53.565586: train_loss -0.8517\n",
      "2025-12-22 02:54:53.565586: val_loss -0.8775\n",
      "2025-12-22 02:54:53.565586: Pseudo dice [0.9299, 0.9617, 0.9336]\n",
      "2025-12-22 02:54:53.581423: Epoch time: 137.93 s\n",
      "2025-12-22 02:54:54.371119: \n",
      "2025-12-22 02:54:54.371119: Epoch 695\n",
      "2025-12-22 02:54:54.371119: Current learning rate: 0.00343\n",
      "2025-12-22 02:57:12.223473: train_loss -0.8534\n",
      "2025-12-22 02:57:12.223473: val_loss -0.8804\n",
      "2025-12-22 02:57:12.228406: Pseudo dice [0.9299, 0.9606, 0.94]\n",
      "2025-12-22 02:57:12.234412: Epoch time: 137.85 s\n",
      "2025-12-22 02:57:13.062403: \n",
      "2025-12-22 02:57:13.062403: Epoch 696\n",
      "2025-12-22 02:57:13.073807: Current learning rate: 0.00342\n",
      "2025-12-22 02:59:30.900673: train_loss -0.8522\n",
      "2025-12-22 02:59:30.900673: val_loss -0.8807\n",
      "2025-12-22 02:59:30.906418: Pseudo dice [0.9299, 0.9537, 0.9462]\n",
      "2025-12-22 02:59:30.910583: Epoch time: 137.84 s\n",
      "2025-12-22 02:59:31.552975: \n",
      "2025-12-22 02:59:31.552975: Epoch 697\n",
      "2025-12-22 02:59:31.568737: Current learning rate: 0.00341\n",
      "2025-12-22 03:01:49.458650: train_loss -0.8557\n",
      "2025-12-22 03:01:49.458650: val_loss -0.8686\n",
      "2025-12-22 03:01:49.466397: Pseudo dice [0.9235, 0.9562, 0.9374]\n",
      "2025-12-22 03:01:49.474149: Epoch time: 137.91 s\n",
      "2025-12-22 03:01:50.284595: \n",
      "2025-12-22 03:01:50.284595: Epoch 698\n",
      "2025-12-22 03:01:50.284595: Current learning rate: 0.0034\n",
      "2025-12-22 03:04:08.169424: train_loss -0.8565\n",
      "2025-12-22 03:04:08.169424: val_loss -0.8742\n",
      "2025-12-22 03:04:08.177432: Pseudo dice [0.9254, 0.9512, 0.941]\n",
      "2025-12-22 03:04:08.183378: Epoch time: 137.88 s\n",
      "2025-12-22 03:04:08.832474: \n",
      "2025-12-22 03:04:08.832474: Epoch 699\n",
      "2025-12-22 03:04:08.832474: Current learning rate: 0.00339\n",
      "2025-12-22 03:06:26.889152: train_loss -0.8532\n",
      "2025-12-22 03:06:26.889152: val_loss -0.8702\n",
      "2025-12-22 03:06:26.894896: Pseudo dice [0.9256, 0.9515, 0.939]\n",
      "2025-12-22 03:06:26.900394: Epoch time: 138.06 s\n",
      "2025-12-22 03:06:27.812209: \n",
      "2025-12-22 03:06:27.812209: Epoch 700\n",
      "2025-12-22 03:06:27.819671: Current learning rate: 0.00338\n",
      "2025-12-22 03:08:45.905268: train_loss -0.8542\n",
      "2025-12-22 03:08:45.907270: val_loss -0.8807\n",
      "2025-12-22 03:08:45.913014: Pseudo dice [0.9312, 0.9566, 0.9414]\n",
      "2025-12-22 03:08:45.917018: Epoch time: 138.09 s\n",
      "2025-12-22 03:08:46.677697: \n",
      "2025-12-22 03:08:46.677697: Epoch 701\n",
      "2025-12-22 03:08:46.677697: Current learning rate: 0.00337\n",
      "2025-12-22 03:11:04.889113: train_loss -0.85\n",
      "2025-12-22 03:11:04.890910: val_loss -0.8928\n",
      "2025-12-22 03:11:04.896917: Pseudo dice [0.9421, 0.9635, 0.9423]\n",
      "2025-12-22 03:11:04.900922: Epoch time: 138.21 s\n",
      "2025-12-22 03:11:05.602534: \n",
      "2025-12-22 03:11:05.602534: Epoch 702\n",
      "2025-12-22 03:11:05.611452: Current learning rate: 0.00336\n",
      "2025-12-22 03:13:23.746785: train_loss -0.8548\n",
      "2025-12-22 03:13:23.748787: val_loss -0.8482\n",
      "2025-12-22 03:13:23.754793: Pseudo dice [0.9064, 0.9406, 0.9412]\n",
      "2025-12-22 03:13:23.762542: Epoch time: 138.14 s\n",
      "2025-12-22 03:13:24.408852: \n",
      "2025-12-22 03:13:24.408852: Epoch 703\n",
      "2025-12-22 03:13:24.424790: Current learning rate: 0.00335\n",
      "2025-12-22 03:15:42.422636: train_loss -0.8543\n",
      "2025-12-22 03:15:42.422636: val_loss -0.8751\n",
      "2025-12-22 03:15:42.440721: Pseudo dice [0.9281, 0.9568, 0.9402]\n",
      "2025-12-22 03:15:42.444725: Epoch time: 138.01 s\n",
      "2025-12-22 03:15:43.199191: \n",
      "2025-12-22 03:15:43.199191: Epoch 704\n",
      "2025-12-22 03:15:43.214890: Current learning rate: 0.00334\n",
      "2025-12-22 03:18:01.249511: train_loss -0.8557\n",
      "2025-12-22 03:18:01.249511: val_loss -0.8651\n",
      "2025-12-22 03:18:01.253515: Pseudo dice [0.9185, 0.948, 0.9419]\n",
      "2025-12-22 03:18:01.259456: Epoch time: 138.05 s\n",
      "2025-12-22 03:18:01.940958: \n",
      "2025-12-22 03:18:01.940958: Epoch 705\n",
      "2025-12-22 03:18:01.940958: Current learning rate: 0.00333\n",
      "2025-12-22 03:20:19.704291: train_loss -0.8609\n",
      "2025-12-22 03:20:19.704291: val_loss -0.8789\n",
      "2025-12-22 03:20:19.713302: Pseudo dice [0.9298, 0.9541, 0.947]\n",
      "2025-12-22 03:20:19.721052: Epoch time: 137.76 s\n",
      "2025-12-22 03:20:20.372752: \n",
      "2025-12-22 03:20:20.372752: Epoch 706\n",
      "2025-12-22 03:20:20.372752: Current learning rate: 0.00332\n",
      "2025-12-22 03:22:38.401258: train_loss -0.8544\n",
      "2025-12-22 03:22:38.401258: val_loss -0.8815\n",
      "2025-12-22 03:22:38.411051: Pseudo dice [0.9299, 0.9562, 0.9463]\n",
      "2025-12-22 03:22:38.417062: Epoch time: 138.03 s\n",
      "2025-12-22 03:22:39.184130: \n",
      "2025-12-22 03:22:39.184130: Epoch 707\n",
      "2025-12-22 03:22:39.184130: Current learning rate: 0.00331\n",
      "2025-12-22 03:24:56.995249: train_loss -0.8559\n",
      "2025-12-22 03:24:56.995249: val_loss -0.8831\n",
      "2025-12-22 03:24:57.002229: Pseudo dice [0.9341, 0.9563, 0.9424]\n",
      "2025-12-22 03:24:57.007235: Epoch time: 137.81 s\n",
      "2025-12-22 03:24:57.812435: \n",
      "2025-12-22 03:24:57.812435: Epoch 708\n",
      "2025-12-22 03:24:57.828265: Current learning rate: 0.0033\n",
      "2025-12-22 03:27:15.701309: train_loss -0.8547\n",
      "2025-12-22 03:27:15.703311: val_loss -0.8788\n",
      "2025-12-22 03:27:15.711059: Pseudo dice [0.932, 0.9574, 0.9373]\n",
      "2025-12-22 03:27:15.717066: Epoch time: 137.89 s\n",
      "2025-12-22 03:27:16.417654: \n",
      "2025-12-22 03:27:16.417654: Epoch 709\n",
      "2025-12-22 03:27:16.433775: Current learning rate: 0.00329\n",
      "2025-12-22 03:29:34.404319: train_loss -0.8513\n",
      "2025-12-22 03:29:34.406320: val_loss -0.8781\n",
      "2025-12-22 03:29:34.412326: Pseudo dice [0.9314, 0.9544, 0.9434]\n",
      "2025-12-22 03:29:34.416331: Epoch time: 137.99 s\n",
      "2025-12-22 03:29:35.069429: \n",
      "2025-12-22 03:29:35.069429: Epoch 710\n",
      "2025-12-22 03:29:35.078073: Current learning rate: 0.00328\n",
      "2025-12-22 03:31:52.940218: train_loss -0.8517\n",
      "2025-12-22 03:31:52.940218: val_loss -0.8753\n",
      "2025-12-22 03:31:52.948226: Pseudo dice [0.9261, 0.9593, 0.9422]\n",
      "2025-12-22 03:31:52.953970: Epoch time: 137.87 s\n",
      "2025-12-22 03:31:53.601627: \n",
      "2025-12-22 03:31:53.601627: Epoch 711\n",
      "2025-12-22 03:31:53.617409: Current learning rate: 0.00327\n",
      "2025-12-22 03:34:11.565405: train_loss -0.8501\n",
      "2025-12-22 03:34:11.565405: val_loss -0.874\n",
      "2025-12-22 03:34:11.571411: Pseudo dice [0.9259, 0.9548, 0.944]\n",
      "2025-12-22 03:34:11.576893: Epoch time: 137.96 s\n",
      "2025-12-22 03:34:12.303762: \n",
      "2025-12-22 03:34:12.303762: Epoch 712\n",
      "2025-12-22 03:34:12.308688: Current learning rate: 0.00326\n",
      "2025-12-22 03:36:30.317484: train_loss -0.8567\n",
      "2025-12-22 03:36:30.319486: val_loss -0.8765\n",
      "2025-12-22 03:36:30.325491: Pseudo dice [0.9262, 0.9557, 0.9422]\n",
      "2025-12-22 03:36:30.329233: Epoch time: 138.01 s\n",
      "2025-12-22 03:36:31.015341: \n",
      "2025-12-22 03:36:31.015341: Epoch 713\n",
      "2025-12-22 03:36:31.015341: Current learning rate: 0.00325\n",
      "2025-12-22 03:38:49.012972: train_loss -0.8562\n",
      "2025-12-22 03:38:49.012972: val_loss -0.8776\n",
      "2025-12-22 03:38:49.030438: Pseudo dice [0.9248, 0.9557, 0.9422]\n",
      "2025-12-22 03:38:49.030438: Epoch time: 138.0 s\n",
      "2025-12-22 03:38:49.853793: \n",
      "2025-12-22 03:38:49.853793: Epoch 714\n",
      "2025-12-22 03:38:49.865425: Current learning rate: 0.00324\n",
      "2025-12-22 03:41:07.857295: train_loss -0.855\n",
      "2025-12-22 03:41:07.857295: val_loss -0.8686\n",
      "2025-12-22 03:41:07.865042: Pseudo dice [0.9228, 0.9478, 0.9456]\n",
      "2025-12-22 03:41:07.871049: Epoch time: 138.0 s\n",
      "2025-12-22 03:41:08.517257: \n",
      "2025-12-22 03:41:08.517257: Epoch 715\n",
      "2025-12-22 03:41:08.517257: Current learning rate: 0.00323\n",
      "2025-12-22 03:43:26.451504: train_loss -0.8548\n",
      "2025-12-22 03:43:26.451504: val_loss -0.8701\n",
      "2025-12-22 03:43:26.467318: Pseudo dice [0.9257, 0.9574, 0.9302]\n",
      "2025-12-22 03:43:26.469486: Epoch time: 137.93 s\n",
      "2025-12-22 03:43:27.102762: \n",
      "2025-12-22 03:43:27.102762: Epoch 716\n",
      "2025-12-22 03:43:27.117361: Current learning rate: 0.00322\n",
      "2025-12-22 03:45:45.138753: train_loss -0.853\n",
      "2025-12-22 03:45:45.138753: val_loss -0.8629\n",
      "2025-12-22 03:45:45.147268: Pseudo dice [0.9226, 0.9514, 0.9371]\n",
      "2025-12-22 03:45:45.153274: Epoch time: 138.04 s\n",
      "2025-12-22 03:45:45.841764: \n",
      "2025-12-22 03:45:45.841764: Epoch 717\n",
      "2025-12-22 03:45:45.841764: Current learning rate: 0.00321\n",
      "2025-12-22 03:48:03.770503: train_loss -0.8566\n",
      "2025-12-22 03:48:03.770503: val_loss -0.8654\n",
      "2025-12-22 03:48:03.776510: Pseudo dice [0.9225, 0.9525, 0.9381]\n",
      "2025-12-22 03:48:03.780514: Epoch time: 137.93 s\n",
      "2025-12-22 03:48:04.512967: \n",
      "2025-12-22 03:48:04.512967: Epoch 718\n",
      "2025-12-22 03:48:04.518313: Current learning rate: 0.0032\n",
      "2025-12-22 03:50:22.373031: train_loss -0.8565\n",
      "2025-12-22 03:50:22.373031: val_loss -0.8861\n",
      "2025-12-22 03:50:22.373031: Pseudo dice [0.9343, 0.9609, 0.9431]\n",
      "2025-12-22 03:50:22.373031: Epoch time: 137.86 s\n",
      "2025-12-22 03:50:23.098006: \n",
      "2025-12-22 03:50:23.098006: Epoch 719\n",
      "2025-12-22 03:50:23.100864: Current learning rate: 0.00319\n",
      "2025-12-22 03:52:41.108930: train_loss -0.8526\n",
      "2025-12-22 03:52:41.108930: val_loss -0.8836\n",
      "2025-12-22 03:52:41.124783: Pseudo dice [0.9342, 0.9583, 0.9418]\n",
      "2025-12-22 03:52:41.124783: Epoch time: 138.03 s\n",
      "2025-12-22 03:52:41.934446: \n",
      "2025-12-22 03:52:41.934446: Epoch 720\n",
      "2025-12-22 03:52:41.951736: Current learning rate: 0.00318\n",
      "2025-12-22 03:54:59.882560: train_loss -0.8491\n",
      "2025-12-22 03:54:59.882560: val_loss -0.8722\n",
      "2025-12-22 03:54:59.898452: Pseudo dice [0.9257, 0.9533, 0.9428]\n",
      "2025-12-22 03:54:59.898452: Epoch time: 137.95 s\n",
      "2025-12-22 03:55:00.707142: \n",
      "2025-12-22 03:55:00.707142: Epoch 721\n",
      "2025-12-22 03:55:00.716424: Current learning rate: 0.00317\n",
      "2025-12-22 03:57:18.570202: train_loss -0.86\n",
      "2025-12-22 03:57:18.570202: val_loss -0.8879\n",
      "2025-12-22 03:57:18.585904: Pseudo dice [0.9339, 0.9624, 0.942]\n",
      "2025-12-22 03:57:18.585904: Epoch time: 137.86 s\n",
      "2025-12-22 03:57:19.234333: \n",
      "2025-12-22 03:57:19.234333: Epoch 722\n",
      "2025-12-22 03:57:19.234333: Current learning rate: 0.00316\n",
      "2025-12-22 03:59:37.099791: train_loss -0.8552\n",
      "2025-12-22 03:59:37.101794: val_loss -0.8732\n",
      "2025-12-22 03:59:37.107538: Pseudo dice [0.9243, 0.9555, 0.9401]\n",
      "2025-12-22 03:59:37.113544: Epoch time: 137.87 s\n",
      "2025-12-22 03:59:37.770129: \n",
      "2025-12-22 03:59:37.770129: Epoch 723\n",
      "2025-12-22 03:59:37.770129: Current learning rate: 0.00315\n",
      "2025-12-22 04:01:55.689904: train_loss -0.8517\n",
      "2025-12-22 04:01:55.689904: val_loss -0.8774\n",
      "2025-12-22 04:01:55.697635: Pseudo dice [0.9278, 0.9557, 0.9425]\n",
      "2025-12-22 04:01:55.701640: Epoch time: 137.92 s\n",
      "2025-12-22 04:01:56.455043: \n",
      "2025-12-22 04:01:56.455043: Epoch 724\n",
      "2025-12-22 04:01:56.470798: Current learning rate: 0.00314\n",
      "2025-12-22 04:04:14.410331: train_loss -0.8579\n",
      "2025-12-22 04:04:14.410331: val_loss -0.8708\n",
      "2025-12-22 04:04:14.416337: Pseudo dice [0.9266, 0.9546, 0.9399]\n",
      "2025-12-22 04:04:14.422344: Epoch time: 137.96 s\n",
      "2025-12-22 04:04:15.069208: \n",
      "2025-12-22 04:04:15.085182: Epoch 725\n",
      "2025-12-22 04:04:15.087075: Current learning rate: 0.00313\n",
      "2025-12-22 04:06:33.242154: train_loss -0.851\n",
      "2025-12-22 04:06:33.252174: val_loss -0.8694\n",
      "2025-12-22 04:06:33.252174: Pseudo dice [0.9259, 0.9524, 0.9424]\n",
      "2025-12-22 04:06:33.252174: Epoch time: 138.17 s\n",
      "2025-12-22 04:06:33.903325: \n",
      "2025-12-22 04:06:33.903325: Epoch 726\n",
      "2025-12-22 04:06:33.903325: Current learning rate: 0.00312\n",
      "2025-12-22 04:08:52.205573: train_loss -0.8536\n",
      "2025-12-22 04:08:52.205573: val_loss -0.8704\n",
      "2025-12-22 04:08:52.211272: Pseudo dice [0.9237, 0.9553, 0.9467]\n",
      "2025-12-22 04:08:52.218771: Epoch time: 138.3 s\n",
      "2025-12-22 04:08:52.966732: \n",
      "2025-12-22 04:08:52.966732: Epoch 727\n",
      "2025-12-22 04:08:52.970815: Current learning rate: 0.00311\n",
      "2025-12-22 04:11:11.185755: train_loss -0.853\n",
      "2025-12-22 04:11:11.185755: val_loss -0.8791\n",
      "2025-12-22 04:11:11.199851: Pseudo dice [0.9309, 0.9556, 0.9441]\n",
      "2025-12-22 04:11:11.199851: Epoch time: 138.22 s\n",
      "2025-12-22 04:11:11.864891: \n",
      "2025-12-22 04:11:11.864891: Epoch 728\n",
      "2025-12-22 04:11:11.864891: Current learning rate: 0.0031\n",
      "2025-12-22 04:13:29.827852: train_loss -0.8591\n",
      "2025-12-22 04:13:29.827852: val_loss -0.8762\n",
      "2025-12-22 04:13:29.833858: Pseudo dice [0.9272, 0.956, 0.9426]\n",
      "2025-12-22 04:13:29.837600: Epoch time: 137.96 s\n",
      "2025-12-22 04:13:30.491096: \n",
      "2025-12-22 04:13:30.491096: Epoch 729\n",
      "2025-12-22 04:13:30.491096: Current learning rate: 0.00309\n",
      "2025-12-22 04:15:48.498650: train_loss -0.8504\n",
      "2025-12-22 04:15:48.498650: val_loss -0.869\n",
      "2025-12-22 04:15:48.506135: Pseudo dice [0.9209, 0.952, 0.9343]\n",
      "2025-12-22 04:15:48.512738: Epoch time: 138.02 s\n",
      "2025-12-22 04:15:49.157201: \n",
      "2025-12-22 04:15:49.157201: Epoch 730\n",
      "2025-12-22 04:15:49.157201: Current learning rate: 0.00308\n",
      "2025-12-22 04:18:07.079018: train_loss -0.8523\n",
      "2025-12-22 04:18:07.079018: val_loss -0.8878\n",
      "2025-12-22 04:18:07.081021: Pseudo dice [0.9352, 0.962, 0.9407]\n",
      "2025-12-22 04:18:07.090884: Epoch time: 137.92 s\n",
      "2025-12-22 04:18:07.743232: \n",
      "2025-12-22 04:18:07.743232: Epoch 731\n",
      "2025-12-22 04:18:07.758997: Current learning rate: 0.00307\n",
      "2025-12-22 04:20:25.667367: train_loss -0.8569\n",
      "2025-12-22 04:20:25.667367: val_loss -0.8694\n",
      "2025-12-22 04:20:25.674460: Pseudo dice [0.9228, 0.9519, 0.9385]\n",
      "2025-12-22 04:20:25.674460: Epoch time: 137.92 s\n",
      "2025-12-22 04:20:26.556020: \n",
      "2025-12-22 04:20:26.556020: Epoch 732\n",
      "2025-12-22 04:20:26.571762: Current learning rate: 0.00306\n",
      "2025-12-22 04:22:44.522094: train_loss -0.8529\n",
      "2025-12-22 04:22:44.522094: val_loss -0.8816\n",
      "2025-12-22 04:22:44.522094: Pseudo dice [0.9341, 0.96, 0.9423]\n",
      "2025-12-22 04:22:44.522094: Epoch time: 137.97 s\n",
      "2025-12-22 04:22:45.173545: \n",
      "2025-12-22 04:22:45.173545: Epoch 733\n",
      "2025-12-22 04:22:45.189307: Current learning rate: 0.00305\n",
      "2025-12-22 04:25:03.388798: train_loss -0.8492\n",
      "2025-12-22 04:25:03.390800: val_loss -0.8821\n",
      "2025-12-22 04:25:03.396545: Pseudo dice [0.9292, 0.9552, 0.9539]\n",
      "2025-12-22 04:25:03.402552: Epoch time: 138.22 s\n",
      "2025-12-22 04:25:04.138227: \n",
      "2025-12-22 04:25:04.138227: Epoch 734\n",
      "2025-12-22 04:25:04.154136: Current learning rate: 0.00304\n",
      "2025-12-22 04:27:22.141192: train_loss -0.8587\n",
      "2025-12-22 04:27:22.141192: val_loss -0.8768\n",
      "2025-12-22 04:27:22.158901: Pseudo dice [0.9288, 0.9572, 0.9437]\n",
      "2025-12-22 04:27:22.164907: Epoch time: 138.0 s\n",
      "2025-12-22 04:27:22.822396: \n",
      "2025-12-22 04:27:22.822396: Epoch 735\n",
      "2025-12-22 04:27:22.838365: Current learning rate: 0.00303\n",
      "2025-12-22 04:29:40.695619: train_loss -0.854\n",
      "2025-12-22 04:29:40.695619: val_loss -0.8921\n",
      "2025-12-22 04:29:40.695619: Pseudo dice [0.9375, 0.964, 0.9463]\n",
      "2025-12-22 04:29:40.711297: Epoch time: 137.87 s\n",
      "2025-12-22 04:29:40.711297: Yayy! New best EMA pseudo Dice: 0.9428\n",
      "2025-12-22 04:29:41.644639: \n",
      "2025-12-22 04:29:41.644639: Epoch 736\n",
      "2025-12-22 04:29:41.644639: Current learning rate: 0.00302\n",
      "2025-12-22 04:31:59.608574: train_loss -0.8531\n",
      "2025-12-22 04:31:59.608574: val_loss -0.8887\n",
      "2025-12-22 04:31:59.624430: Pseudo dice [0.9376, 0.9645, 0.9441]\n",
      "2025-12-22 04:31:59.624430: Epoch time: 137.98 s\n",
      "2025-12-22 04:31:59.624430: Yayy! New best EMA pseudo Dice: 0.9434\n",
      "2025-12-22 04:32:00.560676: \n",
      "2025-12-22 04:32:00.560676: Epoch 737\n",
      "2025-12-22 04:32:00.576521: Current learning rate: 0.00301\n",
      "2025-12-22 04:34:18.456606: train_loss -0.8568\n",
      "2025-12-22 04:34:18.456606: val_loss -0.8825\n",
      "2025-12-22 04:34:18.456606: Pseudo dice [0.9297, 0.9579, 0.9455]\n",
      "2025-12-22 04:34:18.464928: Epoch time: 137.9 s\n",
      "2025-12-22 04:34:18.464928: Yayy! New best EMA pseudo Dice: 0.9435\n",
      "2025-12-22 04:34:19.594061: \n",
      "2025-12-22 04:34:19.594061: Epoch 738\n",
      "2025-12-22 04:34:19.596063: Current learning rate: 0.003\n",
      "2025-12-22 04:36:37.754931: train_loss -0.8538\n",
      "2025-12-22 04:36:37.754931: val_loss -0.8849\n",
      "2025-12-22 04:36:37.756935: Pseudo dice [0.9367, 0.9621, 0.9381]\n",
      "2025-12-22 04:36:37.756935: Epoch time: 138.16 s\n",
      "2025-12-22 04:36:37.756935: Yayy! New best EMA pseudo Dice: 0.9437\n",
      "2025-12-22 04:36:38.745351: \n",
      "2025-12-22 04:36:38.745351: Epoch 739\n",
      "2025-12-22 04:36:38.747093: Current learning rate: 0.00299\n",
      "2025-12-22 04:38:56.805083: train_loss -0.8575\n",
      "2025-12-22 04:38:56.807085: val_loss -0.8646\n",
      "2025-12-22 04:38:56.814326: Pseudo dice [0.9187, 0.9523, 0.9456]\n",
      "2025-12-22 04:38:56.818330: Epoch time: 138.06 s\n",
      "2025-12-22 04:38:57.481443: \n",
      "2025-12-22 04:38:57.481443: Epoch 740\n",
      "2025-12-22 04:38:57.486726: Current learning rate: 0.00297\n",
      "2025-12-22 04:41:15.572710: train_loss -0.8564\n",
      "2025-12-22 04:41:15.572710: val_loss -0.8719\n",
      "2025-12-22 04:41:15.580719: Pseudo dice [0.9241, 0.9522, 0.9454]\n",
      "2025-12-22 04:41:15.586463: Epoch time: 138.09 s\n",
      "2025-12-22 04:41:16.295465: \n",
      "2025-12-22 04:41:16.295465: Epoch 741\n",
      "2025-12-22 04:41:16.295465: Current learning rate: 0.00296\n",
      "2025-12-22 04:43:34.268062: train_loss -0.8581\n",
      "2025-12-22 04:43:34.268062: val_loss -0.8732\n",
      "2025-12-22 04:43:34.287640: Pseudo dice [0.9255, 0.9538, 0.9472]\n",
      "2025-12-22 04:43:34.291644: Epoch time: 137.97 s\n",
      "2025-12-22 04:43:34.996113: \n",
      "2025-12-22 04:43:34.996113: Epoch 742\n",
      "2025-12-22 04:43:34.998116: Current learning rate: 0.00295\n",
      "2025-12-22 04:45:53.134762: train_loss -0.854\n",
      "2025-12-22 04:45:53.134762: val_loss -0.8709\n",
      "2025-12-22 04:45:53.142419: Pseudo dice [0.9258, 0.9524, 0.9497]\n",
      "2025-12-22 04:45:53.148425: Epoch time: 138.14 s\n",
      "2025-12-22 04:45:53.958216: \n",
      "2025-12-22 04:45:53.958216: Epoch 743\n",
      "2025-12-22 04:45:53.958216: Current learning rate: 0.00294\n",
      "2025-12-22 04:48:11.847335: train_loss -0.8563\n",
      "2025-12-22 04:48:11.847335: val_loss -0.8769\n",
      "2025-12-22 04:48:11.863086: Pseudo dice [0.9279, 0.9549, 0.9464]\n",
      "2025-12-22 04:48:11.863086: Epoch time: 137.89 s\n",
      "2025-12-22 04:48:12.514644: \n",
      "2025-12-22 04:48:12.514644: Epoch 744\n",
      "2025-12-22 04:48:12.514644: Current learning rate: 0.00293\n",
      "2025-12-22 04:50:30.371578: train_loss -0.8534\n",
      "2025-12-22 04:50:30.371578: val_loss -0.866\n",
      "2025-12-22 04:50:30.387634: Pseudo dice [0.9156, 0.9538, 0.9468]\n",
      "2025-12-22 04:50:30.395187: Epoch time: 137.86 s\n",
      "2025-12-22 04:50:31.052876: \n",
      "2025-12-22 04:50:31.052876: Epoch 745\n",
      "2025-12-22 04:50:31.052876: Current learning rate: 0.00292\n",
      "2025-12-22 04:52:48.878254: train_loss -0.8575\n",
      "2025-12-22 04:52:48.878254: val_loss -0.8874\n",
      "2025-12-22 04:52:48.888004: Pseudo dice [0.935, 0.9622, 0.9481]\n",
      "2025-12-22 04:52:48.895752: Epoch time: 137.83 s\n",
      "2025-12-22 04:52:49.548977: \n",
      "2025-12-22 04:52:49.548977: Epoch 746\n",
      "2025-12-22 04:52:49.548977: Current learning rate: 0.00291\n",
      "2025-12-22 04:55:07.506163: train_loss -0.8529\n",
      "2025-12-22 04:55:07.506163: val_loss -0.8913\n",
      "2025-12-22 04:55:07.512171: Pseudo dice [0.9406, 0.9644, 0.939]\n",
      "2025-12-22 04:55:07.516176: Epoch time: 137.96 s\n",
      "2025-12-22 04:55:08.283026: \n",
      "2025-12-22 04:55:08.283026: Epoch 747\n",
      "2025-12-22 04:55:08.283026: Current learning rate: 0.0029\n",
      "2025-12-22 04:57:26.215366: train_loss -0.859\n",
      "2025-12-22 04:57:26.217369: val_loss -0.8795\n",
      "2025-12-22 04:57:26.227386: Pseudo dice [0.9307, 0.9581, 0.9441]\n",
      "2025-12-22 04:57:26.233130: Epoch time: 137.93 s\n",
      "2025-12-22 04:57:26.889094: \n",
      "2025-12-22 04:57:26.889094: Epoch 748\n",
      "2025-12-22 04:57:26.889094: Current learning rate: 0.00289\n",
      "2025-12-22 04:59:44.854905: train_loss -0.8503\n",
      "2025-12-22 04:59:44.856908: val_loss -0.8805\n",
      "2025-12-22 04:59:44.861379: Pseudo dice [0.9321, 0.9579, 0.9389]\n",
      "2025-12-22 04:59:44.861379: Epoch time: 137.97 s\n",
      "2025-12-22 04:59:45.684533: \n",
      "2025-12-22 04:59:45.684533: Epoch 749\n",
      "2025-12-22 04:59:45.684533: Current learning rate: 0.00288\n",
      "2025-12-22 05:02:03.559279: train_loss -0.86\n",
      "2025-12-22 05:02:03.559279: val_loss -0.8804\n",
      "2025-12-22 05:02:03.575028: Pseudo dice [0.9297, 0.9583, 0.9399]\n",
      "2025-12-22 05:02:03.575028: Epoch time: 137.88 s\n",
      "2025-12-22 05:02:04.678001: \n",
      "2025-12-22 05:02:04.678001: Epoch 750\n",
      "2025-12-22 05:02:04.681396: Current learning rate: 0.00287\n",
      "2025-12-22 05:04:22.641504: train_loss -0.8571\n",
      "2025-12-22 05:04:22.641504: val_loss -0.8719\n",
      "2025-12-22 05:04:22.650664: Pseudo dice [0.9235, 0.9557, 0.9434]\n",
      "2025-12-22 05:04:22.656671: Epoch time: 137.98 s\n",
      "2025-12-22 05:04:23.312343: \n",
      "2025-12-22 05:04:23.314345: Epoch 751\n",
      "2025-12-22 05:04:23.316553: Current learning rate: 0.00286\n",
      "2025-12-22 05:06:41.174544: train_loss -0.8595\n",
      "2025-12-22 05:06:41.174544: val_loss -0.8677\n",
      "2025-12-22 05:06:41.182164: Pseudo dice [0.9204, 0.954, 0.9416]\n",
      "2025-12-22 05:06:41.186168: Epoch time: 137.86 s\n",
      "2025-12-22 05:06:41.843044: \n",
      "2025-12-22 05:06:41.843044: Epoch 752\n",
      "2025-12-22 05:06:41.843044: Current learning rate: 0.00285\n",
      "2025-12-22 05:08:59.909691: train_loss -0.8594\n",
      "2025-12-22 05:08:59.911693: val_loss -0.8869\n",
      "2025-12-22 05:08:59.919702: Pseudo dice [0.9349, 0.96, 0.9428]\n",
      "2025-12-22 05:08:59.925709: Epoch time: 138.07 s\n",
      "2025-12-22 05:09:00.724816: \n",
      "2025-12-22 05:09:00.724816: Epoch 753\n",
      "2025-12-22 05:09:00.724816: Current learning rate: 0.00284\n",
      "2025-12-22 05:11:19.052457: train_loss -0.8611\n",
      "2025-12-22 05:11:19.052457: val_loss -0.877\n",
      "2025-12-22 05:11:19.058463: Pseudo dice [0.9268, 0.9554, 0.9437]\n",
      "2025-12-22 05:11:19.062468: Epoch time: 138.33 s\n",
      "2025-12-22 05:11:19.716049: \n",
      "2025-12-22 05:11:19.716049: Epoch 754\n",
      "2025-12-22 05:11:19.716049: Current learning rate: 0.00283\n",
      "2025-12-22 05:13:37.840485: train_loss -0.8578\n",
      "2025-12-22 05:13:37.840485: val_loss -0.885\n",
      "2025-12-22 05:13:37.848405: Pseudo dice [0.9335, 0.9597, 0.9431]\n",
      "2025-12-22 05:13:37.854411: Epoch time: 138.12 s\n",
      "2025-12-22 05:13:38.664168: \n",
      "2025-12-22 05:13:38.664168: Epoch 755\n",
      "2025-12-22 05:13:38.680138: Current learning rate: 0.00282\n",
      "2025-12-22 05:15:56.540751: train_loss -0.8643\n",
      "2025-12-22 05:15:56.540751: val_loss -0.8938\n",
      "2025-12-22 05:15:56.540751: Pseudo dice [0.9359, 0.9636, 0.9502]\n",
      "2025-12-22 05:15:56.540751: Epoch time: 137.88 s\n",
      "2025-12-22 05:15:56.556519: Yayy! New best EMA pseudo Dice: 0.9439\n",
      "2025-12-22 05:15:57.655391: \n",
      "2025-12-22 05:15:57.657394: Epoch 756\n",
      "2025-12-22 05:15:57.663193: Current learning rate: 0.00281\n",
      "2025-12-22 05:18:15.364626: train_loss -0.861\n",
      "2025-12-22 05:18:15.365628: val_loss -0.8765\n",
      "2025-12-22 05:18:15.373573: Pseudo dice [0.9296, 0.9576, 0.9389]\n",
      "2025-12-22 05:18:15.377578: Epoch time: 137.71 s\n",
      "2025-12-22 05:18:16.041732: \n",
      "2025-12-22 05:18:16.041732: Epoch 757\n",
      "2025-12-22 05:18:16.041732: Current learning rate: 0.0028\n",
      "2025-12-22 05:20:34.075479: train_loss -0.8531\n",
      "2025-12-22 05:20:34.075479: val_loss -0.8905\n",
      "2025-12-22 05:20:34.095152: Pseudo dice [0.9379, 0.964, 0.9456]\n",
      "2025-12-22 05:20:34.099157: Epoch time: 138.04 s\n",
      "2025-12-22 05:20:34.104901: Yayy! New best EMA pseudo Dice: 0.9442\n",
      "2025-12-22 05:20:35.024275: \n",
      "2025-12-22 05:20:35.024275: Epoch 758\n",
      "2025-12-22 05:20:35.044348: Current learning rate: 0.00279\n",
      "2025-12-22 05:22:52.951247: train_loss -0.8574\n",
      "2025-12-22 05:22:52.951247: val_loss -0.8785\n",
      "2025-12-22 05:22:52.963186: Pseudo dice [0.9315, 0.9592, 0.939]\n",
      "2025-12-22 05:22:52.967190: Epoch time: 137.93 s\n",
      "2025-12-22 05:22:53.734592: \n",
      "2025-12-22 05:22:53.734592: Epoch 759\n",
      "2025-12-22 05:22:53.750674: Current learning rate: 0.00278\n",
      "2025-12-22 05:25:11.745105: train_loss -0.8563\n",
      "2025-12-22 05:25:11.745105: val_loss -0.8836\n",
      "2025-12-22 05:25:11.751110: Pseudo dice [0.9315, 0.9608, 0.9425]\n",
      "2025-12-22 05:25:11.757237: Epoch time: 138.01 s\n",
      "2025-12-22 05:25:12.407234: \n",
      "2025-12-22 05:25:12.407234: Epoch 760\n",
      "2025-12-22 05:25:12.407234: Current learning rate: 0.00277\n",
      "2025-12-22 05:27:30.388370: train_loss -0.8585\n",
      "2025-12-22 05:27:30.390373: val_loss -0.8857\n",
      "2025-12-22 05:27:30.400392: Pseudo dice [0.9319, 0.9579, 0.9486]\n",
      "2025-12-22 05:27:30.406139: Epoch time: 137.98 s\n",
      "2025-12-22 05:27:30.414153: Yayy! New best EMA pseudo Dice: 0.9444\n",
      "2025-12-22 05:27:31.526138: \n",
      "2025-12-22 05:27:31.526138: Epoch 761\n",
      "2025-12-22 05:27:31.537838: Current learning rate: 0.00276\n",
      "2025-12-22 05:29:49.377275: train_loss -0.8594\n",
      "2025-12-22 05:29:49.377275: val_loss -0.8809\n",
      "2025-12-22 05:29:49.389074: Pseudo dice [0.9299, 0.9598, 0.942]\n",
      "2025-12-22 05:29:49.389074: Epoch time: 137.85 s\n",
      "2025-12-22 05:29:50.187867: \n",
      "2025-12-22 05:29:50.187867: Epoch 762\n",
      "2025-12-22 05:29:50.189871: Current learning rate: 0.00275\n",
      "2025-12-22 05:32:07.990030: train_loss -0.859\n",
      "2025-12-22 05:32:07.992037: val_loss -0.8786\n",
      "2025-12-22 05:32:08.001791: Pseudo dice [0.9312, 0.9559, 0.9394]\n",
      "2025-12-22 05:32:08.005795: Epoch time: 137.8 s\n",
      "2025-12-22 05:32:08.678184: \n",
      "2025-12-22 05:32:08.678184: Epoch 763\n",
      "2025-12-22 05:32:08.678184: Current learning rate: 0.00274\n",
      "2025-12-22 05:34:26.528809: train_loss -0.8558\n",
      "2025-12-22 05:34:26.528809: val_loss -0.8767\n",
      "2025-12-22 05:34:26.548286: Pseudo dice [0.9298, 0.954, 0.943]\n",
      "2025-12-22 05:34:26.552291: Epoch time: 137.85 s\n",
      "2025-12-22 05:34:27.209230: \n",
      "2025-12-22 05:34:27.209230: Epoch 764\n",
      "2025-12-22 05:34:27.225327: Current learning rate: 0.00273\n",
      "2025-12-22 05:36:45.140338: train_loss -0.8562\n",
      "2025-12-22 05:36:45.140338: val_loss -0.8811\n",
      "2025-12-22 05:36:45.140338: Pseudo dice [0.9298, 0.9573, 0.9466]\n",
      "2025-12-22 05:36:45.156044: Epoch time: 137.93 s\n",
      "2025-12-22 05:36:45.807284: \n",
      "2025-12-22 05:36:45.807284: Epoch 765\n",
      "2025-12-22 05:36:45.807284: Current learning rate: 0.00272\n",
      "2025-12-22 05:39:03.741243: train_loss -0.8568\n",
      "2025-12-22 05:39:03.741243: val_loss -0.8762\n",
      "2025-12-22 05:39:03.741243: Pseudo dice [0.9285, 0.9562, 0.9418]\n",
      "2025-12-22 05:39:03.757174: Epoch time: 137.93 s\n",
      "2025-12-22 05:39:04.568699: \n",
      "2025-12-22 05:39:04.568699: Epoch 766\n",
      "2025-12-22 05:39:04.584780: Current learning rate: 0.00271\n",
      "2025-12-22 05:41:22.708686: train_loss -0.8577\n",
      "2025-12-22 05:41:22.708686: val_loss -0.8734\n",
      "2025-12-22 05:41:22.724731: Pseudo dice [0.9231, 0.9517, 0.9442]\n",
      "2025-12-22 05:41:22.724731: Epoch time: 138.14 s\n",
      "2025-12-22 05:41:23.377875: \n",
      "2025-12-22 05:41:23.377875: Epoch 767\n",
      "2025-12-22 05:41:23.391565: Current learning rate: 0.0027\n",
      "2025-12-22 05:43:41.232213: train_loss -0.8596\n",
      "2025-12-22 05:43:41.234215: val_loss -0.8698\n",
      "2025-12-22 05:43:41.239774: Pseudo dice [0.9224, 0.9515, 0.943]\n",
      "2025-12-22 05:43:41.239774: Epoch time: 137.85 s\n",
      "2025-12-22 05:43:41.896152: \n",
      "2025-12-22 05:43:41.896152: Epoch 768\n",
      "2025-12-22 05:43:41.911782: Current learning rate: 0.00268\n",
      "2025-12-22 05:45:59.642117: train_loss -0.8618\n",
      "2025-12-22 05:45:59.642117: val_loss -0.8988\n",
      "2025-12-22 05:45:59.657886: Pseudo dice [0.9424, 0.9662, 0.947]\n",
      "2025-12-22 05:45:59.657886: Epoch time: 137.75 s\n",
      "2025-12-22 05:46:00.309427: \n",
      "2025-12-22 05:46:00.309427: Epoch 769\n",
      "2025-12-22 05:46:00.325119: Current learning rate: 0.00267\n",
      "2025-12-22 05:48:18.060037: train_loss -0.8579\n",
      "2025-12-22 05:48:18.060037: val_loss -0.8802\n",
      "2025-12-22 05:48:18.066040: Pseudo dice [0.9325, 0.9579, 0.9405]\n",
      "2025-12-22 05:48:18.066040: Epoch time: 137.75 s\n",
      "2025-12-22 05:48:18.769289: \n",
      "2025-12-22 05:48:18.769289: Epoch 770\n",
      "2025-12-22 05:48:18.784956: Current learning rate: 0.00266\n",
      "2025-12-22 05:50:36.766386: train_loss -0.8592\n",
      "2025-12-22 05:50:36.766386: val_loss -0.8651\n",
      "2025-12-22 05:50:36.774135: Pseudo dice [0.9199, 0.951, 0.9395]\n",
      "2025-12-22 05:50:36.780143: Epoch time: 138.0 s\n",
      "2025-12-22 05:50:37.448971: \n",
      "2025-12-22 05:50:37.448971: Epoch 771\n",
      "2025-12-22 05:50:37.448971: Current learning rate: 0.00265\n",
      "2025-12-22 05:52:55.455119: train_loss -0.857\n",
      "2025-12-22 05:52:55.455119: val_loss -0.8906\n",
      "2025-12-22 05:52:55.470782: Pseudo dice [0.9388, 0.9621, 0.9424]\n",
      "2025-12-22 05:52:55.474604: Epoch time: 138.01 s\n",
      "2025-12-22 05:52:56.340276: \n",
      "2025-12-22 05:52:56.356062: Epoch 772\n",
      "2025-12-22 05:52:56.356062: Current learning rate: 0.00264\n",
      "2025-12-22 05:55:14.307402: train_loss -0.8564\n",
      "2025-12-22 05:55:14.307402: val_loss -0.8732\n",
      "2025-12-22 05:55:14.323517: Pseudo dice [0.9254, 0.9501, 0.9443]\n",
      "2025-12-22 05:55:14.323517: Epoch time: 137.97 s\n",
      "2025-12-22 05:55:15.134061: \n",
      "2025-12-22 05:55:15.134061: Epoch 773\n",
      "2025-12-22 05:55:15.134061: Current learning rate: 0.00263\n",
      "2025-12-22 05:57:33.101418: train_loss -0.8568\n",
      "2025-12-22 05:57:33.101418: val_loss -0.8935\n",
      "2025-12-22 05:57:33.111166: Pseudo dice [0.9404, 0.9629, 0.9434]\n",
      "2025-12-22 05:57:33.117172: Epoch time: 137.97 s\n",
      "2025-12-22 05:57:33.776093: \n",
      "2025-12-22 05:57:33.776093: Epoch 774\n",
      "2025-12-22 05:57:33.776093: Current learning rate: 0.00262\n",
      "2025-12-22 05:59:51.869177: train_loss -0.8558\n",
      "2025-12-22 05:59:51.869177: val_loss -0.8921\n",
      "2025-12-22 05:59:51.884836: Pseudo dice [0.9399, 0.9616, 0.9421]\n",
      "2025-12-22 05:59:51.884836: Epoch time: 138.09 s\n",
      "2025-12-22 05:59:52.551645: \n",
      "2025-12-22 05:59:52.551645: Epoch 775\n",
      "2025-12-22 05:59:52.551645: Current learning rate: 0.00261\n",
      "2025-12-22 06:02:10.410004: train_loss -0.8585\n",
      "2025-12-22 06:02:10.410004: val_loss -0.8608\n",
      "2025-12-22 06:02:10.419754: Pseudo dice [0.9114, 0.9486, 0.9417]\n",
      "2025-12-22 06:02:10.425760: Epoch time: 137.86 s\n",
      "2025-12-22 06:02:11.210625: \n",
      "2025-12-22 06:02:11.210625: Epoch 776\n",
      "2025-12-22 06:02:11.210625: Current learning rate: 0.0026\n",
      "2025-12-22 06:04:29.335406: train_loss -0.8558\n",
      "2025-12-22 06:04:29.335406: val_loss -0.8702\n",
      "2025-12-22 06:04:29.335406: Pseudo dice [0.9263, 0.9516, 0.9377]\n",
      "2025-12-22 06:04:29.335406: Epoch time: 138.12 s\n",
      "2025-12-22 06:04:30.002516: \n",
      "2025-12-22 06:04:30.002516: Epoch 777\n",
      "2025-12-22 06:04:30.002516: Current learning rate: 0.00259\n",
      "2025-12-22 06:06:47.961687: train_loss -0.8597\n",
      "2025-12-22 06:06:47.961687: val_loss -0.8843\n",
      "2025-12-22 06:06:47.969694: Pseudo dice [0.9324, 0.9595, 0.9428]\n",
      "2025-12-22 06:06:47.973438: Epoch time: 137.96 s\n",
      "2025-12-22 06:06:48.800284: \n",
      "2025-12-22 06:06:48.800284: Epoch 778\n",
      "2025-12-22 06:06:48.814204: Current learning rate: 0.00258\n",
      "2025-12-22 06:09:07.150464: train_loss -0.8552\n",
      "2025-12-22 06:09:07.150464: val_loss -0.8772\n",
      "2025-12-22 06:09:07.156471: Pseudo dice [0.9279, 0.9543, 0.9462]\n",
      "2025-12-22 06:09:07.162214: Epoch time: 138.35 s\n",
      "2025-12-22 06:09:07.971235: \n",
      "2025-12-22 06:09:07.971235: Epoch 779\n",
      "2025-12-22 06:09:07.986884: Current learning rate: 0.00257\n",
      "2025-12-22 06:11:26.141641: train_loss -0.8579\n",
      "2025-12-22 06:11:26.141641: val_loss -0.8758\n",
      "2025-12-22 06:11:26.143382: Pseudo dice [0.925, 0.9512, 0.9491]\n",
      "2025-12-22 06:11:26.143382: Epoch time: 138.17 s\n",
      "2025-12-22 06:11:26.809535: \n",
      "2025-12-22 06:11:26.809535: Epoch 780\n",
      "2025-12-22 06:11:26.823550: Current learning rate: 0.00256\n",
      "2025-12-22 06:13:44.658343: train_loss -0.8608\n",
      "2025-12-22 06:13:44.658343: val_loss -0.8831\n",
      "2025-12-22 06:13:44.666089: Pseudo dice [0.9304, 0.9563, 0.9439]\n",
      "2025-12-22 06:13:44.668091: Epoch time: 137.85 s\n",
      "2025-12-22 06:13:45.328240: \n",
      "2025-12-22 06:13:45.328240: Epoch 781\n",
      "2025-12-22 06:13:45.344087: Current learning rate: 0.00255\n",
      "2025-12-22 06:16:03.139479: train_loss -0.8638\n",
      "2025-12-22 06:16:03.141481: val_loss -0.8728\n",
      "2025-12-22 06:16:03.149228: Pseudo dice [0.9249, 0.9546, 0.9429]\n",
      "2025-12-22 06:16:03.157236: Epoch time: 137.81 s\n",
      "2025-12-22 06:16:03.934850: \n",
      "2025-12-22 06:16:03.934850: Epoch 782\n",
      "2025-12-22 06:16:03.950670: Current learning rate: 0.00254\n",
      "2025-12-22 06:18:21.916819: train_loss -0.8567\n",
      "2025-12-22 06:18:21.916819: val_loss -0.8789\n",
      "2025-12-22 06:18:21.916819: Pseudo dice [0.9316, 0.9587, 0.936]\n",
      "2025-12-22 06:18:21.932717: Epoch time: 137.98 s\n",
      "2025-12-22 06:18:22.584273: \n",
      "2025-12-22 06:18:22.584273: Epoch 783\n",
      "2025-12-22 06:18:22.598292: Current learning rate: 0.00253\n",
      "2025-12-22 06:20:40.542322: train_loss -0.8591\n",
      "2025-12-22 06:20:40.542322: val_loss -0.8765\n",
      "2025-12-22 06:20:40.552334: Pseudo dice [0.9289, 0.9538, 0.9392]\n",
      "2025-12-22 06:20:40.558078: Epoch time: 137.96 s\n",
      "2025-12-22 06:20:41.380320: \n",
      "2025-12-22 06:20:41.380320: Epoch 784\n",
      "2025-12-22 06:20:41.396393: Current learning rate: 0.00252\n",
      "2025-12-22 06:22:59.389866: train_loss -0.8612\n",
      "2025-12-22 06:22:59.389866: val_loss -0.8801\n",
      "2025-12-22 06:22:59.397876: Pseudo dice [0.9309, 0.9566, 0.9429]\n",
      "2025-12-22 06:22:59.401618: Epoch time: 138.01 s\n",
      "2025-12-22 06:23:00.141880: \n",
      "2025-12-22 06:23:00.141880: Epoch 785\n",
      "2025-12-22 06:23:00.141880: Current learning rate: 0.00251\n",
      "2025-12-22 06:25:18.064107: train_loss -0.8613\n",
      "2025-12-22 06:25:18.064107: val_loss -0.8853\n",
      "2025-12-22 06:25:18.064107: Pseudo dice [0.936, 0.9577, 0.949]\n",
      "2025-12-22 06:25:18.082127: Epoch time: 137.92 s\n",
      "2025-12-22 06:25:18.730330: \n",
      "2025-12-22 06:25:18.730330: Epoch 786\n",
      "2025-12-22 06:25:18.746380: Current learning rate: 0.0025\n",
      "2025-12-22 06:27:36.517255: train_loss -0.8621\n",
      "2025-12-22 06:27:36.517255: val_loss -0.8918\n",
      "2025-12-22 06:27:36.517255: Pseudo dice [0.9361, 0.9641, 0.9504]\n",
      "2025-12-22 06:27:36.517255: Epoch time: 137.79 s\n",
      "2025-12-22 06:27:37.200779: \n",
      "2025-12-22 06:27:37.216764: Epoch 787\n",
      "2025-12-22 06:27:37.220742: Current learning rate: 0.00249\n",
      "2025-12-22 06:29:55.268990: train_loss -0.8558\n",
      "2025-12-22 06:29:55.268990: val_loss -0.8673\n",
      "2025-12-22 06:29:55.268990: Pseudo dice [0.9234, 0.9529, 0.9374]\n",
      "2025-12-22 06:29:55.284728: Epoch time: 138.07 s\n",
      "2025-12-22 06:29:56.028655: \n",
      "2025-12-22 06:29:56.028655: Epoch 788\n",
      "2025-12-22 06:29:56.028655: Current learning rate: 0.00248\n",
      "2025-12-22 06:32:13.848489: train_loss -0.858\n",
      "2025-12-22 06:32:13.848489: val_loss -0.8681\n",
      "2025-12-22 06:32:13.854233: Pseudo dice [0.9195, 0.9548, 0.9404]\n",
      "2025-12-22 06:32:13.859977: Epoch time: 137.82 s\n",
      "2025-12-22 06:32:14.502635: \n",
      "2025-12-22 06:32:14.502635: Epoch 789\n",
      "2025-12-22 06:32:14.518759: Current learning rate: 0.00247\n",
      "2025-12-22 06:34:32.478941: train_loss -0.8579\n",
      "2025-12-22 06:34:32.478941: val_loss -0.8821\n",
      "2025-12-22 06:34:32.494963: Pseudo dice [0.9286, 0.9573, 0.9478]\n",
      "2025-12-22 06:34:32.494963: Epoch time: 137.98 s\n",
      "2025-12-22 06:34:33.383771: \n",
      "2025-12-22 06:34:33.383771: Epoch 790\n",
      "2025-12-22 06:34:33.383771: Current learning rate: 0.00245\n",
      "2025-12-22 06:36:51.192839: train_loss -0.8604\n",
      "2025-12-22 06:36:51.192839: val_loss -0.8757\n",
      "2025-12-22 06:36:51.204624: Pseudo dice [0.926, 0.9532, 0.9457]\n",
      "2025-12-22 06:36:51.204624: Epoch time: 137.81 s\n",
      "2025-12-22 06:36:51.916280: \n",
      "2025-12-22 06:36:51.916280: Epoch 791\n",
      "2025-12-22 06:36:51.916280: Current learning rate: 0.00244\n",
      "2025-12-22 06:39:09.751312: train_loss -0.8623\n",
      "2025-12-22 06:39:09.751312: val_loss -0.8654\n",
      "2025-12-22 06:39:09.751312: Pseudo dice [0.9137, 0.9465, 0.9535]\n",
      "2025-12-22 06:39:09.762926: Epoch time: 137.84 s\n",
      "2025-12-22 06:39:10.426251: \n",
      "2025-12-22 06:39:10.426251: Epoch 792\n",
      "2025-12-22 06:39:10.441953: Current learning rate: 0.00243\n",
      "2025-12-22 06:41:28.426996: train_loss -0.8598\n",
      "2025-12-22 06:41:28.426996: val_loss -0.8583\n",
      "2025-12-22 06:41:28.432747: Pseudo dice [0.9158, 0.9475, 0.9371]\n",
      "2025-12-22 06:41:28.438757: Epoch time: 138.0 s\n",
      "2025-12-22 06:41:29.236118: \n",
      "2025-12-22 06:41:29.236118: Epoch 793\n",
      "2025-12-22 06:41:29.251844: Current learning rate: 0.00242\n",
      "2025-12-22 06:43:47.233563: train_loss -0.8577\n",
      "2025-12-22 06:43:47.235566: val_loss -0.8761\n",
      "2025-12-22 06:43:47.239570: Pseudo dice [0.9267, 0.9536, 0.952]\n",
      "2025-12-22 06:43:47.245575: Epoch time: 138.0 s\n",
      "2025-12-22 06:43:47.914461: \n",
      "2025-12-22 06:43:47.914461: Epoch 794\n",
      "2025-12-22 06:43:47.914461: Current learning rate: 0.00241\n",
      "2025-12-22 06:46:05.774909: train_loss -0.8606\n",
      "2025-12-22 06:46:05.774909: val_loss -0.8855\n",
      "2025-12-22 06:46:05.782917: Pseudo dice [0.9327, 0.9599, 0.9446]\n",
      "2025-12-22 06:46:05.790665: Epoch time: 137.86 s\n",
      "2025-12-22 06:46:06.450628: \n",
      "2025-12-22 06:46:06.450628: Epoch 795\n",
      "2025-12-22 06:46:06.450628: Current learning rate: 0.0024\n",
      "2025-12-22 06:48:24.434414: train_loss -0.8577\n",
      "2025-12-22 06:48:24.436154: val_loss -0.8754\n",
      "2025-12-22 06:48:24.446169: Pseudo dice [0.9288, 0.9506, 0.9438]\n",
      "2025-12-22 06:48:24.455923: Epoch time: 137.98 s\n",
      "2025-12-22 06:48:25.498317: \n",
      "2025-12-22 06:48:25.498317: Epoch 796\n",
      "2025-12-22 06:48:25.498317: Current learning rate: 0.00239\n",
      "2025-12-22 06:50:43.366136: train_loss -0.8583\n",
      "2025-12-22 06:50:43.366136: val_loss -0.874\n",
      "2025-12-22 06:50:43.380137: Pseudo dice [0.9248, 0.9517, 0.9435]\n",
      "2025-12-22 06:50:43.380137: Epoch time: 137.87 s\n",
      "2025-12-22 06:50:44.047715: \n",
      "2025-12-22 06:50:44.047715: Epoch 797\n",
      "2025-12-22 06:50:44.061762: Current learning rate: 0.00238\n",
      "2025-12-22 06:53:01.881540: train_loss -0.8604\n",
      "2025-12-22 06:53:01.881540: val_loss -0.8824\n",
      "2025-12-22 06:53:01.886292: Pseudo dice [0.93, 0.9552, 0.9476]\n",
      "2025-12-22 06:53:01.892298: Epoch time: 137.83 s\n",
      "2025-12-22 06:53:02.548578: \n",
      "2025-12-22 06:53:02.548578: Epoch 798\n",
      "2025-12-22 06:53:02.548578: Current learning rate: 0.00237\n",
      "2025-12-22 06:55:20.378083: train_loss -0.8578\n",
      "2025-12-22 06:55:20.378083: val_loss -0.8846\n",
      "2025-12-22 06:55:20.394169: Pseudo dice [0.9321, 0.9606, 0.9411]\n",
      "2025-12-22 06:55:20.394169: Epoch time: 137.83 s\n",
      "2025-12-22 06:55:21.215801: \n",
      "2025-12-22 06:55:21.215801: Epoch 799\n",
      "2025-12-22 06:55:21.215801: Current learning rate: 0.00236\n",
      "2025-12-22 06:57:39.017294: train_loss -0.859\n",
      "2025-12-22 06:57:39.017294: val_loss -0.8826\n",
      "2025-12-22 06:57:39.017294: Pseudo dice [0.931, 0.9567, 0.9467]\n",
      "2025-12-22 06:57:39.017294: Epoch time: 137.8 s\n",
      "2025-12-22 06:57:39.947645: \n",
      "2025-12-22 06:57:39.947645: Epoch 800\n",
      "2025-12-22 06:57:39.963469: Current learning rate: 0.00235\n",
      "2025-12-22 06:59:57.863880: train_loss -0.8565\n",
      "2025-12-22 06:59:57.863880: val_loss -0.8886\n",
      "2025-12-22 06:59:57.863880: Pseudo dice [0.937, 0.961, 0.9454]\n",
      "2025-12-22 06:59:57.872146: Epoch time: 137.92 s\n",
      "2025-12-22 06:59:58.595566: \n",
      "2025-12-22 06:59:58.595566: Epoch 801\n",
      "2025-12-22 06:59:58.595566: Current learning rate: 0.00234\n",
      "2025-12-22 07:02:16.385267: train_loss -0.8559\n",
      "2025-12-22 07:02:16.385267: val_loss -0.8707\n",
      "2025-12-22 07:02:16.393280: Pseudo dice [0.9245, 0.9513, 0.9364]\n",
      "2025-12-22 07:02:16.401026: Epoch time: 137.79 s\n",
      "2025-12-22 07:02:17.262094: \n",
      "2025-12-22 07:02:17.262094: Epoch 802\n",
      "2025-12-22 07:02:17.275790: Current learning rate: 0.00233\n",
      "2025-12-22 07:04:35.183285: train_loss -0.8613\n",
      "2025-12-22 07:04:35.183285: val_loss -0.8679\n",
      "2025-12-22 07:04:35.188128: Pseudo dice [0.9194, 0.9527, 0.9381]\n",
      "2025-12-22 07:04:35.194007: Epoch time: 137.92 s\n",
      "2025-12-22 07:04:35.865772: \n",
      "2025-12-22 07:04:35.865772: Epoch 803\n",
      "2025-12-22 07:04:35.865772: Current learning rate: 0.00232\n",
      "2025-12-22 07:06:53.699028: train_loss -0.8596\n",
      "2025-12-22 07:06:53.699028: val_loss -0.8847\n",
      "2025-12-22 07:06:53.707037: Pseudo dice [0.9326, 0.9573, 0.9467]\n",
      "2025-12-22 07:06:53.713892: Epoch time: 137.83 s\n",
      "2025-12-22 07:06:54.379019: \n",
      "2025-12-22 07:06:54.379019: Epoch 804\n",
      "2025-12-22 07:06:54.379019: Current learning rate: 0.00231\n",
      "2025-12-22 07:09:12.318235: train_loss -0.8573\n",
      "2025-12-22 07:09:12.318235: val_loss -0.8754\n",
      "2025-12-22 07:09:12.340460: Pseudo dice [0.9282, 0.9555, 0.9442]\n",
      "2025-12-22 07:09:12.344464: Epoch time: 137.94 s\n",
      "2025-12-22 07:09:13.014360: \n",
      "2025-12-22 07:09:13.014360: Epoch 805\n",
      "2025-12-22 07:09:13.030259: Current learning rate: 0.0023\n",
      "2025-12-22 07:11:31.080812: train_loss -0.8587\n",
      "2025-12-22 07:11:31.080812: val_loss -0.875\n",
      "2025-12-22 07:11:31.090317: Pseudo dice [0.9267, 0.953, 0.9427]\n",
      "2025-12-22 07:11:31.094322: Epoch time: 138.07 s\n",
      "2025-12-22 07:11:31.760356: \n",
      "2025-12-22 07:11:31.760356: Epoch 806\n",
      "2025-12-22 07:11:31.760356: Current learning rate: 0.00229\n",
      "2025-12-22 07:13:49.488621: train_loss -0.8661\n",
      "2025-12-22 07:13:49.488621: val_loss -0.8766\n",
      "2025-12-22 07:13:49.498637: Pseudo dice [0.931, 0.9541, 0.9425]\n",
      "2025-12-22 07:13:49.504653: Epoch time: 137.73 s\n",
      "2025-12-22 07:13:50.364573: \n",
      "2025-12-22 07:13:50.364573: Epoch 807\n",
      "2025-12-22 07:13:50.380388: Current learning rate: 0.00228\n",
      "2025-12-22 07:16:08.509593: train_loss -0.8606\n",
      "2025-12-22 07:16:08.509593: val_loss -0.8906\n",
      "2025-12-22 07:16:08.509593: Pseudo dice [0.9351, 0.9627, 0.9466]\n",
      "2025-12-22 07:16:08.525586: Epoch time: 138.15 s\n",
      "2025-12-22 07:16:09.191755: \n",
      "2025-12-22 07:16:09.191755: Epoch 808\n",
      "2025-12-22 07:16:09.191755: Current learning rate: 0.00226\n",
      "2025-12-22 07:18:26.939220: train_loss -0.8606\n",
      "2025-12-22 07:18:26.939220: val_loss -0.8751\n",
      "2025-12-22 07:18:26.939220: Pseudo dice [0.9317, 0.9554, 0.9332]\n",
      "2025-12-22 07:18:26.939220: Epoch time: 137.75 s\n",
      "2025-12-22 07:18:27.607477: \n",
      "2025-12-22 07:18:27.607477: Epoch 809\n",
      "2025-12-22 07:18:27.607477: Current learning rate: 0.00225\n",
      "2025-12-22 07:20:45.438909: train_loss -0.8582\n",
      "2025-12-22 07:20:45.438909: val_loss -0.8691\n",
      "2025-12-22 07:20:45.447726: Pseudo dice [0.9214, 0.9496, 0.9417]\n",
      "2025-12-22 07:20:45.448727: Epoch time: 137.83 s\n",
      "2025-12-22 07:20:46.162246: \n",
      "2025-12-22 07:20:46.162246: Epoch 810\n",
      "2025-12-22 07:20:46.162246: Current learning rate: 0.00224\n",
      "2025-12-22 07:23:04.059863: train_loss -0.8604\n",
      "2025-12-22 07:23:04.061865: val_loss -0.8936\n",
      "2025-12-22 07:23:04.067872: Pseudo dice [0.9386, 0.9628, 0.9448]\n",
      "2025-12-22 07:23:04.071876: Epoch time: 137.9 s\n",
      "2025-12-22 07:23:04.745722: \n",
      "2025-12-22 07:23:04.745722: Epoch 811\n",
      "2025-12-22 07:23:04.750135: Current learning rate: 0.00223\n",
      "2025-12-22 07:25:22.831534: train_loss -0.8522\n",
      "2025-12-22 07:25:22.831534: val_loss -0.8755\n",
      "2025-12-22 07:25:22.847407: Pseudo dice [0.9232, 0.9509, 0.9463]\n",
      "2025-12-22 07:25:22.856658: Epoch time: 138.09 s\n",
      "2025-12-22 07:25:23.518538: \n",
      "2025-12-22 07:25:23.518538: Epoch 812\n",
      "2025-12-22 07:25:23.534200: Current learning rate: 0.00222\n",
      "2025-12-22 07:27:41.312077: train_loss -0.8556\n",
      "2025-12-22 07:27:41.314671: val_loss -0.8536\n",
      "2025-12-22 07:27:41.318676: Pseudo dice [0.9111, 0.9493, 0.9351]\n",
      "2025-12-22 07:27:41.324682: Epoch time: 137.79 s\n",
      "2025-12-22 07:27:42.326397: \n",
      "2025-12-22 07:27:42.326397: Epoch 813\n",
      "2025-12-22 07:27:42.334966: Current learning rate: 0.00221\n",
      "2025-12-22 07:30:00.052508: train_loss -0.8602\n",
      "2025-12-22 07:30:00.052508: val_loss -0.8821\n",
      "2025-12-22 07:30:00.060516: Pseudo dice [0.9282, 0.956, 0.9501]\n",
      "2025-12-22 07:30:00.064520: Epoch time: 137.73 s\n",
      "2025-12-22 07:30:00.737934: \n",
      "2025-12-22 07:30:00.739936: Epoch 814\n",
      "2025-12-22 07:30:00.739936: Current learning rate: 0.0022\n",
      "2025-12-22 07:32:18.789417: train_loss -0.8569\n",
      "2025-12-22 07:32:18.789417: val_loss -0.8714\n",
      "2025-12-22 07:32:18.808922: Pseudo dice [0.9265, 0.9512, 0.9411]\n",
      "2025-12-22 07:32:18.814928: Epoch time: 138.05 s\n",
      "2025-12-22 07:32:19.485265: \n",
      "2025-12-22 07:32:19.485265: Epoch 815\n",
      "2025-12-22 07:32:19.485265: Current learning rate: 0.00219\n",
      "2025-12-22 07:34:37.335125: train_loss -0.8563\n",
      "2025-12-22 07:34:37.335125: val_loss -0.8811\n",
      "2025-12-22 07:34:37.341131: Pseudo dice [0.9307, 0.9565, 0.9447]\n",
      "2025-12-22 07:34:37.345136: Epoch time: 137.87 s\n",
      "2025-12-22 07:34:38.138575: \n",
      "2025-12-22 07:34:38.138575: Epoch 816\n",
      "2025-12-22 07:34:38.149122: Current learning rate: 0.00218\n",
      "2025-12-22 07:36:56.005769: train_loss -0.8585\n",
      "2025-12-22 07:36:56.005769: val_loss -0.8917\n",
      "2025-12-22 07:36:56.005769: Pseudo dice [0.9417, 0.9656, 0.9358]\n",
      "2025-12-22 07:36:56.014610: Epoch time: 137.87 s\n",
      "2025-12-22 07:36:56.673159: \n",
      "2025-12-22 07:36:56.676666: Epoch 817\n",
      "2025-12-22 07:36:56.676666: Current learning rate: 0.00217\n",
      "2025-12-22 07:39:14.623063: train_loss -0.8575\n",
      "2025-12-22 07:39:14.623063: val_loss -0.8743\n",
      "2025-12-22 07:39:14.629068: Pseudo dice [0.9268, 0.9568, 0.9378]\n",
      "2025-12-22 07:39:14.634573: Epoch time: 137.95 s\n",
      "2025-12-22 07:39:15.308204: \n",
      "2025-12-22 07:39:15.308204: Epoch 818\n",
      "2025-12-22 07:39:15.308204: Current learning rate: 0.00216\n",
      "2025-12-22 07:41:33.336002: train_loss -0.8568\n",
      "2025-12-22 07:41:33.338005: val_loss -0.8783\n",
      "2025-12-22 07:41:33.343762: Pseudo dice [0.9248, 0.9583, 0.948]\n",
      "2025-12-22 07:41:33.349771: Epoch time: 138.03 s\n",
      "2025-12-22 07:41:34.318309: \n",
      "2025-12-22 07:41:34.318309: Epoch 819\n",
      "2025-12-22 07:41:34.336052: Current learning rate: 0.00215\n",
      "2025-12-22 07:43:52.153049: train_loss -0.857\n",
      "2025-12-22 07:43:52.153049: val_loss -0.8882\n",
      "2025-12-22 07:43:52.161058: Pseudo dice [0.9333, 0.9588, 0.9517]\n",
      "2025-12-22 07:43:52.168798: Epoch time: 137.83 s\n",
      "2025-12-22 07:43:52.812573: \n",
      "2025-12-22 07:43:52.812573: Epoch 820\n",
      "2025-12-22 07:43:52.812573: Current learning rate: 0.00214\n",
      "2025-12-22 07:46:10.626189: train_loss -0.8575\n",
      "2025-12-22 07:46:10.626189: val_loss -0.87\n",
      "2025-12-22 07:46:10.645607: Pseudo dice [0.9243, 0.9513, 0.9393]\n",
      "2025-12-22 07:46:10.651613: Epoch time: 137.82 s\n",
      "2025-12-22 07:46:11.289327: \n",
      "2025-12-22 07:46:11.291067: Epoch 821\n",
      "2025-12-22 07:46:11.292070: Current learning rate: 0.00213\n",
      "2025-12-22 07:48:29.243127: train_loss -0.8583\n",
      "2025-12-22 07:48:29.245130: val_loss -0.8781\n",
      "2025-12-22 07:48:29.251137: Pseudo dice [0.9278, 0.9568, 0.9432]\n",
      "2025-12-22 07:48:29.256642: Epoch time: 137.95 s\n",
      "2025-12-22 07:48:30.024406: \n",
      "2025-12-22 07:48:30.024406: Epoch 822\n",
      "2025-12-22 07:48:30.028413: Current learning rate: 0.00212\n",
      "2025-12-22 07:50:47.887023: train_loss -0.8598\n",
      "2025-12-22 07:50:47.887023: val_loss -0.8706\n",
      "2025-12-22 07:50:47.906669: Pseudo dice [0.9239, 0.9492, 0.9492]\n",
      "2025-12-22 07:50:47.912675: Epoch time: 137.87 s\n",
      "2025-12-22 07:50:48.568259: \n",
      "2025-12-22 07:50:48.568259: Epoch 823\n",
      "2025-12-22 07:50:48.586823: Current learning rate: 0.0021\n",
      "2025-12-22 07:53:06.466476: train_loss -0.8612\n",
      "2025-12-22 07:53:06.466476: val_loss -0.8815\n",
      "2025-12-22 07:53:06.479011: Pseudo dice [0.9252, 0.9563, 0.9494]\n",
      "2025-12-22 07:53:06.482516: Epoch time: 137.9 s\n",
      "2025-12-22 07:53:07.177584: \n",
      "2025-12-22 07:53:07.177584: Epoch 824\n",
      "2025-12-22 07:53:07.177584: Current learning rate: 0.00209\n",
      "2025-12-22 07:55:25.188069: train_loss -0.8589\n",
      "2025-12-22 07:55:25.188069: val_loss -0.8635\n",
      "2025-12-22 07:55:25.204076: Pseudo dice [0.9191, 0.9459, 0.9438]\n",
      "2025-12-22 07:55:25.204076: Epoch time: 138.01 s\n",
      "2025-12-22 07:55:26.042799: \n",
      "2025-12-22 07:55:26.042799: Epoch 825\n",
      "2025-12-22 07:55:26.042799: Current learning rate: 0.00208\n",
      "2025-12-22 07:57:43.850818: train_loss -0.8608\n",
      "2025-12-22 07:57:43.852612: val_loss -0.8751\n",
      "2025-12-22 07:57:43.856616: Pseudo dice [0.9238, 0.9535, 0.9487]\n",
      "2025-12-22 07:57:43.862360: Epoch time: 137.81 s\n",
      "2025-12-22 07:57:44.495705: \n",
      "2025-12-22 07:57:44.497707: Epoch 826\n",
      "2025-12-22 07:57:44.497707: Current learning rate: 0.00207\n",
      "2025-12-22 08:00:02.438935: train_loss -0.859\n",
      "2025-12-22 08:00:02.440937: val_loss -0.8789\n",
      "2025-12-22 08:00:02.448684: Pseudo dice [0.9287, 0.9582, 0.9452]\n",
      "2025-12-22 08:00:02.454029: Epoch time: 137.94 s\n",
      "2025-12-22 08:00:03.098588: \n",
      "2025-12-22 08:00:03.098588: Epoch 827\n",
      "2025-12-22 08:00:03.100329: Current learning rate: 0.00206\n",
      "2025-12-22 08:02:20.994948: train_loss -0.8646\n",
      "2025-12-22 08:02:20.994948: val_loss -0.8745\n",
      "2025-12-22 08:02:21.000954: Pseudo dice [0.9271, 0.9549, 0.9424]\n",
      "2025-12-22 08:02:21.006697: Epoch time: 137.9 s\n",
      "2025-12-22 08:02:21.636974: \n",
      "2025-12-22 08:02:21.652893: Epoch 828\n",
      "2025-12-22 08:02:21.652893: Current learning rate: 0.00205\n",
      "2025-12-22 08:04:39.731093: train_loss -0.8601\n",
      "2025-12-22 08:04:39.733095: val_loss -0.8882\n",
      "2025-12-22 08:04:39.740842: Pseudo dice [0.9357, 0.9614, 0.9408]\n",
      "2025-12-22 08:04:39.746848: Epoch time: 138.09 s\n",
      "2025-12-22 08:04:40.449214: \n",
      "2025-12-22 08:04:40.451216: Epoch 829\n",
      "2025-12-22 08:04:40.451216: Current learning rate: 0.00204\n",
      "2025-12-22 08:06:58.343635: train_loss -0.8652\n",
      "2025-12-22 08:06:58.343635: val_loss -0.8608\n",
      "2025-12-22 08:06:58.353394: Pseudo dice [0.9156, 0.9481, 0.9462]\n",
      "2025-12-22 08:06:58.361402: Epoch time: 137.89 s\n",
      "2025-12-22 08:06:59.079183: \n",
      "2025-12-22 08:06:59.079183: Epoch 830\n",
      "2025-12-22 08:06:59.079183: Current learning rate: 0.00203\n",
      "2025-12-22 08:09:17.202122: train_loss -0.8635\n",
      "2025-12-22 08:09:17.202122: val_loss -0.8829\n",
      "2025-12-22 08:09:17.218235: Pseudo dice [0.9328, 0.9637, 0.9399]\n",
      "2025-12-22 08:09:17.218235: Epoch time: 138.12 s\n",
      "2025-12-22 08:09:17.864143: \n",
      "2025-12-22 08:09:17.864143: Epoch 831\n",
      "2025-12-22 08:09:17.868175: Current learning rate: 0.00202\n",
      "2025-12-22 08:11:35.980361: train_loss -0.8602\n",
      "2025-12-22 08:11:35.980361: val_loss -0.8753\n",
      "2025-12-22 08:11:35.980361: Pseudo dice [0.9284, 0.9551, 0.9379]\n",
      "2025-12-22 08:11:35.980361: Epoch time: 138.12 s\n",
      "2025-12-22 08:11:36.852009: \n",
      "2025-12-22 08:11:36.852009: Epoch 832\n",
      "2025-12-22 08:11:36.868121: Current learning rate: 0.00201\n",
      "2025-12-22 08:13:54.803708: train_loss -0.8555\n",
      "2025-12-22 08:13:54.803708: val_loss -0.8901\n",
      "2025-12-22 08:13:54.803708: Pseudo dice [0.932, 0.9606, 0.9506]\n",
      "2025-12-22 08:13:54.812091: Epoch time: 137.95 s\n",
      "2025-12-22 08:13:55.476395: \n",
      "2025-12-22 08:13:55.476395: Epoch 833\n",
      "2025-12-22 08:13:55.492485: Current learning rate: 0.002\n",
      "2025-12-22 08:16:13.383896: train_loss -0.8653\n",
      "2025-12-22 08:16:13.383896: val_loss -0.8748\n",
      "2025-12-22 08:16:13.399757: Pseudo dice [0.925, 0.9516, 0.9444]\n",
      "2025-12-22 08:16:13.399757: Epoch time: 137.91 s\n",
      "2025-12-22 08:16:14.031551: \n",
      "2025-12-22 08:16:14.031551: Epoch 834\n",
      "2025-12-22 08:16:14.047492: Current learning rate: 0.00199\n",
      "2025-12-22 08:18:31.957349: train_loss -0.8604\n",
      "2025-12-22 08:18:31.966280: val_loss -0.8957\n",
      "2025-12-22 08:18:31.972594: Pseudo dice [0.9424, 0.963, 0.9434]\n",
      "2025-12-22 08:18:31.978097: Epoch time: 137.93 s\n",
      "2025-12-22 08:18:32.613929: \n",
      "2025-12-22 08:18:32.614934: Epoch 835\n",
      "2025-12-22 08:18:32.619855: Current learning rate: 0.00198\n",
      "2025-12-22 08:20:51.181790: train_loss -0.8605\n",
      "2025-12-22 08:20:51.181790: val_loss -0.8841\n",
      "2025-12-22 08:20:51.187537: Pseudo dice [0.9303, 0.9567, 0.9513]\n",
      "2025-12-22 08:20:51.191394: Epoch time: 138.57 s\n",
      "2025-12-22 08:20:51.846957: \n",
      "2025-12-22 08:20:51.846957: Epoch 836\n",
      "2025-12-22 08:20:51.853300: Current learning rate: 0.00196\n",
      "2025-12-22 08:23:09.880911: train_loss -0.8577\n",
      "2025-12-22 08:23:09.880911: val_loss -0.8838\n",
      "2025-12-22 08:23:09.888658: Pseudo dice [0.9314, 0.959, 0.9455]\n",
      "2025-12-22 08:23:09.896404: Epoch time: 138.04 s\n",
      "2025-12-22 08:23:10.534555: \n",
      "2025-12-22 08:23:10.534555: Epoch 837\n",
      "2025-12-22 08:23:10.534555: Current learning rate: 0.00195\n",
      "2025-12-22 08:25:28.584388: train_loss -0.8596\n",
      "2025-12-22 08:25:28.584388: val_loss -0.8697\n",
      "2025-12-22 08:25:28.600220: Pseudo dice [0.9211, 0.9527, 0.943]\n",
      "2025-12-22 08:25:28.604343: Epoch time: 138.05 s\n",
      "2025-12-22 08:25:29.404242: \n",
      "2025-12-22 08:25:29.404242: Epoch 838\n",
      "2025-12-22 08:25:29.404242: Current learning rate: 0.00194\n",
      "2025-12-22 08:27:47.279467: train_loss -0.8661\n",
      "2025-12-22 08:27:47.279467: val_loss -0.8838\n",
      "2025-12-22 08:27:47.285211: Pseudo dice [0.9327, 0.9591, 0.9445]\n",
      "2025-12-22 08:27:47.285211: Epoch time: 137.88 s\n",
      "2025-12-22 08:27:47.939932: \n",
      "2025-12-22 08:27:47.939932: Epoch 839\n",
      "2025-12-22 08:27:47.939932: Current learning rate: 0.00193\n",
      "2025-12-22 08:30:05.827425: train_loss -0.8593\n",
      "2025-12-22 08:30:05.827425: val_loss -0.8799\n",
      "2025-12-22 08:30:05.827425: Pseudo dice [0.9261, 0.9588, 0.9448]\n",
      "2025-12-22 08:30:05.827425: Epoch time: 137.89 s\n",
      "2025-12-22 08:30:06.507776: \n",
      "2025-12-22 08:30:06.507776: Epoch 840\n",
      "2025-12-22 08:30:06.507776: Current learning rate: 0.00192\n",
      "2025-12-22 08:32:24.393453: train_loss -0.8629\n",
      "2025-12-22 08:32:24.409165: val_loss -0.8896\n",
      "2025-12-22 08:32:24.409165: Pseudo dice [0.9327, 0.9612, 0.9521]\n",
      "2025-12-22 08:32:24.409165: Epoch time: 137.89 s\n",
      "2025-12-22 08:32:25.058927: \n",
      "2025-12-22 08:32:25.058927: Epoch 841\n",
      "2025-12-22 08:32:25.058927: Current learning rate: 0.00191\n",
      "2025-12-22 08:34:43.014620: train_loss -0.8616\n",
      "2025-12-22 08:34:43.014620: val_loss -0.8783\n",
      "2025-12-22 08:34:43.014620: Pseudo dice [0.9278, 0.9575, 0.9462]\n",
      "2025-12-22 08:34:43.014620: Epoch time: 137.96 s\n",
      "2025-12-22 08:34:43.743092: \n",
      "2025-12-22 08:34:43.743092: Epoch 842\n",
      "2025-12-22 08:34:43.758859: Current learning rate: 0.0019\n",
      "2025-12-22 08:37:01.578072: train_loss -0.8633\n",
      "2025-12-22 08:37:01.578072: val_loss -0.8814\n",
      "2025-12-22 08:37:01.580075: Pseudo dice [0.9295, 0.9595, 0.9448]\n",
      "2025-12-22 08:37:01.593919: Epoch time: 137.83 s\n",
      "2025-12-22 08:37:02.211619: \n",
      "2025-12-22 08:37:02.211619: Epoch 843\n",
      "2025-12-22 08:37:02.227625: Current learning rate: 0.00189\n",
      "2025-12-22 08:39:20.149318: train_loss -0.8599\n",
      "2025-12-22 08:39:20.151320: val_loss -0.8696\n",
      "2025-12-22 08:39:20.157328: Pseudo dice [0.9219, 0.9535, 0.938]\n",
      "2025-12-22 08:39:20.157328: Epoch time: 137.94 s\n",
      "2025-12-22 08:39:20.966929: \n",
      "2025-12-22 08:39:20.966929: Epoch 844\n",
      "2025-12-22 08:39:20.966929: Current learning rate: 0.00188\n",
      "2025-12-22 08:41:38.722310: train_loss -0.8646\n",
      "2025-12-22 08:41:38.722310: val_loss -0.84\n",
      "2025-12-22 08:41:38.730317: Pseudo dice [0.9011, 0.9344, 0.9429]\n",
      "2025-12-22 08:41:38.733797: Epoch time: 137.76 s\n",
      "2025-12-22 08:41:39.369149: \n",
      "2025-12-22 08:41:39.369149: Epoch 845\n",
      "2025-12-22 08:41:39.385082: Current learning rate: 0.00187\n",
      "2025-12-22 08:43:57.495560: train_loss -0.8562\n",
      "2025-12-22 08:43:57.495560: val_loss -0.8776\n",
      "2025-12-22 08:43:57.511306: Pseudo dice [0.9249, 0.9546, 0.9459]\n",
      "2025-12-22 08:43:57.511306: Epoch time: 138.13 s\n",
      "2025-12-22 08:43:58.144860: \n",
      "2025-12-22 08:43:58.144860: Epoch 846\n",
      "2025-12-22 08:43:58.144860: Current learning rate: 0.00186\n",
      "2025-12-22 08:46:16.015881: train_loss -0.8598\n",
      "2025-12-22 08:46:16.015881: val_loss -0.8692\n",
      "2025-12-22 08:46:16.021888: Pseudo dice [0.9206, 0.952, 0.9422]\n",
      "2025-12-22 08:46:16.027894: Epoch time: 137.87 s\n",
      "2025-12-22 08:46:16.665683: \n",
      "2025-12-22 08:46:16.665683: Epoch 847\n",
      "2025-12-22 08:46:16.665683: Current learning rate: 0.00185\n",
      "2025-12-22 08:48:34.576946: train_loss -0.8588\n",
      "2025-12-22 08:48:34.576946: val_loss -0.8827\n",
      "2025-12-22 08:48:34.576946: Pseudo dice [0.9323, 0.9583, 0.9436]\n",
      "2025-12-22 08:48:34.576946: Epoch time: 137.91 s\n",
      "2025-12-22 08:48:35.226261: \n",
      "2025-12-22 08:48:35.226261: Epoch 848\n",
      "2025-12-22 08:48:35.226261: Current learning rate: 0.00184\n",
      "2025-12-22 08:50:53.347166: train_loss -0.8631\n",
      "2025-12-22 08:50:53.347166: val_loss -0.8747\n",
      "2025-12-22 08:50:53.350908: Pseudo dice [0.927, 0.9552, 0.9418]\n",
      "2025-12-22 08:50:53.350908: Epoch time: 138.12 s\n",
      "2025-12-22 08:50:53.996608: \n",
      "2025-12-22 08:50:53.996608: Epoch 849\n",
      "2025-12-22 08:50:53.996608: Current learning rate: 0.00182\n",
      "2025-12-22 08:53:11.777123: train_loss -0.8598\n",
      "2025-12-22 08:53:11.777123: val_loss -0.8799\n",
      "2025-12-22 08:53:11.784869: Pseudo dice [0.9305, 0.9581, 0.9422]\n",
      "2025-12-22 08:53:11.788873: Epoch time: 137.8 s\n",
      "2025-12-22 08:53:12.924760: \n",
      "2025-12-22 08:53:12.924760: Epoch 850\n",
      "2025-12-22 08:53:12.924760: Current learning rate: 0.00181\n",
      "2025-12-22 08:55:31.021726: train_loss -0.8606\n",
      "2025-12-22 08:55:31.021726: val_loss -0.875\n",
      "2025-12-22 08:55:31.021726: Pseudo dice [0.9262, 0.9525, 0.9462]\n",
      "2025-12-22 08:55:31.021726: Epoch time: 138.1 s\n",
      "2025-12-22 08:55:31.641981: \n",
      "2025-12-22 08:55:31.641981: Epoch 851\n",
      "2025-12-22 08:55:31.659446: Current learning rate: 0.0018\n",
      "2025-12-22 08:57:49.381356: train_loss -0.8612\n",
      "2025-12-22 08:57:49.381356: val_loss -0.8867\n",
      "2025-12-22 08:57:49.389363: Pseudo dice [0.9361, 0.9613, 0.9402]\n",
      "2025-12-22 08:57:49.394598: Epoch time: 137.74 s\n",
      "2025-12-22 08:57:50.023493: \n",
      "2025-12-22 08:57:50.023493: Epoch 852\n",
      "2025-12-22 08:57:50.023493: Current learning rate: 0.00179\n",
      "2025-12-22 09:00:07.987538: train_loss -0.856\n",
      "2025-12-22 09:00:07.987538: val_loss -0.8858\n",
      "2025-12-22 09:00:07.994262: Pseudo dice [0.9327, 0.9608, 0.9415]\n",
      "2025-12-22 09:00:08.000268: Epoch time: 137.98 s\n",
      "2025-12-22 09:00:08.661752: \n",
      "2025-12-22 09:00:08.661752: Epoch 853\n",
      "2025-12-22 09:00:08.661752: Current learning rate: 0.00178\n",
      "2025-12-22 09:02:26.983343: train_loss -0.8587\n",
      "2025-12-22 09:02:26.983343: val_loss -0.892\n",
      "2025-12-22 09:02:26.990411: Pseudo dice [0.938, 0.9626, 0.9481]\n",
      "2025-12-22 09:02:26.996417: Epoch time: 138.32 s\n",
      "2025-12-22 09:02:27.633207: \n",
      "2025-12-22 09:02:27.635209: Epoch 854\n",
      "2025-12-22 09:02:27.635209: Current learning rate: 0.00177\n",
      "2025-12-22 09:04:45.602196: train_loss -0.8626\n",
      "2025-12-22 09:04:45.602196: val_loss -0.8681\n",
      "2025-12-22 09:04:45.615955: Pseudo dice [0.9207, 0.9493, 0.9448]\n",
      "2025-12-22 09:04:45.619872: Epoch time: 137.97 s\n",
      "2025-12-22 09:04:46.258737: \n",
      "2025-12-22 09:04:46.258737: Epoch 855\n",
      "2025-12-22 09:04:46.260741: Current learning rate: 0.00176\n",
      "2025-12-22 09:07:04.049012: train_loss -0.861\n",
      "2025-12-22 09:07:04.049012: val_loss -0.8828\n",
      "2025-12-22 09:07:04.064943: Pseudo dice [0.9298, 0.9595, 0.9398]\n",
      "2025-12-22 09:07:04.070953: Epoch time: 137.79 s\n",
      "2025-12-22 09:07:04.829953: \n",
      "2025-12-22 09:07:04.829953: Epoch 856\n",
      "2025-12-22 09:07:04.829953: Current learning rate: 0.00175\n",
      "2025-12-22 09:09:23.080045: train_loss -0.8615\n",
      "2025-12-22 09:09:23.080045: val_loss -0.8908\n",
      "2025-12-22 09:09:23.085789: Pseudo dice [0.9364, 0.9625, 0.9472]\n",
      "2025-12-22 09:09:23.089793: Epoch time: 138.25 s\n",
      "2025-12-22 09:09:23.961469: \n",
      "2025-12-22 09:09:23.961469: Epoch 857\n",
      "2025-12-22 09:09:23.967712: Current learning rate: 0.00174\n",
      "2025-12-22 09:11:41.775815: train_loss -0.8586\n",
      "2025-12-22 09:11:41.775815: val_loss -0.8839\n",
      "2025-12-22 09:11:41.791583: Pseudo dice [0.9314, 0.9588, 0.9476]\n",
      "2025-12-22 09:11:41.791583: Epoch time: 137.81 s\n",
      "2025-12-22 09:11:42.410599: \n",
      "2025-12-22 09:11:42.410599: Epoch 858\n",
      "2025-12-22 09:11:42.426273: Current learning rate: 0.00173\n",
      "2025-12-22 09:14:00.440221: train_loss -0.8654\n",
      "2025-12-22 09:14:00.442223: val_loss -0.8778\n",
      "2025-12-22 09:14:00.448228: Pseudo dice [0.9283, 0.9551, 0.9449]\n",
      "2025-12-22 09:14:00.453782: Epoch time: 138.03 s\n",
      "2025-12-22 09:14:01.192161: \n",
      "2025-12-22 09:14:01.192161: Epoch 859\n",
      "2025-12-22 09:14:01.192161: Current learning rate: 0.00172\n",
      "2025-12-22 09:16:19.140736: train_loss -0.8651\n",
      "2025-12-22 09:16:19.142738: val_loss -0.8799\n",
      "2025-12-22 09:16:19.148746: Pseudo dice [0.9298, 0.9547, 0.9451]\n",
      "2025-12-22 09:16:19.154490: Epoch time: 137.96 s\n",
      "2025-12-22 09:16:19.781982: \n",
      "2025-12-22 09:16:19.783984: Epoch 860\n",
      "2025-12-22 09:16:19.785727: Current learning rate: 0.0017\n",
      "2025-12-22 09:18:37.471238: train_loss -0.8645\n",
      "2025-12-22 09:18:37.473240: val_loss -0.8745\n",
      "2025-12-22 09:18:37.479028: Pseudo dice [0.9301, 0.9549, 0.9355]\n",
      "2025-12-22 09:18:37.485034: Epoch time: 137.69 s\n",
      "2025-12-22 09:18:38.093082: \n",
      "2025-12-22 09:18:38.093082: Epoch 861\n",
      "2025-12-22 09:18:38.108910: Current learning rate: 0.00169\n",
      "2025-12-22 09:20:55.922387: train_loss -0.8558\n",
      "2025-12-22 09:20:55.922387: val_loss -0.8819\n",
      "2025-12-22 09:20:55.927891: Pseudo dice [0.9305, 0.96, 0.9451]\n",
      "2025-12-22 09:20:55.933899: Epoch time: 137.83 s\n",
      "2025-12-22 09:20:56.646804: \n",
      "2025-12-22 09:20:56.646804: Epoch 862\n",
      "2025-12-22 09:20:56.646804: Current learning rate: 0.00168\n",
      "2025-12-22 09:23:14.649064: train_loss -0.8619\n",
      "2025-12-22 09:23:14.651066: val_loss -0.8765\n",
      "2025-12-22 09:23:14.657072: Pseudo dice [0.926, 0.9548, 0.949]\n",
      "2025-12-22 09:23:14.661077: Epoch time: 138.0 s\n",
      "2025-12-22 09:23:15.534715: \n",
      "2025-12-22 09:23:15.534715: Epoch 863\n",
      "2025-12-22 09:23:15.539016: Current learning rate: 0.00167\n",
      "2025-12-22 09:25:33.258098: train_loss -0.8633\n",
      "2025-12-22 09:25:33.258098: val_loss -0.8634\n",
      "2025-12-22 09:25:33.266938: Pseudo dice [0.9199, 0.9514, 0.9367]\n",
      "2025-12-22 09:25:33.270942: Epoch time: 137.72 s\n",
      "2025-12-22 09:25:33.986837: \n",
      "2025-12-22 09:25:33.986837: Epoch 864\n",
      "2025-12-22 09:25:33.988840: Current learning rate: 0.00166\n",
      "2025-12-22 09:27:51.788672: train_loss -0.8586\n",
      "2025-12-22 09:27:51.788672: val_loss -0.8864\n",
      "2025-12-22 09:27:51.794679: Pseudo dice [0.9335, 0.9575, 0.948]\n",
      "2025-12-22 09:27:51.802182: Epoch time: 137.8 s\n",
      "2025-12-22 09:27:52.590415: \n",
      "2025-12-22 09:27:52.590415: Epoch 865\n",
      "2025-12-22 09:27:52.599262: Current learning rate: 0.00165\n",
      "2025-12-22 09:30:10.414006: train_loss -0.866\n",
      "2025-12-22 09:30:10.414006: val_loss -0.8613\n",
      "2025-12-22 09:30:10.423513: Pseudo dice [0.9138, 0.948, 0.9391]\n",
      "2025-12-22 09:30:10.429519: Epoch time: 137.82 s\n",
      "2025-12-22 09:30:11.046664: \n",
      "2025-12-22 09:30:11.046664: Epoch 866\n",
      "2025-12-22 09:30:11.062406: Current learning rate: 0.00164\n",
      "2025-12-22 09:32:29.060141: train_loss -0.8541\n",
      "2025-12-22 09:32:29.060141: val_loss -0.8922\n",
      "2025-12-22 09:32:29.069651: Pseudo dice [0.9385, 0.9634, 0.9415]\n",
      "2025-12-22 09:32:29.073655: Epoch time: 138.01 s\n",
      "2025-12-22 09:32:29.745426: \n",
      "2025-12-22 09:32:29.745426: Epoch 867\n",
      "2025-12-22 09:32:29.761321: Current learning rate: 0.00163\n",
      "2025-12-22 09:34:47.730822: train_loss -0.8584\n",
      "2025-12-22 09:34:47.730822: val_loss -0.8807\n",
      "2025-12-22 09:34:47.738570: Pseudo dice [0.9293, 0.9595, 0.9427]\n",
      "2025-12-22 09:34:47.746579: Epoch time: 137.99 s\n",
      "2025-12-22 09:34:48.368087: \n",
      "2025-12-22 09:34:48.368087: Epoch 868\n",
      "2025-12-22 09:34:48.383800: Current learning rate: 0.00162\n",
      "2025-12-22 09:37:06.232759: train_loss -0.8648\n",
      "2025-12-22 09:37:06.232759: val_loss -0.8651\n",
      "2025-12-22 09:37:06.250792: Pseudo dice [0.9197, 0.9492, 0.9443]\n",
      "2025-12-22 09:37:06.256373: Epoch time: 137.86 s\n",
      "2025-12-22 09:37:06.868227: \n",
      "2025-12-22 09:37:06.868227: Epoch 869\n",
      "2025-12-22 09:37:06.886320: Current learning rate: 0.00161\n",
      "2025-12-22 09:39:24.826097: train_loss -0.8627\n",
      "2025-12-22 09:39:24.827837: val_loss -0.8822\n",
      "2025-12-22 09:39:24.836343: Pseudo dice [0.9311, 0.9552, 0.9443]\n",
      "2025-12-22 09:39:24.842428: Epoch time: 137.96 s\n",
      "2025-12-22 09:39:25.699316: \n",
      "2025-12-22 09:39:25.699316: Epoch 870\n",
      "2025-12-22 09:39:25.717099: Current learning rate: 0.00159\n",
      "2025-12-22 09:41:43.624029: train_loss -0.8592\n",
      "2025-12-22 09:41:43.624029: val_loss -0.8804\n",
      "2025-12-22 09:41:43.629214: Pseudo dice [0.9343, 0.9561, 0.9375]\n",
      "2025-12-22 09:41:43.635724: Epoch time: 137.92 s\n",
      "2025-12-22 09:41:44.268689: \n",
      "2025-12-22 09:41:44.268689: Epoch 871\n",
      "2025-12-22 09:41:44.268689: Current learning rate: 0.00158\n",
      "2025-12-22 09:44:02.149193: train_loss -0.8651\n",
      "2025-12-22 09:44:02.151195: val_loss -0.8978\n",
      "2025-12-22 09:44:02.160716: Pseudo dice [0.9433, 0.9655, 0.9432]\n",
      "2025-12-22 09:44:02.166723: Epoch time: 137.88 s\n",
      "2025-12-22 09:44:02.789164: \n",
      "2025-12-22 09:44:02.789164: Epoch 872\n",
      "2025-12-22 09:44:02.805170: Current learning rate: 0.00157\n",
      "2025-12-22 09:46:20.636817: train_loss -0.8644\n",
      "2025-12-22 09:46:20.636817: val_loss -0.895\n",
      "2025-12-22 09:46:20.642825: Pseudo dice [0.939, 0.9632, 0.9481]\n",
      "2025-12-22 09:46:20.648831: Epoch time: 137.85 s\n",
      "2025-12-22 09:46:21.368814: \n",
      "2025-12-22 09:46:21.368814: Epoch 873\n",
      "2025-12-22 09:46:21.368814: Current learning rate: 0.00156\n",
      "2025-12-22 09:48:39.205363: train_loss -0.8548\n",
      "2025-12-22 09:48:39.205363: val_loss -0.8693\n",
      "2025-12-22 09:48:39.210869: Pseudo dice [0.9249, 0.954, 0.9392]\n",
      "2025-12-22 09:48:39.210869: Epoch time: 137.84 s\n",
      "2025-12-22 09:48:39.844475: \n",
      "2025-12-22 09:48:39.844475: Epoch 874\n",
      "2025-12-22 09:48:39.844475: Current learning rate: 0.00155\n",
      "2025-12-22 09:50:57.622627: train_loss -0.8595\n",
      "2025-12-22 09:50:57.624629: val_loss -0.8865\n",
      "2025-12-22 09:50:57.630635: Pseudo dice [0.9366, 0.9601, 0.937]\n",
      "2025-12-22 09:50:57.638038: Epoch time: 137.78 s\n",
      "2025-12-22 09:50:58.252263: \n",
      "2025-12-22 09:50:58.252263: Epoch 875\n",
      "2025-12-22 09:50:58.269001: Current learning rate: 0.00154\n",
      "2025-12-22 09:53:16.164147: train_loss -0.8628\n",
      "2025-12-22 09:53:16.166149: val_loss -0.8865\n",
      "2025-12-22 09:53:16.177910: Pseudo dice [0.933, 0.9602, 0.9494]\n",
      "2025-12-22 09:53:16.187405: Epoch time: 137.91 s\n",
      "2025-12-22 09:53:17.120829: \n",
      "2025-12-22 09:53:17.120829: Epoch 876\n",
      "2025-12-22 09:53:17.136616: Current learning rate: 0.00153\n",
      "2025-12-22 09:55:35.270168: train_loss -0.8637\n",
      "2025-12-22 09:55:35.270168: val_loss -0.8809\n",
      "2025-12-22 09:55:35.272171: Pseudo dice [0.9291, 0.9538, 0.9492]\n",
      "2025-12-22 09:55:35.272171: Epoch time: 138.15 s\n",
      "2025-12-22 09:55:35.904432: \n",
      "2025-12-22 09:55:35.904432: Epoch 877\n",
      "2025-12-22 09:55:35.904432: Current learning rate: 0.00152\n",
      "2025-12-22 09:57:53.732839: train_loss -0.8603\n",
      "2025-12-22 09:57:53.732839: val_loss -0.892\n",
      "2025-12-22 09:57:53.732839: Pseudo dice [0.937, 0.9645, 0.9457]\n",
      "2025-12-22 09:57:53.748803: Epoch time: 137.84 s\n",
      "2025-12-22 09:57:53.757393: Yayy! New best EMA pseudo Dice: 0.9445\n",
      "2025-12-22 09:57:54.659646: \n",
      "2025-12-22 09:57:54.659646: Epoch 878\n",
      "2025-12-22 09:57:54.661387: Current learning rate: 0.00151\n",
      "2025-12-22 10:00:12.422456: train_loss -0.8601\n",
      "2025-12-22 10:00:12.422456: val_loss -0.8744\n",
      "2025-12-22 10:00:12.431159: Pseudo dice [0.9267, 0.9525, 0.9386]\n",
      "2025-12-22 10:00:12.435164: Epoch time: 137.76 s\n",
      "2025-12-22 10:00:13.185829: \n",
      "2025-12-22 10:00:13.185829: Epoch 879\n",
      "2025-12-22 10:00:13.185829: Current learning rate: 0.00149\n",
      "2025-12-22 10:02:31.129750: train_loss -0.8645\n",
      "2025-12-22 10:02:31.131752: val_loss -0.8778\n",
      "2025-12-22 10:02:31.139336: Pseudo dice [0.9229, 0.9568, 0.952]\n",
      "2025-12-22 10:02:31.143340: Epoch time: 137.95 s\n",
      "2025-12-22 10:02:31.851432: \n",
      "2025-12-22 10:02:31.851432: Epoch 880\n",
      "2025-12-22 10:02:31.853434: Current learning rate: 0.00148\n",
      "2025-12-22 10:04:49.864416: train_loss -0.8598\n",
      "2025-12-22 10:04:49.864416: val_loss -0.8796\n",
      "2025-12-22 10:04:49.874166: Pseudo dice [0.9312, 0.9541, 0.9442]\n",
      "2025-12-22 10:04:49.879911: Epoch time: 138.01 s\n",
      "2025-12-22 10:04:50.567720: \n",
      "2025-12-22 10:04:50.567720: Epoch 881\n",
      "2025-12-22 10:04:50.567720: Current learning rate: 0.00147\n",
      "2025-12-22 10:07:08.378225: train_loss -0.8635\n",
      "2025-12-22 10:07:08.380227: val_loss -0.8758\n",
      "2025-12-22 10:07:08.386233: Pseudo dice [0.9298, 0.9546, 0.9425]\n",
      "2025-12-22 10:07:08.393216: Epoch time: 137.81 s\n",
      "2025-12-22 10:07:09.196802: \n",
      "2025-12-22 10:07:09.196802: Epoch 882\n",
      "2025-12-22 10:07:09.212860: Current learning rate: 0.00146\n",
      "2025-12-22 10:09:27.337934: train_loss -0.8637\n",
      "2025-12-22 10:09:27.337934: val_loss -0.8939\n",
      "2025-12-22 10:09:27.343678: Pseudo dice [0.9374, 0.9616, 0.9488]\n",
      "2025-12-22 10:09:27.351274: Epoch time: 138.14 s\n",
      "2025-12-22 10:09:28.213221: \n",
      "2025-12-22 10:09:28.213221: Epoch 883\n",
      "2025-12-22 10:09:28.229050: Current learning rate: 0.00145\n",
      "2025-12-22 10:11:46.084639: train_loss -0.8634\n",
      "2025-12-22 10:11:46.084639: val_loss -0.8866\n",
      "2025-12-22 10:11:46.090645: Pseudo dice [0.9354, 0.9591, 0.9374]\n",
      "2025-12-22 10:11:46.096651: Epoch time: 137.87 s\n",
      "2025-12-22 10:11:46.727219: \n",
      "2025-12-22 10:11:46.727219: Epoch 884\n",
      "2025-12-22 10:11:46.728961: Current learning rate: 0.00144\n",
      "2025-12-22 10:14:04.766331: train_loss -0.8666\n",
      "2025-12-22 10:14:04.766331: val_loss -0.875\n",
      "2025-12-22 10:14:04.779275: Pseudo dice [0.9246, 0.9509, 0.9467]\n",
      "2025-12-22 10:14:04.786546: Epoch time: 138.04 s\n",
      "2025-12-22 10:14:05.512424: \n",
      "2025-12-22 10:14:05.512424: Epoch 885\n",
      "2025-12-22 10:14:05.512424: Current learning rate: 0.00143\n",
      "2025-12-22 10:16:23.416654: train_loss -0.8636\n",
      "2025-12-22 10:16:23.416654: val_loss -0.8739\n",
      "2025-12-22 10:16:23.432670: Pseudo dice [0.9234, 0.9522, 0.9442]\n",
      "2025-12-22 10:16:23.432670: Epoch time: 137.91 s\n",
      "2025-12-22 10:16:24.062229: \n",
      "2025-12-22 10:16:24.064231: Epoch 886\n",
      "2025-12-22 10:16:24.064231: Current learning rate: 0.00142\n",
      "2025-12-22 10:18:41.876976: train_loss -0.8623\n",
      "2025-12-22 10:18:41.876976: val_loss -0.8769\n",
      "2025-12-22 10:18:41.883985: Pseudo dice [0.9271, 0.9563, 0.9434]\n",
      "2025-12-22 10:18:41.889729: Epoch time: 137.81 s\n",
      "2025-12-22 10:18:42.512120: \n",
      "2025-12-22 10:18:42.512120: Epoch 887\n",
      "2025-12-22 10:18:42.512120: Current learning rate: 0.00141\n",
      "2025-12-22 10:21:00.486575: train_loss -0.8631\n",
      "2025-12-22 10:21:00.490072: val_loss -0.8656\n",
      "2025-12-22 10:21:00.496078: Pseudo dice [0.9202, 0.9487, 0.9397]\n",
      "2025-12-22 10:21:00.502084: Epoch time: 137.98 s\n",
      "2025-12-22 10:21:01.119708: \n",
      "2025-12-22 10:21:01.119708: Epoch 888\n",
      "2025-12-22 10:21:01.135631: Current learning rate: 0.00139\n",
      "2025-12-22 10:23:19.011152: train_loss -0.8658\n",
      "2025-12-22 10:23:19.011152: val_loss -0.8812\n",
      "2025-12-22 10:23:19.026960: Pseudo dice [0.9319, 0.9563, 0.9396]\n",
      "2025-12-22 10:23:19.026960: Epoch time: 137.89 s\n",
      "2025-12-22 10:23:19.847908: \n",
      "2025-12-22 10:23:19.847908: Epoch 889\n",
      "2025-12-22 10:23:19.853921: Current learning rate: 0.00138\n",
      "2025-12-22 10:25:37.694591: train_loss -0.8599\n",
      "2025-12-22 10:25:37.694591: val_loss -0.8746\n",
      "2025-12-22 10:25:37.706344: Pseudo dice [0.9244, 0.9538, 0.9487]\n",
      "2025-12-22 10:25:37.712350: Epoch time: 137.85 s\n",
      "2025-12-22 10:25:38.351972: \n",
      "2025-12-22 10:25:38.351972: Epoch 890\n",
      "2025-12-22 10:25:38.351972: Current learning rate: 0.00137\n",
      "2025-12-22 10:27:56.286879: train_loss -0.8633\n",
      "2025-12-22 10:27:56.288620: val_loss -0.8765\n",
      "2025-12-22 10:27:56.296629: Pseudo dice [0.9299, 0.9555, 0.9442]\n",
      "2025-12-22 10:27:56.304436: Epoch time: 137.93 s\n",
      "2025-12-22 10:27:56.939589: \n",
      "2025-12-22 10:27:56.939589: Epoch 891\n",
      "2025-12-22 10:27:56.939589: Current learning rate: 0.00136\n",
      "2025-12-22 10:30:14.971488: train_loss -0.8614\n",
      "2025-12-22 10:30:14.973490: val_loss -0.8856\n",
      "2025-12-22 10:30:14.981338: Pseudo dice [0.9342, 0.9583, 0.9451]\n",
      "2025-12-22 10:30:14.989174: Epoch time: 138.03 s\n",
      "2025-12-22 10:30:15.623641: \n",
      "2025-12-22 10:30:15.623641: Epoch 892\n",
      "2025-12-22 10:30:15.629702: Current learning rate: 0.00135\n",
      "2025-12-22 10:32:33.509281: train_loss -0.8635\n",
      "2025-12-22 10:32:33.509281: val_loss -0.8717\n",
      "2025-12-22 10:32:33.517291: Pseudo dice [0.9199, 0.9509, 0.9426]\n",
      "2025-12-22 10:32:33.525132: Epoch time: 137.89 s\n",
      "2025-12-22 10:32:34.165112: \n",
      "2025-12-22 10:32:34.165112: Epoch 893\n",
      "2025-12-22 10:32:34.169410: Current learning rate: 0.00134\n",
      "2025-12-22 10:34:52.259030: train_loss -0.8579\n",
      "2025-12-22 10:34:52.262576: val_loss -0.8808\n",
      "2025-12-22 10:34:52.269078: Pseudo dice [0.9293, 0.9578, 0.9438]\n",
      "2025-12-22 10:34:52.274822: Epoch time: 138.09 s\n",
      "2025-12-22 10:34:52.907628: \n",
      "2025-12-22 10:34:52.907628: Epoch 894\n",
      "2025-12-22 10:34:52.907628: Current learning rate: 0.00133\n",
      "2025-12-22 10:37:11.039912: train_loss -0.8611\n",
      "2025-12-22 10:37:11.039912: val_loss -0.8733\n",
      "2025-12-22 10:37:11.047208: Pseudo dice [0.9266, 0.9535, 0.9375]\n",
      "2025-12-22 10:37:11.053214: Epoch time: 138.13 s\n",
      "2025-12-22 10:37:11.676252: \n",
      "2025-12-22 10:37:11.676252: Epoch 895\n",
      "2025-12-22 10:37:11.682243: Current learning rate: 0.00132\n",
      "2025-12-22 10:39:29.622567: train_loss -0.8597\n",
      "2025-12-22 10:39:29.622567: val_loss -0.8742\n",
      "2025-12-22 10:39:29.638593: Pseudo dice [0.9254, 0.9526, 0.94]\n",
      "2025-12-22 10:39:29.638593: Epoch time: 137.95 s\n",
      "2025-12-22 10:39:30.495646: \n",
      "2025-12-22 10:39:30.495646: Epoch 896\n",
      "2025-12-22 10:39:30.495646: Current learning rate: 0.0013\n",
      "2025-12-22 10:41:48.452803: train_loss -0.8607\n",
      "2025-12-22 10:41:48.452803: val_loss -0.8694\n",
      "2025-12-22 10:41:48.462408: Pseudo dice [0.9227, 0.9521, 0.9461]\n",
      "2025-12-22 10:41:48.468414: Epoch time: 137.96 s\n",
      "2025-12-22 10:41:49.104404: \n",
      "2025-12-22 10:41:49.104404: Epoch 897\n",
      "2025-12-22 10:41:49.104404: Current learning rate: 0.00129\n",
      "2025-12-22 10:44:07.008424: train_loss -0.8622\n",
      "2025-12-22 10:44:07.008424: val_loss -0.8908\n",
      "2025-12-22 10:44:07.018437: Pseudo dice [0.9365, 0.9621, 0.9414]\n",
      "2025-12-22 10:44:07.026187: Epoch time: 137.9 s\n",
      "2025-12-22 10:44:07.653412: \n",
      "2025-12-22 10:44:07.653412: Epoch 898\n",
      "2025-12-22 10:44:07.669342: Current learning rate: 0.00128\n",
      "2025-12-22 10:46:25.719336: train_loss -0.8643\n",
      "2025-12-22 10:46:25.719336: val_loss -0.8738\n",
      "2025-12-22 10:46:25.730453: Pseudo dice [0.9265, 0.9517, 0.9451]\n",
      "2025-12-22 10:46:25.734683: Epoch time: 138.07 s\n",
      "2025-12-22 10:46:26.495292: \n",
      "2025-12-22 10:46:26.511050: Epoch 899\n",
      "2025-12-22 10:46:26.517207: Current learning rate: 0.00127\n",
      "2025-12-22 10:48:44.404581: train_loss -0.8658\n",
      "2025-12-22 10:48:44.404581: val_loss -0.877\n",
      "2025-12-22 10:48:44.404581: Pseudo dice [0.9239, 0.9543, 0.9473]\n",
      "2025-12-22 10:48:44.424015: Epoch time: 137.91 s\n",
      "2025-12-22 10:48:45.307891: \n",
      "2025-12-22 10:48:45.307891: Epoch 900\n",
      "2025-12-22 10:48:45.307891: Current learning rate: 0.00126\n",
      "2025-12-22 10:51:03.123864: train_loss -0.8632\n",
      "2025-12-22 10:51:03.125609: val_loss -0.8819\n",
      "2025-12-22 10:51:03.137623: Pseudo dice [0.9302, 0.957, 0.9518]\n",
      "2025-12-22 10:51:03.143368: Epoch time: 137.82 s\n",
      "2025-12-22 10:51:03.776702: \n",
      "2025-12-22 10:51:03.776702: Epoch 901\n",
      "2025-12-22 10:51:03.776702: Current learning rate: 0.00125\n",
      "2025-12-22 10:53:21.685697: train_loss -0.8649\n",
      "2025-12-22 10:53:21.687700: val_loss -0.8882\n",
      "2025-12-22 10:53:21.696712: Pseudo dice [0.9342, 0.9596, 0.9505]\n",
      "2025-12-22 10:53:21.704459: Epoch time: 137.91 s\n",
      "2025-12-22 10:53:22.636829: \n",
      "2025-12-22 10:53:22.636829: Epoch 902\n",
      "2025-12-22 10:53:22.636829: Current learning rate: 0.00124\n",
      "2025-12-22 10:55:40.695425: train_loss -0.8632\n",
      "2025-12-22 10:55:40.695425: val_loss -0.8839\n",
      "2025-12-22 10:55:40.705436: Pseudo dice [0.9352, 0.9575, 0.946]\n",
      "2025-12-22 10:55:40.713185: Epoch time: 138.06 s\n",
      "2025-12-22 10:55:41.414716: \n",
      "2025-12-22 10:55:41.414716: Epoch 903\n",
      "2025-12-22 10:55:41.414716: Current learning rate: 0.00122\n",
      "2025-12-22 10:57:59.318177: train_loss -0.8609\n",
      "2025-12-22 10:57:59.318177: val_loss -0.8922\n",
      "2025-12-22 10:57:59.325690: Pseudo dice [0.9383, 0.9634, 0.9476]\n",
      "2025-12-22 10:57:59.331696: Epoch time: 137.9 s\n",
      "2025-12-22 10:57:59.961990: \n",
      "2025-12-22 10:57:59.961990: Epoch 904\n",
      "2025-12-22 10:57:59.961990: Current learning rate: 0.00121\n",
      "2025-12-22 11:00:17.992548: train_loss -0.8635\n",
      "2025-12-22 11:00:17.992548: val_loss -0.8726\n",
      "2025-12-22 11:00:18.000556: Pseudo dice [0.9278, 0.9528, 0.939]\n",
      "2025-12-22 11:00:18.006423: Epoch time: 138.03 s\n",
      "2025-12-22 11:00:18.729879: \n",
      "2025-12-22 11:00:18.729879: Epoch 905\n",
      "2025-12-22 11:00:18.729879: Current learning rate: 0.0012\n",
      "2025-12-22 11:02:36.735091: train_loss -0.8648\n",
      "2025-12-22 11:02:36.735091: val_loss -0.8774\n",
      "2025-12-22 11:02:36.738833: Pseudo dice [0.9259, 0.9521, 0.9432]\n",
      "2025-12-22 11:02:36.748384: Epoch time: 138.01 s\n",
      "2025-12-22 11:02:37.372746: \n",
      "2025-12-22 11:02:37.372746: Epoch 906\n",
      "2025-12-22 11:02:37.372746: Current learning rate: 0.00119\n",
      "2025-12-22 11:04:55.474852: train_loss -0.8657\n",
      "2025-12-22 11:04:55.474852: val_loss -0.878\n",
      "2025-12-22 11:04:55.487670: Pseudo dice [0.9249, 0.9521, 0.9532]\n",
      "2025-12-22 11:04:55.490674: Epoch time: 138.1 s\n",
      "2025-12-22 11:04:56.122167: \n",
      "2025-12-22 11:04:56.122167: Epoch 907\n",
      "2025-12-22 11:04:56.122167: Current learning rate: 0.00118\n",
      "2025-12-22 11:07:14.209427: train_loss -0.8641\n",
      "2025-12-22 11:07:14.211429: val_loss -0.8803\n",
      "2025-12-22 11:07:14.217261: Pseudo dice [0.9278, 0.9549, 0.9507]\n",
      "2025-12-22 11:07:14.221265: Epoch time: 138.09 s\n",
      "2025-12-22 11:07:15.031990: \n",
      "2025-12-22 11:07:15.042226: Epoch 908\n",
      "2025-12-22 11:07:15.047785: Current learning rate: 0.00117\n",
      "2025-12-22 11:09:33.177063: train_loss -0.8633\n",
      "2025-12-22 11:09:33.177063: val_loss -0.872\n",
      "2025-12-22 11:09:33.190336: Pseudo dice [0.9227, 0.9498, 0.9501]\n",
      "2025-12-22 11:09:33.192338: Epoch time: 138.15 s\n",
      "2025-12-22 11:09:34.028558: \n",
      "2025-12-22 11:09:34.028558: Epoch 909\n",
      "2025-12-22 11:09:34.028558: Current learning rate: 0.00116\n",
      "2025-12-22 11:11:52.004702: train_loss -0.8595\n",
      "2025-12-22 11:11:52.004702: val_loss -0.8857\n",
      "2025-12-22 11:11:52.020656: Pseudo dice [0.934, 0.9589, 0.9423]\n",
      "2025-12-22 11:11:52.024523: Epoch time: 137.98 s\n",
      "2025-12-22 11:11:52.652179: \n",
      "2025-12-22 11:11:52.654182: Epoch 910\n",
      "2025-12-22 11:11:52.654182: Current learning rate: 0.00115\n",
      "2025-12-22 11:14:10.629416: train_loss -0.8619\n",
      "2025-12-22 11:14:10.629416: val_loss -0.8764\n",
      "2025-12-22 11:14:10.639374: Pseudo dice [0.9255, 0.9552, 0.9451]\n",
      "2025-12-22 11:14:10.645511: Epoch time: 137.98 s\n",
      "2025-12-22 11:14:11.388807: \n",
      "2025-12-22 11:14:11.388807: Epoch 911\n",
      "2025-12-22 11:14:11.388807: Current learning rate: 0.00113\n",
      "2025-12-22 11:16:29.848597: train_loss -0.8637\n",
      "2025-12-22 11:16:29.848597: val_loss -0.8859\n",
      "2025-12-22 11:16:29.856048: Pseudo dice [0.9334, 0.9587, 0.9487]\n",
      "2025-12-22 11:16:29.861833: Epoch time: 138.46 s\n",
      "2025-12-22 11:16:30.559253: \n",
      "2025-12-22 11:16:30.574392: Epoch 912\n",
      "2025-12-22 11:16:30.580927: Current learning rate: 0.00112\n",
      "2025-12-22 11:18:48.664574: train_loss -0.8589\n",
      "2025-12-22 11:18:48.664574: val_loss -0.877\n",
      "2025-12-22 11:18:48.673306: Pseudo dice [0.9254, 0.9572, 0.9436]\n",
      "2025-12-22 11:18:48.677310: Epoch time: 138.11 s\n",
      "2025-12-22 11:18:49.296468: \n",
      "2025-12-22 11:18:49.296468: Epoch 913\n",
      "2025-12-22 11:18:49.312285: Current learning rate: 0.00111\n",
      "2025-12-22 11:21:07.517057: train_loss -0.8595\n",
      "2025-12-22 11:21:07.518057: val_loss -0.8915\n",
      "2025-12-22 11:21:07.525164: Pseudo dice [0.9376, 0.9626, 0.9469]\n",
      "2025-12-22 11:21:07.530164: Epoch time: 138.22 s\n",
      "2025-12-22 11:21:08.285797: \n",
      "2025-12-22 11:21:08.285797: Epoch 914\n",
      "2025-12-22 11:21:08.293312: Current learning rate: 0.0011\n",
      "2025-12-22 11:23:26.293665: train_loss -0.8612\n",
      "2025-12-22 11:23:26.293665: val_loss -0.8929\n",
      "2025-12-22 11:23:26.303414: Pseudo dice [0.9408, 0.9636, 0.9457]\n",
      "2025-12-22 11:23:26.308909: Epoch time: 138.01 s\n",
      "2025-12-22 11:23:26.315362: Yayy! New best EMA pseudo Dice: 0.9446\n",
      "2025-12-22 11:23:27.214592: \n",
      "2025-12-22 11:23:27.218097: Epoch 915\n",
      "2025-12-22 11:23:27.218097: Current learning rate: 0.00109\n",
      "2025-12-22 11:25:45.104016: train_loss -0.8641\n",
      "2025-12-22 11:25:45.104016: val_loss -0.8838\n",
      "2025-12-22 11:25:45.116044: Pseudo dice [0.9333, 0.9544, 0.9419]\n",
      "2025-12-22 11:25:45.121791: Epoch time: 137.89 s\n",
      "2025-12-22 11:25:45.977381: \n",
      "2025-12-22 11:25:45.977381: Epoch 916\n",
      "2025-12-22 11:25:45.977381: Current learning rate: 0.00108\n",
      "2025-12-22 11:28:03.711467: train_loss -0.8633\n",
      "2025-12-22 11:28:03.711467: val_loss -0.881\n",
      "2025-12-22 11:28:03.725083: Pseudo dice [0.9322, 0.9554, 0.9401]\n",
      "2025-12-22 11:28:03.731431: Epoch time: 137.73 s\n",
      "2025-12-22 11:28:04.483582: \n",
      "2025-12-22 11:28:04.483582: Epoch 917\n",
      "2025-12-22 11:28:04.493589: Current learning rate: 0.00106\n",
      "2025-12-22 11:30:22.413954: train_loss -0.8599\n",
      "2025-12-22 11:30:22.413954: val_loss -0.8863\n",
      "2025-12-22 11:30:22.415956: Pseudo dice [0.9334, 0.9601, 0.9425]\n",
      "2025-12-22 11:30:22.428006: Epoch time: 137.93 s\n",
      "2025-12-22 11:30:23.054746: \n",
      "2025-12-22 11:30:23.054746: Epoch 918\n",
      "2025-12-22 11:30:23.057221: Current learning rate: 0.00105\n",
      "2025-12-22 11:32:40.967548: train_loss -0.8654\n",
      "2025-12-22 11:32:40.967548: val_loss -0.8825\n",
      "2025-12-22 11:32:40.982877: Pseudo dice [0.9305, 0.9573, 0.9475]\n",
      "2025-12-22 11:32:40.990635: Epoch time: 137.91 s\n",
      "2025-12-22 11:32:41.694549: \n",
      "2025-12-22 11:32:41.694549: Epoch 919\n",
      "2025-12-22 11:32:41.694549: Current learning rate: 0.00104\n",
      "2025-12-22 11:34:59.737281: train_loss -0.8612\n",
      "2025-12-22 11:34:59.737281: val_loss -0.8864\n",
      "2025-12-22 11:34:59.744829: Pseudo dice [0.9358, 0.9601, 0.9418]\n",
      "2025-12-22 11:34:59.750836: Epoch time: 138.04 s\n",
      "2025-12-22 11:35:00.369999: \n",
      "2025-12-22 11:35:00.369999: Epoch 920\n",
      "2025-12-22 11:35:00.369999: Current learning rate: 0.00103\n",
      "2025-12-22 11:37:18.351891: train_loss -0.8645\n",
      "2025-12-22 11:37:18.353893: val_loss -0.881\n",
      "2025-12-22 11:37:18.364913: Pseudo dice [0.9278, 0.9566, 0.9478]\n",
      "2025-12-22 11:37:18.368918: Epoch time: 137.98 s\n",
      "2025-12-22 11:37:18.988699: \n",
      "2025-12-22 11:37:19.002216: Epoch 921\n",
      "2025-12-22 11:37:19.006223: Current learning rate: 0.00102\n",
      "2025-12-22 11:39:36.811407: train_loss -0.8612\n",
      "2025-12-22 11:39:36.813410: val_loss -0.8752\n",
      "2025-12-22 11:39:36.819571: Pseudo dice [0.9271, 0.9543, 0.9434]\n",
      "2025-12-22 11:39:36.819571: Epoch time: 137.82 s\n",
      "2025-12-22 11:39:37.660706: \n",
      "2025-12-22 11:39:37.660706: Epoch 922\n",
      "2025-12-22 11:39:37.660706: Current learning rate: 0.00101\n",
      "2025-12-22 11:41:55.448290: train_loss -0.865\n",
      "2025-12-22 11:41:55.448290: val_loss -0.8906\n",
      "2025-12-22 11:41:55.456357: Pseudo dice [0.9377, 0.9623, 0.9457]\n",
      "2025-12-22 11:41:55.462364: Epoch time: 137.79 s\n",
      "2025-12-22 11:41:55.468108: Yayy! New best EMA pseudo Dice: 0.9447\n",
      "2025-12-22 11:41:56.417685: \n",
      "2025-12-22 11:41:56.417685: Epoch 923\n",
      "2025-12-22 11:41:56.417685: Current learning rate: 0.001\n",
      "2025-12-22 11:44:14.301144: train_loss -0.8653\n",
      "2025-12-22 11:44:14.301144: val_loss -0.8878\n",
      "2025-12-22 11:44:14.301144: Pseudo dice [0.9339, 0.9587, 0.9446]\n",
      "2025-12-22 11:44:14.316925: Epoch time: 137.88 s\n",
      "2025-12-22 11:44:14.316925: Yayy! New best EMA pseudo Dice: 0.9448\n",
      "2025-12-22 11:44:15.279531: \n",
      "2025-12-22 11:44:15.279531: Epoch 924\n",
      "2025-12-22 11:44:15.295597: Current learning rate: 0.00098\n",
      "2025-12-22 11:46:33.221920: train_loss -0.8625\n",
      "2025-12-22 11:46:33.221920: val_loss -0.884\n",
      "2025-12-22 11:46:33.229165: Pseudo dice [0.9313, 0.9587, 0.9454]\n",
      "2025-12-22 11:46:33.235171: Epoch time: 137.94 s\n",
      "2025-12-22 11:46:33.239175: Yayy! New best EMA pseudo Dice: 0.9448\n",
      "2025-12-22 11:46:34.221844: \n",
      "2025-12-22 11:46:34.223847: Epoch 925\n",
      "2025-12-22 11:46:34.230207: Current learning rate: 0.00097\n",
      "2025-12-22 11:48:52.099466: train_loss -0.8618\n",
      "2025-12-22 11:48:52.099466: val_loss -0.8756\n",
      "2025-12-22 11:48:52.107312: Pseudo dice [0.9235, 0.9557, 0.95]\n",
      "2025-12-22 11:48:52.113319: Epoch time: 137.88 s\n",
      "2025-12-22 11:48:52.732183: \n",
      "2025-12-22 11:48:52.732183: Epoch 926\n",
      "2025-12-22 11:48:52.747968: Current learning rate: 0.00096\n",
      "2025-12-22 11:51:10.548391: train_loss -0.8605\n",
      "2025-12-22 11:51:10.548391: val_loss -0.8821\n",
      "2025-12-22 11:51:10.566202: Pseudo dice [0.929, 0.9569, 0.9485]\n",
      "2025-12-22 11:51:10.573706: Epoch time: 137.82 s\n",
      "2025-12-22 11:51:11.207510: \n",
      "2025-12-22 11:51:11.207510: Epoch 927\n",
      "2025-12-22 11:51:11.213829: Current learning rate: 0.00095\n",
      "2025-12-22 11:53:28.818222: train_loss -0.8659\n",
      "2025-12-22 11:53:28.818222: val_loss -0.8936\n",
      "2025-12-22 11:53:28.825973: Pseudo dice [0.9407, 0.9629, 0.9455]\n",
      "2025-12-22 11:53:28.829979: Epoch time: 137.61 s\n",
      "2025-12-22 11:53:28.835988: Yayy! New best EMA pseudo Dice: 0.9452\n",
      "2025-12-22 11:53:30.110327: \n",
      "2025-12-22 11:53:30.110327: Epoch 928\n",
      "2025-12-22 11:53:30.131206: Current learning rate: 0.00094\n",
      "2025-12-22 11:55:47.853731: train_loss -0.8629\n",
      "2025-12-22 11:55:47.855473: val_loss -0.8759\n",
      "2025-12-22 11:55:47.859941: Pseudo dice [0.9264, 0.9535, 0.9448]\n",
      "2025-12-22 11:55:47.859941: Epoch time: 137.74 s\n",
      "2025-12-22 11:55:48.488600: \n",
      "2025-12-22 11:55:48.488600: Epoch 929\n",
      "2025-12-22 11:55:48.506396: Current learning rate: 0.00092\n",
      "2025-12-22 11:58:06.341980: train_loss -0.8615\n",
      "2025-12-22 11:58:06.343983: val_loss -0.8867\n",
      "2025-12-22 11:58:06.351903: Pseudo dice [0.9353, 0.959, 0.9424]\n",
      "2025-12-22 11:58:06.359914: Epoch time: 137.85 s\n",
      "2025-12-22 11:58:06.982687: \n",
      "2025-12-22 11:58:06.982687: Epoch 930\n",
      "2025-12-22 11:58:06.998380: Current learning rate: 0.00091\n",
      "2025-12-22 12:00:24.834393: train_loss -0.8652\n",
      "2025-12-22 12:00:24.834393: val_loss -0.8854\n",
      "2025-12-22 12:00:24.841904: Pseudo dice [0.9325, 0.9572, 0.9516]\n",
      "2025-12-22 12:00:24.841904: Epoch time: 137.85 s\n",
      "2025-12-22 12:00:25.550134: \n",
      "2025-12-22 12:00:25.550134: Epoch 931\n",
      "2025-12-22 12:00:25.550134: Current learning rate: 0.0009\n",
      "2025-12-22 12:02:43.498595: train_loss -0.8611\n",
      "2025-12-22 12:02:43.498595: val_loss -0.8884\n",
      "2025-12-22 12:02:43.504340: Pseudo dice [0.932, 0.9614, 0.9491]\n",
      "2025-12-22 12:02:43.504340: Epoch time: 137.95 s\n",
      "2025-12-22 12:02:43.518181: Yayy! New best EMA pseudo Dice: 0.9453\n",
      "2025-12-22 12:02:44.421759: \n",
      "2025-12-22 12:02:44.421759: Epoch 932\n",
      "2025-12-22 12:02:44.421759: Current learning rate: 0.00089\n",
      "2025-12-22 12:05:02.253435: train_loss -0.8638\n",
      "2025-12-22 12:05:02.255438: val_loss -0.8821\n",
      "2025-12-22 12:05:02.261445: Pseudo dice [0.9289, 0.9553, 0.948]\n",
      "2025-12-22 12:05:02.267451: Epoch time: 137.83 s\n",
      "2025-12-22 12:05:02.895387: \n",
      "2025-12-22 12:05:02.895387: Epoch 933\n",
      "2025-12-22 12:05:02.895387: Current learning rate: 0.00088\n",
      "2025-12-22 12:07:20.717290: train_loss -0.8695\n",
      "2025-12-22 12:07:20.719292: val_loss -0.8739\n",
      "2025-12-22 12:07:20.721294: Pseudo dice [0.9232, 0.9522, 0.9458]\n",
      "2025-12-22 12:07:20.730797: Epoch time: 137.82 s\n",
      "2025-12-22 12:07:21.654193: \n",
      "2025-12-22 12:07:21.670115: Epoch 934\n",
      "2025-12-22 12:07:21.676387: Current learning rate: 0.00087\n",
      "2025-12-22 12:09:39.812705: train_loss -0.8665\n",
      "2025-12-22 12:09:39.812705: val_loss -0.9017\n",
      "2025-12-22 12:09:39.820284: Pseudo dice [0.9442, 0.969, 0.9418]\n",
      "2025-12-22 12:09:39.826290: Epoch time: 138.16 s\n",
      "2025-12-22 12:09:39.832296: Yayy! New best EMA pseudo Dice: 0.9454\n",
      "2025-12-22 12:09:40.778757: \n",
      "2025-12-22 12:09:40.778757: Epoch 935\n",
      "2025-12-22 12:09:40.778757: Current learning rate: 0.00085\n",
      "2025-12-22 12:11:58.700069: train_loss -0.8649\n",
      "2025-12-22 12:11:58.701071: val_loss -0.8851\n",
      "2025-12-22 12:11:58.706910: Pseudo dice [0.9334, 0.9594, 0.9515]\n",
      "2025-12-22 12:11:58.712916: Epoch time: 137.92 s\n",
      "2025-12-22 12:11:58.716919: Yayy! New best EMA pseudo Dice: 0.9457\n",
      "2025-12-22 12:11:59.646506: \n",
      "2025-12-22 12:11:59.646506: Epoch 936\n",
      "2025-12-22 12:11:59.646506: Current learning rate: 0.00084\n",
      "2025-12-22 12:14:17.513627: train_loss -0.8675\n",
      "2025-12-22 12:14:17.513627: val_loss -0.8812\n",
      "2025-12-22 12:14:17.521377: Pseudo dice [0.927, 0.9565, 0.9474]\n",
      "2025-12-22 12:14:17.529385: Epoch time: 137.87 s\n",
      "2025-12-22 12:14:18.260326: \n",
      "2025-12-22 12:14:18.260326: Epoch 937\n",
      "2025-12-22 12:14:18.262329: Current learning rate: 0.00083\n",
      "2025-12-22 12:16:36.122338: train_loss -0.8663\n",
      "2025-12-22 12:16:36.122338: val_loss -0.8687\n",
      "2025-12-22 12:16:36.143791: Pseudo dice [0.9196, 0.9521, 0.9451]\n",
      "2025-12-22 12:16:36.147796: Epoch time: 137.86 s\n",
      "2025-12-22 12:16:36.803311: \n",
      "2025-12-22 12:16:36.803311: Epoch 938\n",
      "2025-12-22 12:16:36.803311: Current learning rate: 0.00082\n",
      "2025-12-22 12:18:54.842666: train_loss -0.8654\n",
      "2025-12-22 12:18:54.842666: val_loss -0.8684\n",
      "2025-12-22 12:18:54.850500: Pseudo dice [0.9194, 0.9533, 0.9424]\n",
      "2025-12-22 12:18:54.854504: Epoch time: 138.04 s\n",
      "2025-12-22 12:18:55.553470: \n",
      "2025-12-22 12:18:55.553470: Epoch 939\n",
      "2025-12-22 12:18:55.553470: Current learning rate: 0.00081\n",
      "2025-12-22 12:21:13.238447: train_loss -0.8664\n",
      "2025-12-22 12:21:13.238447: val_loss -0.88\n",
      "2025-12-22 12:21:13.248458: Pseudo dice [0.9291, 0.9557, 0.9423]\n",
      "2025-12-22 12:21:13.256374: Epoch time: 137.68 s\n",
      "2025-12-22 12:21:13.918401: \n",
      "2025-12-22 12:21:13.918401: Epoch 940\n",
      "2025-12-22 12:21:13.934106: Current learning rate: 0.00079\n",
      "2025-12-22 12:23:31.779044: train_loss -0.8636\n",
      "2025-12-22 12:23:31.779044: val_loss -0.8797\n",
      "2025-12-22 12:23:31.785051: Pseudo dice [0.9313, 0.96, 0.9423]\n",
      "2025-12-22 12:23:31.791057: Epoch time: 137.86 s\n",
      "2025-12-22 12:23:32.458781: \n",
      "2025-12-22 12:23:32.458781: Epoch 941\n",
      "2025-12-22 12:23:32.474583: Current learning rate: 0.00078\n",
      "2025-12-22 12:25:50.193522: train_loss -0.8677\n",
      "2025-12-22 12:25:50.193522: val_loss -0.8846\n",
      "2025-12-22 12:25:50.197264: Pseudo dice [0.9321, 0.9576, 0.9466]\n",
      "2025-12-22 12:25:50.206507: Epoch time: 137.73 s\n",
      "2025-12-22 12:25:50.928034: \n",
      "2025-12-22 12:25:50.930036: Epoch 942\n",
      "2025-12-22 12:25:50.930036: Current learning rate: 0.00077\n",
      "2025-12-22 12:28:08.748343: train_loss -0.8656\n",
      "2025-12-22 12:28:08.750345: val_loss -0.881\n",
      "2025-12-22 12:28:08.756846: Pseudo dice [0.9266, 0.9563, 0.9489]\n",
      "2025-12-22 12:28:08.762851: Epoch time: 137.82 s\n",
      "2025-12-22 12:28:09.475412: \n",
      "2025-12-22 12:28:09.475412: Epoch 943\n",
      "2025-12-22 12:28:09.480062: Current learning rate: 0.00076\n",
      "2025-12-22 12:30:27.337078: train_loss -0.8629\n",
      "2025-12-22 12:30:27.337078: val_loss -0.8794\n",
      "2025-12-22 12:30:27.343990: Pseudo dice [0.9267, 0.9556, 0.9468]\n",
      "2025-12-22 12:30:27.343990: Epoch time: 137.86 s\n",
      "2025-12-22 12:30:28.028182: \n",
      "2025-12-22 12:30:28.028182: Epoch 944\n",
      "2025-12-22 12:30:28.028182: Current learning rate: 0.00075\n",
      "2025-12-22 12:32:45.825244: train_loss -0.8634\n",
      "2025-12-22 12:32:45.825244: val_loss -0.8736\n",
      "2025-12-22 12:32:45.831250: Pseudo dice [0.924, 0.952, 0.9461]\n",
      "2025-12-22 12:32:45.834254: Epoch time: 137.8 s\n",
      "2025-12-22 12:32:46.606212: \n",
      "2025-12-22 12:32:46.606212: Epoch 945\n",
      "2025-12-22 12:32:46.606212: Current learning rate: 0.00074\n",
      "2025-12-22 12:35:04.405692: train_loss -0.8718\n",
      "2025-12-22 12:35:04.405692: val_loss -0.879\n",
      "2025-12-22 12:35:04.405692: Pseudo dice [0.9277, 0.9564, 0.9464]\n",
      "2025-12-22 12:35:04.421690: Epoch time: 137.8 s\n",
      "2025-12-22 12:35:05.040814: \n",
      "2025-12-22 12:35:05.040814: Epoch 946\n",
      "2025-12-22 12:35:05.056582: Current learning rate: 0.00072\n",
      "2025-12-22 12:37:22.907859: train_loss -0.8669\n",
      "2025-12-22 12:37:22.908862: val_loss -0.8828\n",
      "2025-12-22 12:37:22.914424: Pseudo dice [0.9315, 0.9578, 0.9478]\n",
      "2025-12-22 12:37:22.920430: Epoch time: 137.87 s\n",
      "2025-12-22 12:37:23.701844: \n",
      "2025-12-22 12:37:23.701844: Epoch 947\n",
      "2025-12-22 12:37:23.717689: Current learning rate: 0.00071\n",
      "2025-12-22 12:39:41.522835: train_loss -0.8626\n",
      "2025-12-22 12:39:41.522835: val_loss -0.8694\n",
      "2025-12-22 12:39:41.528840: Pseudo dice [0.9187, 0.9492, 0.9487]\n",
      "2025-12-22 12:39:41.536268: Epoch time: 137.82 s\n",
      "2025-12-22 12:39:42.320286: \n",
      "2025-12-22 12:39:42.320286: Epoch 948\n",
      "2025-12-22 12:39:42.328296: Current learning rate: 0.0007\n",
      "2025-12-22 12:42:00.181080: train_loss -0.8625\n",
      "2025-12-22 12:42:00.181080: val_loss -0.8797\n",
      "2025-12-22 12:42:00.188652: Pseudo dice [0.9272, 0.9546, 0.9506]\n",
      "2025-12-22 12:42:00.194658: Epoch time: 137.86 s\n",
      "2025-12-22 12:42:00.846325: \n",
      "2025-12-22 12:42:00.846325: Epoch 949\n",
      "2025-12-22 12:42:00.851860: Current learning rate: 0.00069\n",
      "2025-12-22 12:44:18.539746: train_loss -0.8665\n",
      "2025-12-22 12:44:18.539746: val_loss -0.8972\n",
      "2025-12-22 12:44:18.547492: Pseudo dice [0.9386, 0.9652, 0.9491]\n",
      "2025-12-22 12:44:18.553498: Epoch time: 137.7 s\n",
      "2025-12-22 12:44:19.438853: \n",
      "2025-12-22 12:44:19.438853: Epoch 950\n",
      "2025-12-22 12:44:19.454890: Current learning rate: 0.00067\n",
      "2025-12-22 12:46:37.220404: train_loss -0.8662\n",
      "2025-12-22 12:46:37.220404: val_loss -0.8834\n",
      "2025-12-22 12:46:37.228280: Pseudo dice [0.9266, 0.9568, 0.9467]\n",
      "2025-12-22 12:46:37.232284: Epoch time: 137.78 s\n",
      "2025-12-22 12:46:37.968798: \n",
      "2025-12-22 12:46:37.968798: Epoch 951\n",
      "2025-12-22 12:46:37.977250: Current learning rate: 0.00066\n",
      "2025-12-22 12:48:55.814580: train_loss -0.8663\n",
      "2025-12-22 12:48:55.816583: val_loss -0.8683\n",
      "2025-12-22 12:48:55.816583: Pseudo dice [0.9198, 0.948, 0.945]\n",
      "2025-12-22 12:48:55.824258: Epoch time: 137.85 s\n",
      "2025-12-22 12:48:56.474036: \n",
      "2025-12-22 12:48:56.474036: Epoch 952\n",
      "2025-12-22 12:48:56.491830: Current learning rate: 0.00065\n",
      "2025-12-22 12:51:14.425612: train_loss -0.8647\n",
      "2025-12-22 12:51:14.425612: val_loss -0.8826\n",
      "2025-12-22 12:51:14.430854: Pseudo dice [0.9293, 0.9601, 0.9421]\n",
      "2025-12-22 12:51:14.430854: Epoch time: 137.95 s\n",
      "2025-12-22 12:51:15.310738: \n",
      "2025-12-22 12:51:15.310738: Epoch 953\n",
      "2025-12-22 12:51:15.310738: Current learning rate: 0.00064\n",
      "2025-12-22 12:53:33.290846: train_loss -0.8655\n",
      "2025-12-22 12:53:33.290846: val_loss -0.8762\n",
      "2025-12-22 12:53:33.290846: Pseudo dice [0.9239, 0.9531, 0.9417]\n",
      "2025-12-22 12:53:33.308802: Epoch time: 137.98 s\n",
      "2025-12-22 12:53:34.082306: \n",
      "2025-12-22 12:53:34.082306: Epoch 954\n",
      "2025-12-22 12:53:34.082306: Current learning rate: 0.00063\n",
      "2025-12-22 12:55:51.813989: train_loss -0.8618\n",
      "2025-12-22 12:55:51.813989: val_loss -0.8661\n",
      "2025-12-22 12:55:51.831708: Pseudo dice [0.9227, 0.949, 0.9422]\n",
      "2025-12-22 12:55:51.831708: Epoch time: 137.73 s\n",
      "2025-12-22 12:55:52.466164: \n",
      "2025-12-22 12:55:52.466164: Epoch 955\n",
      "2025-12-22 12:55:52.466164: Current learning rate: 0.00061\n",
      "2025-12-22 12:58:10.353802: train_loss -0.8667\n",
      "2025-12-22 12:58:10.353802: val_loss -0.8565\n",
      "2025-12-22 12:58:10.353802: Pseudo dice [0.9127, 0.9468, 0.9429]\n",
      "2025-12-22 12:58:10.367535: Epoch time: 137.9 s\n",
      "2025-12-22 12:58:11.000102: \n",
      "2025-12-22 12:58:11.000102: Epoch 956\n",
      "2025-12-22 12:58:11.016226: Current learning rate: 0.0006\n",
      "2025-12-22 13:00:29.043654: train_loss -0.8622\n",
      "2025-12-22 13:00:29.045657: val_loss -0.8825\n",
      "2025-12-22 13:00:29.055666: Pseudo dice [0.9346, 0.9564, 0.9391]\n",
      "2025-12-22 13:00:29.061301: Epoch time: 138.04 s\n",
      "2025-12-22 13:00:29.695061: \n",
      "2025-12-22 13:00:29.695061: Epoch 957\n",
      "2025-12-22 13:00:29.709059: Current learning rate: 0.00059\n",
      "2025-12-22 13:02:47.456319: train_loss -0.8673\n",
      "2025-12-22 13:02:47.458060: val_loss -0.8836\n",
      "2025-12-22 13:02:47.467306: Pseudo dice [0.9315, 0.9568, 0.949]\n",
      "2025-12-22 13:02:47.473808: Epoch time: 137.76 s\n",
      "2025-12-22 13:02:48.092392: \n",
      "2025-12-22 13:02:48.108301: Epoch 958\n",
      "2025-12-22 13:02:48.110081: Current learning rate: 0.00058\n",
      "2025-12-22 13:05:06.118757: train_loss -0.8677\n",
      "2025-12-22 13:05:06.118757: val_loss -0.8843\n",
      "2025-12-22 13:05:06.134592: Pseudo dice [0.9308, 0.9571, 0.948]\n",
      "2025-12-22 13:05:06.134592: Epoch time: 138.03 s\n",
      "2025-12-22 13:05:06.768662: \n",
      "2025-12-22 13:05:06.768662: Epoch 959\n",
      "2025-12-22 13:05:06.784636: Current learning rate: 0.00056\n",
      "2025-12-22 13:07:24.791106: train_loss -0.8647\n",
      "2025-12-22 13:07:24.791106: val_loss -0.8844\n",
      "2025-12-22 13:07:24.795112: Pseudo dice [0.9314, 0.9558, 0.9485]\n",
      "2025-12-22 13:07:24.795112: Epoch time: 138.02 s\n",
      "2025-12-22 13:07:25.648983: \n",
      "2025-12-22 13:07:25.648983: Epoch 960\n",
      "2025-12-22 13:07:25.664913: Current learning rate: 0.00055\n",
      "2025-12-22 13:09:43.900657: train_loss -0.8669\n",
      "2025-12-22 13:09:43.900657: val_loss -0.8819\n",
      "2025-12-22 13:09:43.916559: Pseudo dice [0.9311, 0.9581, 0.938]\n",
      "2025-12-22 13:09:43.916559: Epoch time: 138.25 s\n",
      "2025-12-22 13:09:44.597755: \n",
      "2025-12-22 13:09:44.597755: Epoch 961\n",
      "2025-12-22 13:09:44.597755: Current learning rate: 0.00054\n",
      "2025-12-22 13:12:02.356848: train_loss -0.8652\n",
      "2025-12-22 13:12:02.356848: val_loss -0.8827\n",
      "2025-12-22 13:12:02.368649: Pseudo dice [0.93, 0.9582, 0.9444]\n",
      "2025-12-22 13:12:02.368649: Epoch time: 137.76 s\n",
      "2025-12-22 13:12:03.017315: \n",
      "2025-12-22 13:12:03.017315: Epoch 962\n",
      "2025-12-22 13:12:03.017315: Current learning rate: 0.00053\n",
      "2025-12-22 13:14:20.904479: train_loss -0.8667\n",
      "2025-12-22 13:14:20.904479: val_loss -0.8818\n",
      "2025-12-22 13:14:20.912247: Pseudo dice [0.9285, 0.9567, 0.9443]\n",
      "2025-12-22 13:14:20.912247: Epoch time: 137.89 s\n",
      "2025-12-22 13:14:21.543638: \n",
      "2025-12-22 13:14:21.543638: Epoch 963\n",
      "2025-12-22 13:14:21.543638: Current learning rate: 0.00051\n",
      "2025-12-22 13:16:39.362556: train_loss -0.8646\n",
      "2025-12-22 13:16:39.362556: val_loss -0.8826\n",
      "2025-12-22 13:16:39.362556: Pseudo dice [0.9336, 0.9578, 0.943]\n",
      "2025-12-22 13:16:39.380084: Epoch time: 137.82 s\n",
      "2025-12-22 13:16:40.015296: \n",
      "2025-12-22 13:16:40.015296: Epoch 964\n",
      "2025-12-22 13:16:40.015296: Current learning rate: 0.0005\n",
      "2025-12-22 13:18:57.747010: train_loss -0.8634\n",
      "2025-12-22 13:18:57.747010: val_loss -0.8773\n",
      "2025-12-22 13:18:57.762940: Pseudo dice [0.9293, 0.9565, 0.9381]\n",
      "2025-12-22 13:18:57.769304: Epoch time: 137.73 s\n",
      "2025-12-22 13:18:58.508727: \n",
      "2025-12-22 13:18:58.508727: Epoch 965\n",
      "2025-12-22 13:18:58.524352: Current learning rate: 0.00049\n",
      "2025-12-22 13:21:16.445064: train_loss -0.8653\n",
      "2025-12-22 13:21:16.445064: val_loss -0.8903\n",
      "2025-12-22 13:21:16.454843: Pseudo dice [0.9354, 0.9592, 0.9538]\n",
      "2025-12-22 13:21:16.466445: Epoch time: 137.94 s\n",
      "2025-12-22 13:21:17.265109: \n",
      "2025-12-22 13:21:17.265109: Epoch 966\n",
      "2025-12-22 13:21:17.281075: Current learning rate: 0.00048\n",
      "2025-12-22 13:23:35.103041: train_loss -0.8638\n",
      "2025-12-22 13:23:35.103041: val_loss -0.9026\n",
      "2025-12-22 13:23:35.113053: Pseudo dice [0.9425, 0.9668, 0.9518]\n",
      "2025-12-22 13:23:35.118736: Epoch time: 137.84 s\n",
      "2025-12-22 13:23:35.797873: \n",
      "2025-12-22 13:23:35.797873: Epoch 967\n",
      "2025-12-22 13:23:35.805201: Current learning rate: 0.00046\n",
      "2025-12-22 13:25:53.746734: train_loss -0.8615\n",
      "2025-12-22 13:25:53.746734: val_loss -0.878\n",
      "2025-12-22 13:25:53.756485: Pseudo dice [0.9224, 0.9532, 0.9512]\n",
      "2025-12-22 13:25:53.766509: Epoch time: 137.95 s\n",
      "2025-12-22 13:25:54.512628: \n",
      "2025-12-22 13:25:54.514440: Epoch 968\n",
      "2025-12-22 13:25:54.514440: Current learning rate: 0.00045\n",
      "2025-12-22 13:28:12.293613: train_loss -0.8635\n",
      "2025-12-22 13:28:12.293613: val_loss -0.8835\n",
      "2025-12-22 13:28:12.293613: Pseudo dice [0.93, 0.9594, 0.9423]\n",
      "2025-12-22 13:28:12.309353: Epoch time: 137.8 s\n",
      "2025-12-22 13:28:12.931072: \n",
      "2025-12-22 13:28:12.945091: Epoch 969\n",
      "2025-12-22 13:28:12.950971: Current learning rate: 0.00044\n",
      "2025-12-22 13:30:30.638098: train_loss -0.8655\n",
      "2025-12-22 13:30:30.638098: val_loss -0.893\n",
      "2025-12-22 13:30:30.649114: Pseudo dice [0.938, 0.9634, 0.9509]\n",
      "2025-12-22 13:30:30.655121: Epoch time: 137.71 s\n",
      "2025-12-22 13:30:31.294858: \n",
      "2025-12-22 13:30:31.294858: Epoch 970\n",
      "2025-12-22 13:30:31.294858: Current learning rate: 0.00043\n",
      "2025-12-22 13:32:48.975547: train_loss -0.8693\n",
      "2025-12-22 13:32:48.975547: val_loss -0.8826\n",
      "2025-12-22 13:32:48.991320: Pseudo dice [0.9279, 0.9552, 0.9527]\n",
      "2025-12-22 13:32:49.000825: Epoch time: 137.68 s\n",
      "2025-12-22 13:32:49.626588: \n",
      "2025-12-22 13:32:49.626588: Epoch 971\n",
      "2025-12-22 13:32:49.640563: Current learning rate: 0.00041\n",
      "2025-12-22 13:35:07.318259: train_loss -0.8659\n",
      "2025-12-22 13:35:07.318259: val_loss -0.8855\n",
      "2025-12-22 13:35:07.333999: Pseudo dice [0.9336, 0.9585, 0.9457]\n",
      "2025-12-22 13:35:07.333999: Epoch time: 137.69 s\n",
      "2025-12-22 13:35:08.144284: \n",
      "2025-12-22 13:35:08.144284: Epoch 972\n",
      "2025-12-22 13:35:08.144284: Current learning rate: 0.0004\n",
      "2025-12-22 13:37:26.097316: train_loss -0.8633\n",
      "2025-12-22 13:37:26.097316: val_loss -0.8736\n",
      "2025-12-22 13:37:26.105289: Pseudo dice [0.9249, 0.9554, 0.9422]\n",
      "2025-12-22 13:37:26.105289: Epoch time: 137.95 s\n",
      "2025-12-22 13:37:26.741235: \n",
      "2025-12-22 13:37:26.741235: Epoch 973\n",
      "2025-12-22 13:37:26.755205: Current learning rate: 0.00039\n",
      "2025-12-22 13:39:44.424290: train_loss -0.8618\n",
      "2025-12-22 13:39:44.424290: val_loss -0.868\n",
      "2025-12-22 13:39:44.440192: Pseudo dice [0.9171, 0.9497, 0.951]\n",
      "2025-12-22 13:39:44.440192: Epoch time: 137.68 s\n",
      "2025-12-22 13:39:45.074953: \n",
      "2025-12-22 13:39:45.074953: Epoch 974\n",
      "2025-12-22 13:39:45.074953: Current learning rate: 0.00037\n",
      "2025-12-22 13:42:02.889152: train_loss -0.8614\n",
      "2025-12-22 13:42:02.889152: val_loss -0.8855\n",
      "2025-12-22 13:42:02.899163: Pseudo dice [0.9333, 0.9601, 0.9469]\n",
      "2025-12-22 13:42:02.906910: Epoch time: 137.81 s\n",
      "2025-12-22 13:42:03.535829: \n",
      "2025-12-22 13:42:03.535829: Epoch 975\n",
      "2025-12-22 13:42:03.551744: Current learning rate: 0.00036\n",
      "2025-12-22 13:44:21.392026: train_loss -0.8655\n",
      "2025-12-22 13:44:21.392026: val_loss -0.8865\n",
      "2025-12-22 13:44:21.403786: Pseudo dice [0.9311, 0.96, 0.9472]\n",
      "2025-12-22 13:44:21.411287: Epoch time: 137.86 s\n",
      "2025-12-22 13:44:22.177020: \n",
      "2025-12-22 13:44:22.177020: Epoch 976\n",
      "2025-12-22 13:44:22.177020: Current learning rate: 0.00035\n",
      "2025-12-22 13:46:39.872353: train_loss -0.8668\n",
      "2025-12-22 13:46:39.872353: val_loss -0.8821\n",
      "2025-12-22 13:46:39.884194: Pseudo dice [0.9277, 0.9547, 0.9459]\n",
      "2025-12-22 13:46:39.890200: Epoch time: 137.7 s\n",
      "2025-12-22 13:46:40.593047: \n",
      "2025-12-22 13:46:40.593047: Epoch 977\n",
      "2025-12-22 13:46:40.593047: Current learning rate: 0.00034\n",
      "2025-12-22 13:48:58.226204: train_loss -0.8638\n",
      "2025-12-22 13:48:58.228205: val_loss -0.8829\n",
      "2025-12-22 13:48:58.234210: Pseudo dice [0.9328, 0.9595, 0.9406]\n",
      "2025-12-22 13:48:58.237770: Epoch time: 137.63 s\n",
      "2025-12-22 13:48:58.868611: \n",
      "2025-12-22 13:48:58.868611: Epoch 978\n",
      "2025-12-22 13:48:58.886513: Current learning rate: 0.00032\n",
      "2025-12-22 13:51:16.598686: train_loss -0.8691\n",
      "2025-12-22 13:51:16.598686: val_loss -0.8878\n",
      "2025-12-22 13:51:16.605926: Pseudo dice [0.9361, 0.9598, 0.9426]\n",
      "2025-12-22 13:51:16.605926: Epoch time: 137.73 s\n",
      "2025-12-22 13:51:17.462154: \n",
      "2025-12-22 13:51:17.462154: Epoch 979\n",
      "2025-12-22 13:51:17.462154: Current learning rate: 0.00031\n",
      "2025-12-22 13:53:35.194525: train_loss -0.8671\n",
      "2025-12-22 13:53:35.194525: val_loss -0.8809\n",
      "2025-12-22 13:53:35.208544: Pseudo dice [0.9285, 0.9554, 0.9496]\n",
      "2025-12-22 13:53:35.216292: Epoch time: 137.73 s\n",
      "2025-12-22 13:53:35.842880: \n",
      "2025-12-22 13:53:35.842880: Epoch 980\n",
      "2025-12-22 13:53:35.858549: Current learning rate: 0.0003\n",
      "2025-12-22 13:55:53.665803: train_loss -0.8642\n",
      "2025-12-22 13:55:53.681455: val_loss -0.899\n",
      "2025-12-22 13:55:53.687461: Pseudo dice [0.9437, 0.964, 0.9444]\n",
      "2025-12-22 13:55:53.691465: Epoch time: 137.82 s\n",
      "2025-12-22 13:55:54.329812: \n",
      "2025-12-22 13:55:54.329812: Epoch 981\n",
      "2025-12-22 13:55:54.329812: Current learning rate: 0.00028\n",
      "2025-12-22 13:58:12.151977: train_loss -0.8643\n",
      "2025-12-22 13:58:12.151977: val_loss -0.882\n",
      "2025-12-22 13:58:12.151977: Pseudo dice [0.9333, 0.9563, 0.9406]\n",
      "2025-12-22 13:58:12.151977: Epoch time: 137.82 s\n",
      "2025-12-22 13:58:12.785918: \n",
      "2025-12-22 13:58:12.801765: Epoch 982\n",
      "2025-12-22 13:58:12.801765: Current learning rate: 0.00027\n",
      "2025-12-22 14:00:30.478902: train_loss -0.8633\n",
      "2025-12-22 14:00:30.478902: val_loss -0.8884\n",
      "2025-12-22 14:00:30.486648: Pseudo dice [0.9331, 0.9586, 0.9495]\n",
      "2025-12-22 14:00:30.488650: Epoch time: 137.69 s\n",
      "2025-12-22 14:00:31.119989: \n",
      "2025-12-22 14:00:31.119989: Epoch 983\n",
      "2025-12-22 14:00:31.135815: Current learning rate: 0.00026\n",
      "2025-12-22 14:02:49.452609: train_loss -0.8656\n",
      "2025-12-22 14:02:49.452609: val_loss -0.8733\n",
      "2025-12-22 14:02:49.462369: Pseudo dice [0.9211, 0.9543, 0.9439]\n",
      "2025-12-22 14:02:49.470380: Epoch time: 138.33 s\n",
      "2025-12-22 14:02:50.205011: \n",
      "2025-12-22 14:02:50.205011: Epoch 984\n",
      "2025-12-22 14:02:50.220968: Current learning rate: 0.00024\n",
      "2025-12-22 14:05:08.627758: train_loss -0.8664\n",
      "2025-12-22 14:05:08.627758: val_loss -0.8719\n",
      "2025-12-22 14:05:08.631600: Pseudo dice [0.9215, 0.9505, 0.9543]\n",
      "2025-12-22 14:05:08.631600: Epoch time: 138.42 s\n",
      "2025-12-22 14:05:09.449492: \n",
      "2025-12-22 14:05:09.449492: Epoch 985\n",
      "2025-12-22 14:05:09.449492: Current learning rate: 0.00023\n",
      "2025-12-22 14:07:27.721973: train_loss -0.8683\n",
      "2025-12-22 14:07:27.721973: val_loss -0.9018\n",
      "2025-12-22 14:07:27.735821: Pseudo dice [0.9431, 0.9671, 0.9516]\n",
      "2025-12-22 14:07:27.735821: Epoch time: 138.27 s\n",
      "2025-12-22 14:07:28.384240: \n",
      "2025-12-22 14:07:28.384240: Epoch 986\n",
      "2025-12-22 14:07:28.384240: Current learning rate: 0.00021\n",
      "2025-12-22 14:09:46.975533: train_loss -0.8637\n",
      "2025-12-22 14:09:46.975533: val_loss -0.8964\n",
      "2025-12-22 14:09:46.985282: Pseudo dice [0.9401, 0.9648, 0.9504]\n",
      "2025-12-22 14:09:46.994541: Epoch time: 138.59 s\n",
      "2025-12-22 14:09:47.000548: Yayy! New best EMA pseudo Dice: 0.946\n",
      "2025-12-22 14:09:47.914744: \n",
      "2025-12-22 14:09:47.914744: Epoch 987\n",
      "2025-12-22 14:09:47.930758: Current learning rate: 0.0002\n",
      "2025-12-22 14:12:06.201102: train_loss -0.8645\n",
      "2025-12-22 14:12:06.201102: val_loss -0.8704\n",
      "2025-12-22 14:12:06.201102: Pseudo dice [0.9175, 0.9525, 0.9485]\n",
      "2025-12-22 14:12:06.201102: Epoch time: 138.29 s\n",
      "2025-12-22 14:12:06.862996: \n",
      "2025-12-22 14:12:06.862996: Epoch 988\n",
      "2025-12-22 14:12:06.862996: Current learning rate: 0.00019\n",
      "2025-12-22 14:14:24.861650: train_loss -0.8646\n",
      "2025-12-22 14:14:24.861650: val_loss -0.8803\n",
      "2025-12-22 14:14:24.871399: Pseudo dice [0.9295, 0.9577, 0.9467]\n",
      "2025-12-22 14:14:24.879146: Epoch time: 138.0 s\n",
      "2025-12-22 14:14:25.511923: \n",
      "2025-12-22 14:14:25.511923: Epoch 989\n",
      "2025-12-22 14:14:25.511923: Current learning rate: 0.00017\n",
      "2025-12-22 14:16:43.296999: train_loss -0.8649\n",
      "2025-12-22 14:16:43.296999: val_loss -0.8791\n",
      "2025-12-22 14:16:43.305006: Pseudo dice [0.9279, 0.9597, 0.9407]\n",
      "2025-12-22 14:16:43.309750: Epoch time: 137.79 s\n",
      "2025-12-22 14:16:43.940325: \n",
      "2025-12-22 14:16:43.940325: Epoch 990\n",
      "2025-12-22 14:16:43.940325: Current learning rate: 0.00016\n",
      "2025-12-22 14:19:01.685904: train_loss -0.8651\n",
      "2025-12-22 14:19:01.685904: val_loss -0.8862\n",
      "2025-12-22 14:19:01.693912: Pseudo dice [0.9323, 0.9585, 0.9489]\n",
      "2025-12-22 14:19:01.695652: Epoch time: 137.75 s\n",
      "2025-12-22 14:19:02.502652: \n",
      "2025-12-22 14:19:02.502652: Epoch 991\n",
      "2025-12-22 14:19:02.518608: Current learning rate: 0.00014\n",
      "2025-12-22 14:21:20.228263: train_loss -0.8666\n",
      "2025-12-22 14:21:20.228263: val_loss -0.875\n",
      "2025-12-22 14:21:20.248114: Pseudo dice [0.9235, 0.9542, 0.9469]\n",
      "2025-12-22 14:21:20.254121: Epoch time: 137.73 s\n",
      "2025-12-22 14:21:20.939986: \n",
      "2025-12-22 14:21:20.939986: Epoch 992\n",
      "2025-12-22 14:21:20.939986: Current learning rate: 0.00013\n",
      "2025-12-22 14:23:38.779339: train_loss -0.8653\n",
      "2025-12-22 14:23:38.781342: val_loss -0.891\n",
      "2025-12-22 14:23:38.787056: Pseudo dice [0.9328, 0.9601, 0.9509]\n",
      "2025-12-22 14:23:38.794155: Epoch time: 137.84 s\n",
      "2025-12-22 14:23:39.541124: \n",
      "2025-12-22 14:23:39.541124: Epoch 993\n",
      "2025-12-22 14:23:39.557148: Current learning rate: 0.00011\n",
      "2025-12-22 14:25:57.202301: train_loss -0.8704\n",
      "2025-12-22 14:25:57.204303: val_loss -0.8874\n",
      "2025-12-22 14:25:57.212049: Pseudo dice [0.9347, 0.9629, 0.9452]\n",
      "2025-12-22 14:25:57.218055: Epoch time: 137.66 s\n",
      "2025-12-22 14:25:57.861353: \n",
      "2025-12-22 14:25:57.861353: Epoch 994\n",
      "2025-12-22 14:25:57.861353: Current learning rate: 0.0001\n",
      "2025-12-22 14:28:15.602134: train_loss -0.8639\n",
      "2025-12-22 14:28:15.618134: val_loss -0.8826\n",
      "2025-12-22 14:28:15.620137: Pseudo dice [0.9305, 0.9575, 0.9489]\n",
      "2025-12-22 14:28:15.631543: Epoch time: 137.74 s\n",
      "2025-12-22 14:28:16.268336: \n",
      "2025-12-22 14:28:16.270079: Epoch 995\n",
      "2025-12-22 14:28:16.276147: Current learning rate: 8e-05\n",
      "2025-12-22 14:30:33.911439: train_loss -0.8733\n",
      "2025-12-22 14:30:33.911439: val_loss -0.882\n",
      "2025-12-22 14:30:33.920916: Pseudo dice [0.926, 0.9546, 0.9529]\n",
      "2025-12-22 14:30:33.927260: Epoch time: 137.66 s\n",
      "2025-12-22 14:30:34.687521: \n",
      "2025-12-22 14:30:34.689524: Epoch 996\n",
      "2025-12-22 14:30:34.689524: Current learning rate: 7e-05\n",
      "2025-12-22 14:32:52.576103: train_loss -0.8637\n",
      "2025-12-22 14:32:52.576103: val_loss -0.8789\n",
      "2025-12-22 14:32:52.583849: Pseudo dice [0.9271, 0.9555, 0.9437]\n",
      "2025-12-22 14:32:52.583849: Epoch time: 137.89 s\n",
      "2025-12-22 14:32:53.391068: \n",
      "2025-12-22 14:32:53.391068: Epoch 997\n",
      "2025-12-22 14:32:53.408792: Current learning rate: 5e-05\n",
      "2025-12-22 14:35:11.253652: train_loss -0.8652\n",
      "2025-12-22 14:35:11.253652: val_loss -0.8927\n",
      "2025-12-22 14:35:11.255910: Pseudo dice [0.936, 0.9632, 0.9482]\n",
      "2025-12-22 14:35:11.268223: Epoch time: 137.86 s\n",
      "2025-12-22 14:35:11.970302: \n",
      "2025-12-22 14:35:11.970302: Epoch 998\n",
      "2025-12-22 14:35:11.970302: Current learning rate: 4e-05\n",
      "2025-12-22 14:37:29.867473: train_loss -0.8633\n",
      "2025-12-22 14:37:29.867473: val_loss -0.8831\n",
      "2025-12-22 14:37:29.874912: Pseudo dice [0.9293, 0.9586, 0.9479]\n",
      "2025-12-22 14:37:29.880041: Epoch time: 137.9 s\n",
      "2025-12-22 14:37:30.515674: \n",
      "2025-12-22 14:37:30.515674: Epoch 999\n",
      "2025-12-22 14:37:30.535290: Current learning rate: 2e-05\n",
      "2025-12-22 14:39:48.321467: train_loss -0.8647\n",
      "2025-12-22 14:39:48.321467: val_loss -0.8909\n",
      "2025-12-22 14:39:48.337349: Pseudo dice [0.935, 0.9601, 0.9468]\n",
      "2025-12-22 14:39:48.337349: Epoch time: 137.81 s\n",
      "2025-12-22 14:39:49.402870: Training done.\n",
      "2025-12-22 14:39:49.481074: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-22 14:39:49.484225: The split file contains 5 splits.\n",
      "2025-12-22 14:39:49.493895: Desired fold for training: 4\n",
      "2025-12-22 14:39:49.493895: This split has 400 training and 100 validation cases.\n",
      "2025-12-22 14:39:49.509643: predicting OAS30014_MR_d0196_1\n",
      "2025-12-22 14:39:49.667350: OAS30014_MR_d0196_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:40:09.716271: predicting OAS30014_MR_d0196_5\n",
      "2025-12-22 14:40:09.732485: OAS30014_MR_d0196_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:40:26.381360: predicting OAS30017_MR_d0054_2\n",
      "2025-12-22 14:40:26.397046: OAS30017_MR_d0054_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:40:43.136339: predicting OAS30025_MR_d0210_3\n",
      "2025-12-22 14:40:43.138341: OAS30025_MR_d0210_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:40:59.891798: predicting OAS30025_MR_d0210_9\n",
      "2025-12-22 14:40:59.909732: OAS30025_MR_d0210_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:41:16.605964: predicting OAS30036_MR_d0059_4\n",
      "2025-12-22 14:41:16.628471: OAS30036_MR_d0059_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:41:33.336272: predicting OAS30036_MR_d0059_6\n",
      "2025-12-22 14:41:33.360141: OAS30036_MR_d0059_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:41:50.043370: predicting OAS30039_MR_d1203_3\n",
      "2025-12-22 14:41:50.055212: OAS30039_MR_d1203_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:42:06.744204: predicting OAS30052_MR_d0693_8\n",
      "2025-12-22 14:42:06.758809: OAS30052_MR_d0693_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:42:23.417730: predicting OAS30052_MR_d0693_9\n",
      "2025-12-22 14:42:23.435826: OAS30052_MR_d0693_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:42:40.114251: predicting OAS30078_MR_d0210_2\n",
      "2025-12-22 14:42:40.132235: OAS30078_MR_d0210_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:42:56.854941: predicting OAS30078_MR_d0210_3\n",
      "2025-12-22 14:42:56.866078: OAS30078_MR_d0210_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:43:13.585897: predicting OAS30078_MR_d0210_4\n",
      "2025-12-22 14:43:13.601564: OAS30078_MR_d0210_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:43:30.290226: predicting OAS30083_MR_d0465_2\n",
      "2025-12-22 14:43:30.300247: OAS30083_MR_d0465_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:43:46.973828: predicting OAS30083_MR_d0465_4\n",
      "2025-12-22 14:43:46.993556: OAS30083_MR_d0465_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:44:03.661756: predicting OAS30083_MR_d0465_6\n",
      "2025-12-22 14:44:03.677723: OAS30083_MR_d0465_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:44:20.374346: predicting OAS30083_MR_d0465_7\n",
      "2025-12-22 14:44:20.398025: OAS30083_MR_d0465_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:44:37.093187: predicting OAS30087_MR_d0260_1\n",
      "2025-12-22 14:44:37.103044: OAS30087_MR_d0260_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:44:53.815038: predicting OAS30087_MR_d0260_10\n",
      "2025-12-22 14:44:53.826684: OAS30087_MR_d0260_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:45:10.519905: predicting OAS30087_MR_d0260_2\n",
      "2025-12-22 14:45:10.529272: OAS30087_MR_d0260_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:45:27.214906: predicting OAS30087_MR_d0260_3\n",
      "2025-12-22 14:45:27.227958: OAS30087_MR_d0260_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:45:43.894430: predicting OAS30099_MR_d0032_10\n",
      "2025-12-22 14:45:43.917419: OAS30099_MR_d0032_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:46:00.593201: predicting OAS30102_MR_d0024_5\n",
      "2025-12-22 14:46:00.608994: OAS30102_MR_d0024_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:46:17.298918: predicting OAS30102_MR_d0024_7\n",
      "2025-12-22 14:46:17.310690: OAS30102_MR_d0024_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:46:34.019213: predicting OAS30102_MR_d0024_8\n",
      "2025-12-22 14:46:34.032792: OAS30102_MR_d0024_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:46:50.749479: predicting OAS30104_MR_d0328_8\n",
      "2025-12-22 14:46:50.759035: OAS30104_MR_d0328_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:47:07.439041: predicting OAS30104_MR_d0328_9\n",
      "2025-12-22 14:47:07.453118: OAS30104_MR_d0328_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:47:24.137558: predicting OAS30107_MR_d0387_4\n",
      "2025-12-22 14:47:24.155419: OAS30107_MR_d0387_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:47:40.876390: predicting OAS30107_MR_d0387_9\n",
      "2025-12-22 14:47:40.888414: OAS30107_MR_d0387_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:47:57.604997: predicting OAS30125_MR_d0201_3\n",
      "2025-12-22 14:47:57.615075: OAS30125_MR_d0201_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:48:14.338019: predicting OAS30125_MR_d0201_8\n",
      "2025-12-22 14:48:14.347235: OAS30125_MR_d0201_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:48:31.029907: predicting OAS30127_MR_d0098_2\n",
      "2025-12-22 14:48:31.038854: OAS30127_MR_d0098_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:48:47.735379: predicting OAS30127_MR_d0098_3\n",
      "2025-12-22 14:48:47.757903: OAS30127_MR_d0098_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:49:04.496110: predicting OAS30127_MR_d0098_6\n",
      "2025-12-22 14:49:04.506126: OAS30127_MR_d0098_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:49:21.239371: predicting OAS30134_MR_d0080_5\n",
      "2025-12-22 14:49:21.259157: OAS30134_MR_d0080_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:49:37.924852: predicting OAS30140_MR_d0172_6\n",
      "2025-12-22 14:49:37.940516: OAS30140_MR_d0172_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:49:54.630769: predicting OAS30147_MR_d0048_10\n",
      "2025-12-22 14:49:54.644123: OAS30147_MR_d0048_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:50:11.334312: predicting OAS30147_MR_d0048_2\n",
      "2025-12-22 14:50:11.346261: OAS30147_MR_d0048_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:50:28.052670: predicting OAS30147_MR_d0048_8\n",
      "2025-12-22 14:50:28.063851: OAS30147_MR_d0048_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:50:44.813647: predicting OAS30165_MR_d1763_5\n",
      "2025-12-22 14:50:44.829376: OAS30165_MR_d1763_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:51:01.532190: predicting OAS30165_MR_d1763_9\n",
      "2025-12-22 14:51:01.543668: OAS30165_MR_d1763_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:51:18.254567: predicting OAS30176_MR_d0000_10\n",
      "2025-12-22 14:51:18.279015: OAS30176_MR_d0000_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:51:34.967426: predicting OAS30176_MR_d0000_8\n",
      "2025-12-22 14:51:34.979514: OAS30176_MR_d0000_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:51:51.661648: predicting OAS30195_MR_d1596_1\n",
      "2025-12-22 14:51:51.671708: OAS30195_MR_d1596_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:52:08.360058: predicting OAS30195_MR_d1596_8\n",
      "2025-12-22 14:52:08.377499: OAS30195_MR_d1596_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:52:25.038704: predicting OAS30226_MR_d0183_6\n",
      "2025-12-22 14:52:25.056751: OAS30226_MR_d0183_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:52:41.758747: predicting OAS30226_MR_d0183_7\n",
      "2025-12-22 14:52:41.772969: OAS30226_MR_d0183_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:52:58.452410: predicting OAS30234_MR_d2098_10\n",
      "2025-12-22 14:52:58.472300: OAS30234_MR_d2098_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:53:15.180443: predicting OAS30234_MR_d2098_5\n",
      "2025-12-22 14:53:15.190840: OAS30234_MR_d2098_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:53:31.911132: predicting OAS30238_MR_d0037_7\n",
      "2025-12-22 14:53:31.920965: OAS30238_MR_d0037_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:53:48.609536: predicting OAS30238_MR_d0037_8\n",
      "2025-12-22 14:53:48.631171: OAS30238_MR_d0037_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:54:05.360716: predicting OAS30250_MR_d0389_10\n",
      "2025-12-22 14:54:05.368468: OAS30250_MR_d0389_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:54:22.081140: predicting OAS30250_MR_d0389_2\n",
      "2025-12-22 14:54:22.092364: OAS30250_MR_d0389_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:54:38.789834: predicting OAS30250_MR_d0389_7\n",
      "2025-12-22 14:54:38.798817: OAS30250_MR_d0389_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:54:55.519737: predicting OAS30250_MR_d0389_9\n",
      "2025-12-22 14:54:55.537575: OAS30250_MR_d0389_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:55:12.209631: predicting OAS30262_MR_d0037_10\n",
      "2025-12-22 14:55:12.230784: OAS30262_MR_d0037_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:55:28.931482: predicting OAS30262_MR_d0037_3\n",
      "2025-12-22 14:55:28.953171: OAS30262_MR_d0037_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:55:45.641903: predicting OAS30262_MR_d0037_7\n",
      "2025-12-22 14:55:45.655070: OAS30262_MR_d0037_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:56:02.368399: predicting OAS30274_MR_d3332_8\n",
      "2025-12-22 14:56:02.386464: OAS30274_MR_d3332_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:56:19.087112: predicting OAS30274_MR_d3332_9\n",
      "2025-12-22 14:56:19.104982: OAS30274_MR_d3332_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:56:35.782923: predicting OAS30292_MR_d0165_5\n",
      "2025-12-22 14:56:35.804602: OAS30292_MR_d0165_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:56:52.512400: predicting OAS30292_MR_d0165_7\n",
      "2025-12-22 14:56:52.532071: OAS30292_MR_d0165_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:57:09.257203: predicting OAS30292_MR_d0165_8\n",
      "2025-12-22 14:57:09.272453: OAS30292_MR_d0165_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:57:25.946822: predicting OAS30297_MR_d1712_4\n",
      "2025-12-22 14:57:25.956777: OAS30297_MR_d1712_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:57:42.652165: predicting OAS30297_MR_d1712_7\n",
      "2025-12-22 14:57:42.664532: OAS30297_MR_d1712_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:57:59.353750: predicting OAS30300_MR_d0100_2\n",
      "2025-12-22 14:57:59.363265: OAS30300_MR_d0100_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:58:16.058629: predicting OAS30300_MR_d0100_9\n",
      "2025-12-22 14:58:16.067755: OAS30300_MR_d0100_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:58:32.784796: predicting OAS30302_MR_d0262_5\n",
      "2025-12-22 14:58:32.800868: OAS30302_MR_d0262_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:58:49.485019: predicting OAS30321_MR_d3003_10\n",
      "2025-12-22 14:58:49.508759: OAS30321_MR_d3003_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:59:06.246440: predicting OAS30321_MR_d3003_7\n",
      "2025-12-22 14:59:06.262444: OAS30321_MR_d3003_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:59:23.000753: predicting OAS30325_MR_d0032_3\n",
      "2025-12-22 14:59:23.013815: OAS30325_MR_d0032_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:59:39.728926: predicting OAS30325_MR_d0032_4\n",
      "2025-12-22 14:59:39.754313: OAS30325_MR_d0032_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 14:59:56.413007: predicting OAS30325_MR_d0032_7\n",
      "2025-12-22 14:59:56.435364: OAS30325_MR_d0032_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:00:13.133623: predicting OAS30343_MR_d4178_3\n",
      "2025-12-22 15:00:13.144390: OAS30343_MR_d4178_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:00:29.866040: predicting OAS30343_MR_d4178_7\n",
      "2025-12-22 15:00:29.885836: OAS30343_MR_d4178_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:00:46.616010: predicting OAS30349_MR_d0699_9\n",
      "2025-12-22 15:00:46.630475: OAS30349_MR_d0699_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:01:03.316524: predicting OAS30350_MR_d0018_1\n",
      "2025-12-22 15:01:03.328438: OAS30350_MR_d0018_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:01:20.038378: predicting OAS30350_MR_d0018_10\n",
      "2025-12-22 15:01:20.038378: OAS30350_MR_d0018_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:01:36.743700: predicting OAS30350_MR_d0018_4\n",
      "2025-12-22 15:01:36.753583: OAS30350_MR_d0018_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:01:53.444924: predicting OAS30350_MR_d0018_9\n",
      "2025-12-22 15:01:53.455737: OAS30350_MR_d0018_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:02:10.163438: predicting OAS30352_MR_d0099_2\n",
      "2025-12-22 15:02:10.177790: OAS30352_MR_d0099_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:02:26.879320: predicting OAS30352_MR_d0099_7\n",
      "2025-12-22 15:02:26.891307: OAS30352_MR_d0099_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:02:43.597914: predicting OAS30352_MR_d0099_9\n",
      "2025-12-22 15:02:43.609600: OAS30352_MR_d0099_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:03:00.322513: predicting OAS30354_MR_d0056_8\n",
      "2025-12-22 15:03:00.340498: OAS30354_MR_d0056_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:03:17.025145: predicting OAS30355_MR_d0048_5\n",
      "2025-12-22 15:03:17.033155: OAS30355_MR_d0048_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:03:33.717803: predicting OAS30367_MR_d1540_1\n",
      "2025-12-22 15:03:33.733548: OAS30367_MR_d1540_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:03:50.414814: predicting OAS30367_MR_d1540_5\n",
      "2025-12-22 15:03:50.434684: OAS30367_MR_d1540_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:04:07.136345: predicting OAS30369_MR_d4058_4\n",
      "2025-12-22 15:04:07.156029: OAS30369_MR_d4058_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:04:23.838588: predicting OAS30369_MR_d4058_5\n",
      "2025-12-22 15:04:23.858477: OAS30369_MR_d4058_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:04:40.562621: predicting OAS30369_MR_d4058_9\n",
      "2025-12-22 15:04:40.582402: OAS30369_MR_d4058_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:04:57.251385: predicting OAS30371_MR_d0338_1\n",
      "2025-12-22 15:04:57.266731: OAS30371_MR_d0338_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:05:13.935996: predicting OAS30371_MR_d0338_10\n",
      "2025-12-22 15:05:13.954034: OAS30371_MR_d0338_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:05:30.641183: predicting OAS30371_MR_d0338_8\n",
      "2025-12-22 15:05:30.651418: OAS30371_MR_d0338_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:05:47.339211: predicting OAS30371_MR_d0338_9\n",
      "2025-12-22 15:05:47.346873: OAS30371_MR_d0338_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:06:04.079604: predicting OAS30373_MR_d1211_1\n",
      "2025-12-22 15:06:04.093123: OAS30373_MR_d1211_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:06:20.818209: predicting OAS30380_MR_d3446_10\n",
      "2025-12-22 15:06:20.827700: OAS30380_MR_d3446_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:06:37.536097: predicting OAS30383_MR_d0134_2\n",
      "2025-12-22 15:06:37.551996: OAS30383_MR_d0134_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:06:54.277583: predicting OAS30383_MR_d0134_5\n",
      "2025-12-22 15:06:54.287484: OAS30383_MR_d0134_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:07:11.231818: predicting OAS30388_MR_d0073_1\n",
      "2025-12-22 15:07:11.251713: OAS30388_MR_d0073_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:07:28.323700: predicting OAS30388_MR_d0073_10\n",
      "2025-12-22 15:07:28.341530: OAS30388_MR_d0073_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-22 15:08:11.815394: Validation complete\n",
      "2025-12-22 15:08:11.815394: Mean Validation Dice:  0.933902076342387\n"
     ]
    }
   ],
   "source": [
    "#Train nnU-Net on fold 4\n",
    "!nnUNetv2_train 500 3d_lowres 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0a6ae-77a3-4557-b944-2dc3a5383f6b",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c800529-9a10-4525-9727-dfc227174357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 100 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 100 cases that I would like to predict\n",
      "\n",
      "Predicting sub-OAS30003_sess-d4954_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30003_sess-d4954_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30007_ses-d1641_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30007_ses-d1641_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30008_ses-d0061_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30008_ses-d0061_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30009_ses-d2457_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30009_ses-d2457_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30014_ses-d1176_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30014_ses-d1176_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30016_ses-d0021_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30016_ses-d0021_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30018_ses-d0070_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30018_ses-d0070_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30033_ses-d1267_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30033_ses-d1267_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30035_ses-d2218_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30035_ses-d2218_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30043_ses-d0145_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30043_ses-d0145_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30052_ses-d2709_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30052_ses-d2709_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30053_ses-d0428_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30053_ses-d0428_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30061_ses-d0035_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30061_ses-d0035_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30070_ses-d0070_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30070_ses-d0070_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30073_ses-d1587_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30073_ses-d1587_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30074_ses-d2812_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30074_ses-d2812_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30076_ses-d0534_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30076_ses-d0534_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30103_sess-d7334_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30103_sess-d7334_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30109_ses-d0997_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30109_ses-d0997_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30121_ses-d2392_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30121_ses-d2392_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30134_sess-d2927_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30134_sess-d2927_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30135_ses-d0341_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30135_ses-d0341_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30137_sess-d5772_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30137_sess-d5772_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30138_sess-d0074_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30138_sess-d0074_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30141_ses-d0544_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30141_ses-d0544_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30145_ses-d4247_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30145_ses-d4247_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30150_ses-d0100_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30150_ses-d0100_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30164_ses-d0233_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30164_ses-d0233_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30165_ses-d0563_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30165_ses-d0563_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30168_ses-d0059_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30168_ses-d0059_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30171_ses-d0139_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30171_ses-d0139_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30172_ses-d0028_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30172_ses-d0028_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30173_ses-d3841_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30173_ses-d3841_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30178_ses-d0516_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30178_ses-d0516_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30179_ses-d0061_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30179_ses-d0061_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30184_ses-d0876_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30184_ses-d0876_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30189_ses-d0072_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30189_ses-d0072_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30200_ses-d0075_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30200_ses-d0075_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30202_ses-d0175_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30202_ses-d0175_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30205_ses-d0061_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30205_ses-d0061_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30207_sess-d2812_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30207_sess-d2812_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30208_ses-d0436_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30208_ses-d0436_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30228_sess-d1211_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30228_sess-d1211_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30231_ses-d3617_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30231_ses-d3617_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30232_ses-d0695_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30232_ses-d0695_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30233_ses-d3867_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30233_ses-d3867_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30235_ses-d0139_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30235_ses-d0139_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30244_ses-d1526_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30244_ses-d1526_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30247_ses-d0168_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30247_ses-d0168_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30250_ses-d1233_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30250_ses-d1233_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30255_ses-d0019_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30255_ses-d0019_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30261_ses-d0785_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30261_ses-d0785_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30265_ses-d0192_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30265_ses-d0192_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30272_ses-d0057_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30272_ses-d0057_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30276_ses-d1200_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30276_ses-d1200_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30277_ses-d0198_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30277_ses-d0198_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30284_ses-d3510_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30284_ses-d3510_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30285_ses-d0055_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30285_ses-d0055_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30287_ses-d0890_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30287_ses-d0890_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30289_ses-d0160_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30289_ses-d0160_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30298_ses-d0292_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30298_ses-d0292_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30299_ses-d0119_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30299_ses-d0119_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30304_sess-d1272_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30304_sess-d1272_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30306_ses-d0473_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30306_ses-d0473_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30308_ses-d0423_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30308_ses-d0423_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30310_ses-d0191_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30310_ses-d0191_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30311_ses-d0127_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30311_ses-d0127_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30316_ses-d0018_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30316_ses-d0018_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30317_ses-d0088_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30317_ses-d0088_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30318_ses-d2975_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30318_ses-d2975_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30319_ses-d0043_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30319_ses-d0043_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30321_ses-d0075_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30321_ses-d0075_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30322_ses-d1630_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30322_ses-d1630_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30326_ses-d0189_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30326_ses-d0189_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30327_ses-d0281_run-02_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30327_ses-d0281_run-02_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30328_sess-d1274_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30328_sess-d1274_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30329_ses-d0376_run-02_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30329_ses-d0376_run-02_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30331_ses-d2824_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30331_ses-d2824_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30335_ses-d0693_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30335_ses-d0693_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30336_ses-d0012_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30336_ses-d0012_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30344_ses-d1052_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30344_ses-d1052_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30350_ses-d0509_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30350_ses-d0509_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30351_sess-d2542_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30351_sess-d2542_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30357_ses-d0521_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30357_ses-d0521_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30361_ses-d2151_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30361_ses-d2151_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30362_ses-d0032_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30362_ses-d0032_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30363_ses-d0087_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30363_ses-d0087_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30368_ses-d0056_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30368_ses-d0056_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30369_ses-d4921_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30369_ses-d4921_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30372_ses-d3464_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30372_ses-d3464_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30373_ses-d0048_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30373_ses-d0048_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30374_ses-d0084_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30374_ses-d0084_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30382_ses-d0051_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30382_ses-d0051_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30387_ses-d0616_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30387_ses-d0616_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30390_ses-d2428_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30390_ses-d2428_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30391_ses-d0192_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30391_ses-d0192_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30394_ses-d0039_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30394_ses-d0039_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30395_ses-d1241_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30395_ses-d1241_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30397_ses-d0043_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30397_ses-d0043_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30399_ses-d0380_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30399_ses-d0380_T2w_stripped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:04<00:00,  4.80s/it]\n",
      "100%|##########| 1/1 [00:04<00:00,  4.80s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.14it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.14it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.20it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.19it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.18it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.33it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.22it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.19it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.28it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.15it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.18it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.29it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.29it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.30it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.28it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.24it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.31it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.26it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.16it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.15it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.12it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.14it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.11it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.28it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.43it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.18it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.16it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.34s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.37s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.37s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.37s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.19it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.87it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.87it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.23it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.23it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.79it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.79it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.57it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.17it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.33it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.80it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.69it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.80it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.34it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.65it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.38it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.38it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.18it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.15it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.11it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.01s/it]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.00it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.03s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.01s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.03s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.11it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.11it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.04s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.02s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.22it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.54it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.27it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.13it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.04s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.27it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.50it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.54it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.37it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.04it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.60it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.52it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.46it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.24it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.76it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.76it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.69it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.19it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.19it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.52it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.40it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.13it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.13it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.70it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.96it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.19it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.19it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.32s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.11it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.53it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.53it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.17it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.33s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.42it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.66it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.76it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.46it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.04s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.80it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.43it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.40it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.82it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.76it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.33it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.33it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.71it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.81it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.53it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.49it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.22it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.64it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.21it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.28it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.29it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.29it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.68it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.22it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.25it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.32s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.33s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.33s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.65it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.48it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.51it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.33s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.34s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.25it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.25it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.65it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.58it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.69it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.16it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.37it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.05s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.18it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.51it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.04s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.46it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.49it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.49it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.16it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.16it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.35s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.29it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.42it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.42it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.79it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.86it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.01s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.20it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.46it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.53it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.47it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.02s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.31it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.06s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.70it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.44it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.26s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.58it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.39it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.36it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.34it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.22it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.24it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.12it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.12it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.12it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.12it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.13it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.43it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.43it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.37it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.28it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.07s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.16s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.11s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.10it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.08s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.13s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.02it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.12s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.09s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.05it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.05it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.05it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.05it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.39it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  1.39it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.06it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.20s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.18s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.19s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.21s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.24s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.33s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.27s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.22s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.08it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.31s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.28s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.23s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.05it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.39s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.32s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.38s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.31s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.25s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.05it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.37s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.31s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.07it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.37s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.09it/s]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.36s/it]\n",
      "100%|##########| 2/2 [00:02<00:00,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.80it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.88it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.74it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.74it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.88it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.98it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.74it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.63it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.90it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.98it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.87it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.74it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.79it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.94it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.76it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.82it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.94it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.87it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.80it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_predict -i nnUNet_raw/Dataset500_MRI/imagesTs -o C:\\Users\\Anna\\Documents\\TFM\\predictions -d 500 -c 3d_lowres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f382875-0838-4e37-a78b-ee9bec8fb8b8",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e1aca4c-ebdc-4536-a097-31a7b1467aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!D:\\ProgramData\\Anaconda3\\envs\\nnunet\\Scripts\\nnUNetv2_evaluate_simple.exe \\\n",
    "    \"C:\\Users\\Anna\\Documents\\TFM\\nnUNet_raw\\Dataset500_MRI\\labelsTs\" \\\n",
    "    \"C:\\Users\\Anna\\Documents\\TFM\\predictions\" \\\n",
    "    -l 1 2 3 \\\n",
    "    -o \"C:\\Users\\Anna\\Documents\\TFM\\pred_eval\\eval.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9d9e1-a6f8-497a-b2f3-fbc7fe85ef77",
   "metadata": {},
   "source": [
    "## 7. Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee46ac69-9ccb-4899-8ede-460a6979ec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 3d_lowres\n",
      "Exporting fold_0\n",
      "Exporting fold_1\n",
      "Exporting fold_2\n",
      "Exporting fold_3\n",
      "Exporting fold_4\n",
      "No ensemble directory found for task 500\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_export_model_to_zip -d 500 -o segmentation_model.zip -c 3d_lowres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
