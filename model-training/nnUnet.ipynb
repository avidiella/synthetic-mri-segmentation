{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf74476f-414b-48dc-a93d-4950a50f6efc",
   "metadata": {},
   "source": [
    "# nnU-Net Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809fb75-80f5-412b-9ad4-798eced7a4cb",
   "metadata": {},
   "source": [
    "## 1. Install compatible versions of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88afcd61-3f2c-4b3d-9461-d58d85c52f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.9.1\n",
      "Uninstalling torch-2.9.1:\n",
      "  Successfully uninstalled torch-2.9.1\n",
      "Found existing installation: torchvision 0.15.2+cu118\n",
      "Uninstalling torchvision-0.15.2+cu118:\n",
      "  Successfully uninstalled torchvision-0.15.2+cu118\n",
      "Found existing installation: torchaudio 2.0.2+cu118\n",
      "Uninstalling torchaudio-2.0.2+cu118:\n",
      "  Successfully uninstalled torchaudio-2.0.2+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torchmetrics as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio torchmetrics -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36bcdd47-826b-4ef4-a73b-0a84e2b9bdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.0.1+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-win_amd64.whl (2619.1 MB)\n",
      "Collecting torchvision==0.15.2+cu118\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-win_amd64.whl (4.9 MB)\n",
      "Requirement already satisfied: torchaudio==2.0.2+cu118 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (2.0.2+cu118)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (4.15.0)\n",
      "Requirement already satisfied: sympy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch==2.0.1+cu118) (3.1.6)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torchvision==0.15.2+cu118) (2.1.2)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torchvision==0.15.2+cu118) (2.32.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torchvision==0.15.2+cu118) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->torchvision==0.15.2+cu118) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "\n",
      "  Attempting uninstall: torch\n",
      "\n",
      "    Found existing installation: torch 2.9.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "    Uninstalling torch-2.9.1:\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "      Successfully uninstalled torch-2.9.1\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "  Attempting uninstall: torchvision\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "    Found existing installation: torchvision 0.24.1\n",
      "   ---------------------------------------- 0/2 [torch]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "    Uninstalling torchvision-0.24.1:\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "      Successfully uninstalled torchvision-0.24.1\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchvision]\n",
      "   ---------------------------------------- 2/2 [torchvision]\n",
      "\n",
      "Successfully installed torch-2.0.1+cu118 torchvision-0.15.2+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nnunetv2 2.6.2 requires torch>=2.1.2, but you have torch 2.0.1+cu118 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad9e66f2-9844-4aad-a844-94ca72887c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da79652a-5129-45ee-abf4-1a9c51b1bdee",
   "metadata": {},
   "source": [
    "## 2. Install nnU-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789ad7e1-42fb-4866-8f61-bea769986789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nnunetv2\n",
      "  Downloading nnunetv2-2.6.2.tar.gz (211 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting torch>=2.1.2 (from nnunetv2)\n",
      "  Downloading torch-2.9.1-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Collecting acvl-utils<0.3,>=0.2.3 (from nnunetv2)\n",
      "  Using cached acvl_utils-0.2.5.tar.gz (29 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting dynamic-network-architectures<0.5,>=0.4.1 (from nnunetv2)\n",
      "  Using cached dynamic_network_architectures-0.4.2.tar.gz (28 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tqdm (from nnunetv2)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy (from nnunetv2)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting batchgenerators>=0.25.1 (from nnunetv2)\n",
      "  Using cached batchgenerators-0.25.1.tar.gz (76 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.24 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2) (2.1.2)\n",
      "Collecting scikit-learn (from nnunetv2)\n",
      "  Using cached scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scikit-image>=0.19.3 (from nnunetv2)\n",
      "  Using cached scikit_image-0.25.2-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting SimpleITK>=2.2.1 (from nnunetv2)\n",
      "  Using cached simpleitk-2.5.3-cp310-cp310-win_amd64.whl.metadata (7.3 kB)\n",
      "Collecting pandas (from nnunetv2)\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting graphviz (from nnunetv2)\n",
      "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tifffile (from nnunetv2)\n",
      "  Using cached tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2) (2.32.5)\n",
      "Collecting nibabel (from nnunetv2)\n",
      "  Downloading nibabel-5.3.3-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting matplotlib (from nnunetv2)\n",
      "  Using cached matplotlib-3.10.7-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from nnunetv2)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting imagecodecs (from nnunetv2)\n",
      "  Using cached imagecodecs-2025.3.30-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting yacs (from nnunetv2)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting batchgeneratorsv2>=0.3.0 (from nnunetv2)\n",
      "  Downloading batchgeneratorsv2-0.3.0.tar.gz (44 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting einops (from nnunetv2)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting blosc2>=3.0.0b1 (from nnunetv2)\n",
      "  Downloading blosc2-3.12.2-cp310-cp310-win_amd64.whl.metadata (7.1 kB)\n",
      "Collecting connected-components-3d (from acvl-utils<0.3,>=0.2.3->nnunetv2)\n",
      "  Using cached connected_components_3d-3.26.1-cp310-cp310-win_amd64.whl.metadata (33 kB)\n",
      "Collecting timm (from dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached timm-1.0.22-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25.1->nnunetv2) (11.3.0)\n",
      "Collecting future (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting unittest2 (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading unittest2-1.1.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting threadpoolctl (from batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fft-conv-pytorch (from batchgeneratorsv2>=0.3.0->nnunetv2)\n",
      "  Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting ndindex (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached ndindex-1.10.1-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting msgpack (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached msgpack-1.1.2-cp310-cp310-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: platformdirs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b1->nnunetv2) (4.5.0)\n",
      "Collecting numexpr>=2.14.1 (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached numexpr-2.14.1-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting py-cpuinfo (from blosc2>=3.0.0b1->nnunetv2)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2) (3.3)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image>=0.19.3->nnunetv2)\n",
      "  Using cached imageio-2.37.2-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: packaging>=21 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2) (25.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.19.3->nnunetv2)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch>=2.1.2->nnunetv2)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from jinja2->torch>=2.1.2->nnunetv2) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->nnunetv2)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->nnunetv2)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->nnunetv2)\n",
      "  Using cached fonttools-4.61.0-cp310-cp310-win_amd64.whl.metadata (115 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->nnunetv2)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib->nnunetv2)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2) (1.17.0)\n",
      "Collecting importlib-resources>=5.12 (from nibabel->nnunetv2)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->nnunetv2)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->nnunetv2)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2) (2025.11.12)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->nnunetv2)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: torchvision in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.15.2+cu118)\n",
      "Requirement already satisfied: pyyaml in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (6.0.3)\n",
      "Collecting huggingface_hub (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Downloading huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.28.1)\n",
      "Collecting shellingham (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (0.16.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from tqdm->nnunetv2) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Downloading torchvision-0.24.1-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub->timm->dynamic-network-architectures<0.5,>=0.4.1->nnunetv2)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting traceback2 (from unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Downloading traceback2-1.4.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting linecache2 (from traceback2->unittest2->batchgenerators>=0.25.1->nnunetv2)\n",
      "  Using cached linecache2-1.0.0-py2.py3-none-any.whl.metadata (1000 bytes)\n",
      "Downloading blosc2-3.12.2-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 16.2 MB/s  0:00:00\n",
      "Downloading numexpr-2.14.1-cp310-cp310-win_amd64.whl (160 kB)\n",
      "Downloading scikit_image-0.25.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 6.8/12.8 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 40.1 MB/s  0:00:00\n",
      "Downloading imageio-2.37.2-py3-none-any.whl (317 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "   ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 21.8/41.3 MB 105.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.2/41.3 MB 109.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.3/41.3 MB 97.1 MB/s  0:00:00\n",
      "Downloading simpleitk-2.5.3-cp310-cp310-win_amd64.whl (18.8 MB)\n",
      "   ---------------------------------------- 0.0/18.8 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.1/18.8 MB 16.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 5.2/18.8 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 6.6/18.8 MB 9.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 8.4/18.8 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 9.4/18.8 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 11.5/18.8 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 12.6/18.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 14.7/18.8 MB 8.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.8/18.8 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.8/18.8 MB 8.9 MB/s  0:00:02\n",
      "Downloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "Downloading torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "   ---------------------------------------- 0.0/111.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 18.6/111.0 MB 83.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 37.7/111.0 MB 88.8 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 42.7/111.0 MB 67.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 59.5/111.0 MB 70.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 78.6/111.0 MB 74.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 98.8/111.0 MB 77.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 79.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 111.0/111.0 MB 73.8 MB/s  0:00:01\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Downloading connected_components_3d-3.26.1-cp310-cp310-win_amd64.whl (521 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading fft_conv_pytorch-1.2.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Downloading imagecodecs-2025.3.30-cp310-cp310-win_amd64.whl (28.9 MB)\n",
      "   ---------------------------------------- 0.0/28.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 3.1/28.9 MB 14.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 5.2/28.9 MB 12.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 7.3/28.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 9.4/28.9 MB 12.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 12.6/28.9 MB 11.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 14.7/28.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.8/28.9 MB 11.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 18.9/28.9 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 21.0/28.9 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 23.1/28.9 MB 11.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 25.2/28.9 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 27.3/28.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.9/28.9 MB 10.8 MB/s  0:00:02\n",
      "Downloading matplotlib-3.10.7-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 83.5 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 85.6 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading msgpack-1.1.2-cp310-cp310-win_amd64.whl (71 kB)\n",
      "Downloading ndindex-1.10.1-cp310-cp310-win_amd64.whl (157 kB)\n",
      "Downloading nibabel-5.3.3-py3-none-any.whl (3.3 MB)\n",
      "   ---------------------------------------- 0.0/3.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.3/3.3 MB 64.5 MB/s  0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.3/11.3 MB 88.7 MB/s  0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.9/8.9 MB 61.2 MB/s  0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading timm-1.0.22-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 70.9 MB/s  0:00:00\n",
      "Downloading huggingface_hub-1.2.1-py3-none-any.whl (520 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 85.3 MB/s  0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading torchvision-0.24.1-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "   ---------------------------------------- 0.0/3.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.7/3.7 MB 110.8 MB/s  0:00:00\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading unittest2-1.1.0-py2.py3-none-any.whl (96 kB)\n",
      "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Downloading traceback2-1.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading linecache2-1.0.0-py2.py3-none-any.whl (12 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: nnunetv2, acvl-utils, dynamic-network-architectures, batchgenerators, batchgeneratorsv2\n",
      "  Building wheel for nnunetv2 (pyproject.toml): started\n",
      "  Building wheel for nnunetv2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.6.2-py3-none-any.whl size=285951 sha256=a7a44d68002164ee30463aadda0c7df27ad8f6392cdcee74d03485d54c5ad3af\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\7b\\93\\c5\\742da4755f3e7838e41c346eb3ef391ed49ec53c620c7550ef\n",
      "  Building wheel for acvl-utils (pyproject.toml): started\n",
      "  Building wheel for acvl-utils (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for acvl-utils: filename=acvl_utils-0.2.5-py3-none-any.whl size=27250 sha256=bb802262a73cd3726398181828eef4af2c33e33516c9a5ed575060d8cd3f8d91\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\11\\68\\32\\42aed9ef6a7c3ff868f84b243120481ef8a19d0ddbc2551133\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): started\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.4.2-py3-none-any.whl size=39089 sha256=5a3192eb7584824329d098ed04859ef5c230548e084466662184ec5778484418\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\08\\23\\62\\cbf95e3b955e69700d71fd056ba2ed436cfff0ba3f9ae2ebb9\n",
      "  Building wheel for batchgenerators (pyproject.toml): started\n",
      "  Building wheel for batchgenerators (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for batchgenerators: filename=batchgenerators-0.25.1-py3-none-any.whl size=93167 sha256=8af42eec23d5219281538ea957556e6a14913677c919f54e9fbc764c7a6e1584\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\be\\1b\\30\\b3f066999ad01855fc903fe7c93c25682333dd5645d5c75434\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml): started\n",
      "  Building wheel for batchgeneratorsv2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for batchgeneratorsv2: filename=batchgeneratorsv2-0.3.0-py3-none-any.whl size=65242 sha256=a54d62bc3f48ed0620dec0555264b448e05cc969a7ca0f134fb6e97b1d1edba7\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\85\\91\\18\\f246cf6cecb275d0e293271390794a0c090621cfc1ab1501c7\n",
      "Successfully built nnunetv2 acvl-utils dynamic-network-architectures batchgenerators batchgeneratorsv2\n",
      "Installing collected packages: SimpleITK, pytz, py-cpuinfo, linecache2, argparse, yacs, tzdata, traceback2, tqdm, tifffile, threadpoolctl, shellingham, scipy, safetensors, pyparsing, numexpr, ndindex, msgpack, lazy-loader, kiwisolver, joblib, importlib-resources, imageio, imagecodecs, hf-xet, graphviz, future, fsspec, fonttools, einops, cycler, contourpy, connected-components-3d, click, unittest2, typer-slim, torch, scikit-learn, scikit-image, pandas, nibabel, matplotlib, blosc2, torchvision, seaborn, fft-conv-pytorch, batchgenerators, huggingface_hub, batchgeneratorsv2, acvl-utils, timm, dynamic-network-architectures, nnunetv2\n",
      "\n",
      "   ----------------------------------------  0/53 [SimpleITK]\n",
      "   ----------------------------------------  0/53 [SimpleITK]\n",
      "   ----------------------------------------  0/53 [SimpleITK]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "    ---------------------------------------  1/53 [pytz]\n",
      "   - --------------------------------------  2/53 [py-cpuinfo]\n",
      "   -- -------------------------------------  3/53 [linecache2]\n",
      "   --- ------------------------------------  4/53 [argparse]\n",
      "   ---- -----------------------------------  6/53 [tzdata]\n",
      "   ---- -----------------------------------  6/53 [tzdata]\n",
      "   ---- -----------------------------------  6/53 [tzdata]\n",
      "   ----- ----------------------------------  7/53 [traceback2]\n",
      "   ----- ----------------------------------  7/53 [traceback2]\n",
      "   ------ ---------------------------------  8/53 [tqdm]\n",
      "   ------ ---------------------------------  8/53 [tqdm]\n",
      "   ------ ---------------------------------  9/53 [tifffile]\n",
      "   ------- -------------------------------- 10/53 [threadpoolctl]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 12/53 [scipy]\n",
      "   --------- ------------------------------ 13/53 [safetensors]\n",
      "   ---------- ----------------------------- 14/53 [pyparsing]\n",
      "   ----------- ---------------------------- 15/53 [numexpr]\n",
      "   ------------ --------------------------- 16/53 [ndindex]\n",
      "   ------------ --------------------------- 17/53 [msgpack]\n",
      "   -------------- ------------------------- 19/53 [kiwisolver]\n",
      "   --------------- ------------------------ 20/53 [joblib]\n",
      "   --------------- ------------------------ 20/53 [joblib]\n",
      "   --------------- ------------------------ 21/53 [importlib-resources]\n",
      "   --------------- ------------------------ 21/53 [importlib-resources]\n",
      "   ---------------- ----------------------- 22/53 [imageio]\n",
      "   ---------------- ----------------------- 22/53 [imageio]\n",
      "   ----------------- ---------------------- 23/53 [imagecodecs]\n",
      "   ----------------- ---------------------- 23/53 [imagecodecs]\n",
      "   ----------------- ---------------------- 23/53 [imagecodecs]\n",
      "   ------------------ --------------------- 24/53 [hf-xet]\n",
      "   ------------------ --------------------- 25/53 [graphviz]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   ------------------- -------------------- 26/53 [future]\n",
      "   -------------------- ------------------- 27/53 [fsspec]\n",
      "   -------------------- ------------------- 27/53 [fsspec]\n",
      "   -------------------- ------------------- 27/53 [fsspec]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 28/53 [fonttools]\n",
      "   --------------------- ------------------ 29/53 [einops]\n",
      "   --------------------- ------------------ 29/53 [einops]\n",
      "   --------------------- ------------------ 29/53 [einops]\n",
      "   ---------------------- ----------------- 30/53 [cycler]\n",
      "   ----------------------- ---------------- 31/53 [contourpy]\n",
      "   ------------------------ --------------- 32/53 [connected-components-3d]\n",
      "   ------------------------ --------------- 33/53 [click]\n",
      "   ------------------------- -------------- 34/53 [unittest2]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "  Attempting uninstall: torch\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "   -------------------------- ------------- 35/53 [typer-slim]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 36/53 [torch]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   --------------------------- ------------ 37/53 [scikit-learn]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ---------------------------- ----------- 38/53 [scikit-image]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ----------------------------- ---------- 39/53 [pandas]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 40/53 [nibabel]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------ --------- 41/53 [matplotlib]\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "  Attempting uninstall: torchvision\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "    Found existing installation: torchvision 0.15.2+cu118\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "    Uninstalling torchvision-0.15.2+cu118:\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "      Successfully uninstalled torchvision-0.15.2+cu118\n",
      "   ------------------------------- -------- 42/53 [blosc2]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   -------------------------------- ------- 43/53 [torchvision]\n",
      "   --------------------------------- ------ 44/53 [seaborn]\n",
      "   --------------------------------- ------ 44/53 [seaborn]\n",
      "   --------------------------------- ------ 45/53 [fft-conv-pytorch]\n",
      "   ---------------------------------- ----- 46/53 [batchgenerators]\n",
      "   ---------------------------------- ----- 46/53 [batchgenerators]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ----------------------------------- ---- 47/53 [huggingface_hub]\n",
      "   ------------------------------------ --- 48/53 [batchgeneratorsv2]\n",
      "   ------------------------------------ --- 49/53 [acvl-utils]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------- -- 50/53 [timm]\n",
      "   ------------------------------------ - 51/53 [dynamic-network-architectures]\n",
      "   ------------------------------------ - 51/53 [dynamic-network-architectures]\n",
      "   ------------------------------------ - 51/53 [dynamic-network-architectures]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------  52/53 [nnunetv2]\n",
      "   ---------------------------------------- 53/53 [nnunetv2]\n",
      "\n",
      "Successfully installed SimpleITK-2.5.3 acvl-utils-0.2.5 argparse-1.4.0 batchgenerators-0.25.1 batchgeneratorsv2-0.3.0 blosc2-3.12.2 click-8.3.1 connected-components-3d-3.26.1 contourpy-1.3.2 cycler-0.12.1 dynamic-network-architectures-0.4.2 einops-0.8.1 fft-conv-pytorch-1.2.0 fonttools-4.61.0 fsspec-2025.12.0 future-1.0.0 graphviz-0.21 hf-xet-1.2.0 huggingface_hub-1.2.1 imagecodecs-2025.3.30 imageio-2.37.2 importlib-resources-6.5.2 joblib-1.5.2 kiwisolver-1.4.9 lazy-loader-0.4 linecache2-1.0.0 matplotlib-3.10.7 msgpack-1.1.2 ndindex-1.10.1 nibabel-5.3.3 nnunetv2-2.6.2 numexpr-2.14.1 pandas-2.3.3 py-cpuinfo-9.0.0 pyparsing-3.2.5 pytz-2025.2 safetensors-0.7.0 scikit-image-0.25.2 scikit-learn-1.7.2 scipy-1.15.3 seaborn-0.13.2 shellingham-1.5.4 threadpoolctl-3.6.0 tifffile-2025.5.10 timm-1.0.22 torch-2.9.1 torchvision-0.24.1 tqdm-4.67.1 traceback2-1.4.0 typer-slim-0.20.0 tzdata-2025.2 unittest2-1.1.0 yacs-0.1.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\ProgramData\\Anaconda3\\envs\\nnunet\\Lib\\site-packages\\~vfuser'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\ProgramData\\Anaconda3\\envs\\nnunet\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install nnunetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adda6336-368a-4c71-b280-9eda0b7c21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: nnunetv2 2.6.2\n",
      "Uninstalling nnunetv2-2.6.2:\n",
      "  Successfully uninstalled nnunetv2-2.6.2\n",
      "Collecting nnunetv2==2.4.2\n",
      "  Downloading nnunetv2-2.4.2.tar.gz (184 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting torch>=2.1.2 (from nnunetv2==2.4.2)\n",
      "  Using cached torch-2.9.1-cp310-cp310-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: acvl-utils<0.3,>=0.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.2.5)\n",
      "Collecting dynamic-network-architectures<0.4,>=0.3.1 (from nnunetv2==2.4.2)\n",
      "  Downloading dynamic_network_architectures-0.3.1.tar.gz (20 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (4.67.1)\n",
      "Collecting dicom2nifti (from nnunetv2==2.4.2)\n",
      "  Using cached dicom2nifti-2.6.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: scipy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (1.15.3)\n",
      "Requirement already satisfied: batchgenerators>=0.25 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.25.1)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (1.7.2)\n",
      "Requirement already satisfied: scikit-image>=0.19.3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.25.2)\n",
      "Requirement already satisfied: SimpleITK>=2.2.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.5.3)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.3.3)\n",
      "Requirement already satisfied: graphviz in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.21)\n",
      "Requirement already satisfied: tifffile in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2025.5.10)\n",
      "Requirement already satisfied: requests in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2.32.5)\n",
      "Requirement already satisfied: nibabel in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (5.3.3)\n",
      "Requirement already satisfied: matplotlib in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (3.10.7)\n",
      "Requirement already satisfied: seaborn in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.13.2)\n",
      "Requirement already satisfied: imagecodecs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (2025.3.30)\n",
      "Requirement already satisfied: yacs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nnunetv2==2.4.2) (0.1.8)\n",
      "Requirement already satisfied: connected-components-3d in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (3.26.1)\n",
      "Requirement already satisfied: blosc2>=3.0.0b4 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (3.12.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (11.3.0)\n",
      "Requirement already satisfied: future in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (1.0.0)\n",
      "Requirement already satisfied: unittest2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from batchgenerators>=0.25->nnunetv2==2.4.2) (3.6.0)\n",
      "Requirement already satisfied: ndindex in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (1.10.1)\n",
      "Requirement already satisfied: msgpack in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (1.1.2)\n",
      "Requirement already satisfied: platformdirs in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (4.5.0)\n",
      "Requirement already satisfied: numexpr>=2.14.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (2.14.1)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from blosc2>=3.0.0b4->acvl-utils<0.3,>=0.2->nnunetv2==2.4.2) (9.0.0)\n",
      "Requirement already satisfied: networkx>=3.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (3.3)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (2.37.2)\n",
      "Requirement already satisfied: packaging>=21 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-image>=0.19.3->nnunetv2==2.4.2) (0.4)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from torch>=2.1.2->nnunetv2==2.4.2) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from sympy>=1.13.3->torch>=2.1.2->nnunetv2==2.4.2) (1.3.0)\n",
      "Collecting pydicom>=3.0.0 (from dicom2nifti->nnunetv2==2.4.2)\n",
      "  Using cached pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting python-gdcm (from dicom2nifti->nnunetv2==2.4.2)\n",
      "  Using cached python_gdcm-3.2.2-cp310-cp310-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from jinja2->torch>=2.1.2->nnunetv2==2.4.2) (3.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from matplotlib->nnunetv2==2.4.2) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->nnunetv2==2.4.2) (1.17.0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from nibabel->nnunetv2==2.4.2) (6.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from pandas->nnunetv2==2.4.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from pandas->nnunetv2==2.4.2) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from requests->nnunetv2==2.4.2) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from scikit-learn->nnunetv2==2.4.2) (1.5.2)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from tqdm->nnunetv2==2.4.2) (0.4.6)\n",
      "Collecting argparse (from unittest2->batchgenerators>=0.25->nnunetv2==2.4.2)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: traceback2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from unittest2->batchgenerators>=0.25->nnunetv2==2.4.2) (1.4.0)\n",
      "Requirement already satisfied: linecache2 in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from traceback2->unittest2->batchgenerators>=0.25->nnunetv2==2.4.2) (1.0.0)\n",
      "Requirement already satisfied: PyYAML in d:\\programdata\\anaconda3\\envs\\nnunet\\lib\\site-packages (from yacs->nnunetv2==2.4.2) (6.0.3)\n",
      "Using cached torch-2.9.1-cp310-cp310-win_amd64.whl (111.0 MB)\n",
      "Downloading dicom2nifti-2.6.2-py3-none-any.whl (43 kB)\n",
      "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 13.6 MB/s  0:00:00\n",
      "Downloading python_gdcm-3.2.2-cp310-cp310-win_amd64.whl (34.2 MB)\n",
      "   ---------------------------------------- 0.0/34.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 6.6/34.2 MB 33.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 21.0/34.2 MB 53.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  34.1/34.2 MB 56.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 34.2/34.2 MB 54.3 MB/s  0:00:00\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Building wheels for collected packages: nnunetv2, dynamic-network-architectures\n",
      "  Building wheel for nnunetv2 (pyproject.toml): started\n",
      "  Building wheel for nnunetv2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nnunetv2: filename=nnunetv2-2.4.2-py3-none-any.whl size=248849 sha256=b14498d8ae2982b5b94510857a830bc0dfc1d8570651be61e2f73bbaa1033683\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\a8\\24\\6a\\632a19c700123ed30f3f6380d84ea3f9e1d068bf5db817e804\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): started\n",
      "  Building wheel for dynamic-network-architectures (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for dynamic-network-architectures: filename=dynamic_network_architectures-0.3.1-py3-none-any.whl size=30114 sha256=7b4fbf41e49a08498b3c608efc6c13057f7a30ac9909566150c7ee15cbcd6bd6\n",
      "  Stored in directory: c:\\users\\anna\\appdata\\local\\pip\\cache\\wheels\\55\\1b\\13\\a6419c8dbf998b9343710355ec3edc5c8e24d9b7b22eec95fb\n",
      "Successfully built nnunetv2 dynamic-network-architectures\n",
      "Installing collected packages: argparse, python-gdcm, pydicom, torch, dicom2nifti, dynamic-network-architectures, nnunetv2\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----- ---------------------------------- 1/7 [python-gdcm]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "  Attempting uninstall: torch\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "    Found existing installation: torch 2.0.1+cu118\n",
      "   ----------- ---------------------------- 2/7 [pydicom]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "    Uninstalling torch-2.0.1+cu118:\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "      Successfully uninstalled torch-2.0.1+cu118\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ----------------- ---------------------- 3/7 [torch]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "  Attempting uninstall: dynamic-network-architectures\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "    Found existing installation: dynamic_network_architectures 0.4.2\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "    Uninstalling dynamic_network_architectures-0.4.2:\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "      Successfully uninstalled dynamic_network_architectures-0.4.2\n",
      "   ---------------------- ----------------- 4/7 [dicom2nifti]\n",
      "   ---------------------------- ----------- 5/7 [dynamic-network-architectures]\n",
      "   ---------------------------- ----------- 5/7 [dynamic-network-architectures]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------- ----- 6/7 [nnunetv2]\n",
      "   ---------------------------------------- 7/7 [nnunetv2]\n",
      "\n",
      "Successfully installed argparse-1.4.0 dicom2nifti-2.6.2 dynamic-network-architectures-0.3.1 nnunetv2-2.4.2 pydicom-3.0.1 python-gdcm-3.2.2 torch-2.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.9.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall nnunetv2 -y\n",
    "!pip install nnunetv2==2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106d2f6-e26c-4c60-97ba-302628c4336a",
   "metadata": {},
   "source": [
    "## 3. System configuration and training environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d85b8f3-9d23-4a74-b142-cc5773c06eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  7 12:01:50 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.80                 Driver Version: 576.80         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  |   00000000:05:00.0  On |                  N/A |\n",
      "|  0%   43C    P8             19W /  170W |     619MiB /  12288MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            8200    C+G   ...cord\\app-1.0.9217\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A            8440    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A            9796    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           12580    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           13160    C+G   ...Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A           15448    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           16700    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           18716    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           18896    C+G   ...crosoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           23004    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A           23180    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A           31764    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           31904    C+G   ....0.3650.66\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A           31920    C+G   ...8wekyb3d8bbwe\\M365Copilot.exe      N/A      |\n",
      "|    0   N/A  N/A           33440    C+G   ...1g1gvanyjgm\\WhatsApp.Root.exe      N/A      |\n",
      "|    0   N/A  N/A           34228    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd49396-f7ad-4943-843c-39cce140a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu118\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17102f3-e657-4d05-8b2d-f93c9e8f0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_raw\n",
      "nnUNet_preprocessed: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\n",
      "nnUNet_results: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_path = r\"C:\\Users\\Anna\\Documents\\TFM\"\n",
    "\n",
    "os.environ['nnUNet_raw'] = os.path.join(base_path, \"nnUNet_raw\")\n",
    "os.environ['nnUNet_preprocessed'] = os.path.join(base_path, \"nnUNet_preprocessed\")\n",
    "os.environ['nnUNet_results'] = os.path.join(base_path, \"nnUNet_results\")\n",
    "\n",
    "print(\"nnUNet_raw:\", os.environ['nnUNet_raw'])\n",
    "print(\"nnUNet_preprocessed:\", os.environ['nnUNet_preprocessed'])\n",
    "print(\"nnUNet_results:\", os.environ['nnUNet_results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baadb9a-89d4-4f86-9872-80c4adccdde5",
   "metadata": {},
   "source": [
    "## 4. nnU-Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9893577b-e6f6-4ca7-9558-5cfa1ca7c04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint extraction...\n",
      "Dataset500_MRI\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.03 1.03 1.03]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [248.54368932 248.54368932 248.54368932]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.0609 1.0609 1.0609]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [241.30455274 241.30455274 241.30455274]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.092727 1.092727 1.092727]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [234.27626479 234.27626479 234.27626479]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.12550881 1.12550881 1.12550881]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [227.45268427 227.45268427 227.45268427]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.15927407 1.15927407 1.15927407]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [220.8278488 220.8278488 220.8278488]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.1940523 1.1940523 1.1940523]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [214.39596971 214.39596971 214.39596971]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.22987387 1.22987387 1.22987387]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [208.1514269 208.1514269 208.1514269]\n",
      "Attempting to find 3d_lowres config. \n",
      "Current spacing: [1.26677008 1.26677008 1.26677008]. \n",
      "Current patch size: (128, 128, 128). \n",
      "Current median shape: [202.08876398 202.08876398 202.08876398]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 49, 'patch_size': (256, 256), 'median_image_size_in_voxels': array([256., 256.]), 'spacing': array([1., 1.]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 7, 'features_per_stage': (32, 64, 128, 256, 512, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D lowres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (128, 128, 128), 'median_image_size_in_voxels': (202, 202, 202), 'spacing': array([1.26677008, 1.26677008, 1.26677008]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'}\n",
      "\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (128, 128, 128), 'median_image_size_in_voxels': array([256., 256., 256.]), 'spacing': array([1., 1., 1.]), 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Plans were saved to C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset500_MRI\n",
      "Configuration: 2d...\n",
      "Configuration: 3d_fullres...\n",
      "Configuration: 3d_lowres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:03<31:36,  3.80s/it]\n",
      "  0%|          | 2/500 [00:03<13:30,  1.63s/it]\n",
      "  1%|          | 4/500 [00:04<05:18,  1.56it/s]\n",
      "  1%|1         | 6/500 [00:04<03:12,  2.56it/s]\n",
      "  2%|1         | 8/500 [00:04<02:05,  3.94it/s]\n",
      "  2%|2         | 10/500 [00:04<01:58,  4.15it/s]\n",
      "  2%|2         | 12/500 [00:04<01:26,  5.65it/s]\n",
      "  3%|2         | 14/500 [00:05<01:15,  6.47it/s]\n",
      "  3%|3         | 16/500 [00:05<00:59,  8.17it/s]\n",
      "  4%|3         | 18/500 [00:05<01:21,  5.91it/s]\n",
      "  4%|4         | 21/500 [00:06<01:09,  6.92it/s]\n",
      "  5%|5         | 25/500 [00:06<01:01,  7.76it/s]\n",
      "  5%|5         | 26/500 [00:06<00:59,  7.92it/s]\n",
      "  6%|5         | 29/500 [00:06<00:56,  8.36it/s]\n",
      "  6%|6         | 31/500 [00:07<00:48,  9.74it/s]\n",
      "  7%|6         | 33/500 [00:07<00:55,  8.39it/s]\n",
      "  7%|7         | 35/500 [00:07<00:47,  9.89it/s]\n",
      "  7%|7         | 37/500 [00:07<01:02,  7.45it/s]\n",
      "  8%|8         | 41/500 [00:08<00:50,  9.03it/s]\n",
      "  9%|8         | 43/500 [00:08<00:44, 10.29it/s]\n",
      "  9%|9         | 45/500 [00:08<00:58,  7.79it/s]\n",
      " 10%|9         | 48/500 [00:08<00:43, 10.45it/s]\n",
      " 10%|#         | 50/500 [00:09<00:50,  8.87it/s]\n",
      " 10%|#         | 52/500 [00:09<00:43, 10.29it/s]\n",
      " 11%|#         | 54/500 [00:09<00:58,  7.69it/s]\n",
      " 12%|#1        | 58/500 [00:10<00:53,  8.28it/s]\n",
      " 12%|#2        | 61/500 [00:10<00:56,  7.77it/s]\n",
      " 13%|#2        | 63/500 [00:10<00:48,  8.98it/s]\n",
      " 13%|#3        | 66/500 [00:11<00:47,  9.05it/s]\n",
      " 14%|#3        | 68/500 [00:11<00:41, 10.31it/s]\n",
      " 14%|#4        | 70/500 [00:11<00:55,  7.80it/s]\n",
      " 15%|#4        | 74/500 [00:12<00:51,  8.33it/s]\n",
      " 15%|#5        | 77/500 [00:12<00:54,  7.81it/s]\n",
      " 16%|#5        | 79/500 [00:12<00:46,  9.01it/s]\n",
      " 16%|#6        | 82/500 [00:12<00:46,  9.08it/s]\n",
      " 17%|#6        | 84/500 [00:13<00:40, 10.32it/s]\n",
      " 17%|#7        | 86/500 [00:13<00:52,  7.82it/s]\n",
      " 18%|#8        | 90/500 [00:13<00:49,  8.34it/s]\n",
      " 19%|#8        | 93/500 [00:14<00:47,  8.59it/s]\n",
      " 19%|#8        | 94/500 [00:14<00:46,  8.68it/s]\n",
      " 20%|#9        | 98/500 [00:14<00:45,  8.87it/s]\n",
      " 20%|##        | 101/500 [00:15<00:49,  8.13it/s]\n",
      " 20%|##        | 102/500 [00:15<00:48,  8.26it/s]\n",
      " 21%|##1       | 106/500 [00:15<00:45,  8.64it/s]\n",
      " 22%|##1       | 109/500 [00:16<00:44,  8.80it/s]\n",
      " 22%|##2       | 110/500 [00:16<00:44,  8.86it/s]\n",
      " 23%|##2       | 114/500 [00:16<00:42,  9.00it/s]\n",
      " 23%|##3       | 117/500 [00:16<00:37, 10.08it/s]\n",
      " 24%|##3       | 119/500 [00:17<00:48,  7.92it/s]\n",
      " 24%|##4       | 122/500 [00:17<00:45,  8.30it/s]\n",
      " 25%|##5       | 125/500 [00:17<00:35, 10.69it/s]\n",
      " 25%|##5       | 127/500 [00:18<00:45,  8.18it/s]\n",
      " 26%|##6       | 130/500 [00:18<00:48,  7.70it/s]\n",
      " 27%|##6       | 134/500 [00:19<00:48,  7.55it/s]\n",
      " 28%|##7       | 138/500 [00:19<00:44,  8.06it/s]\n",
      " 28%|##8       | 142/500 [00:20<00:42,  8.39it/s]\n",
      " 29%|##9       | 145/500 [00:20<00:34, 10.31it/s]\n",
      " 29%|##9       | 147/500 [00:20<00:42,  8.24it/s]\n",
      " 30%|###       | 150/500 [00:20<00:45,  7.78it/s]\n",
      " 31%|###       | 153/500 [00:21<00:34,  9.94it/s]\n",
      " 31%|###1      | 155/500 [00:21<00:43,  7.90it/s]\n",
      " 32%|###1      | 158/500 [00:21<00:45,  7.55it/s]\n",
      " 32%|###2      | 162/500 [00:22<00:41,  8.09it/s]\n",
      " 33%|###3      | 166/500 [00:22<00:39,  8.45it/s]\n",
      " 33%|###3      | 167/500 [00:22<00:39,  8.53it/s]\n",
      " 34%|###4      | 170/500 [00:23<00:41,  7.91it/s]\n",
      " 35%|###4      | 174/500 [00:23<00:38,  8.36it/s]\n",
      " 36%|###5      | 178/500 [00:24<00:37,  8.62it/s]\n",
      " 36%|###6      | 181/500 [00:24<00:30, 10.60it/s]\n",
      " 37%|###6      | 183/500 [00:24<00:37,  8.34it/s]\n",
      " 37%|###7      | 186/500 [00:25<00:40,  7.84it/s]\n",
      " 38%|###8      | 190/500 [00:25<00:37,  8.29it/s]\n",
      " 39%|###8      | 193/500 [00:25<00:29, 10.34it/s]\n",
      " 39%|###9      | 195/500 [00:26<00:33,  9.02it/s]\n",
      " 39%|###9      | 197/500 [00:26<00:29, 10.22it/s]\n",
      " 40%|###9      | 199/500 [00:26<00:38,  7.83it/s]\n",
      " 40%|####      | 202/500 [00:26<00:36,  8.24it/s]\n",
      " 41%|####      | 204/500 [00:27<00:30,  9.55it/s]\n",
      " 41%|####1     | 206/500 [00:27<00:39,  7.45it/s]\n",
      " 42%|####2     | 210/500 [00:27<00:32,  8.95it/s]\n",
      " 42%|####2     | 212/500 [00:27<00:28, 10.17it/s]\n",
      " 43%|####2     | 214/500 [00:28<00:32,  8.73it/s]\n",
      " 43%|####3     | 216/500 [00:28<00:28, 10.13it/s]\n",
      " 44%|####3     | 218/500 [00:28<00:32,  8.57it/s]\n",
      " 44%|####4     | 220/500 [00:28<00:32,  8.73it/s]\n",
      " 44%|####4     | 222/500 [00:29<00:35,  7.78it/s]\n",
      " 45%|####4     | 223/500 [00:29<00:34,  7.99it/s]\n",
      " 45%|####5     | 226/500 [00:29<00:28,  9.73it/s]\n",
      " 46%|####5     | 228/500 [00:29<00:28,  9.56it/s]\n",
      " 46%|####6     | 230/500 [00:30<00:32,  8.22it/s]\n",
      " 46%|####6     | 231/500 [00:30<00:32,  8.36it/s]\n",
      " 47%|####6     | 234/500 [00:30<00:22, 11.86it/s]\n",
      " 47%|####7     | 236/500 [00:30<00:28,  9.35it/s]\n",
      " 48%|####7     | 238/500 [00:31<00:32,  8.11it/s]\n",
      " 48%|####8     | 240/500 [00:31<00:26,  9.71it/s]\n",
      " 48%|####8     | 242/500 [00:31<00:27,  9.54it/s]\n",
      " 49%|####8     | 244/500 [00:31<00:27,  9.45it/s]\n",
      " 49%|####9     | 246/500 [00:31<00:31,  8.12it/s]\n",
      " 49%|####9     | 247/500 [00:31<00:30,  8.29it/s]\n",
      " 50%|#####     | 250/500 [00:32<00:21, 11.84it/s]\n",
      " 50%|#####     | 252/500 [00:32<00:30,  8.10it/s]\n",
      " 51%|#####     | 254/500 [00:32<00:33,  7.39it/s]\n",
      " 51%|#####1    | 256/500 [00:32<00:27,  8.98it/s]\n",
      " 52%|#####1    | 259/500 [00:33<00:29,  8.03it/s]\n",
      " 52%|#####2    | 262/500 [00:33<00:31,  7.58it/s]\n",
      " 53%|#####3    | 267/500 [00:34<00:25,  8.98it/s]\n",
      " 54%|#####3    | 269/500 [00:34<00:23, 10.04it/s]\n",
      " 54%|#####4    | 271/500 [00:34<00:29,  7.88it/s]\n",
      " 55%|#####5    | 275/500 [00:35<00:26,  8.35it/s]\n",
      " 56%|#####5    | 278/500 [00:35<00:25,  8.59it/s]\n",
      " 56%|#####5    | 279/500 [00:35<00:25,  8.66it/s]\n",
      " 56%|#####6    | 280/500 [00:35<00:25,  8.74it/s]\n",
      " 57%|#####6    | 283/500 [00:36<00:27,  7.91it/s]\n",
      " 57%|#####7    | 286/500 [00:36<00:22,  9.35it/s]\n",
      " 58%|#####7    | 288/500 [00:36<00:22,  9.31it/s]\n",
      " 58%|#####7    | 289/500 [00:36<00:22,  9.29it/s]\n",
      " 58%|#####8    | 291/500 [00:37<00:26,  8.04it/s]\n",
      " 59%|#####8    | 294/500 [00:37<00:21,  9.63it/s]\n",
      " 59%|#####9    | 296/500 [00:37<00:24,  8.33it/s]\n",
      " 60%|#####9    | 299/500 [00:38<00:25,  7.75it/s]\n",
      " 60%|######    | 302/500 [00:38<00:19, 10.31it/s]\n",
      " 61%|######    | 304/500 [00:38<00:24,  7.87it/s]\n",
      " 61%|######1   | 307/500 [00:39<00:25,  7.55it/s]\n",
      " 62%|######2   | 311/500 [00:39<00:23,  8.14it/s]\n",
      " 63%|######3   | 315/500 [00:40<00:23,  7.84it/s]\n",
      " 64%|######3   | 319/500 [00:40<00:21,  8.26it/s]\n",
      " 65%|######4   | 323/500 [00:40<00:20,  8.56it/s]\n",
      " 65%|######5   | 327/500 [00:41<00:19,  8.75it/s]\n",
      " 66%|######5   | 328/500 [00:41<00:19,  8.79it/s]\n",
      " 66%|######6   | 331/500 [00:41<00:20,  8.11it/s]\n",
      " 67%|######7   | 335/500 [00:42<00:19,  8.47it/s]\n",
      " 68%|######7   | 339/500 [00:42<00:18,  8.69it/s]\n",
      " 69%|######8   | 343/500 [00:43<00:17,  8.81it/s]\n",
      " 69%|######9   | 347/500 [00:43<00:18,  8.30it/s]\n",
      " 70%|#######   | 351/500 [00:44<00:16,  9.23it/s]\n",
      " 70%|#######   | 352/500 [00:44<00:16,  9.24it/s]\n",
      " 71%|#######1  | 355/500 [00:44<00:17,  8.39it/s]\n",
      " 72%|#######1  | 359/500 [00:45<00:16,  8.66it/s]\n",
      " 72%|#######2  | 362/500 [00:45<00:12, 10.74it/s]\n",
      " 73%|#######2  | 364/500 [00:45<00:16,  8.37it/s]\n",
      " 73%|#######3  | 367/500 [00:45<00:15,  8.61it/s]\n",
      " 74%|#######3  | 369/500 [00:46<00:13,  9.83it/s]\n",
      " 74%|#######4  | 371/500 [00:46<00:16,  7.67it/s]\n",
      " 75%|#######5  | 375/500 [00:46<00:15,  8.22it/s]\n",
      " 76%|#######5  | 378/500 [00:47<00:11, 10.48it/s]\n",
      " 76%|#######6  | 380/500 [00:47<00:13,  9.03it/s]\n",
      " 76%|#######6  | 382/500 [00:47<00:11, 10.31it/s]\n",
      " 77%|#######6  | 384/500 [00:47<00:14,  7.80it/s]\n",
      " 77%|#######7  | 387/500 [00:48<00:15,  7.45it/s]\n",
      " 78%|#######8  | 391/500 [00:48<00:13,  8.07it/s]\n",
      " 79%|#######8  | 393/500 [00:48<00:11,  9.23it/s]\n",
      " 79%|#######9  | 395/500 [00:49<00:14,  7.41it/s]\n",
      " 80%|#######9  | 399/500 [00:49<00:11,  8.85it/s]\n",
      " 80%|########  | 401/500 [00:49<00:09, 10.04it/s]\n",
      " 81%|########  | 403/500 [00:50<00:12,  7.79it/s]\n",
      " 81%|########1 | 405/500 [00:50<00:10,  9.14it/s]\n",
      " 81%|########1 | 407/500 [00:50<00:11,  8.08it/s]\n",
      " 82%|########1 | 409/500 [00:50<00:09,  9.56it/s]\n",
      " 82%|########2 | 411/500 [00:51<00:12,  7.32it/s]\n",
      " 83%|########2 | 415/500 [00:51<00:08, 10.04it/s]\n",
      " 83%|########3 | 417/500 [00:51<00:09,  8.71it/s]\n",
      " 84%|########3 | 419/500 [00:52<00:11,  7.07it/s]\n",
      " 85%|########4 | 423/500 [00:52<00:07, 10.76it/s]\n",
      " 85%|########5 | 425/500 [00:52<00:08,  9.15it/s]\n",
      " 85%|########5 | 427/500 [00:52<00:09,  7.34it/s]\n",
      " 86%|########5 | 429/500 [00:53<00:08,  8.72it/s]\n",
      " 86%|########6 | 432/500 [00:53<00:07,  8.89it/s]\n",
      " 87%|########6 | 434/500 [00:53<00:06, 10.24it/s]\n",
      " 87%|########7 | 436/500 [00:53<00:08,  7.71it/s]\n",
      " 88%|########7 | 439/500 [00:54<00:05, 10.46it/s]\n",
      " 88%|########8 | 441/500 [00:54<00:06,  8.85it/s]\n",
      " 89%|########8 | 443/500 [00:54<00:06,  8.94it/s]\n",
      " 89%|########9 | 445/500 [00:54<00:06,  7.91it/s]\n",
      " 90%|########9 | 448/500 [00:55<00:06,  7.50it/s]\n",
      " 90%|######### | 451/500 [00:55<00:04, 10.05it/s]\n",
      " 91%|######### | 453/500 [00:55<00:06,  7.76it/s]\n",
      " 91%|#########1| 456/500 [00:56<00:05,  7.43it/s]\n",
      " 92%|#########2| 460/500 [00:56<00:04,  8.04it/s]\n",
      " 93%|#########2| 464/500 [00:57<00:04,  8.42it/s]\n",
      " 93%|#########3| 466/500 [00:57<00:03,  9.49it/s]\n",
      " 94%|#########3| 468/500 [00:57<00:04,  7.65it/s]\n",
      " 94%|#########4| 472/500 [00:58<00:03,  8.19it/s]\n",
      " 95%|#########5| 476/500 [00:58<00:02,  8.53it/s]\n",
      " 96%|#########5| 478/500 [00:58<00:02,  9.58it/s]\n",
      " 96%|#########6| 480/500 [00:59<00:02,  8.49it/s]\n",
      " 96%|#########6| 482/500 [00:59<00:01,  9.79it/s]\n",
      " 97%|#########6| 484/500 [00:59<00:02,  7.58it/s]\n",
      " 98%|#########7| 488/500 [00:59<00:01,  8.98it/s]\n",
      " 98%|#########8| 490/500 [01:00<00:00, 10.25it/s]\n",
      " 98%|#########8| 492/500 [01:00<00:01,  7.84it/s]\n",
      " 99%|#########8| 494/500 [01:00<00:00,  9.22it/s]\n",
      " 99%|#########9| 496/500 [01:00<00:00,  9.24it/s]\n",
      "100%|#########9| 498/500 [01:00<00:00, 10.76it/s]\n",
      "100%|##########| 500/500 [01:01<00:00, 12.22it/s]\n",
      "100%|##########| 500/500 [01:01<00:00,  8.18it/s]\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:08<1:14:49,  9.00s/it]\n",
      "  1%|          | 3/500 [00:09<19:59,  2.41s/it]  \n",
      "  1%|          | 4/500 [00:09<13:58,  1.69s/it]\n",
      "  1%|1         | 5/500 [00:09<09:55,  1.20s/it]\n",
      "  1%|1         | 7/500 [00:09<05:30,  1.49it/s]\n",
      "  2%|1         | 8/500 [00:10<04:32,  1.80it/s]\n",
      "  2%|1         | 9/500 [00:14<12:52,  1.57s/it]\n",
      "  2%|2         | 10/500 [00:14<10:01,  1.23s/it]\n",
      "  2%|2         | 11/500 [00:14<07:24,  1.10it/s]\n",
      "  2%|2         | 12/500 [00:15<05:30,  1.47it/s]\n",
      "  3%|2         | 13/500 [00:15<04:09,  1.95it/s]\n",
      "  3%|2         | 14/500 [00:15<03:26,  2.35it/s]\n",
      "  3%|3         | 15/500 [00:15<03:12,  2.52it/s]\n",
      "  3%|3         | 17/500 [00:19<09:32,  1.19s/it]\n",
      "  4%|3         | 18/500 [00:20<07:36,  1.06it/s]\n",
      "  4%|4         | 20/500 [00:20<04:58,  1.61it/s]\n",
      "  4%|4         | 21/500 [00:20<04:13,  1.89it/s]\n",
      "  4%|4         | 22/500 [00:20<03:23,  2.35it/s]\n",
      "  5%|4         | 23/500 [00:21<03:24,  2.34it/s]\n",
      "  5%|5         | 25/500 [00:24<07:37,  1.04it/s]\n",
      "  5%|5         | 26/500 [00:25<07:37,  1.04it/s]\n",
      "  5%|5         | 27/500 [00:25<05:55,  1.33it/s]\n",
      "  6%|5         | 28/500 [00:25<04:48,  1.64it/s]\n",
      "  6%|5         | 29/500 [00:26<03:43,  2.11it/s]\n",
      "  6%|6         | 30/500 [00:26<02:54,  2.69it/s]\n",
      "  6%|6         | 31/500 [00:26<02:33,  3.06it/s]\n",
      "  6%|6         | 32/500 [00:26<02:18,  3.39it/s]\n",
      "  7%|6         | 33/500 [00:30<09:48,  1.26s/it]\n",
      "  7%|6         | 34/500 [00:30<07:54,  1.02s/it]\n",
      "  7%|7         | 35/500 [00:30<06:17,  1.23it/s]\n",
      "  7%|7         | 36/500 [00:31<04:39,  1.66it/s]\n",
      "  7%|7         | 37/500 [00:31<03:30,  2.19it/s]\n",
      "  8%|7         | 38/500 [00:31<02:57,  2.60it/s]\n",
      "  8%|7         | 39/500 [00:31<02:34,  2.99it/s]\n",
      "  8%|8         | 40/500 [00:31<02:17,  3.34it/s]\n",
      "  8%|8         | 41/500 [00:35<10:49,  1.42s/it]\n",
      "  8%|8         | 42/500 [00:36<08:03,  1.06s/it]\n",
      "  9%|8         | 43/500 [00:36<06:37,  1.15it/s]\n",
      "  9%|9         | 45/500 [00:36<03:44,  2.03it/s]\n",
      "  9%|9         | 46/500 [00:37<03:37,  2.09it/s]\n",
      "  9%|9         | 47/500 [00:37<03:06,  2.43it/s]\n",
      " 10%|9         | 49/500 [00:40<07:22,  1.02it/s]\n",
      " 10%|#         | 50/500 [00:41<06:23,  1.17it/s]\n",
      " 10%|#         | 51/500 [00:41<06:11,  1.21it/s]\n",
      " 11%|#         | 54/500 [00:42<03:14,  2.29it/s]\n",
      " 11%|#1        | 55/500 [00:42<02:45,  2.69it/s]\n",
      " 11%|#1        | 56/500 [00:42<03:01,  2.44it/s]\n",
      " 11%|#1        | 57/500 [00:46<08:04,  1.09s/it]\n",
      " 12%|#1        | 58/500 [00:46<06:48,  1.08it/s]\n",
      " 12%|#1        | 59/500 [00:46<05:37,  1.31it/s]\n",
      " 12%|#2        | 60/500 [00:47<04:56,  1.48it/s]\n",
      " 12%|#2        | 62/500 [00:47<02:55,  2.50it/s]\n",
      " 13%|#2        | 63/500 [00:48<03:32,  2.05it/s]\n",
      " 13%|#2        | 64/500 [00:48<03:26,  2.11it/s]\n",
      " 13%|#3        | 65/500 [00:50<06:43,  1.08it/s]\n",
      " 13%|#3        | 66/500 [00:51<06:48,  1.06it/s]\n",
      " 13%|#3        | 67/500 [00:52<06:25,  1.12it/s]\n",
      " 14%|#3        | 68/500 [00:52<04:47,  1.50it/s]\n",
      " 14%|#3        | 69/500 [00:52<04:18,  1.67it/s]\n",
      " 14%|#4        | 71/500 [00:53<02:31,  2.84it/s]\n",
      " 14%|#4        | 72/500 [00:53<02:50,  2.51it/s]\n",
      " 15%|#4        | 73/500 [00:55<05:30,  1.29it/s]\n",
      " 15%|#4        | 74/500 [00:56<06:44,  1.05it/s]\n",
      " 15%|#5        | 75/500 [00:57<05:42,  1.24it/s]\n",
      " 15%|#5        | 76/500 [00:58<05:22,  1.31it/s]\n",
      " 16%|#5        | 78/500 [00:58<03:07,  2.25it/s]\n",
      " 16%|#5        | 79/500 [00:58<02:55,  2.40it/s]\n",
      " 16%|#6        | 80/500 [00:59<03:20,  2.10it/s]\n",
      " 16%|#6        | 81/500 [01:00<04:52,  1.43it/s]\n",
      " 16%|#6        | 82/500 [01:01<06:27,  1.08it/s]\n",
      " 17%|#6        | 83/500 [01:02<06:06,  1.14it/s]\n",
      " 17%|#6        | 84/500 [01:02<04:33,  1.52it/s]\n",
      " 17%|#7        | 85/500 [01:03<03:52,  1.79it/s]\n",
      " 17%|#7        | 86/500 [01:03<03:36,  1.92it/s]\n",
      " 17%|#7        | 87/500 [01:04<03:38,  1.89it/s]\n",
      " 18%|#7        | 88/500 [01:04<02:59,  2.29it/s]\n",
      " 18%|#7        | 89/500 [01:05<04:45,  1.44it/s]\n",
      " 18%|#8        | 90/500 [01:07<06:12,  1.10it/s]\n",
      " 18%|#8        | 91/500 [01:07<04:33,  1.49it/s]\n",
      " 18%|#8        | 92/500 [01:08<05:23,  1.26it/s]\n",
      " 19%|#8        | 93/500 [01:08<04:12,  1.61it/s]\n",
      " 19%|#8        | 94/500 [01:08<03:49,  1.77it/s]\n",
      " 19%|#9        | 95/500 [01:09<03:33,  1.90it/s]\n",
      " 19%|#9        | 96/500 [01:09<03:21,  2.01it/s]\n",
      " 19%|#9        | 97/500 [01:11<05:11,  1.29it/s]\n",
      " 20%|#9        | 98/500 [01:12<06:14,  1.07it/s]\n",
      " 20%|#9        | 99/500 [01:12<04:34,  1.46it/s]\n",
      " 20%|##        | 100/500 [01:13<04:42,  1.41it/s]\n",
      " 20%|##        | 101/500 [01:13<03:56,  1.69it/s]\n",
      " 20%|##        | 102/500 [01:14<03:36,  1.84it/s]\n",
      " 21%|##        | 103/500 [01:14<02:44,  2.42it/s]\n",
      " 21%|##        | 104/500 [01:14<02:58,  2.21it/s]\n",
      " 21%|##1       | 105/500 [01:16<05:44,  1.15it/s]\n",
      " 21%|##1       | 106/500 [01:17<05:05,  1.29it/s]\n",
      " 21%|##1       | 107/500 [01:17<04:36,  1.42it/s]\n",
      " 22%|##1       | 108/500 [01:18<04:43,  1.38it/s]\n",
      " 22%|##1       | 109/500 [01:18<03:55,  1.66it/s]\n",
      " 22%|##2       | 110/500 [01:19<03:22,  1.92it/s]\n",
      " 22%|##2       | 111/500 [01:19<03:37,  1.79it/s]\n",
      " 23%|##2       | 113/500 [01:22<05:20,  1.21it/s]\n",
      " 23%|##2       | 114/500 [01:22<04:21,  1.48it/s]\n",
      " 23%|##3       | 115/500 [01:22<03:34,  1.80it/s]\n",
      " 23%|##3       | 116/500 [01:24<05:25,  1.18it/s]\n",
      " 23%|##3       | 117/500 [01:24<04:05,  1.56it/s]\n",
      " 24%|##3       | 118/500 [01:24<03:06,  2.05it/s]\n",
      " 24%|##3       | 119/500 [01:24<02:36,  2.44it/s]\n",
      " 24%|##4       | 120/500 [01:25<03:26,  1.84it/s]\n",
      " 24%|##4       | 121/500 [01:25<03:13,  1.96it/s]\n",
      " 24%|##4       | 122/500 [01:27<05:31,  1.14it/s]\n",
      " 25%|##4       | 123/500 [01:27<04:40,  1.34it/s]\n",
      " 25%|##4       | 124/500 [01:28<05:06,  1.23it/s]\n",
      " 25%|##5       | 125/500 [01:29<05:11,  1.20it/s]\n",
      " 25%|##5       | 127/500 [01:30<03:05,  2.01it/s]\n",
      " 26%|##5       | 128/500 [01:30<02:59,  2.07it/s]\n",
      " 26%|##5       | 129/500 [01:30<02:54,  2.13it/s]\n",
      " 26%|##6       | 130/500 [01:31<03:33,  1.73it/s]\n",
      " 26%|##6       | 131/500 [01:33<05:10,  1.19it/s]\n",
      " 26%|##6       | 132/500 [01:34<05:58,  1.03it/s]\n",
      " 27%|##6       | 133/500 [01:35<05:11,  1.18it/s]\n",
      " 27%|##6       | 134/500 [01:35<03:51,  1.58it/s]\n",
      " 27%|##7       | 135/500 [01:35<02:54,  2.09it/s]\n",
      " 27%|##7       | 136/500 [01:36<03:24,  1.78it/s]\n",
      " 28%|##7       | 138/500 [01:36<02:52,  2.10it/s]\n",
      " 28%|##7       | 139/500 [01:37<03:37,  1.66it/s]\n",
      " 28%|##8       | 140/500 [01:39<04:32,  1.32it/s]\n",
      " 28%|##8       | 141/500 [01:40<05:14,  1.14it/s]\n",
      " 28%|##8       | 142/500 [01:40<04:29,  1.33it/s]\n",
      " 29%|##8       | 143/500 [01:40<03:23,  1.76it/s]\n",
      " 29%|##8       | 144/500 [01:41<02:46,  2.14it/s]\n",
      " 29%|##9       | 145/500 [01:41<02:53,  2.05it/s]\n",
      " 29%|##9       | 146/500 [01:42<03:21,  1.76it/s]\n",
      " 29%|##9       | 147/500 [01:43<03:51,  1.52it/s]\n",
      " 30%|##9       | 148/500 [01:43<04:01,  1.46it/s]\n",
      " 30%|##9       | 149/500 [01:45<05:39,  1.03it/s]\n",
      " 30%|###       | 150/500 [01:45<04:19,  1.35it/s]\n",
      " 30%|###       | 151/500 [01:46<03:35,  1.62it/s]\n",
      " 30%|###       | 152/500 [01:46<03:04,  1.88it/s]\n",
      " 31%|###       | 153/500 [01:46<02:31,  2.29it/s]\n",
      " 31%|###       | 154/500 [01:47<02:30,  2.29it/s]\n",
      " 31%|###1      | 155/500 [01:47<02:41,  2.14it/s]\n",
      " 31%|###1      | 156/500 [01:48<03:55,  1.46it/s]\n",
      " 31%|###1      | 157/500 [01:50<05:10,  1.11it/s]\n",
      " 32%|###1      | 158/500 [01:50<03:58,  1.43it/s]\n",
      " 32%|###1      | 159/500 [01:51<04:04,  1.40it/s]\n",
      " 32%|###2      | 161/500 [01:51<03:01,  1.87it/s]\n",
      " 33%|###2      | 163/500 [01:52<02:10,  2.57it/s]\n",
      " 33%|###2      | 164/500 [01:53<03:34,  1.57it/s]\n",
      " 33%|###3      | 165/500 [01:55<05:10,  1.08it/s]\n",
      " 33%|###3      | 166/500 [01:55<04:27,  1.25it/s]\n",
      " 33%|###3      | 167/500 [01:56<03:54,  1.42it/s]\n",
      " 34%|###3      | 168/500 [01:56<03:08,  1.76it/s]\n",
      " 34%|###3      | 169/500 [01:56<02:25,  2.28it/s]\n",
      " 34%|###4      | 170/500 [01:57<02:34,  2.13it/s]\n",
      " 34%|###4      | 171/500 [01:57<02:30,  2.18it/s]\n",
      " 34%|###4      | 172/500 [01:58<02:38,  2.07it/s]\n",
      " 35%|###4      | 173/500 [02:00<06:03,  1.11s/it]\n",
      " 35%|###4      | 174/500 [02:01<05:28,  1.01s/it]\n",
      " 35%|###5      | 175/500 [02:01<04:21,  1.24it/s]\n",
      " 35%|###5      | 177/500 [02:02<02:52,  1.87it/s]\n",
      " 36%|###5      | 178/500 [02:02<02:43,  1.97it/s]\n",
      " 36%|###5      | 179/500 [02:03<02:27,  2.17it/s]\n",
      " 36%|###6      | 180/500 [02:03<02:24,  2.21it/s]\n",
      " 36%|###6      | 181/500 [02:06<05:36,  1.05s/it]\n",
      " 36%|###6      | 182/500 [02:06<04:38,  1.14it/s]\n",
      " 37%|###6      | 183/500 [02:06<03:47,  1.39it/s]\n",
      " 37%|###6      | 184/500 [02:07<03:50,  1.37it/s]\n",
      " 37%|###7      | 185/500 [02:08<03:22,  1.55it/s]\n",
      " 37%|###7      | 187/500 [02:08<01:57,  2.67it/s]\n",
      " 38%|###7      | 188/500 [02:09<02:43,  1.91it/s]\n",
      " 38%|###7      | 189/500 [02:11<05:04,  1.02it/s]\n",
      " 38%|###8      | 190/500 [02:11<04:17,  1.20it/s]\n",
      " 38%|###8      | 191/500 [02:12<03:14,  1.59it/s]\n",
      " 38%|###8      | 192/500 [02:13<04:13,  1.22it/s]\n",
      " 39%|###8      | 193/500 [02:13<03:28,  1.48it/s]\n",
      " 39%|###8      | 194/500 [02:14<02:55,  1.74it/s]\n",
      " 39%|###9      | 195/500 [02:14<02:13,  2.29it/s]\n",
      " 39%|###9      | 197/500 [02:16<04:12,  1.20it/s]\n",
      " 40%|###9      | 199/500 [02:16<02:37,  1.91it/s]\n",
      " 40%|####      | 200/500 [02:18<03:56,  1.27it/s]\n",
      " 40%|####      | 201/500 [02:18<03:07,  1.59it/s]\n",
      " 41%|####      | 203/500 [02:19<02:29,  1.98it/s]\n",
      " 41%|####      | 204/500 [02:19<02:17,  2.15it/s]\n",
      " 41%|####1     | 205/500 [02:21<04:03,  1.21it/s]\n",
      " 41%|####1     | 206/500 [02:22<03:32,  1.38it/s]\n",
      " 41%|####1     | 207/500 [02:22<02:43,  1.79it/s]\n",
      " 42%|####1     | 208/500 [02:22<03:08,  1.55it/s]\n",
      " 42%|####1     | 209/500 [02:23<02:41,  1.81it/s]\n",
      " 42%|####2     | 210/500 [02:23<02:12,  2.19it/s]\n",
      " 42%|####2     | 211/500 [02:24<03:32,  1.36it/s]\n",
      " 42%|####2     | 212/500 [02:25<02:47,  1.72it/s]\n",
      " 43%|####2     | 213/500 [02:25<03:02,  1.58it/s]\n",
      " 43%|####2     | 214/500 [02:26<03:21,  1.42it/s]\n",
      " 43%|####3     | 215/500 [02:27<03:16,  1.45it/s]\n",
      " 43%|####3     | 216/500 [02:28<03:40,  1.29it/s]\n",
      " 43%|####3     | 217/500 [02:28<02:52,  1.64it/s]\n",
      " 44%|####3     | 218/500 [02:29<03:04,  1.53it/s]\n",
      " 44%|####4     | 220/500 [02:30<02:34,  1.81it/s]\n",
      " 44%|####4     | 221/500 [02:31<03:10,  1.46it/s]\n",
      " 44%|####4     | 222/500 [02:31<02:51,  1.62it/s]\n",
      " 45%|####4     | 223/500 [02:32<03:10,  1.46it/s]\n",
      " 45%|####4     | 224/500 [02:33<03:31,  1.30it/s]\n",
      " 45%|####5     | 225/500 [02:34<03:30,  1.31it/s]\n",
      " 45%|####5     | 226/500 [02:34<02:46,  1.65it/s]\n",
      " 45%|####5     | 227/500 [02:35<02:40,  1.70it/s]\n",
      " 46%|####5     | 228/500 [02:35<02:27,  1.84it/s]\n",
      " 46%|####5     | 229/500 [02:36<03:10,  1.42it/s]\n",
      " 46%|####6     | 230/500 [02:36<02:22,  1.90it/s]\n",
      " 46%|####6     | 231/500 [02:38<03:41,  1.22it/s]\n",
      " 46%|####6     | 232/500 [02:39<03:52,  1.15it/s]\n",
      " 47%|####6     | 233/500 [02:39<03:00,  1.48it/s]\n",
      " 47%|####6     | 234/500 [02:39<02:40,  1.66it/s]\n",
      " 47%|####6     | 235/500 [02:40<02:34,  1.71it/s]\n",
      " 47%|####7     | 237/500 [02:41<02:54,  1.51it/s]\n",
      " 48%|####7     | 238/500 [02:42<02:18,  1.90it/s]\n",
      " 48%|####7     | 239/500 [02:43<03:47,  1.15it/s]\n",
      " 48%|####8     | 240/500 [02:45<04:01,  1.08it/s]\n",
      " 48%|####8     | 241/500 [02:45<03:01,  1.43it/s]\n",
      " 48%|####8     | 242/500 [02:45<02:16,  1.88it/s]\n",
      " 49%|####8     | 243/500 [02:45<02:09,  1.99it/s]\n",
      " 49%|####8     | 244/500 [02:46<02:11,  1.94it/s]\n",
      " 49%|####9     | 245/500 [02:46<02:29,  1.71it/s]\n",
      " 49%|####9     | 246/500 [02:47<02:41,  1.57it/s]\n",
      " 49%|####9     | 247/500 [02:48<02:34,  1.64it/s]\n",
      " 50%|####9     | 248/500 [02:50<04:06,  1.02it/s]\n",
      " 50%|####9     | 249/500 [02:50<03:08,  1.33it/s]\n",
      " 50%|#####     | 250/500 [02:50<02:36,  1.60it/s]\n",
      " 50%|#####     | 252/500 [02:50<01:29,  2.77it/s]\n",
      " 51%|#####     | 253/500 [02:51<01:46,  2.31it/s]\n",
      " 51%|#####     | 254/500 [02:53<03:17,  1.25it/s]\n",
      " 51%|#####1    | 255/500 [02:53<03:06,  1.31it/s]\n",
      " 51%|#####1    | 256/500 [02:55<03:27,  1.17it/s]\n",
      " 51%|#####1    | 257/500 [02:56<03:36,  1.12it/s]\n",
      " 52%|#####1    | 258/500 [02:56<02:48,  1.44it/s]\n",
      " 52%|#####2    | 260/500 [02:56<01:43,  2.33it/s]\n",
      " 52%|#####2    | 261/500 [02:56<01:49,  2.19it/s]\n",
      " 52%|#####2    | 262/500 [02:58<02:47,  1.42it/s]\n",
      " 53%|#####2    | 263/500 [02:59<03:11,  1.24it/s]\n",
      " 53%|#####2    | 264/500 [03:00<03:14,  1.21it/s]\n",
      " 53%|#####3    | 265/500 [03:01<03:09,  1.24it/s]\n",
      " 53%|#####3    | 266/500 [03:01<03:12,  1.21it/s]\n",
      " 53%|#####3    | 267/500 [03:02<02:30,  1.55it/s]\n",
      " 54%|#####3    | 268/500 [03:02<02:00,  1.92it/s]\n",
      " 54%|#####3    | 269/500 [03:02<01:39,  2.32it/s]\n",
      " 54%|#####4    | 270/500 [03:03<02:01,  1.89it/s]\n",
      " 54%|#####4    | 271/500 [03:04<02:23,  1.59it/s]\n",
      " 54%|#####4    | 272/500 [03:05<03:09,  1.20it/s]\n",
      " 55%|#####4    | 273/500 [03:06<03:32,  1.07it/s]\n",
      " 55%|#####4    | 274/500 [03:07<02:57,  1.27it/s]\n",
      " 55%|#####5    | 275/500 [03:07<02:11,  1.71it/s]\n",
      " 55%|#####5    | 276/500 [03:07<01:46,  2.11it/s]\n",
      " 55%|#####5    | 277/500 [03:07<01:35,  2.33it/s]\n",
      " 56%|#####5    | 278/500 [03:08<02:04,  1.78it/s]\n",
      " 56%|#####5    | 279/500 [03:09<02:45,  1.33it/s]\n",
      " 56%|#####6    | 280/500 [03:10<02:16,  1.61it/s]\n",
      " 56%|#####6    | 281/500 [03:12<03:50,  1.05s/it]\n",
      " 56%|#####6    | 282/500 [03:12<02:55,  1.24it/s]\n",
      " 57%|#####6    | 283/500 [03:12<02:09,  1.68it/s]\n",
      " 57%|#####6    | 284/500 [03:12<01:37,  2.23it/s]\n",
      " 57%|#####6    | 285/500 [03:13<01:56,  1.84it/s]\n",
      " 57%|#####7    | 286/500 [03:13<01:42,  2.10it/s]\n",
      " 57%|#####7    | 287/500 [03:14<02:13,  1.60it/s]\n",
      " 58%|#####7    | 288/500 [03:15<02:28,  1.43it/s]\n",
      " 58%|#####7    | 289/500 [03:17<03:47,  1.08s/it]\n",
      " 58%|#####8    | 290/500 [03:18<03:05,  1.13it/s]\n",
      " 58%|#####8    | 291/500 [03:18<02:22,  1.46it/s]\n",
      " 59%|#####8    | 293/500 [03:18<01:42,  2.03it/s]\n",
      " 59%|#####8    | 294/500 [03:19<01:33,  2.21it/s]\n",
      " 59%|#####8    | 295/500 [03:20<02:00,  1.70it/s]\n",
      " 59%|#####9    | 296/500 [03:20<02:09,  1.57it/s]\n",
      " 59%|#####9    | 297/500 [03:22<03:11,  1.06it/s]\n",
      " 60%|#####9    | 298/500 [03:22<02:28,  1.36it/s]\n",
      " 60%|#####9    | 299/500 [03:23<02:16,  1.47it/s]\n",
      " 60%|######    | 300/500 [03:23<01:42,  1.95it/s]\n",
      " 60%|######    | 301/500 [03:24<01:43,  1.92it/s]\n",
      " 60%|######    | 302/500 [03:24<02:03,  1.60it/s]\n",
      " 61%|######    | 303/500 [03:25<02:04,  1.58it/s]\n",
      " 61%|######    | 304/500 [03:25<01:52,  1.74it/s]\n",
      " 61%|######1   | 305/500 [03:27<03:00,  1.08it/s]\n",
      " 61%|######1   | 306/500 [03:27<02:18,  1.40it/s]\n",
      " 62%|######1   | 308/500 [03:28<01:18,  2.45it/s]\n",
      " 62%|######1   | 309/500 [03:29<02:00,  1.59it/s]\n",
      " 62%|######2   | 310/500 [03:29<01:39,  1.91it/s]\n",
      " 62%|######2   | 311/500 [03:30<02:07,  1.48it/s]\n",
      " 62%|######2   | 312/500 [03:30<01:48,  1.73it/s]\n",
      " 63%|######2   | 313/500 [03:33<03:07,  1.00s/it]\n",
      " 63%|######2   | 314/500 [03:33<02:24,  1.29it/s]\n",
      " 63%|######3   | 315/500 [03:33<01:58,  1.56it/s]\n",
      " 63%|######3   | 317/500 [03:34<01:44,  1.75it/s]\n",
      " 64%|######3   | 318/500 [03:34<01:33,  1.95it/s]\n",
      " 64%|######3   | 319/500 [03:34<01:13,  2.45it/s]\n",
      " 64%|######4   | 320/500 [03:36<01:57,  1.54it/s]\n",
      " 64%|######4   | 321/500 [03:38<03:06,  1.04s/it]\n",
      " 65%|######4   | 323/500 [03:38<01:51,  1.59it/s]\n",
      " 65%|######4   | 324/500 [03:39<01:47,  1.64it/s]\n",
      " 65%|######5   | 325/500 [03:39<01:29,  1.97it/s]\n",
      " 65%|######5   | 326/500 [03:39<01:09,  2.49it/s]\n",
      " 65%|######5   | 327/500 [03:41<02:08,  1.35it/s]\n",
      " 66%|######5   | 328/500 [03:41<02:08,  1.34it/s]\n",
      " 66%|######5   | 329/500 [03:43<02:35,  1.10it/s]\n",
      " 66%|######6   | 330/500 [03:43<02:26,  1.16it/s]\n",
      " 66%|######6   | 331/500 [03:43<01:48,  1.56it/s]\n",
      " 66%|######6   | 332/500 [03:44<01:37,  1.73it/s]\n",
      " 67%|######6   | 333/500 [03:44<01:18,  2.12it/s]\n",
      " 67%|######6   | 334/500 [03:45<01:16,  2.17it/s]\n",
      " 67%|######7   | 335/500 [03:45<01:14,  2.21it/s]\n",
      " 67%|######7   | 336/500 [03:46<02:01,  1.35it/s]\n",
      " 67%|######7   | 337/500 [03:47<01:45,  1.54it/s]\n",
      " 68%|######7   | 338/500 [03:48<02:11,  1.23it/s]\n",
      " 68%|######7   | 339/500 [03:48<01:52,  1.43it/s]\n",
      " 68%|######8   | 340/500 [03:49<01:54,  1.40it/s]\n",
      " 68%|######8   | 341/500 [03:49<01:30,  1.77it/s]\n",
      " 68%|######8   | 342/500 [03:50<01:23,  1.90it/s]\n",
      " 69%|######8   | 343/500 [03:50<01:13,  2.15it/s]\n",
      " 69%|######8   | 344/500 [03:52<02:17,  1.14it/s]\n",
      " 69%|######9   | 346/500 [03:53<01:59,  1.29it/s]\n",
      " 70%|######9   | 348/500 [03:54<01:17,  1.96it/s]\n",
      " 70%|######9   | 349/500 [03:54<01:07,  2.25it/s]\n",
      " 70%|#######   | 350/500 [03:55<01:29,  1.67it/s]\n",
      " 70%|#######   | 351/500 [03:56<01:39,  1.50it/s]\n",
      " 70%|#######   | 352/500 [03:57<02:12,  1.12it/s]\n",
      " 71%|#######   | 353/500 [03:57<01:44,  1.41it/s]\n",
      " 71%|#######   | 354/500 [03:58<01:45,  1.38it/s]\n",
      " 71%|#######1  | 355/500 [03:59<01:37,  1.49it/s]\n",
      " 71%|#######1  | 356/500 [03:59<01:31,  1.58it/s]\n",
      " 71%|#######1  | 357/500 [03:59<01:08,  2.09it/s]\n",
      " 72%|#######1  | 358/500 [04:00<01:24,  1.68it/s]\n",
      " 72%|#######1  | 359/500 [04:01<01:44,  1.35it/s]\n",
      " 72%|#######2  | 360/500 [04:02<01:58,  1.18it/s]\n",
      " 72%|#######2  | 361/500 [04:03<01:31,  1.52it/s]\n",
      " 72%|#######2  | 362/500 [04:04<01:39,  1.39it/s]\n",
      " 73%|#######2  | 363/500 [04:04<01:13,  1.86it/s]\n",
      " 73%|#######2  | 364/500 [04:04<00:55,  2.45it/s]\n",
      " 73%|#######3  | 365/500 [04:05<01:09,  1.95it/s]\n",
      " 73%|#######3  | 366/500 [04:06<01:57,  1.14it/s]\n",
      " 73%|#######3  | 367/500 [04:07<01:34,  1.40it/s]\n",
      " 74%|#######3  | 368/500 [04:07<01:40,  1.31it/s]\n",
      " 74%|#######3  | 369/500 [04:08<01:22,  1.59it/s]\n",
      " 74%|#######4  | 370/500 [04:08<01:01,  2.11it/s]\n",
      " 74%|#######4  | 371/500 [04:09<01:07,  1.90it/s]\n",
      " 74%|#######4  | 372/500 [04:09<01:12,  1.77it/s]\n",
      " 75%|#######4  | 373/500 [04:10<01:27,  1.46it/s]\n",
      " 75%|#######4  | 374/500 [04:12<01:53,  1.11it/s]\n",
      " 75%|#######5  | 375/500 [04:12<01:27,  1.43it/s]\n",
      " 75%|#######5  | 376/500 [04:12<01:24,  1.46it/s]\n",
      " 75%|#######5  | 377/500 [04:13<01:14,  1.64it/s]\n",
      " 76%|#######5  | 378/500 [04:13<01:11,  1.70it/s]\n",
      " 76%|#######5  | 379/500 [04:14<00:53,  2.25it/s]\n",
      " 76%|#######6  | 380/500 [04:14<01:08,  1.75it/s]\n",
      " 76%|#######6  | 381/500 [04:15<00:55,  2.14it/s]\n",
      " 76%|#######6  | 382/500 [04:17<02:10,  1.11s/it]\n",
      " 77%|#######6  | 384/500 [04:18<01:17,  1.49it/s]\n",
      " 77%|#######7  | 385/500 [04:18<01:10,  1.63it/s]\n",
      " 77%|#######7  | 386/500 [04:19<01:07,  1.68it/s]\n",
      " 77%|#######7  | 387/500 [04:19<01:12,  1.56it/s]\n",
      " 78%|#######7  | 388/500 [04:20<01:08,  1.63it/s]\n",
      " 78%|#######7  | 389/500 [04:20<00:55,  2.00it/s]\n",
      " 78%|#######8  | 390/500 [04:22<01:51,  1.01s/it]\n",
      " 78%|#######8  | 391/500 [04:22<01:21,  1.34it/s]\n",
      " 79%|#######8  | 393/500 [04:24<01:12,  1.47it/s]\n",
      " 79%|#######8  | 394/500 [04:24<00:57,  1.85it/s]\n",
      " 79%|#######9  | 395/500 [04:25<01:08,  1.53it/s]\n",
      " 79%|#######9  | 396/500 [04:25<01:07,  1.53it/s]\n",
      " 79%|#######9  | 397/500 [04:26<00:54,  1.88it/s]\n",
      " 80%|#######9  | 398/500 [04:27<01:26,  1.18it/s]\n",
      " 80%|#######9  | 399/500 [04:28<01:22,  1.22it/s]\n",
      " 80%|########  | 400/500 [04:28<01:04,  1.55it/s]\n",
      " 80%|########  | 401/500 [04:29<00:57,  1.72it/s]\n",
      " 80%|########  | 402/500 [04:29<00:55,  1.75it/s]\n",
      " 81%|########  | 403/500 [04:30<01:03,  1.52it/s]\n",
      " 81%|########  | 404/500 [04:31<01:12,  1.33it/s]\n",
      " 81%|########1 | 405/500 [04:31<00:53,  1.78it/s]\n",
      " 81%|########1 | 406/500 [04:32<01:07,  1.39it/s]\n",
      " 81%|########1 | 407/500 [04:33<01:04,  1.43it/s]\n",
      " 82%|########1 | 408/500 [04:34<01:14,  1.23it/s]\n",
      " 82%|########1 | 409/500 [04:35<01:09,  1.31it/s]\n",
      " 82%|########2 | 411/500 [04:36<00:56,  1.57it/s]\n",
      " 83%|########2 | 413/500 [04:37<00:50,  1.73it/s]\n",
      " 83%|########2 | 414/500 [04:37<00:44,  1.92it/s]\n",
      " 83%|########2 | 415/500 [04:39<01:06,  1.27it/s]\n",
      " 83%|########3 | 416/500 [04:39<01:10,  1.20it/s]\n",
      " 83%|########3 | 417/500 [04:40<00:57,  1.43it/s]\n",
      " 84%|########3 | 418/500 [04:40<00:48,  1.68it/s]\n",
      " 84%|########3 | 419/500 [04:41<00:49,  1.64it/s]\n",
      " 84%|########4 | 420/500 [04:41<00:44,  1.78it/s]\n",
      " 84%|########4 | 422/500 [04:43<00:48,  1.60it/s]\n",
      " 85%|########4 | 423/500 [04:44<01:04,  1.18it/s]\n",
      " 85%|########4 | 424/500 [04:44<00:53,  1.41it/s]\n",
      " 85%|########5 | 425/500 [04:45<00:43,  1.73it/s]\n",
      " 85%|########5 | 426/500 [04:45<00:42,  1.76it/s]\n",
      " 85%|########5 | 427/500 [04:46<00:40,  1.78it/s]\n",
      " 86%|########5 | 428/500 [04:46<00:37,  1.91it/s]\n",
      " 86%|########5 | 429/500 [04:47<00:37,  1.89it/s]\n",
      " 86%|########6 | 430/500 [04:47<00:32,  2.13it/s]\n",
      " 86%|########6 | 431/500 [04:50<01:20,  1.17s/it]\n",
      " 86%|########6 | 432/500 [04:50<01:04,  1.05it/s]\n",
      " 87%|########6 | 434/500 [04:51<00:42,  1.57it/s]\n",
      " 87%|########7 | 435/500 [04:51<00:38,  1.70it/s]\n",
      " 87%|########7 | 436/500 [04:52<00:36,  1.74it/s]\n",
      " 87%|########7 | 437/500 [04:52<00:32,  1.96it/s]\n",
      " 88%|########7 | 438/500 [04:52<00:26,  2.34it/s]\n",
      " 88%|########7 | 439/500 [04:55<01:09,  1.14s/it]\n",
      " 88%|########8 | 440/500 [04:55<00:50,  1.19it/s]\n",
      " 88%|########8 | 441/500 [04:56<00:42,  1.38it/s]\n",
      " 89%|########8 | 443/500 [04:57<00:33,  1.69it/s]\n",
      " 89%|########8 | 444/500 [04:57<00:29,  1.90it/s]\n",
      " 89%|########9 | 445/500 [04:57<00:26,  2.11it/s]\n",
      " 89%|########9 | 446/500 [04:58<00:25,  2.16it/s]\n",
      " 89%|########9 | 447/500 [05:00<00:56,  1.06s/it]\n",
      " 90%|########9 | 448/500 [05:01<00:41,  1.27it/s]\n",
      " 90%|########9 | 449/500 [05:01<00:36,  1.39it/s]\n",
      " 90%|######### | 450/500 [05:01<00:26,  1.85it/s]\n",
      " 90%|######### | 451/500 [05:02<00:23,  2.10it/s]\n",
      " 90%|######### | 452/500 [05:02<00:28,  1.69it/s]\n",
      " 91%|######### | 453/500 [05:03<00:30,  1.55it/s]\n",
      " 91%|######### | 454/500 [05:03<00:25,  1.82it/s]\n",
      " 91%|#########1| 455/500 [05:06<00:45,  1.00s/it]\n",
      " 91%|#########1| 456/500 [05:06<00:36,  1.20it/s]\n",
      " 91%|#########1| 457/500 [05:06<00:29,  1.47it/s]\n",
      " 92%|#########1| 458/500 [05:06<00:21,  1.96it/s]\n",
      " 92%|#########1| 459/500 [05:07<00:19,  2.05it/s]\n",
      " 92%|#########2| 460/500 [05:08<00:22,  1.76it/s]\n",
      " 92%|#########2| 461/500 [05:08<00:21,  1.78it/s]\n",
      " 92%|#########2| 462/500 [05:08<00:16,  2.35it/s]\n",
      " 93%|#########2| 463/500 [05:11<00:41,  1.12s/it]\n",
      " 93%|#########2| 464/500 [05:11<00:29,  1.23it/s]\n",
      " 93%|#########3| 465/500 [05:11<00:22,  1.58it/s]\n",
      " 93%|#########3| 466/500 [05:11<00:16,  2.10it/s]\n",
      " 93%|#########3| 467/500 [05:12<00:15,  2.16it/s]\n",
      " 94%|#########3| 468/500 [05:13<00:22,  1.40it/s]\n",
      " 94%|#########3| 469/500 [05:13<00:16,  1.87it/s]\n",
      " 94%|#########3| 470/500 [05:14<00:16,  1.86it/s]\n",
      " 94%|#########4| 471/500 [05:15<00:24,  1.20it/s]\n",
      " 94%|#########4| 472/500 [05:16<00:20,  1.34it/s]\n",
      " 95%|#########4| 473/500 [05:17<00:19,  1.39it/s]\n",
      " 95%|#########4| 474/500 [05:17<00:14,  1.76it/s]\n",
      " 95%|#########5| 475/500 [05:17<00:14,  1.68it/s]\n",
      " 95%|#########5| 476/500 [05:19<00:17,  1.35it/s]\n",
      " 95%|#########5| 477/500 [05:19<00:12,  1.81it/s]\n",
      " 96%|#########5| 478/500 [05:19<00:13,  1.63it/s]\n",
      " 96%|#########5| 479/500 [05:21<00:16,  1.27it/s]\n",
      " 96%|#########6| 480/500 [05:21<00:14,  1.40it/s]\n",
      " 96%|#########6| 481/500 [05:21<00:11,  1.67it/s]\n",
      " 96%|#########6| 482/500 [05:22<00:09,  1.94it/s]\n",
      " 97%|#########6| 483/500 [05:23<00:11,  1.53it/s]\n",
      " 97%|#########6| 484/500 [05:23<00:09,  1.70it/s]\n",
      " 97%|#########7| 485/500 [05:24<00:09,  1.56it/s]\n",
      " 97%|#########7| 486/500 [05:24<00:08,  1.64it/s]\n",
      " 97%|#########7| 487/500 [05:26<00:10,  1.22it/s]\n",
      " 98%|#########7| 488/500 [05:27<00:10,  1.11it/s]\n",
      " 98%|#########7| 489/500 [05:27<00:07,  1.51it/s]\n",
      " 98%|#########8| 490/500 [05:27<00:05,  1.79it/s]\n",
      " 98%|#########8| 491/500 [05:28<00:05,  1.53it/s]\n",
      " 98%|#########8| 492/500 [05:28<00:04,  1.92it/s]\n",
      " 99%|#########8| 493/500 [05:29<00:03,  2.16it/s]\n",
      " 99%|#########8| 494/500 [05:29<00:03,  1.82it/s]\n",
      " 99%|#########9| 495/500 [05:31<00:04,  1.14it/s]\n",
      " 99%|#########9| 496/500 [05:32<00:03,  1.06it/s]\n",
      " 99%|#########9| 497/500 [05:32<00:02,  1.45it/s]\n",
      "100%|#########9| 499/500 [05:33<00:00,  1.67it/s]\n",
      "100%|##########| 500/500 [05:33<00:00,  1.98it/s]\n",
      "100%|##########| 500/500 [05:34<00:00,  1.50it/s]\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:08<1:11:36,  8.61s/it]\n",
      "  1%|          | 3/500 [00:08<19:28,  2.35s/it]  \n",
      "  1%|          | 4/500 [00:09<13:19,  1.61s/it]\n",
      "  1%|1         | 5/500 [00:14<23:01,  2.79s/it]\n",
      "  1%|1         | 7/500 [00:14<12:12,  1.49s/it]\n",
      "  2%|1         | 8/500 [00:15<10:11,  1.24s/it]\n",
      "  2%|1         | 9/500 [00:19<18:15,  2.23s/it]\n",
      "  2%|2         | 10/500 [00:20<13:53,  1.70s/it]\n",
      "  2%|2         | 11/500 [00:20<10:10,  1.25s/it]\n",
      "  3%|2         | 13/500 [00:25<15:09,  1.87s/it]\n",
      "  3%|2         | 14/500 [00:25<11:52,  1.47s/it]\n",
      "  3%|3         | 16/500 [00:26<07:10,  1.12it/s]\n",
      "  3%|3         | 17/500 [00:31<14:45,  1.83s/it]\n",
      "  4%|3         | 18/500 [00:31<11:20,  1.41s/it]\n",
      "  4%|3         | 19/500 [00:31<09:04,  1.13s/it]\n",
      "  4%|4         | 20/500 [00:31<06:50,  1.17it/s]\n",
      "  4%|4         | 21/500 [00:35<14:22,  1.80s/it]\n",
      "  4%|4         | 22/500 [00:36<11:29,  1.44s/it]\n",
      "  5%|4         | 23/500 [00:36<09:08,  1.15s/it]\n",
      "  5%|5         | 25/500 [00:40<11:40,  1.47s/it]\n",
      "  5%|5         | 26/500 [00:41<11:19,  1.43s/it]\n",
      "  5%|5         | 27/500 [00:42<09:15,  1.17s/it]\n",
      "  6%|5         | 28/500 [00:42<07:40,  1.03it/s]\n",
      "  6%|5         | 29/500 [00:45<11:59,  1.53s/it]\n",
      "  6%|6         | 30/500 [00:47<11:28,  1.46s/it]\n",
      "  6%|6         | 31/500 [00:47<08:22,  1.07s/it]\n",
      "  6%|6         | 32/500 [00:47<07:39,  1.02it/s]\n",
      "  7%|6         | 33/500 [00:51<13:22,  1.72s/it]\n",
      "  7%|6         | 34/500 [00:52<10:53,  1.40s/it]\n",
      "  7%|7         | 35/500 [00:52<08:23,  1.08s/it]\n",
      "  7%|7         | 36/500 [00:53<07:52,  1.02s/it]\n",
      "  7%|7         | 37/500 [00:56<12:35,  1.63s/it]\n",
      "  8%|7         | 38/500 [00:56<09:33,  1.24s/it]\n",
      "  8%|7         | 39/500 [00:57<09:11,  1.20s/it]\n",
      "  8%|8         | 40/500 [00:58<07:39,  1.00it/s]\n",
      "  8%|8         | 41/500 [01:02<14:08,  1.85s/it]\n",
      "  8%|8         | 42/500 [01:02<10:37,  1.39s/it]\n",
      "  9%|8         | 43/500 [01:02<08:09,  1.07s/it]\n",
      "  9%|8         | 44/500 [01:03<07:40,  1.01s/it]\n",
      "  9%|9         | 45/500 [01:07<13:49,  1.82s/it]\n",
      "  9%|9         | 46/500 [01:08<11:37,  1.54s/it]\n",
      "  9%|9         | 47/500 [01:08<08:36,  1.14s/it]\n",
      " 10%|9         | 48/500 [01:08<06:30,  1.16it/s]\n",
      " 10%|9         | 49/500 [01:12<14:09,  1.88s/it]\n",
      " 10%|#         | 51/500 [01:13<09:28,  1.27s/it]\n",
      " 10%|#         | 52/500 [01:14<07:18,  1.02it/s]\n",
      " 11%|#         | 53/500 [01:17<12:39,  1.70s/it]\n",
      " 11%|#         | 54/500 [01:18<10:30,  1.41s/it]\n",
      " 11%|#1        | 55/500 [01:18<08:00,  1.08s/it]\n",
      " 11%|#1        | 56/500 [01:19<08:00,  1.08s/it]\n",
      " 11%|#1        | 57/500 [01:23<13:09,  1.78s/it]\n",
      " 12%|#1        | 58/500 [01:23<10:40,  1.45s/it]\n",
      " 12%|#1        | 59/500 [01:24<07:44,  1.05s/it]\n",
      " 12%|#2        | 60/500 [01:24<06:22,  1.15it/s]\n",
      " 12%|#2        | 61/500 [01:28<13:04,  1.79s/it]\n",
      " 12%|#2        | 62/500 [01:29<10:34,  1.45s/it]\n",
      " 13%|#2        | 63/500 [01:29<08:49,  1.21s/it]\n",
      " 13%|#2        | 64/500 [01:30<07:21,  1.01s/it]\n",
      " 13%|#3        | 65/500 [01:32<11:05,  1.53s/it]\n",
      " 13%|#3        | 66/500 [01:34<10:20,  1.43s/it]\n",
      " 13%|#3        | 67/500 [01:35<09:35,  1.33s/it]\n",
      " 14%|#3        | 68/500 [01:35<08:06,  1.13s/it]\n",
      " 14%|#3        | 69/500 [01:38<11:18,  1.57s/it]\n",
      " 14%|#4        | 70/500 [01:39<09:31,  1.33s/it]\n",
      " 14%|#4        | 71/500 [01:40<08:30,  1.19s/it]\n",
      " 14%|#4        | 72/500 [01:41<08:03,  1.13s/it]\n",
      " 15%|#4        | 73/500 [01:43<10:46,  1.51s/it]\n",
      " 15%|#4        | 74/500 [01:44<09:52,  1.39s/it]\n",
      " 15%|#5        | 75/500 [01:45<08:58,  1.27s/it]\n",
      " 15%|#5        | 76/500 [01:45<06:57,  1.01it/s]\n",
      " 15%|#5        | 77/500 [01:48<10:10,  1.44s/it]\n",
      " 16%|#5        | 78/500 [01:49<09:52,  1.40s/it]\n",
      " 16%|#5        | 79/500 [01:51<09:25,  1.34s/it]\n",
      " 16%|#6        | 80/500 [01:51<07:43,  1.10s/it]\n",
      " 16%|#6        | 81/500 [01:53<09:30,  1.36s/it]\n",
      " 16%|#6        | 82/500 [01:54<09:36,  1.38s/it]\n",
      " 17%|#6        | 83/500 [01:56<09:26,  1.36s/it]\n",
      " 17%|#6        | 84/500 [01:56<07:02,  1.02s/it]\n",
      " 17%|#7        | 85/500 [01:58<09:26,  1.37s/it]\n",
      " 17%|#7        | 86/500 [02:00<10:13,  1.48s/it]\n",
      " 17%|#7        | 87/500 [02:01<10:18,  1.50s/it]\n",
      " 18%|#7        | 89/500 [02:03<08:27,  1.23s/it]\n",
      " 18%|#8        | 90/500 [02:05<09:29,  1.39s/it]\n",
      " 18%|#8        | 91/500 [02:06<08:20,  1.22s/it]\n",
      " 18%|#8        | 92/500 [02:07<08:03,  1.19s/it]\n",
      " 19%|#8        | 93/500 [02:09<08:41,  1.28s/it]\n",
      " 19%|#8        | 94/500 [02:10<09:47,  1.45s/it]\n",
      " 19%|#9        | 95/500 [02:11<08:00,  1.19s/it]\n",
      " 19%|#9        | 96/500 [02:12<08:39,  1.29s/it]\n",
      " 19%|#9        | 97/500 [02:14<09:33,  1.42s/it]\n",
      " 20%|#9        | 98/500 [02:16<10:09,  1.52s/it]\n",
      " 20%|#9        | 99/500 [02:16<08:12,  1.23s/it]\n",
      " 20%|##        | 100/500 [02:18<08:20,  1.25s/it]\n",
      " 20%|##        | 101/500 [02:20<09:31,  1.43s/it]\n",
      " 20%|##        | 102/500 [02:21<09:54,  1.49s/it]\n",
      " 21%|##        | 103/500 [02:22<08:26,  1.27s/it]\n",
      " 21%|##        | 104/500 [02:22<06:45,  1.02s/it]\n",
      " 21%|##1       | 105/500 [02:25<10:21,  1.57s/it]\n",
      " 21%|##1       | 106/500 [02:27<09:35,  1.46s/it]\n",
      " 21%|##1       | 107/500 [02:27<06:54,  1.06s/it]\n",
      " 22%|##1       | 108/500 [02:28<07:49,  1.20s/it]\n",
      " 22%|##1       | 109/500 [02:30<09:19,  1.43s/it]\n",
      " 22%|##2       | 110/500 [02:32<09:42,  1.49s/it]\n",
      " 22%|##2       | 111/500 [02:32<07:49,  1.21s/it]\n",
      " 22%|##2       | 112/500 [02:33<07:21,  1.14s/it]\n",
      " 23%|##2       | 113/500 [02:36<10:14,  1.59s/it]\n",
      " 23%|##2       | 114/500 [02:37<08:36,  1.34s/it]\n",
      " 23%|##3       | 115/500 [02:37<06:51,  1.07s/it]\n",
      " 23%|##3       | 116/500 [02:39<08:22,  1.31s/it]\n",
      " 23%|##3       | 117/500 [02:41<09:50,  1.54s/it]\n",
      " 24%|##3       | 118/500 [02:42<08:06,  1.27s/it]\n",
      " 24%|##3       | 119/500 [02:42<06:04,  1.05it/s]\n",
      " 24%|##4       | 120/500 [02:45<09:12,  1.45s/it]\n",
      " 24%|##4       | 121/500 [02:45<07:15,  1.15s/it]\n",
      " 24%|##4       | 122/500 [02:47<09:23,  1.49s/it]\n",
      " 25%|##4       | 123/500 [02:48<06:57,  1.11s/it]\n",
      " 25%|##4       | 124/500 [02:49<08:32,  1.36s/it]\n",
      " 25%|##5       | 125/500 [02:51<07:59,  1.28s/it]\n",
      " 25%|##5       | 126/500 [02:53<09:52,  1.58s/it]\n",
      " 25%|##5       | 127/500 [02:53<07:17,  1.17s/it]\n",
      " 26%|##5       | 128/500 [02:55<08:57,  1.44s/it]\n",
      " 26%|##5       | 129/500 [02:55<06:39,  1.08s/it]\n",
      " 26%|##6       | 130/500 [02:57<08:04,  1.31s/it]\n",
      " 26%|##6       | 131/500 [02:59<08:14,  1.34s/it]\n",
      " 26%|##6       | 132/500 [03:01<09:45,  1.59s/it]\n",
      " 27%|##6       | 134/500 [03:03<08:08,  1.33s/it]\n",
      " 27%|##7       | 135/500 [03:04<07:34,  1.25s/it]\n",
      " 27%|##7       | 136/500 [03:06<09:32,  1.57s/it]\n",
      " 27%|##7       | 137/500 [03:07<07:38,  1.26s/it]\n",
      " 28%|##7       | 138/500 [03:08<07:53,  1.31s/it]\n",
      " 28%|##7       | 139/500 [03:09<06:10,  1.03s/it]\n",
      " 28%|##8       | 140/500 [03:11<08:31,  1.42s/it]\n",
      " 28%|##8       | 141/500 [03:12<08:29,  1.42s/it]\n",
      " 28%|##8       | 142/500 [03:13<07:18,  1.23s/it]\n",
      " 29%|##8       | 143/500 [03:14<06:28,  1.09s/it]\n",
      " 29%|##8       | 144/500 [03:16<07:58,  1.35s/it]\n",
      " 29%|##9       | 145/500 [03:18<08:50,  1.49s/it]\n",
      " 29%|##9       | 146/500 [03:19<08:05,  1.37s/it]\n",
      " 29%|##9       | 147/500 [03:19<06:48,  1.16s/it]\n",
      " 30%|##9       | 148/500 [03:21<07:03,  1.20s/it]\n",
      " 30%|##9       | 149/500 [03:23<09:20,  1.60s/it]\n",
      " 30%|###       | 150/500 [03:25<08:49,  1.51s/it]\n",
      " 30%|###       | 151/500 [03:25<06:32,  1.12s/it]\n",
      " 30%|###       | 152/500 [03:26<07:01,  1.21s/it]\n",
      " 31%|###       | 153/500 [03:27<07:10,  1.24s/it]\n",
      " 31%|###       | 154/500 [03:29<07:27,  1.29s/it]\n",
      " 31%|###1      | 155/500 [03:30<07:04,  1.23s/it]\n",
      " 31%|###1      | 156/500 [03:31<06:59,  1.22s/it]\n",
      " 31%|###1      | 157/500 [03:32<06:22,  1.12s/it]\n",
      " 32%|###1      | 158/500 [03:33<06:52,  1.21s/it]\n",
      " 32%|###1      | 159/500 [03:35<07:25,  1.31s/it]\n",
      " 32%|###2      | 160/500 [03:36<06:50,  1.21s/it]\n",
      " 32%|###2      | 161/500 [03:37<07:10,  1.27s/it]\n",
      " 32%|###2      | 162/500 [03:39<07:02,  1.25s/it]\n",
      " 33%|###2      | 163/500 [03:39<06:11,  1.10s/it]\n",
      " 33%|###2      | 164/500 [03:41<07:04,  1.26s/it]\n",
      " 33%|###3      | 165/500 [03:43<08:25,  1.51s/it]\n",
      " 33%|###3      | 166/500 [03:44<06:46,  1.22s/it]\n",
      " 33%|###3      | 167/500 [03:44<06:10,  1.11s/it]\n",
      " 34%|###3      | 168/500 [03:46<07:34,  1.37s/it]\n",
      " 34%|###3      | 169/500 [03:48<08:32,  1.55s/it]\n",
      " 34%|###4      | 170/500 [03:49<06:40,  1.21s/it]\n",
      " 34%|###4      | 171/500 [03:49<05:32,  1.01s/it]\n",
      " 34%|###4      | 172/500 [03:51<06:32,  1.20s/it]\n",
      " 35%|###4      | 173/500 [03:54<09:21,  1.72s/it]\n",
      " 35%|###4      | 174/500 [03:55<07:56,  1.46s/it]\n",
      " 35%|###5      | 175/500 [03:55<05:42,  1.06s/it]\n",
      " 35%|###5      | 176/500 [03:56<06:27,  1.20s/it]\n",
      " 35%|###5      | 177/500 [04:00<09:37,  1.79s/it]\n",
      " 36%|###5      | 178/500 [04:00<07:56,  1.48s/it]\n",
      " 36%|###5      | 179/500 [04:01<06:24,  1.20s/it]\n",
      " 36%|###6      | 180/500 [04:01<05:21,  1.00s/it]\n",
      " 36%|###6      | 181/500 [04:05<09:28,  1.78s/it]\n",
      " 36%|###6      | 182/500 [04:05<07:07,  1.35s/it]\n",
      " 37%|###6      | 183/500 [04:06<06:10,  1.17s/it]\n",
      " 37%|###6      | 184/500 [04:07<05:10,  1.02it/s]\n",
      " 37%|###7      | 185/500 [04:11<09:50,  1.87s/it]\n",
      " 37%|###7      | 186/500 [04:11<07:12,  1.38s/it]\n",
      " 37%|###7      | 187/500 [04:11<05:32,  1.06s/it]\n",
      " 38%|###7      | 188/500 [04:13<06:04,  1.17s/it]\n",
      " 38%|###7      | 189/500 [04:16<09:32,  1.84s/it]\n",
      " 38%|###8      | 190/500 [04:16<07:09,  1.39s/it]\n",
      " 38%|###8      | 191/500 [04:16<05:09,  1.00s/it]\n",
      " 38%|###8      | 192/500 [04:18<06:06,  1.19s/it]\n",
      " 39%|###8      | 193/500 [04:22<09:38,  1.88s/it]\n",
      " 39%|###8      | 194/500 [04:22<07:43,  1.51s/it]\n",
      " 39%|###9      | 195/500 [04:22<05:33,  1.09s/it]\n",
      " 39%|###9      | 196/500 [04:23<04:41,  1.08it/s]\n",
      " 39%|###9      | 197/500 [04:27<09:15,  1.83s/it]\n",
      " 40%|###9      | 198/500 [04:27<06:56,  1.38s/it]\n",
      " 40%|####      | 200/500 [04:28<05:13,  1.04s/it]\n",
      " 40%|####      | 201/500 [04:32<07:57,  1.60s/it]\n",
      " 40%|####      | 202/500 [04:32<06:00,  1.21s/it]\n",
      " 41%|####      | 203/500 [04:32<05:13,  1.06s/it]\n",
      " 41%|####      | 204/500 [04:34<05:50,  1.18s/it]\n",
      " 41%|####1     | 205/500 [04:37<08:08,  1.66s/it]\n",
      " 41%|####1     | 206/500 [04:37<06:32,  1.33s/it]\n",
      " 41%|####1     | 207/500 [04:38<05:13,  1.07s/it]\n",
      " 42%|####1     | 208/500 [04:39<04:55,  1.01s/it]\n",
      " 42%|####1     | 209/500 [04:42<07:32,  1.56s/it]\n",
      " 42%|####2     | 210/500 [04:42<06:22,  1.32s/it]\n",
      " 42%|####2     | 211/500 [04:44<06:20,  1.32s/it]\n",
      " 42%|####2     | 212/500 [04:44<05:12,  1.09s/it]\n",
      " 43%|####2     | 213/500 [04:46<06:18,  1.32s/it]\n",
      " 43%|####2     | 214/500 [04:47<05:48,  1.22s/it]\n",
      " 43%|####3     | 215/500 [04:49<06:50,  1.44s/it]\n",
      " 43%|####3     | 216/500 [04:50<05:41,  1.20s/it]\n",
      " 43%|####3     | 217/500 [04:51<06:34,  1.40s/it]\n",
      " 44%|####3     | 218/500 [04:53<06:36,  1.40s/it]\n",
      " 44%|####3     | 219/500 [04:54<05:31,  1.18s/it]\n",
      " 44%|####4     | 220/500 [04:55<05:22,  1.15s/it]\n",
      " 44%|####4     | 221/500 [04:57<06:57,  1.50s/it]\n",
      " 44%|####4     | 222/500 [04:58<06:04,  1.31s/it]\n",
      " 45%|####4     | 223/500 [04:59<05:44,  1.24s/it]\n",
      " 45%|####4     | 224/500 [05:00<05:30,  1.20s/it]\n",
      " 45%|####5     | 225/500 [05:03<07:45,  1.69s/it]\n",
      " 45%|####5     | 226/500 [05:04<06:36,  1.45s/it]\n",
      " 45%|####5     | 227/500 [05:04<05:20,  1.17s/it]\n",
      " 46%|####5     | 228/500 [05:05<05:21,  1.18s/it]\n",
      " 46%|####5     | 229/500 [05:08<07:43,  1.71s/it]\n",
      " 46%|####6     | 230/500 [05:09<06:06,  1.36s/it]\n",
      " 46%|####6     | 231/500 [05:10<05:44,  1.28s/it]\n",
      " 46%|####6     | 232/500 [05:11<05:44,  1.29s/it]\n",
      " 47%|####6     | 233/500 [05:14<07:03,  1.59s/it]\n",
      " 47%|####6     | 234/500 [05:14<05:47,  1.31s/it]\n",
      " 47%|####6     | 235/500 [05:15<05:37,  1.27s/it]\n",
      " 47%|####7     | 236/500 [05:16<05:03,  1.15s/it]\n",
      " 47%|####7     | 237/500 [05:19<07:08,  1.63s/it]\n",
      " 48%|####7     | 238/500 [05:20<05:40,  1.30s/it]\n",
      " 48%|####7     | 239/500 [05:21<05:57,  1.37s/it]\n",
      " 48%|####8     | 240/500 [05:22<05:16,  1.22s/it]\n",
      " 48%|####8     | 241/500 [05:25<07:12,  1.67s/it]\n",
      " 48%|####8     | 242/500 [05:25<05:18,  1.23s/it]\n",
      " 49%|####8     | 243/500 [05:26<05:30,  1.29s/it]\n",
      " 49%|####8     | 244/500 [05:28<05:30,  1.29s/it]\n",
      " 49%|####9     | 245/500 [05:30<06:46,  1.59s/it]\n",
      " 49%|####9     | 246/500 [05:30<05:24,  1.28s/it]\n",
      " 49%|####9     | 247/500 [05:31<04:02,  1.04it/s]\n",
      " 50%|####9     | 248/500 [05:33<06:07,  1.46s/it]\n",
      " 50%|####9     | 249/500 [05:35<06:28,  1.55s/it]\n",
      " 50%|#####     | 250/500 [05:35<04:47,  1.15s/it]\n",
      " 50%|#####     | 251/500 [05:36<04:08,  1.00it/s]\n",
      " 50%|#####     | 252/500 [05:38<05:10,  1.25s/it]\n",
      " 51%|#####     | 253/500 [05:40<06:02,  1.47s/it]\n",
      " 51%|#####     | 254/500 [05:41<05:25,  1.32s/it]\n",
      " 51%|#####1    | 255/500 [05:42<04:50,  1.19s/it]\n",
      " 51%|#####1    | 256/500 [05:42<04:26,  1.09s/it]\n",
      " 51%|#####1    | 257/500 [05:45<06:25,  1.59s/it]\n",
      " 52%|#####1    | 258/500 [05:46<05:16,  1.31s/it]\n",
      " 52%|#####1    | 259/500 [05:47<04:43,  1.17s/it]\n",
      " 52%|#####2    | 260/500 [05:48<04:35,  1.15s/it]\n",
      " 52%|#####2    | 261/500 [05:51<06:29,  1.63s/it]\n",
      " 53%|#####2    | 263/500 [05:52<05:02,  1.28s/it]\n",
      " 53%|#####2    | 264/500 [05:53<04:24,  1.12s/it]\n",
      " 53%|#####3    | 265/500 [05:55<05:49,  1.49s/it]\n",
      " 53%|#####3    | 266/500 [05:56<05:01,  1.29s/it]\n",
      " 53%|#####3    | 267/500 [05:58<05:44,  1.48s/it]\n",
      " 54%|#####3    | 268/500 [05:59<04:26,  1.15s/it]\n",
      " 54%|#####3    | 269/500 [06:01<05:57,  1.55s/it]\n",
      " 54%|#####4    | 270/500 [06:01<04:18,  1.13s/it]\n",
      " 54%|#####4    | 271/500 [06:03<05:21,  1.40s/it]\n",
      " 54%|#####4    | 272/500 [06:04<04:36,  1.21s/it]\n",
      " 55%|#####4    | 273/500 [06:07<06:10,  1.63s/it]\n",
      " 55%|#####4    | 274/500 [06:07<04:41,  1.24s/it]\n",
      " 55%|#####5    | 275/500 [06:09<05:20,  1.43s/it]\n",
      " 55%|#####5    | 276/500 [06:09<03:58,  1.06s/it]\n",
      " 55%|#####5    | 277/500 [06:12<05:40,  1.53s/it]\n",
      " 56%|#####5    | 278/500 [06:12<04:47,  1.30s/it]\n",
      " 56%|#####5    | 279/500 [06:14<05:01,  1.37s/it]\n",
      " 56%|#####6    | 280/500 [06:15<04:20,  1.19s/it]\n",
      " 56%|#####6    | 281/500 [06:17<05:53,  1.62s/it]\n",
      " 56%|#####6    | 282/500 [06:18<04:27,  1.23s/it]\n",
      " 57%|#####6    | 283/500 [06:19<04:39,  1.29s/it]\n",
      " 57%|#####6    | 284/500 [06:20<04:03,  1.13s/it]\n",
      " 57%|#####6    | 285/500 [06:23<06:08,  1.71s/it]\n",
      " 57%|#####7    | 286/500 [06:23<04:30,  1.26s/it]\n",
      " 57%|#####7    | 287/500 [06:24<04:10,  1.18s/it]\n",
      " 58%|#####7    | 288/500 [06:25<04:10,  1.18s/it]\n",
      " 58%|#####7    | 289/500 [06:28<05:47,  1.65s/it]\n",
      " 58%|#####8    | 290/500 [06:29<04:36,  1.31s/it]\n",
      " 58%|#####8    | 291/500 [06:30<04:27,  1.28s/it]\n",
      " 58%|#####8    | 292/500 [06:31<04:14,  1.22s/it]\n",
      " 59%|#####8    | 293/500 [06:34<05:47,  1.68s/it]\n",
      " 59%|#####8    | 294/500 [06:34<04:15,  1.24s/it]\n",
      " 59%|#####8    | 295/500 [06:35<04:44,  1.39s/it]\n",
      " 59%|#####9    | 296/500 [06:36<03:58,  1.17s/it]\n",
      " 59%|#####9    | 297/500 [06:39<05:25,  1.61s/it]\n",
      " 60%|#####9    | 298/500 [06:39<04:06,  1.22s/it]\n",
      " 60%|#####9    | 299/500 [06:41<04:30,  1.35s/it]\n",
      " 60%|######    | 300/500 [06:41<03:41,  1.11s/it]\n",
      " 60%|######    | 301/500 [06:44<04:57,  1.49s/it]\n",
      " 60%|######    | 302/500 [06:45<04:31,  1.37s/it]\n",
      " 61%|######    | 303/500 [06:46<04:40,  1.42s/it]\n",
      " 61%|######    | 304/500 [06:47<03:27,  1.06s/it]\n",
      " 61%|######1   | 305/500 [06:49<04:57,  1.53s/it]\n",
      " 61%|######1   | 306/500 [06:50<04:18,  1.33s/it]\n",
      " 61%|######1   | 307/500 [06:51<03:43,  1.16s/it]\n",
      " 62%|######1   | 308/500 [06:51<03:01,  1.06it/s]\n",
      " 62%|######1   | 309/500 [06:55<05:20,  1.68s/it]\n",
      " 62%|######2   | 310/500 [06:55<03:49,  1.21s/it]\n",
      " 62%|######2   | 311/500 [06:56<03:47,  1.20s/it]\n",
      " 62%|######2   | 312/500 [06:56<03:02,  1.03it/s]\n",
      " 63%|######2   | 313/500 [07:00<05:28,  1.76s/it]\n",
      " 63%|######2   | 314/500 [07:00<03:55,  1.26s/it]\n",
      " 63%|######3   | 315/500 [07:01<03:56,  1.28s/it]\n",
      " 63%|######3   | 316/500 [07:02<03:08,  1.03s/it]\n",
      " 63%|######3   | 317/500 [07:05<05:17,  1.74s/it]\n",
      " 64%|######3   | 318/500 [07:05<03:53,  1.28s/it]\n",
      " 64%|######3   | 319/500 [07:06<03:05,  1.03s/it]\n",
      " 64%|######4   | 320/500 [07:07<03:31,  1.18s/it]\n",
      " 64%|######4   | 321/500 [07:11<05:17,  1.77s/it]\n",
      " 65%|######4   | 323/500 [07:11<03:02,  1.03s/it]\n",
      " 65%|######4   | 324/500 [07:13<03:51,  1.31s/it]\n",
      " 65%|######5   | 325/500 [07:15<04:19,  1.48s/it]\n",
      " 65%|######5   | 326/500 [07:16<03:33,  1.23s/it]\n",
      " 65%|######5   | 327/500 [07:17<03:36,  1.25s/it]\n",
      " 66%|######5   | 328/500 [07:19<04:04,  1.42s/it]\n",
      " 66%|######5   | 329/500 [07:20<04:13,  1.49s/it]\n",
      " 66%|######6   | 330/500 [07:21<03:20,  1.18s/it]\n",
      " 66%|######6   | 331/500 [07:23<03:53,  1.38s/it]\n",
      " 66%|######6   | 332/500 [07:24<03:31,  1.26s/it]\n",
      " 67%|######6   | 333/500 [07:26<04:11,  1.51s/it]\n",
      " 67%|######6   | 334/500 [07:26<03:33,  1.28s/it]\n",
      " 67%|######7   | 335/500 [07:27<03:06,  1.13s/it]\n",
      " 67%|######7   | 336/500 [07:29<03:19,  1.22s/it]\n",
      " 67%|######7   | 337/500 [07:30<03:16,  1.21s/it]\n",
      " 68%|######7   | 338/500 [07:31<03:20,  1.24s/it]\n",
      " 68%|######7   | 339/500 [07:33<03:49,  1.42s/it]\n",
      " 68%|######8   | 340/500 [07:33<02:50,  1.06s/it]\n",
      " 68%|######8   | 341/500 [07:35<03:32,  1.33s/it]\n",
      " 68%|######8   | 342/500 [07:37<03:40,  1.40s/it]\n",
      " 69%|######8   | 343/500 [07:38<03:29,  1.34s/it]\n",
      " 69%|######8   | 344/500 [07:39<03:17,  1.26s/it]\n",
      " 69%|######9   | 345/500 [07:41<03:28,  1.34s/it]\n",
      " 69%|######9   | 346/500 [07:42<03:41,  1.44s/it]\n",
      " 69%|######9   | 347/500 [07:43<03:03,  1.20s/it]\n",
      " 70%|######9   | 348/500 [07:43<02:32,  1.00s/it]\n",
      " 70%|######9   | 349/500 [07:45<02:55,  1.16s/it]\n",
      " 70%|#######   | 350/500 [07:47<03:50,  1.53s/it]\n",
      " 70%|#######   | 351/500 [07:48<03:28,  1.40s/it]\n",
      " 70%|#######   | 352/500 [07:49<02:39,  1.08s/it]\n",
      " 71%|#######   | 353/500 [07:50<03:03,  1.25s/it]\n",
      " 71%|#######   | 354/500 [07:53<03:48,  1.56s/it]\n",
      " 71%|#######1  | 355/500 [07:53<03:06,  1.29s/it]\n",
      " 71%|#######1  | 356/500 [07:55<03:01,  1.26s/it]\n",
      " 71%|#######1  | 357/500 [07:56<03:07,  1.31s/it]\n",
      " 72%|#######1  | 358/500 [07:58<03:38,  1.54s/it]\n",
      " 72%|#######1  | 359/500 [07:59<03:08,  1.34s/it]\n",
      " 72%|#######2  | 360/500 [08:00<02:47,  1.20s/it]\n",
      " 72%|#######2  | 361/500 [08:01<02:51,  1.23s/it]\n",
      " 72%|#######2  | 362/500 [08:03<03:38,  1.58s/it]\n",
      " 73%|#######2  | 363/500 [08:04<02:40,  1.17s/it]\n",
      " 73%|#######2  | 364/500 [08:05<02:45,  1.21s/it]\n",
      " 73%|#######3  | 365/500 [08:05<02:07,  1.06it/s]\n",
      " 73%|#######3  | 366/500 [08:09<04:02,  1.81s/it]\n",
      " 73%|#######3  | 367/500 [08:09<03:01,  1.36s/it]\n",
      " 74%|#######3  | 368/500 [08:10<02:36,  1.18s/it]\n",
      " 74%|#######3  | 369/500 [08:10<01:56,  1.12it/s]\n",
      " 74%|#######4  | 370/500 [08:14<03:25,  1.58s/it]\n",
      " 74%|#######4  | 371/500 [08:15<03:13,  1.50s/it]\n",
      " 74%|#######4  | 372/500 [08:15<02:22,  1.12s/it]\n",
      " 75%|#######4  | 373/500 [08:16<02:20,  1.11s/it]\n",
      " 75%|#######4  | 374/500 [08:19<03:24,  1.63s/it]\n",
      " 75%|#######5  | 375/500 [08:20<02:58,  1.43s/it]\n",
      " 75%|#######5  | 376/500 [08:20<02:08,  1.03s/it]\n",
      " 75%|#######5  | 377/500 [08:21<02:17,  1.11s/it]\n",
      " 76%|#######5  | 378/500 [08:25<03:31,  1.73s/it]\n",
      " 76%|#######5  | 379/500 [08:25<02:42,  1.34s/it]\n",
      " 76%|#######6  | 380/500 [08:26<02:16,  1.14s/it]\n",
      " 76%|#######6  | 381/500 [08:26<01:38,  1.21it/s]\n",
      " 76%|#######6  | 382/500 [08:30<03:51,  1.96s/it]\n",
      " 77%|#######6  | 383/500 [08:31<02:48,  1.44s/it]\n",
      " 77%|#######6  | 384/500 [08:31<02:08,  1.11s/it]\n",
      " 77%|#######7  | 385/500 [08:31<01:36,  1.19it/s]\n",
      " 77%|#######7  | 386/500 [08:35<03:32,  1.87s/it]\n",
      " 77%|#######7  | 387/500 [08:37<03:04,  1.63s/it]\n",
      " 78%|#######7  | 388/500 [08:37<02:11,  1.17s/it]\n",
      " 78%|#######8  | 390/500 [08:41<02:49,  1.54s/it]\n",
      " 78%|#######8  | 391/500 [08:41<02:24,  1.32s/it]\n",
      " 78%|#######8  | 392/500 [08:42<02:00,  1.12s/it]\n",
      " 79%|#######8  | 393/500 [08:43<01:52,  1.05s/it]\n",
      " 79%|#######8  | 394/500 [08:46<02:54,  1.65s/it]\n",
      " 79%|#######9  | 395/500 [08:47<02:32,  1.46s/it]\n",
      " 79%|#######9  | 396/500 [08:48<02:07,  1.22s/it]\n",
      " 79%|#######9  | 397/500 [08:48<01:55,  1.12s/it]\n",
      " 80%|#######9  | 398/500 [08:52<02:59,  1.76s/it]\n",
      " 80%|########  | 400/500 [08:53<02:05,  1.25s/it]\n",
      " 80%|########  | 401/500 [08:54<01:59,  1.21s/it]\n",
      " 80%|########  | 402/500 [08:57<02:37,  1.61s/it]\n",
      " 81%|########  | 403/500 [08:57<02:08,  1.32s/it]\n",
      " 81%|########  | 404/500 [08:59<02:09,  1.35s/it]\n",
      " 81%|########1 | 405/500 [09:00<01:52,  1.18s/it]\n",
      " 81%|########1 | 406/500 [09:02<02:24,  1.54s/it]\n",
      " 81%|########1 | 407/500 [09:02<01:52,  1.21s/it]\n",
      " 82%|########1 | 408/500 [09:05<02:18,  1.50s/it]\n",
      " 82%|########1 | 409/500 [09:05<01:59,  1.31s/it]\n",
      " 82%|########2 | 410/500 [09:07<02:18,  1.54s/it]\n",
      " 82%|########2 | 411/500 [09:08<01:44,  1.18s/it]\n",
      " 82%|########2 | 412/500 [09:10<02:07,  1.44s/it]\n",
      " 83%|########2 | 413/500 [09:10<01:33,  1.08s/it]\n",
      " 83%|########2 | 414/500 [09:12<02:01,  1.41s/it]\n",
      " 83%|########2 | 415/500 [09:14<01:57,  1.38s/it]\n",
      " 83%|########3 | 416/500 [09:16<02:10,  1.55s/it]\n",
      " 83%|########3 | 417/500 [09:16<01:33,  1.12s/it]\n",
      " 84%|########3 | 418/500 [09:18<01:49,  1.34s/it]\n",
      " 84%|########3 | 419/500 [09:19<01:50,  1.36s/it]\n",
      " 84%|########4 | 420/500 [09:20<01:50,  1.38s/it]\n",
      " 84%|########4 | 421/500 [09:21<01:42,  1.29s/it]\n",
      " 84%|########4 | 422/500 [09:24<01:58,  1.53s/it]\n",
      " 85%|########4 | 423/500 [09:25<01:49,  1.43s/it]\n",
      " 85%|########4 | 424/500 [09:26<01:38,  1.29s/it]\n",
      " 85%|########5 | 425/500 [09:26<01:20,  1.07s/it]\n",
      " 85%|########5 | 426/500 [09:29<01:51,  1.50s/it]\n",
      " 85%|########5 | 427/500 [09:30<01:47,  1.48s/it]\n",
      " 86%|########5 | 428/500 [09:31<01:28,  1.23s/it]\n",
      " 86%|########5 | 429/500 [09:31<01:10,  1.01it/s]\n",
      " 86%|########6 | 430/500 [09:33<01:32,  1.32s/it]\n",
      " 86%|########6 | 431/500 [09:36<01:58,  1.72s/it]\n",
      " 86%|########6 | 432/500 [09:37<01:41,  1.49s/it]\n",
      " 87%|########6 | 434/500 [09:39<01:28,  1.33s/it]\n",
      " 87%|########7 | 435/500 [09:41<01:40,  1.55s/it]\n",
      " 87%|########7 | 436/500 [09:42<01:25,  1.34s/it]\n",
      " 87%|########7 | 437/500 [09:43<01:16,  1.21s/it]\n",
      " 88%|########7 | 438/500 [09:45<01:20,  1.30s/it]\n",
      " 88%|########7 | 439/500 [09:47<01:42,  1.68s/it]\n",
      " 88%|########8 | 440/500 [09:47<01:13,  1.22s/it]\n",
      " 88%|########8 | 441/500 [09:49<01:15,  1.28s/it]\n",
      " 88%|########8 | 442/500 [09:50<01:07,  1.16s/it]\n",
      " 89%|########8 | 443/500 [09:53<01:38,  1.73s/it]\n",
      " 89%|########8 | 444/500 [09:53<01:11,  1.28s/it]\n",
      " 89%|########9 | 445/500 [09:54<01:14,  1.35s/it]\n",
      " 89%|########9 | 446/500 [09:55<00:56,  1.05s/it]\n",
      " 89%|########9 | 447/500 [09:58<01:27,  1.65s/it]\n",
      " 90%|########9 | 448/500 [09:59<01:11,  1.38s/it]\n",
      " 90%|########9 | 449/500 [10:00<01:04,  1.26s/it]\n",
      " 90%|######### | 450/500 [10:00<00:53,  1.08s/it]\n",
      " 90%|######### | 451/500 [10:02<01:08,  1.41s/it]\n",
      " 90%|######### | 452/500 [10:04<01:09,  1.45s/it]\n",
      " 91%|######### | 453/500 [10:05<01:07,  1.44s/it]\n",
      " 91%|######### | 454/500 [10:06<00:53,  1.17s/it]\n",
      " 91%|#########1| 455/500 [10:08<01:04,  1.44s/it]\n",
      " 91%|#########1| 456/500 [10:09<00:57,  1.30s/it]\n",
      " 91%|#########1| 457/500 [10:11<01:00,  1.40s/it]\n",
      " 92%|#########1| 458/500 [10:11<00:52,  1.24s/it]\n",
      " 92%|#########1| 459/500 [10:13<00:50,  1.23s/it]\n",
      " 92%|#########2| 460/500 [10:14<00:55,  1.38s/it]\n",
      " 92%|#########2| 461/500 [10:16<00:53,  1.36s/it]\n",
      " 92%|#########2| 462/500 [10:16<00:42,  1.11s/it]\n",
      " 93%|#########2| 463/500 [10:19<00:54,  1.47s/it]\n",
      " 93%|#########2| 464/500 [10:20<00:52,  1.45s/it]\n",
      " 93%|#########3| 465/500 [10:21<00:45,  1.31s/it]\n",
      " 93%|#########3| 466/500 [10:21<00:33,  1.02it/s]\n",
      " 93%|#########3| 467/500 [10:23<00:40,  1.21s/it]\n",
      " 94%|#########3| 468/500 [10:26<00:53,  1.67s/it]\n",
      " 94%|#########3| 469/500 [10:26<00:40,  1.30s/it]\n",
      " 94%|#########3| 470/500 [10:27<00:34,  1.14s/it]\n",
      " 94%|#########4| 471/500 [10:28<00:30,  1.06s/it]\n",
      " 94%|#########4| 472/500 [10:30<00:38,  1.39s/it]\n",
      " 95%|#########4| 473/500 [10:31<00:37,  1.40s/it]\n",
      " 95%|#########4| 474/500 [10:32<00:33,  1.27s/it]\n",
      " 95%|#########5| 475/500 [10:33<00:31,  1.25s/it]\n",
      " 95%|#########5| 476/500 [10:36<00:35,  1.49s/it]\n",
      " 95%|#########5| 477/500 [10:37<00:32,  1.40s/it]\n",
      " 96%|#########5| 478/500 [10:38<00:30,  1.41s/it]\n",
      " 96%|#########5| 479/500 [10:39<00:25,  1.21s/it]\n",
      " 96%|#########6| 480/500 [10:41<00:29,  1.47s/it]\n",
      " 96%|#########6| 481/500 [10:42<00:25,  1.32s/it]\n",
      " 96%|#########6| 482/500 [10:43<00:20,  1.15s/it]\n",
      " 97%|#########6| 483/500 [10:44<00:21,  1.26s/it]\n",
      " 97%|#########6| 484/500 [10:46<00:20,  1.31s/it]\n",
      " 97%|#########7| 485/500 [10:47<00:21,  1.44s/it]\n",
      " 97%|#########7| 486/500 [10:48<00:16,  1.17s/it]\n",
      " 97%|#########7| 487/500 [10:50<00:17,  1.32s/it]\n",
      " 98%|#########7| 488/500 [10:51<00:17,  1.48s/it]\n",
      " 98%|#########7| 489/500 [10:53<00:16,  1.53s/it]\n",
      " 98%|#########8| 490/500 [10:54<00:12,  1.20s/it]\n",
      " 98%|#########8| 491/500 [10:55<00:11,  1.33s/it]\n",
      " 98%|#########8| 492/500 [10:57<00:11,  1.46s/it]\n",
      " 99%|#########8| 493/500 [10:58<00:09,  1.32s/it]\n",
      " 99%|#########8| 494/500 [10:59<00:07,  1.21s/it]\n",
      " 99%|#########9| 495/500 [11:01<00:07,  1.40s/it]\n",
      " 99%|#########9| 496/500 [11:03<00:06,  1.54s/it]\n",
      " 99%|#########9| 497/500 [11:04<00:04,  1.37s/it]\n",
      "100%|#########9| 498/500 [11:04<00:02,  1.16s/it]\n",
      "100%|#########9| 499/500 [11:06<00:01,  1.47s/it]\n",
      "100%|##########| 500/500 [11:08<00:00,  1.39s/it]\n",
      "100%|##########| 500/500 [11:08<00:00,  1.34s/it]\n",
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "  0%|          | 1/500 [00:13<1:50:21, 13.27s/it]\n",
      "  0%|          | 2/500 [00:13<46:24,  5.59s/it]  \n",
      "  1%|          | 4/500 [00:13<17:31,  2.12s/it]\n",
      "  1%|1         | 6/500 [00:13<09:25,  1.14s/it]\n",
      "  2%|1         | 9/500 [00:21<15:58,  1.95s/it]\n",
      "  2%|2         | 10/500 [00:22<13:12,  1.62s/it]\n",
      "  2%|2         | 11/500 [00:22<10:31,  1.29s/it]\n",
      "  2%|2         | 12/500 [00:22<08:13,  1.01s/it]\n",
      "  3%|2         | 13/500 [00:22<06:21,  1.28it/s]\n",
      "  3%|2         | 14/500 [00:22<04:54,  1.65it/s]\n",
      "  3%|3         | 16/500 [00:22<02:58,  2.72it/s]\n",
      "  4%|3         | 18/500 [00:30<13:58,  1.74s/it]\n",
      "  4%|3         | 19/500 [00:30<11:08,  1.39s/it]\n",
      "  4%|4         | 21/500 [00:30<07:13,  1.11it/s]\n",
      "  4%|4         | 22/500 [00:31<05:51,  1.36it/s]\n",
      "  5%|4         | 24/500 [00:31<03:47,  2.09it/s]\n",
      "  5%|5         | 26/500 [00:39<13:36,  1.72s/it]\n",
      "  6%|5         | 28/500 [00:39<09:18,  1.18s/it]\n",
      "  6%|5         | 29/500 [00:39<07:39,  1.02it/s]\n",
      "  6%|6         | 30/500 [00:39<06:10,  1.27it/s]\n",
      "  6%|6         | 32/500 [00:39<03:58,  1.96it/s]\n",
      "  7%|6         | 34/500 [00:47<13:26,  1.73s/it]\n",
      "  7%|7         | 36/500 [00:47<09:10,  1.19s/it]\n",
      "  7%|7         | 37/500 [00:48<07:32,  1.02it/s]\n",
      "  8%|7         | 38/500 [00:48<06:05,  1.26it/s]\n",
      "  8%|8         | 40/500 [00:48<03:54,  1.96it/s]\n",
      "  8%|8         | 42/500 [00:56<13:20,  1.75s/it]\n",
      "  9%|8         | 43/500 [00:56<10:48,  1.42s/it]\n",
      "  9%|9         | 45/500 [00:56<07:07,  1.06it/s]\n",
      "  9%|9         | 46/500 [00:56<05:48,  1.30it/s]\n",
      " 10%|9         | 48/500 [00:56<03:47,  1.99it/s]\n",
      " 10%|#         | 50/500 [01:04<12:55,  1.72s/it]\n",
      " 10%|#         | 51/500 [01:05<10:30,  1.40s/it]\n",
      " 11%|#         | 53/500 [01:05<06:48,  1.09it/s]\n",
      " 11%|#1        | 55/500 [01:05<04:45,  1.56it/s]\n",
      " 11%|#1        | 57/500 [01:12<12:08,  1.64s/it]\n",
      " 12%|#1        | 58/500 [01:13<10:52,  1.48s/it]\n",
      " 12%|#2        | 60/500 [01:13<07:09,  1.02it/s]\n",
      " 12%|#2        | 61/500 [01:13<05:52,  1.25it/s]\n",
      " 12%|#2        | 62/500 [01:13<04:43,  1.54it/s]\n",
      " 13%|#2        | 63/500 [01:13<03:46,  1.93it/s]\n",
      " 13%|#2        | 64/500 [01:14<03:00,  2.42it/s]\n",
      " 13%|#3        | 65/500 [01:20<15:18,  2.11s/it]\n",
      " 13%|#3        | 66/500 [01:21<12:59,  1.80s/it]\n",
      " 14%|#3        | 68/500 [01:21<07:33,  1.05s/it]\n",
      " 14%|#3        | 69/500 [01:22<05:55,  1.21it/s]\n",
      " 14%|#4        | 72/500 [01:22<03:09,  2.25it/s]\n",
      " 15%|#4        | 73/500 [01:28<11:43,  1.65s/it]\n",
      " 15%|#4        | 74/500 [01:30<11:00,  1.55s/it]\n",
      " 15%|#5        | 75/500 [01:30<08:34,  1.21s/it]\n",
      " 15%|#5        | 76/500 [01:30<06:46,  1.04it/s]\n",
      " 16%|#5        | 78/500 [01:30<04:04,  1.73it/s]\n",
      " 16%|#5        | 79/500 [01:30<03:18,  2.13it/s]\n",
      " 16%|#6        | 80/500 [01:30<02:40,  2.62it/s]\n",
      " 16%|#6        | 81/500 [01:36<12:40,  1.82s/it]\n",
      " 16%|#6        | 82/500 [01:38<12:54,  1.85s/it]\n",
      " 17%|#6        | 84/500 [01:38<07:43,  1.11s/it]\n",
      " 17%|#7        | 87/500 [01:39<04:04,  1.69it/s]\n",
      " 18%|#7        | 89/500 [01:44<08:53,  1.30s/it]\n",
      " 18%|#8        | 90/500 [01:47<10:05,  1.48s/it]\n",
      " 18%|#8        | 91/500 [01:47<08:07,  1.19s/it]\n",
      " 18%|#8        | 92/500 [01:47<06:35,  1.03it/s]\n",
      " 19%|#8        | 93/500 [01:47<05:08,  1.32it/s]\n",
      " 19%|#9        | 96/500 [01:47<02:36,  2.58it/s]\n",
      " 20%|#9        | 98/500 [01:55<10:21,  1.55s/it]\n",
      " 20%|#9        | 99/500 [01:55<08:30,  1.27s/it]\n",
      " 20%|##        | 101/500 [01:55<05:44,  1.16it/s]\n",
      " 20%|##        | 102/500 [01:55<04:43,  1.41it/s]\n",
      " 21%|##        | 103/500 [01:55<03:49,  1.73it/s]\n",
      " 21%|##1       | 105/500 [02:00<08:45,  1.33s/it]\n",
      " 21%|##1       | 106/500 [02:03<10:33,  1.61s/it]\n",
      " 21%|##1       | 107/500 [02:03<08:42,  1.33s/it]\n",
      " 22%|##1       | 109/500 [02:04<05:18,  1.23it/s]\n",
      " 22%|##2       | 110/500 [02:04<04:25,  1.47it/s]\n",
      " 22%|##2       | 112/500 [02:04<02:49,  2.29it/s]\n",
      " 23%|##2       | 114/500 [02:11<10:18,  1.60s/it]\n",
      " 23%|##3       | 115/500 [02:12<08:45,  1.37s/it]\n",
      " 23%|##3       | 117/500 [02:12<05:45,  1.11it/s]\n",
      " 24%|##3       | 118/500 [02:12<04:41,  1.35it/s]\n",
      " 24%|##4       | 120/500 [02:12<03:11,  1.99it/s]\n",
      " 24%|##4       | 121/500 [02:17<08:17,  1.31s/it]\n",
      " 24%|##4       | 122/500 [02:19<10:28,  1.66s/it]\n",
      " 25%|##4       | 123/500 [02:20<08:41,  1.38s/it]\n",
      " 25%|##4       | 124/500 [02:20<06:45,  1.08s/it]\n",
      " 25%|##5       | 126/500 [02:20<03:59,  1.56it/s]\n",
      " 25%|##5       | 127/500 [02:20<03:12,  1.94it/s]\n",
      " 26%|##5       | 129/500 [02:25<07:10,  1.16s/it]\n",
      " 26%|##6       | 130/500 [02:28<09:34,  1.55s/it]\n",
      " 26%|##6       | 131/500 [02:28<08:12,  1.34s/it]\n",
      " 27%|##6       | 133/500 [02:29<05:22,  1.14it/s]\n",
      " 27%|##7       | 135/500 [02:29<03:30,  1.74it/s]\n",
      " 27%|##7       | 137/500 [02:33<06:28,  1.07s/it]\n",
      " 28%|##7       | 138/500 [02:36<08:24,  1.39s/it]\n",
      " 28%|##7       | 139/500 [02:37<08:16,  1.37s/it]\n",
      " 28%|##8       | 141/500 [02:37<05:10,  1.16it/s]\n",
      " 28%|##8       | 142/500 [02:37<04:11,  1.43it/s]\n",
      " 29%|##8       | 143/500 [02:37<03:29,  1.70it/s]\n",
      " 29%|##8       | 144/500 [02:37<02:45,  2.14it/s]\n",
      " 29%|##9       | 145/500 [02:41<07:40,  1.30s/it]\n",
      " 29%|##9       | 146/500 [02:43<09:13,  1.56s/it]\n",
      " 29%|##9       | 147/500 [02:45<09:40,  1.64s/it]\n",
      " 30%|##9       | 148/500 [02:45<07:14,  1.23s/it]\n",
      " 30%|###       | 150/500 [02:45<04:06,  1.42it/s]\n",
      " 30%|###       | 151/500 [02:46<03:15,  1.79it/s]\n",
      " 31%|###       | 153/500 [02:49<06:25,  1.11s/it]\n",
      " 31%|###       | 154/500 [02:51<07:38,  1.33s/it]\n",
      " 31%|###1      | 155/500 [02:53<08:30,  1.48s/it]\n",
      " 31%|###1      | 156/500 [02:53<06:27,  1.13s/it]\n",
      " 31%|###1      | 157/500 [02:54<05:01,  1.14it/s]\n",
      " 32%|###1      | 158/500 [02:54<03:47,  1.50it/s]\n",
      " 32%|###1      | 159/500 [02:54<02:52,  1.97it/s]\n",
      " 32%|###2      | 160/500 [02:54<02:13,  2.55it/s]\n",
      " 32%|###2      | 161/500 [02:57<06:58,  1.23s/it]\n",
      " 32%|###2      | 162/500 [02:59<08:19,  1.48s/it]\n",
      " 33%|###2      | 163/500 [03:02<10:00,  1.78s/it]\n",
      " 33%|###3      | 165/500 [03:02<05:31,  1.01it/s]\n",
      " 34%|###3      | 168/500 [03:02<03:03,  1.81it/s]\n",
      " 34%|###3      | 169/500 [03:05<05:47,  1.05s/it]\n",
      " 34%|###4      | 170/500 [03:07<06:50,  1.24s/it]\n",
      " 34%|###4      | 171/500 [03:10<08:42,  1.59s/it]\n",
      " 34%|###4      | 172/500 [03:10<06:39,  1.22s/it]\n",
      " 35%|###4      | 173/500 [03:10<05:11,  1.05it/s]\n",
      " 35%|###4      | 174/500 [03:11<03:55,  1.39it/s]\n",
      " 35%|###5      | 176/500 [03:11<02:18,  2.33it/s]\n",
      " 35%|###5      | 177/500 [03:14<05:32,  1.03s/it]\n",
      " 36%|###5      | 178/500 [03:16<06:47,  1.27s/it]\n",
      " 36%|###5      | 179/500 [03:18<08:59,  1.68s/it]\n",
      " 36%|###6      | 181/500 [03:19<05:19,  1.00s/it]\n",
      " 36%|###6      | 182/500 [03:19<04:11,  1.27it/s]\n",
      " 37%|###6      | 183/500 [03:19<03:24,  1.55it/s]\n",
      " 37%|###6      | 184/500 [03:19<02:39,  1.99it/s]\n",
      " 37%|###7      | 185/500 [03:22<05:31,  1.05s/it]\n",
      " 37%|###7      | 186/500 [03:24<07:10,  1.37s/it]\n",
      " 37%|###7      | 187/500 [03:27<09:38,  1.85s/it]\n",
      " 38%|###7      | 189/500 [03:27<05:23,  1.04s/it]\n",
      " 38%|###8      | 190/500 [03:27<04:12,  1.23it/s]\n",
      " 38%|###8      | 191/500 [03:27<03:32,  1.45it/s]\n",
      " 38%|###8      | 192/500 [03:28<02:52,  1.78it/s]\n",
      " 39%|###8      | 193/500 [03:30<05:18,  1.04s/it]\n",
      " 39%|###8      | 194/500 [03:32<06:37,  1.30s/it]\n",
      " 39%|###9      | 195/500 [03:35<09:10,  1.80s/it]\n",
      " 39%|###9      | 196/500 [03:35<06:57,  1.37s/it]\n",
      " 39%|###9      | 197/500 [03:35<05:03,  1.00s/it]\n",
      " 40%|###9      | 199/500 [03:35<02:58,  1.69it/s]\n",
      " 40%|####      | 200/500 [03:36<02:29,  2.00it/s]\n",
      " 40%|####      | 201/500 [03:38<04:47,  1.04it/s]\n",
      " 40%|####      | 202/500 [03:40<05:58,  1.20s/it]\n",
      " 41%|####      | 203/500 [03:43<09:05,  1.84s/it]\n",
      " 41%|####      | 204/500 [03:43<06:37,  1.34s/it]\n",
      " 41%|####1     | 205/500 [03:43<04:51,  1.01it/s]\n",
      " 41%|####1     | 206/500 [03:44<03:34,  1.37it/s]\n",
      " 41%|####1     | 207/500 [03:44<02:49,  1.73it/s]\n",
      " 42%|####1     | 209/500 [03:46<03:55,  1.23it/s]\n",
      " 42%|####2     | 210/500 [03:48<05:17,  1.09s/it]\n",
      " 42%|####2     | 211/500 [03:52<08:24,  1.74s/it]\n",
      " 42%|####2     | 212/500 [03:52<06:31,  1.36s/it]\n",
      " 43%|####2     | 214/500 [03:52<03:46,  1.27it/s]\n",
      " 43%|####3     | 216/500 [03:52<02:30,  1.89it/s]\n",
      " 43%|####3     | 217/500 [03:54<03:49,  1.23it/s]\n",
      " 44%|####3     | 218/500 [03:56<05:27,  1.16s/it]\n",
      " 44%|####3     | 219/500 [04:00<08:25,  1.80s/it]\n",
      " 44%|####4     | 221/500 [04:00<04:58,  1.07s/it]\n",
      " 44%|####4     | 222/500 [04:00<04:09,  1.11it/s]\n",
      " 45%|####4     | 224/500 [04:01<02:41,  1.71it/s]\n",
      " 45%|####5     | 225/500 [04:02<03:37,  1.26it/s]\n",
      " 45%|####5     | 226/500 [04:05<05:15,  1.15s/it]\n",
      " 45%|####5     | 227/500 [04:08<08:19,  1.83s/it]\n",
      " 46%|####5     | 228/500 [04:09<06:28,  1.43s/it]\n",
      " 46%|####6     | 230/500 [04:09<03:44,  1.20it/s]\n",
      " 46%|####6     | 231/500 [04:09<02:57,  1.51it/s]\n",
      " 46%|####6     | 232/500 [04:09<02:19,  1.92it/s]\n",
      " 47%|####6     | 233/500 [04:10<03:06,  1.43it/s]\n",
      " 47%|####6     | 234/500 [04:13<05:26,  1.23s/it]\n",
      " 47%|####6     | 235/500 [04:17<08:37,  1.95s/it]\n",
      " 47%|####7     | 236/500 [04:17<06:15,  1.42s/it]\n",
      " 47%|####7     | 237/500 [04:17<04:50,  1.10s/it]\n",
      " 48%|####7     | 238/500 [04:17<03:32,  1.23it/s]\n",
      " 48%|####7     | 239/500 [04:17<02:37,  1.65it/s]\n",
      " 48%|####8     | 240/500 [04:17<01:58,  2.19it/s]\n",
      " 48%|####8     | 241/500 [04:18<02:21,  1.83it/s]\n",
      " 48%|####8     | 242/500 [04:21<05:16,  1.23s/it]\n",
      " 49%|####8     | 243/500 [04:25<08:40,  2.03s/it]\n",
      " 49%|####8     | 244/500 [04:25<06:20,  1.48s/it]\n",
      " 49%|####9     | 245/500 [04:25<04:41,  1.10s/it]\n",
      " 49%|####9     | 247/500 [04:26<02:49,  1.49it/s]\n",
      " 50%|####9     | 248/500 [04:26<02:13,  1.88it/s]\n",
      " 50%|####9     | 249/500 [04:26<02:07,  1.97it/s]\n",
      " 50%|#####     | 250/500 [04:29<04:44,  1.14s/it]\n",
      " 50%|#####     | 251/500 [04:33<08:11,  1.97s/it]\n",
      " 51%|#####     | 253/500 [04:34<04:54,  1.19s/it]\n",
      " 51%|#####1    | 255/500 [04:34<03:14,  1.26it/s]\n",
      " 51%|#####1    | 256/500 [04:34<02:42,  1.50it/s]\n",
      " 51%|#####1    | 257/500 [04:34<02:16,  1.78it/s]\n",
      " 52%|#####1    | 258/500 [04:37<04:34,  1.13s/it]\n",
      " 52%|#####1    | 259/500 [04:42<08:05,  2.01s/it]\n",
      " 52%|#####2    | 261/500 [04:42<04:50,  1.22s/it]\n",
      " 53%|#####2    | 263/500 [04:42<03:08,  1.26it/s]\n",
      " 53%|#####2    | 264/500 [04:42<02:43,  1.44it/s]\n",
      " 53%|#####3    | 265/500 [04:43<02:16,  1.72it/s]\n",
      " 53%|#####3    | 266/500 [04:45<04:21,  1.12s/it]\n",
      " 53%|#####3    | 267/500 [04:50<07:38,  1.97s/it]\n",
      " 54%|#####3    | 269/500 [04:50<04:35,  1.19s/it]\n",
      " 54%|#####4    | 270/500 [04:50<03:35,  1.07it/s]\n",
      " 54%|#####4    | 271/500 [04:50<02:53,  1.32it/s]\n",
      " 54%|#####4    | 272/500 [04:51<02:26,  1.56it/s]\n",
      " 55%|#####4    | 273/500 [04:51<01:52,  2.01it/s]\n",
      " 55%|#####4    | 274/500 [04:53<03:52,  1.03s/it]\n",
      " 55%|#####5    | 275/500 [04:58<07:38,  2.04s/it]\n",
      " 55%|#####5    | 276/500 [04:58<05:45,  1.54s/it]\n",
      " 55%|#####5    | 277/500 [04:58<04:17,  1.15s/it]\n",
      " 56%|#####5    | 278/500 [04:58<03:14,  1.14it/s]\n",
      " 56%|#####5    | 279/500 [04:59<02:23,  1.54it/s]\n",
      " 56%|#####6    | 280/500 [04:59<02:08,  1.71it/s]\n",
      " 56%|#####6    | 282/500 [05:01<02:58,  1.22it/s]\n",
      " 57%|#####6    | 283/500 [05:06<06:23,  1.77s/it]\n",
      " 57%|#####6    | 284/500 [05:06<04:54,  1.36s/it]\n",
      " 57%|#####6    | 285/500 [05:06<03:46,  1.05s/it]\n",
      " 57%|#####7    | 286/500 [05:07<03:01,  1.18it/s]\n",
      " 57%|#####7    | 287/500 [05:07<02:15,  1.57it/s]\n",
      " 58%|#####7    | 288/500 [05:07<01:49,  1.94it/s]\n",
      " 58%|#####7    | 289/500 [05:07<01:30,  2.34it/s]\n",
      " 58%|#####8    | 290/500 [05:09<03:11,  1.10it/s]\n",
      " 58%|#####8    | 291/500 [05:14<07:10,  2.06s/it]\n",
      " 58%|#####8    | 292/500 [05:14<05:20,  1.54s/it]\n",
      " 59%|#####8    | 293/500 [05:15<03:57,  1.15s/it]\n",
      " 59%|#####8    | 294/500 [05:15<02:59,  1.15it/s]\n",
      " 59%|#####8    | 295/500 [05:15<02:11,  1.56it/s]\n",
      " 59%|#####9    | 296/500 [05:15<01:51,  1.83it/s]\n",
      " 59%|#####9    | 297/500 [05:15<01:24,  2.40it/s]\n",
      " 60%|#####9    | 298/500 [05:17<02:56,  1.14it/s]\n",
      " 60%|#####9    | 299/500 [05:22<06:44,  2.01s/it]\n",
      " 60%|######    | 300/500 [05:23<05:27,  1.64s/it]\n",
      " 60%|######    | 301/500 [05:23<03:54,  1.18s/it]\n",
      " 60%|######    | 302/500 [05:23<02:49,  1.17it/s]\n",
      " 61%|######    | 303/500 [05:23<02:11,  1.50it/s]\n",
      " 61%|######    | 304/500 [05:23<01:37,  2.01it/s]\n",
      " 61%|######1   | 305/500 [05:23<01:14,  2.62it/s]\n",
      " 61%|######1   | 306/500 [05:25<02:39,  1.22it/s]\n",
      " 61%|######1   | 307/500 [05:30<06:21,  1.98s/it]\n",
      " 62%|######1   | 308/500 [05:31<05:40,  1.78s/it]\n",
      " 62%|######1   | 309/500 [05:31<04:03,  1.28s/it]\n",
      " 62%|######2   | 310/500 [05:31<02:55,  1.08it/s]\n",
      " 62%|######2   | 311/500 [05:32<02:08,  1.47it/s]\n",
      " 62%|######2   | 312/500 [05:32<01:41,  1.85it/s]\n",
      " 63%|######2   | 313/500 [05:32<01:16,  2.43it/s]\n",
      " 63%|######2   | 314/500 [05:33<02:11,  1.41it/s]\n",
      " 63%|######3   | 315/500 [05:37<05:27,  1.77s/it]\n",
      " 63%|######3   | 316/500 [05:39<05:23,  1.76s/it]\n",
      " 63%|######3   | 317/500 [05:40<04:02,  1.33s/it]\n",
      " 64%|######3   | 318/500 [05:40<02:54,  1.04it/s]\n",
      " 64%|######4   | 320/500 [05:40<01:46,  1.69it/s]\n",
      " 64%|######4   | 322/500 [05:41<01:45,  1.69it/s]\n",
      " 65%|######4   | 323/500 [05:45<04:07,  1.40s/it]\n",
      " 65%|######4   | 324/500 [05:47<04:29,  1.53s/it]\n",
      " 65%|######5   | 325/500 [05:48<03:44,  1.28s/it]\n",
      " 65%|######5   | 326/500 [05:48<02:48,  1.03it/s]\n",
      " 65%|######5   | 327/500 [05:48<02:06,  1.37it/s]\n",
      " 66%|######5   | 329/500 [05:48<01:13,  2.31it/s]\n",
      " 66%|######6   | 330/500 [05:49<01:35,  1.78it/s]\n",
      " 66%|######6   | 331/500 [05:53<04:08,  1.47s/it]\n",
      " 66%|######6   | 332/500 [05:55<04:33,  1.63s/it]\n",
      " 67%|######6   | 333/500 [05:56<03:52,  1.39s/it]\n",
      " 67%|######6   | 334/500 [05:56<02:50,  1.03s/it]\n",
      " 67%|######7   | 336/500 [05:56<01:41,  1.62it/s]\n",
      " 67%|######7   | 337/500 [05:57<01:20,  2.03it/s]\n",
      " 68%|######7   | 338/500 [05:57<01:17,  2.09it/s]\n",
      " 68%|######7   | 339/500 [06:01<04:04,  1.52s/it]\n",
      " 68%|######8   | 340/500 [06:03<04:21,  1.64s/it]\n",
      " 68%|######8   | 341/500 [06:04<03:55,  1.48s/it]\n",
      " 68%|######8   | 342/500 [06:05<02:51,  1.09s/it]\n",
      " 69%|######8   | 343/500 [06:05<02:10,  1.20it/s]\n",
      " 69%|######9   | 346/500 [06:05<01:09,  2.22it/s]\n",
      " 69%|######9   | 347/500 [06:09<03:06,  1.22s/it]\n",
      " 70%|######9   | 348/500 [06:11<03:27,  1.37s/it]\n",
      " 70%|######9   | 349/500 [06:13<03:32,  1.41s/it]\n",
      " 70%|#######   | 351/500 [06:13<02:09,  1.15it/s]\n",
      " 70%|#######   | 352/500 [06:13<01:42,  1.44it/s]\n",
      " 71%|#######   | 353/500 [06:13<01:20,  1.82it/s]\n",
      " 71%|#######   | 354/500 [06:13<01:07,  2.16it/s]\n",
      " 71%|#######1  | 355/500 [06:17<03:22,  1.40s/it]\n",
      " 71%|#######1  | 356/500 [06:19<03:38,  1.52s/it]\n",
      " 71%|#######1  | 357/500 [06:21<03:55,  1.64s/it]\n",
      " 72%|#######1  | 359/500 [06:21<02:11,  1.07it/s]\n",
      " 72%|#######2  | 360/500 [06:21<01:46,  1.32it/s]\n",
      " 72%|#######2  | 361/500 [06:22<01:25,  1.62it/s]\n",
      " 72%|#######2  | 362/500 [06:22<01:06,  2.08it/s]\n",
      " 73%|#######2  | 363/500 [06:25<03:00,  1.31s/it]\n",
      " 73%|#######2  | 364/500 [06:27<03:19,  1.47s/it]\n",
      " 73%|#######3  | 365/500 [06:29<03:53,  1.73s/it]\n",
      " 73%|#######3  | 366/500 [06:30<02:52,  1.29s/it]\n",
      " 74%|#######3  | 369/500 [06:30<01:23,  1.58it/s]\n",
      " 74%|#######4  | 370/500 [06:30<01:08,  1.91it/s]\n",
      " 74%|#######4  | 371/500 [06:34<02:35,  1.21s/it]\n",
      " 74%|#######4  | 372/500 [06:35<02:47,  1.31s/it]\n",
      " 75%|#######4  | 373/500 [06:38<03:25,  1.62s/it]\n",
      " 75%|#######4  | 374/500 [06:38<02:36,  1.24s/it]\n",
      " 75%|#######5  | 375/500 [06:38<01:55,  1.08it/s]\n",
      " 75%|#######5  | 377/500 [06:38<01:09,  1.78it/s]\n",
      " 76%|#######5  | 378/500 [06:38<00:55,  2.21it/s]\n",
      " 76%|#######5  | 379/500 [06:42<02:22,  1.17s/it]\n",
      " 76%|#######6  | 380/500 [06:43<02:35,  1.29s/it]\n",
      " 76%|#######6  | 381/500 [06:46<03:13,  1.63s/it]\n",
      " 76%|#######6  | 382/500 [06:46<02:32,  1.29s/it]\n",
      " 77%|#######7  | 385/500 [06:46<01:11,  1.61it/s]\n",
      " 77%|#######7  | 386/500 [06:46<00:58,  1.94it/s]\n",
      " 77%|#######7  | 387/500 [06:50<02:06,  1.12s/it]\n",
      " 78%|#######7  | 388/500 [06:51<02:16,  1.22s/it]\n",
      " 78%|#######7  | 389/500 [06:54<03:07,  1.69s/it]\n",
      " 78%|#######8  | 390/500 [06:54<02:22,  1.29s/it]\n",
      " 78%|#######8  | 391/500 [06:55<01:44,  1.04it/s]\n",
      " 79%|#######8  | 393/500 [06:55<01:02,  1.71it/s]\n",
      " 79%|#######8  | 394/500 [06:55<00:49,  2.13it/s]\n",
      " 79%|#######9  | 395/500 [06:58<01:49,  1.04s/it]\n",
      " 79%|#######9  | 396/500 [06:59<02:08,  1.23s/it]\n",
      " 79%|#######9  | 397/500 [07:02<03:01,  1.76s/it]\n",
      " 80%|#######9  | 398/500 [07:03<02:21,  1.39s/it]\n",
      " 80%|########  | 400/500 [07:03<01:19,  1.26it/s]\n",
      " 80%|########  | 401/500 [07:03<01:01,  1.60it/s]\n",
      " 80%|########  | 402/500 [07:03<00:48,  2.03it/s]\n",
      " 81%|########  | 403/500 [07:06<01:37,  1.00s/it]\n",
      " 81%|########  | 404/500 [07:07<01:55,  1.21s/it]\n",
      " 81%|########1 | 405/500 [07:11<02:58,  1.88s/it]\n",
      " 81%|########1 | 406/500 [07:11<02:11,  1.40s/it]\n",
      " 81%|########1 | 407/500 [07:11<01:38,  1.06s/it]\n",
      " 82%|########1 | 408/500 [07:12<01:14,  1.24it/s]\n",
      " 82%|########2 | 411/500 [07:14<01:06,  1.35it/s]\n",
      " 82%|########2 | 412/500 [07:16<01:25,  1.02it/s]\n",
      " 83%|########2 | 413/500 [07:19<02:17,  1.58s/it]\n",
      " 83%|########2 | 414/500 [07:19<01:48,  1.27s/it]\n",
      " 83%|########2 | 415/500 [07:20<01:24,  1.01it/s]\n",
      " 83%|########3 | 416/500 [07:20<01:05,  1.28it/s]\n",
      " 83%|########3 | 417/500 [07:20<00:49,  1.69it/s]\n",
      " 84%|########3 | 418/500 [07:20<00:37,  2.20it/s]\n",
      " 84%|########3 | 419/500 [07:22<01:04,  1.26it/s]\n",
      " 84%|########4 | 420/500 [07:24<01:31,  1.14s/it]\n",
      " 84%|########4 | 421/500 [07:27<02:26,  1.86s/it]\n",
      " 84%|########4 | 422/500 [07:28<01:54,  1.47s/it]\n",
      " 85%|########4 | 423/500 [07:28<01:21,  1.06s/it]\n",
      " 85%|########4 | 424/500 [07:28<00:59,  1.28it/s]\n",
      " 85%|########5 | 425/500 [07:28<00:45,  1.63it/s]\n",
      " 85%|########5 | 426/500 [07:28<00:34,  2.17it/s]\n",
      " 85%|########5 | 427/500 [07:29<00:47,  1.54it/s]\n",
      " 86%|########5 | 428/500 [07:32<01:21,  1.14s/it]\n",
      " 86%|########5 | 429/500 [07:36<02:17,  1.94s/it]\n",
      " 86%|########6 | 430/500 [07:36<01:46,  1.52s/it]\n",
      " 86%|########6 | 431/500 [07:36<01:15,  1.10s/it]\n",
      " 86%|########6 | 432/500 [07:36<00:54,  1.25it/s]\n",
      " 87%|########6 | 433/500 [07:36<00:39,  1.69it/s]\n",
      " 87%|########6 | 434/500 [07:37<00:29,  2.23it/s]\n",
      " 87%|########7 | 435/500 [07:37<00:39,  1.65it/s]\n",
      " 87%|########7 | 436/500 [07:40<01:13,  1.14s/it]\n",
      " 87%|########7 | 437/500 [07:44<02:02,  1.94s/it]\n",
      " 88%|########7 | 438/500 [07:44<01:38,  1.59s/it]\n",
      " 88%|########7 | 439/500 [07:45<01:09,  1.14s/it]\n",
      " 88%|########8 | 440/500 [07:45<00:49,  1.20it/s]\n",
      " 88%|########8 | 441/500 [07:45<00:36,  1.62it/s]\n",
      " 89%|########8 | 443/500 [07:45<00:27,  2.07it/s]\n",
      " 89%|########8 | 444/500 [07:48<00:56,  1.01s/it]\n",
      " 89%|########9 | 445/500 [07:52<01:32,  1.68s/it]\n",
      " 89%|########9 | 446/500 [07:53<01:22,  1.52s/it]\n",
      " 90%|########9 | 448/500 [07:53<00:49,  1.05it/s]\n",
      " 90%|######### | 450/500 [07:53<00:30,  1.65it/s]\n",
      " 90%|######### | 451/500 [07:53<00:25,  1.92it/s]\n",
      " 90%|######### | 452/500 [07:56<00:49,  1.04s/it]\n",
      " 91%|######### | 453/500 [07:59<01:13,  1.57s/it]\n",
      " 91%|######### | 454/500 [08:01<01:11,  1.56s/it]\n",
      " 91%|#########1| 455/500 [08:01<00:53,  1.19s/it]\n",
      " 91%|#########1| 456/500 [08:01<00:38,  1.13it/s]\n",
      " 91%|#########1| 457/500 [08:01<00:29,  1.44it/s]\n",
      " 92%|#########1| 458/500 [08:01<00:21,  1.91it/s]\n",
      " 92%|#########1| 459/500 [08:02<00:16,  2.49it/s]\n",
      " 92%|#########2| 460/500 [08:04<00:39,  1.01it/s]\n",
      " 92%|#########2| 461/500 [08:07<01:06,  1.69s/it]\n",
      " 92%|#########2| 462/500 [08:09<01:03,  1.68s/it]\n",
      " 93%|#########2| 463/500 [08:09<00:44,  1.21s/it]\n",
      " 93%|#########2| 464/500 [08:09<00:32,  1.10it/s]\n",
      " 93%|#########3| 465/500 [08:09<00:23,  1.49it/s]\n",
      " 93%|#########3| 466/500 [08:10<00:19,  1.76it/s]\n",
      " 94%|#########3| 468/500 [08:12<00:26,  1.20it/s]\n",
      " 94%|#########3| 469/500 [08:15<00:43,  1.41s/it]\n",
      " 94%|#########3| 470/500 [08:17<00:46,  1.55s/it]\n",
      " 94%|#########4| 471/500 [08:17<00:33,  1.16s/it]\n",
      " 94%|#########4| 472/500 [08:17<00:25,  1.12it/s]\n",
      " 95%|#########4| 473/500 [08:18<00:18,  1.43it/s]\n",
      " 95%|#########4| 474/500 [08:18<00:14,  1.78it/s]\n",
      " 95%|#########5| 476/500 [08:20<00:18,  1.29it/s]\n",
      " 95%|#########5| 477/500 [08:23<00:31,  1.38s/it]\n",
      " 96%|#########5| 478/500 [08:25<00:34,  1.58s/it]\n",
      " 96%|#########5| 479/500 [08:25<00:24,  1.19s/it]\n",
      " 96%|#########6| 480/500 [08:26<00:18,  1.09it/s]\n",
      " 96%|#########6| 481/500 [08:26<00:13,  1.46it/s]\n",
      " 96%|#########6| 482/500 [08:26<00:10,  1.72it/s]\n",
      " 97%|#########6| 484/500 [08:28<00:11,  1.36it/s]\n",
      " 97%|#########7| 485/500 [08:31<00:20,  1.35s/it]\n",
      " 97%|#########7| 486/500 [08:34<00:22,  1.62s/it]\n",
      " 97%|#########7| 487/500 [08:34<00:16,  1.24s/it]\n",
      " 98%|#########7| 488/500 [08:34<00:11,  1.04it/s]\n",
      " 98%|#########8| 490/500 [08:34<00:06,  1.65it/s]\n",
      " 98%|#########8| 492/500 [08:36<00:05,  1.51it/s]\n",
      " 99%|#########8| 493/500 [08:39<00:08,  1.25s/it]\n",
      " 99%|#########8| 494/500 [08:42<00:09,  1.55s/it]\n",
      " 99%|#########9| 495/500 [08:42<00:05,  1.18s/it]\n",
      " 99%|#########9| 496/500 [08:42<00:03,  1.11it/s]\n",
      " 99%|#########9| 497/500 [08:42<00:02,  1.47it/s]\n",
      "100%|#########9| 498/500 [08:42<00:01,  1.82it/s]\n",
      "100%|##########| 500/500 [08:43<00:00,  2.00it/s]\n",
      "100%|##########| 500/500 [08:43<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset integrity\n",
    "!nnUNetv2_plan_and_preprocess -d 500 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a6aeec-5b7c-4763-8f39-30a0f07a4641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-12-07 12:02:42.624446: do_dummy_2d_data_aug: False\n",
      "2025-12-07 12:02:42.743926: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-07 12:02:42.745929: The split file contains 5 splits.\n",
      "2025-12-07 12:02:42.745929: Desired fold for training: 0\n",
      "2025-12-07 12:02:42.746932: This split has 400 training and 100 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2025-12-07 12:03:16.604067: Using torch.compile...\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_lowres\n",
      " {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [128, 128, 128], 'median_image_size_in_voxels': [202, 202, 202], 'spacing': [1.2667700813876164, 1.2667700813876164, 1.2667700813876164], 'normalization_schemes': ['ZScoreNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset500_MRI', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [256, 256, 256], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.422696590423584, 'median': 0.4194243550300598, 'min': 0.0027002037968486547, 'percentile_00_5': 0.05628390982747078, 'percentile_99_5': 0.8565635681152344, 'std': 0.19347868859767914}}} \n",
      "\n",
      "2025-12-07 12:03:16.607102: unpacking dataset...\n",
      "2025-12-07 12:03:17.183792: unpacking done...\n",
      "2025-12-07 12:03:17.186795: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-12-07 12:03:17.238692: \n",
      "2025-12-07 12:03:17.238692: Epoch 0\n",
      "2025-12-07 12:03:17.238692: Current learning rate: 0.01\n",
      "2025-12-07 12:05:53.139965: train_loss 0.2196\n",
      "2025-12-07 12:05:53.142323: val_loss 0.0403\n",
      "2025-12-07 12:05:53.142323: Pseudo dice [0.4587, 0.5406, 0.1938]\n",
      "2025-12-07 12:05:53.144326: Epoch time: 155.9 s\n",
      "2025-12-07 12:05:53.144326: Yayy! New best EMA pseudo Dice: 0.3977\n",
      "2025-12-07 12:05:54.202162: \n",
      "2025-12-07 12:05:54.202162: Epoch 1\n",
      "2025-12-07 12:05:54.203249: Current learning rate: 0.00999\n",
      "2025-12-07 12:08:12.715917: train_loss -0.0151\n",
      "2025-12-07 12:08:12.715917: val_loss -0.0894\n",
      "2025-12-07 12:08:12.720767: Pseudo dice [0.5204, 0.5247, 0.4031]\n",
      "2025-12-07 12:08:12.720767: Epoch time: 138.51 s\n",
      "2025-12-07 12:08:12.722769: Yayy! New best EMA pseudo Dice: 0.4062\n",
      "2025-12-07 12:08:13.717835: \n",
      "2025-12-07 12:08:13.717835: Epoch 2\n",
      "2025-12-07 12:08:13.717835: Current learning rate: 0.00998\n",
      "2025-12-07 12:10:32.201926: train_loss -0.1272\n",
      "2025-12-07 12:10:32.201926: val_loss -0.2044\n",
      "2025-12-07 12:10:32.201926: Pseudo dice [0.5887, 0.6235, 0.4818]\n",
      "2025-12-07 12:10:32.201926: Epoch time: 138.48 s\n",
      "2025-12-07 12:10:32.201926: Yayy! New best EMA pseudo Dice: 0.422\n",
      "2025-12-07 12:10:33.311255: \n",
      "2025-12-07 12:10:33.311255: Epoch 3\n",
      "2025-12-07 12:10:33.311255: Current learning rate: 0.00997\n",
      "2025-12-07 12:12:51.732815: train_loss -0.1829\n",
      "2025-12-07 12:12:51.732815: val_loss -0.2453\n",
      "2025-12-07 12:12:51.734817: Pseudo dice [0.614, 0.6514, 0.5158]\n",
      "2025-12-07 12:12:51.734817: Epoch time: 138.42 s\n",
      "2025-12-07 12:12:51.736820: Yayy! New best EMA pseudo Dice: 0.4392\n",
      "2025-12-07 12:12:52.677979: \n",
      "2025-12-07 12:12:52.677979: Epoch 4\n",
      "2025-12-07 12:12:52.679981: Current learning rate: 0.00996\n",
      "2025-12-07 12:15:11.186821: train_loss -0.2242\n",
      "2025-12-07 12:15:11.186821: val_loss -0.2876\n",
      "2025-12-07 12:15:11.186821: Pseudo dice [0.6251, 0.6688, 0.5547]\n",
      "2025-12-07 12:15:11.186821: Epoch time: 138.51 s\n",
      "2025-12-07 12:15:11.186821: Yayy! New best EMA pseudo Dice: 0.4569\n",
      "2025-12-07 12:15:12.097795: \n",
      "2025-12-07 12:15:12.109192: Epoch 5\n",
      "2025-12-07 12:15:12.110253: Current learning rate: 0.00995\n",
      "2025-12-07 12:17:30.693334: train_loss -0.2805\n",
      "2025-12-07 12:17:30.694335: val_loss -0.3475\n",
      "2025-12-07 12:17:30.696335: Pseudo dice [0.6791, 0.7091, 0.6116]\n",
      "2025-12-07 12:17:30.697336: Epoch time: 138.6 s\n",
      "2025-12-07 12:17:30.698634: Yayy! New best EMA pseudo Dice: 0.4779\n",
      "2025-12-07 12:17:31.592087: \n",
      "2025-12-07 12:17:31.592087: Epoch 6\n",
      "2025-12-07 12:17:31.592087: Current learning rate: 0.00995\n",
      "2025-12-07 12:19:49.941831: train_loss -0.3535\n",
      "2025-12-07 12:19:49.941831: val_loss -0.4248\n",
      "2025-12-07 12:19:49.945835: Pseudo dice [0.7176, 0.7663, 0.6502]\n",
      "2025-12-07 12:19:49.945835: Epoch time: 138.35 s\n",
      "2025-12-07 12:19:49.947837: Yayy! New best EMA pseudo Dice: 0.5012\n",
      "2025-12-07 12:19:51.041833: \n",
      "2025-12-07 12:19:51.041833: Epoch 7\n",
      "2025-12-07 12:19:51.041833: Current learning rate: 0.00994\n",
      "2025-12-07 12:22:09.351999: train_loss -0.4049\n",
      "2025-12-07 12:22:09.351999: val_loss -0.461\n",
      "2025-12-07 12:22:09.354000: Pseudo dice [0.7487, 0.7912, 0.6669]\n",
      "2025-12-07 12:22:09.356002: Epoch time: 138.31 s\n",
      "2025-12-07 12:22:09.356002: Yayy! New best EMA pseudo Dice: 0.5247\n",
      "2025-12-07 12:22:10.270842: \n",
      "2025-12-07 12:22:10.270842: Epoch 8\n",
      "2025-12-07 12:22:10.271836: Current learning rate: 0.00993\n",
      "2025-12-07 12:24:28.592975: train_loss -0.4353\n",
      "2025-12-07 12:24:28.592975: val_loss -0.4571\n",
      "2025-12-07 12:24:28.594978: Pseudo dice [0.7581, 0.8073, 0.6306]\n",
      "2025-12-07 12:24:28.594978: Epoch time: 138.32 s\n",
      "2025-12-07 12:24:28.596981: Yayy! New best EMA pseudo Dice: 0.5454\n",
      "2025-12-07 12:24:29.511336: \n",
      "2025-12-07 12:24:29.511336: Epoch 9\n",
      "2025-12-07 12:24:29.511336: Current learning rate: 0.00992\n",
      "2025-12-07 12:26:47.756474: train_loss -0.4425\n",
      "2025-12-07 12:26:47.756474: val_loss -0.475\n",
      "2025-12-07 12:26:47.758476: Pseudo dice [0.7401, 0.8154, 0.6627]\n",
      "2025-12-07 12:26:47.758476: Epoch time: 138.25 s\n",
      "2025-12-07 12:26:47.760478: Yayy! New best EMA pseudo Dice: 0.5648\n",
      "2025-12-07 12:26:48.769609: \n",
      "2025-12-07 12:26:48.769609: Epoch 10\n",
      "2025-12-07 12:26:48.769609: Current learning rate: 0.00991\n",
      "2025-12-07 12:29:06.840700: train_loss -0.4845\n",
      "2025-12-07 12:29:06.842703: val_loss -0.5245\n",
      "2025-12-07 12:29:06.843822: Pseudo dice [0.7929, 0.8335, 0.6995]\n",
      "2025-12-07 12:29:06.844824: Epoch time: 138.07 s\n",
      "2025-12-07 12:29:06.844824: Yayy! New best EMA pseudo Dice: 0.5859\n",
      "2025-12-07 12:29:07.730428: \n",
      "2025-12-07 12:29:07.732430: Epoch 11\n",
      "2025-12-07 12:29:07.733517: Current learning rate: 0.0099\n",
      "2025-12-07 12:31:25.607714: train_loss -0.5058\n",
      "2025-12-07 12:31:25.607714: val_loss -0.5183\n",
      "2025-12-07 12:31:25.607714: Pseudo dice [0.7421, 0.8271, 0.7428]\n",
      "2025-12-07 12:31:25.615353: Epoch time: 137.88 s\n",
      "2025-12-07 12:31:25.615353: Yayy! New best EMA pseudo Dice: 0.6043\n",
      "2025-12-07 12:31:26.515118: \n",
      "2025-12-07 12:31:26.515118: Epoch 12\n",
      "2025-12-07 12:31:26.515118: Current learning rate: 0.00989\n",
      "2025-12-07 12:33:44.547827: train_loss -0.503\n",
      "2025-12-07 12:33:44.549829: val_loss -0.5551\n",
      "2025-12-07 12:33:44.549829: Pseudo dice [0.7826, 0.8474, 0.7459]\n",
      "2025-12-07 12:33:44.551833: Epoch time: 138.05 s\n",
      "2025-12-07 12:33:44.553840: Yayy! New best EMA pseudo Dice: 0.6231\n",
      "2025-12-07 12:33:45.812248: \n",
      "2025-12-07 12:33:45.812248: Epoch 13\n",
      "2025-12-07 12:33:45.812248: Current learning rate: 0.00988\n",
      "2025-12-07 12:36:03.736776: train_loss -0.5433\n",
      "2025-12-07 12:36:03.738779: val_loss -0.5758\n",
      "2025-12-07 12:36:03.738779: Pseudo dice [0.7995, 0.8558, 0.7635]\n",
      "2025-12-07 12:36:03.740782: Epoch time: 137.92 s\n",
      "2025-12-07 12:36:03.740782: Yayy! New best EMA pseudo Dice: 0.6414\n",
      "2025-12-07 12:36:04.655371: \n",
      "2025-12-07 12:36:04.655371: Epoch 14\n",
      "2025-12-07 12:36:04.655371: Current learning rate: 0.00987\n",
      "2025-12-07 12:38:22.670951: train_loss -0.5609\n",
      "2025-12-07 12:38:22.670951: val_loss -0.6053\n",
      "2025-12-07 12:38:22.670951: Pseudo dice [0.8259, 0.8745, 0.7606]\n",
      "2025-12-07 12:38:22.670951: Epoch time: 138.02 s\n",
      "2025-12-07 12:38:22.679574: Yayy! New best EMA pseudo Dice: 0.6593\n",
      "2025-12-07 12:38:23.561860: \n",
      "2025-12-07 12:38:23.561860: Epoch 15\n",
      "2025-12-07 12:38:23.561860: Current learning rate: 0.00986\n",
      "2025-12-07 12:40:41.843736: train_loss -0.5783\n",
      "2025-12-07 12:40:41.843736: val_loss -0.6026\n",
      "2025-12-07 12:40:41.843736: Pseudo dice [0.8147, 0.8636, 0.774]\n",
      "2025-12-07 12:40:41.859708: Epoch time: 138.28 s\n",
      "2025-12-07 12:40:41.859708: Yayy! New best EMA pseudo Dice: 0.6751\n",
      "2025-12-07 12:40:42.827409: \n",
      "2025-12-07 12:40:42.827409: Epoch 16\n",
      "2025-12-07 12:40:42.843043: Current learning rate: 0.00986\n",
      "2025-12-07 12:43:00.951350: train_loss -0.6051\n",
      "2025-12-07 12:43:00.951350: val_loss -0.6286\n",
      "2025-12-07 12:43:00.952852: Pseudo dice [0.8275, 0.8794, 0.7966]\n",
      "2025-12-07 12:43:00.954854: Epoch time: 138.12 s\n",
      "2025-12-07 12:43:00.954854: Yayy! New best EMA pseudo Dice: 0.6911\n",
      "2025-12-07 12:43:01.872051: \n",
      "2025-12-07 12:43:01.872051: Epoch 17\n",
      "2025-12-07 12:43:01.873054: Current learning rate: 0.00985\n",
      "2025-12-07 12:45:20.038627: train_loss -0.5668\n",
      "2025-12-07 12:45:20.040629: val_loss -0.5955\n",
      "2025-12-07 12:45:20.040629: Pseudo dice [0.8052, 0.865, 0.7676]\n",
      "2025-12-07 12:45:20.042631: Epoch time: 138.17 s\n",
      "2025-12-07 12:45:20.042631: Yayy! New best EMA pseudo Dice: 0.7032\n",
      "2025-12-07 12:45:20.970897: \n",
      "2025-12-07 12:45:20.971897: Epoch 18\n",
      "2025-12-07 12:45:20.973049: Current learning rate: 0.00984\n",
      "2025-12-07 12:47:39.014487: train_loss -0.586\n",
      "2025-12-07 12:47:39.016227: val_loss -0.6399\n",
      "2025-12-07 12:47:39.018230: Pseudo dice [0.8319, 0.8798, 0.7976]\n",
      "2025-12-07 12:47:39.018230: Epoch time: 138.04 s\n",
      "2025-12-07 12:47:39.020233: Yayy! New best EMA pseudo Dice: 0.7165\n",
      "2025-12-07 12:47:40.126882: \n",
      "2025-12-07 12:47:40.126882: Epoch 19\n",
      "2025-12-07 12:47:40.126882: Current learning rate: 0.00983\n",
      "2025-12-07 12:49:58.042298: train_loss -0.6258\n",
      "2025-12-07 12:49:58.042298: val_loss -0.6484\n",
      "2025-12-07 12:49:58.044300: Pseudo dice [0.8286, 0.8863, 0.8122]\n",
      "2025-12-07 12:49:58.045804: Epoch time: 137.92 s\n",
      "2025-12-07 12:49:58.047810: Yayy! New best EMA pseudo Dice: 0.7291\n",
      "2025-12-07 12:49:58.963232: \n",
      "2025-12-07 12:49:58.963232: Epoch 20\n",
      "2025-12-07 12:49:58.964236: Current learning rate: 0.00982\n",
      "2025-12-07 12:52:17.379574: train_loss -0.6257\n",
      "2025-12-07 12:52:17.379574: val_loss -0.6573\n",
      "2025-12-07 12:52:17.381579: Pseudo dice [0.832, 0.8862, 0.8171]\n",
      "2025-12-07 12:52:17.382582: Epoch time: 138.42 s\n",
      "2025-12-07 12:52:17.383585: Yayy! New best EMA pseudo Dice: 0.7407\n",
      "2025-12-07 12:52:18.296248: \n",
      "2025-12-07 12:52:18.296248: Epoch 21\n",
      "2025-12-07 12:52:18.296248: Current learning rate: 0.00981\n",
      "2025-12-07 12:54:37.452593: train_loss -0.6351\n",
      "2025-12-07 12:54:37.452593: val_loss -0.6621\n",
      "2025-12-07 12:54:37.452593: Pseudo dice [0.8363, 0.8972, 0.8063]\n",
      "2025-12-07 12:54:37.452593: Epoch time: 139.16 s\n",
      "2025-12-07 12:54:37.468580: Yayy! New best EMA pseudo Dice: 0.7513\n",
      "2025-12-07 12:54:38.344551: \n",
      "2025-12-07 12:54:38.344551: Epoch 22\n",
      "2025-12-07 12:54:38.344551: Current learning rate: 0.0098\n",
      "2025-12-07 12:56:56.899885: train_loss -0.6617\n",
      "2025-12-07 12:56:56.899885: val_loss -0.6814\n",
      "2025-12-07 12:56:56.901886: Pseudo dice [0.8482, 0.9002, 0.8436]\n",
      "2025-12-07 12:56:56.902886: Epoch time: 138.56 s\n",
      "2025-12-07 12:56:56.903886: Yayy! New best EMA pseudo Dice: 0.7626\n",
      "2025-12-07 12:56:57.956879: \n",
      "2025-12-07 12:56:57.956879: Epoch 23\n",
      "2025-12-07 12:56:57.956879: Current learning rate: 0.00979\n",
      "2025-12-07 12:59:16.344981: train_loss -0.6593\n",
      "2025-12-07 12:59:16.344981: val_loss -0.669\n",
      "2025-12-07 12:59:16.346983: Pseudo dice [0.8425, 0.8934, 0.8158]\n",
      "2025-12-07 12:59:16.346983: Epoch time: 138.39 s\n",
      "2025-12-07 12:59:16.346983: Yayy! New best EMA pseudo Dice: 0.7714\n",
      "2025-12-07 12:59:17.233570: \n",
      "2025-12-07 12:59:17.233570: Epoch 24\n",
      "2025-12-07 12:59:17.233570: Current learning rate: 0.00978\n",
      "2025-12-07 13:01:35.506209: train_loss -0.6638\n",
      "2025-12-07 13:01:35.506209: val_loss -0.697\n",
      "2025-12-07 13:01:35.508219: Pseudo dice [0.8536, 0.9118, 0.8405]\n",
      "2025-12-07 13:01:35.509218: Epoch time: 138.27 s\n",
      "2025-12-07 13:01:35.510219: Yayy! New best EMA pseudo Dice: 0.7811\n",
      "2025-12-07 13:01:36.388420: \n",
      "2025-12-07 13:01:36.388420: Epoch 25\n",
      "2025-12-07 13:01:36.388420: Current learning rate: 0.00977\n",
      "2025-12-07 13:03:54.779933: train_loss -0.6685\n",
      "2025-12-07 13:03:54.779933: val_loss -0.6985\n",
      "2025-12-07 13:03:54.782322: Pseudo dice [0.8537, 0.9031, 0.8396]\n",
      "2025-12-07 13:03:54.782322: Epoch time: 138.39 s\n",
      "2025-12-07 13:03:54.782322: Yayy! New best EMA pseudo Dice: 0.7895\n",
      "2025-12-07 13:03:55.703962: \n",
      "2025-12-07 13:03:55.704962: Epoch 26\n",
      "2025-12-07 13:03:55.706256: Current learning rate: 0.00977\n",
      "2025-12-07 13:06:14.180086: train_loss -0.6646\n",
      "2025-12-07 13:06:14.181088: val_loss -0.6694\n",
      "2025-12-07 13:06:14.183092: Pseudo dice [0.8369, 0.8871, 0.8324]\n",
      "2025-12-07 13:06:14.183092: Epoch time: 138.48 s\n",
      "2025-12-07 13:06:14.184093: Yayy! New best EMA pseudo Dice: 0.7958\n",
      "2025-12-07 13:06:15.092323: \n",
      "2025-12-07 13:06:15.092323: Epoch 27\n",
      "2025-12-07 13:06:15.092323: Current learning rate: 0.00976\n",
      "2025-12-07 13:08:33.145122: train_loss -0.6767\n",
      "2025-12-07 13:08:33.145122: val_loss -0.684\n",
      "2025-12-07 13:08:33.147125: Pseudo dice [0.8436, 0.8973, 0.8562]\n",
      "2025-12-07 13:08:33.149128: Epoch time: 138.05 s\n",
      "2025-12-07 13:08:33.151134: Yayy! New best EMA pseudo Dice: 0.8028\n",
      "2025-12-07 13:08:34.030535: \n",
      "2025-12-07 13:08:34.030535: Epoch 28\n",
      "2025-12-07 13:08:34.030535: Current learning rate: 0.00975\n",
      "2025-12-07 13:10:51.983656: train_loss -0.6823\n",
      "2025-12-07 13:10:51.983656: val_loss -0.6834\n",
      "2025-12-07 13:10:51.983656: Pseudo dice [0.8393, 0.9003, 0.8465]\n",
      "2025-12-07 13:10:51.983656: Epoch time: 137.95 s\n",
      "2025-12-07 13:10:51.983656: Yayy! New best EMA pseudo Dice: 0.8087\n",
      "2025-12-07 13:10:52.868313: \n",
      "2025-12-07 13:10:52.868313: Epoch 29\n",
      "2025-12-07 13:10:52.868313: Current learning rate: 0.00974\n",
      "2025-12-07 13:13:10.874206: train_loss -0.6954\n",
      "2025-12-07 13:13:10.874206: val_loss -0.699\n",
      "2025-12-07 13:13:10.874206: Pseudo dice [0.8432, 0.9005, 0.8688]\n",
      "2025-12-07 13:13:10.874206: Epoch time: 138.01 s\n",
      "2025-12-07 13:13:10.874206: Yayy! New best EMA pseudo Dice: 0.8149\n",
      "2025-12-07 13:13:11.932128: \n",
      "2025-12-07 13:13:11.932128: Epoch 30\n",
      "2025-12-07 13:13:11.932128: Current learning rate: 0.00973\n",
      "2025-12-07 13:15:29.999683: train_loss -0.6946\n",
      "2025-12-07 13:15:29.999683: val_loss -0.7117\n",
      "2025-12-07 13:15:29.999683: Pseudo dice [0.8577, 0.9027, 0.8537]\n",
      "2025-12-07 13:15:29.999683: Epoch time: 138.07 s\n",
      "2025-12-07 13:15:29.999683: Yayy! New best EMA pseudo Dice: 0.8206\n",
      "2025-12-07 13:15:30.888777: \n",
      "2025-12-07 13:15:30.888777: Epoch 31\n",
      "2025-12-07 13:15:30.888777: Current learning rate: 0.00972\n",
      "2025-12-07 13:17:48.909447: train_loss -0.7042\n",
      "2025-12-07 13:17:48.910448: val_loss -0.7234\n",
      "2025-12-07 13:17:48.911448: Pseudo dice [0.86, 0.9105, 0.8626]\n",
      "2025-12-07 13:17:48.912448: Epoch time: 138.02 s\n",
      "2025-12-07 13:17:48.914448: Yayy! New best EMA pseudo Dice: 0.8263\n",
      "2025-12-07 13:17:49.828179: \n",
      "2025-12-07 13:17:49.829184: Epoch 32\n",
      "2025-12-07 13:17:49.829184: Current learning rate: 0.00971\n",
      "2025-12-07 13:20:07.793944: train_loss -0.7064\n",
      "2025-12-07 13:20:07.793944: val_loss -0.7209\n",
      "2025-12-07 13:20:07.796949: Pseudo dice [0.8592, 0.9071, 0.8653]\n",
      "2025-12-07 13:20:07.798951: Epoch time: 137.97 s\n",
      "2025-12-07 13:20:07.798951: Yayy! New best EMA pseudo Dice: 0.8314\n",
      "2025-12-07 13:20:08.699880: \n",
      "2025-12-07 13:20:08.701621: Epoch 33\n",
      "2025-12-07 13:20:08.701621: Current learning rate: 0.0097\n",
      "2025-12-07 13:22:26.841862: train_loss -0.7052\n",
      "2025-12-07 13:22:26.841862: val_loss -0.7285\n",
      "2025-12-07 13:22:26.843602: Pseudo dice [0.8619, 0.9052, 0.8859]\n",
      "2025-12-07 13:22:26.843602: Epoch time: 138.14 s\n",
      "2025-12-07 13:22:26.843602: Yayy! New best EMA pseudo Dice: 0.8367\n",
      "2025-12-07 13:22:27.740093: \n",
      "2025-12-07 13:22:27.740093: Epoch 34\n",
      "2025-12-07 13:22:27.741402: Current learning rate: 0.00969\n",
      "2025-12-07 13:24:45.795722: train_loss -0.7063\n",
      "2025-12-07 13:24:45.798494: val_loss -0.7337\n",
      "2025-12-07 13:24:45.798494: Pseudo dice [0.8641, 0.9149, 0.8744]\n",
      "2025-12-07 13:24:45.800663: Epoch time: 138.06 s\n",
      "2025-12-07 13:24:45.800663: Yayy! New best EMA pseudo Dice: 0.8414\n",
      "2025-12-07 13:24:46.715203: \n",
      "2025-12-07 13:24:46.715203: Epoch 35\n",
      "2025-12-07 13:24:46.717796: Current learning rate: 0.00968\n",
      "2025-12-07 13:27:04.813298: train_loss -0.7245\n",
      "2025-12-07 13:27:04.813298: val_loss -0.7427\n",
      "2025-12-07 13:27:04.817307: Pseudo dice [0.8612, 0.9097, 0.8896]\n",
      "2025-12-07 13:27:04.819311: Epoch time: 138.1 s\n",
      "2025-12-07 13:27:04.821313: Yayy! New best EMA pseudo Dice: 0.846\n",
      "2025-12-07 13:27:05.921616: \n",
      "2025-12-07 13:27:05.921616: Epoch 36\n",
      "2025-12-07 13:27:05.921616: Current learning rate: 0.00968\n",
      "2025-12-07 13:29:24.091347: train_loss -0.7268\n",
      "2025-12-07 13:29:24.093348: val_loss -0.7458\n",
      "2025-12-07 13:29:24.093348: Pseudo dice [0.8731, 0.9194, 0.8733]\n",
      "2025-12-07 13:29:24.093348: Epoch time: 138.17 s\n",
      "2025-12-07 13:29:24.093348: Yayy! New best EMA pseudo Dice: 0.8503\n",
      "2025-12-07 13:29:24.998879: \n",
      "2025-12-07 13:29:24.999885: Epoch 37\n",
      "2025-12-07 13:29:25.000924: Current learning rate: 0.00967\n",
      "2025-12-07 13:31:43.165655: train_loss -0.7175\n",
      "2025-12-07 13:31:43.165655: val_loss -0.7488\n",
      "2025-12-07 13:31:43.165655: Pseudo dice [0.8687, 0.9196, 0.8825]\n",
      "2025-12-07 13:31:43.165655: Epoch time: 138.17 s\n",
      "2025-12-07 13:31:43.165655: Yayy! New best EMA pseudo Dice: 0.8543\n",
      "2025-12-07 13:31:44.078124: \n",
      "2025-12-07 13:31:44.078124: Epoch 38\n",
      "2025-12-07 13:31:44.078124: Current learning rate: 0.00966\n",
      "2025-12-07 13:34:02.928632: train_loss -0.7176\n",
      "2025-12-07 13:34:02.928632: val_loss -0.7436\n",
      "2025-12-07 13:34:02.930634: Pseudo dice [0.8716, 0.9153, 0.8855]\n",
      "2025-12-07 13:34:02.930634: Epoch time: 138.85 s\n",
      "2025-12-07 13:34:02.932636: Yayy! New best EMA pseudo Dice: 0.8579\n",
      "2025-12-07 13:34:03.874761: \n",
      "2025-12-07 13:34:03.874761: Epoch 39\n",
      "2025-12-07 13:34:03.874761: Current learning rate: 0.00965\n",
      "2025-12-07 13:36:22.391249: train_loss -0.708\n",
      "2025-12-07 13:36:22.391249: val_loss -0.7507\n",
      "2025-12-07 13:36:22.393255: Pseudo dice [0.8793, 0.9222, 0.8788]\n",
      "2025-12-07 13:36:22.393255: Epoch time: 138.52 s\n",
      "2025-12-07 13:36:22.394257: Yayy! New best EMA pseudo Dice: 0.8615\n",
      "2025-12-07 13:36:23.295635: \n",
      "2025-12-07 13:36:23.295635: Epoch 40\n",
      "2025-12-07 13:36:23.295635: Current learning rate: 0.00964\n",
      "2025-12-07 13:38:41.825699: train_loss -0.7291\n",
      "2025-12-07 13:38:41.826824: val_loss -0.7613\n",
      "2025-12-07 13:38:41.827859: Pseudo dice [0.8764, 0.9237, 0.889]\n",
      "2025-12-07 13:38:41.828870: Epoch time: 138.53 s\n",
      "2025-12-07 13:38:41.829872: Yayy! New best EMA pseudo Dice: 0.865\n",
      "2025-12-07 13:38:42.742896: \n",
      "2025-12-07 13:38:42.744899: Epoch 41\n",
      "2025-12-07 13:38:42.744899: Current learning rate: 0.00963\n",
      "2025-12-07 13:41:01.513389: train_loss -0.7332\n",
      "2025-12-07 13:41:01.513389: val_loss -0.7374\n",
      "2025-12-07 13:41:01.513389: Pseudo dice [0.854, 0.8996, 0.8852]\n",
      "2025-12-07 13:41:01.513389: Epoch time: 138.77 s\n",
      "2025-12-07 13:41:01.513389: Yayy! New best EMA pseudo Dice: 0.8664\n",
      "2025-12-07 13:41:02.762189: \n",
      "2025-12-07 13:41:02.762189: Epoch 42\n",
      "2025-12-07 13:41:02.762189: Current learning rate: 0.00962\n",
      "2025-12-07 13:43:21.392246: train_loss -0.7321\n",
      "2025-12-07 13:43:21.392246: val_loss -0.7654\n",
      "2025-12-07 13:43:21.396259: Pseudo dice [0.8763, 0.9273, 0.8869]\n",
      "2025-12-07 13:43:21.398261: Epoch time: 138.63 s\n",
      "2025-12-07 13:43:21.398261: Yayy! New best EMA pseudo Dice: 0.8695\n",
      "2025-12-07 13:43:22.296043: \n",
      "2025-12-07 13:43:22.296043: Epoch 43\n",
      "2025-12-07 13:43:22.296043: Current learning rate: 0.00961\n",
      "2025-12-07 13:45:40.435916: train_loss -0.7375\n",
      "2025-12-07 13:45:40.435916: val_loss -0.7605\n",
      "2025-12-07 13:45:40.439421: Pseudo dice [0.8744, 0.9184, 0.8994]\n",
      "2025-12-07 13:45:40.439421: Epoch time: 138.14 s\n",
      "2025-12-07 13:45:40.441425: Yayy! New best EMA pseudo Dice: 0.8723\n",
      "2025-12-07 13:45:41.341435: \n",
      "2025-12-07 13:45:41.343438: Epoch 44\n",
      "2025-12-07 13:45:41.343438: Current learning rate: 0.0096\n",
      "2025-12-07 13:47:59.490836: train_loss -0.7405\n",
      "2025-12-07 13:47:59.490836: val_loss -0.7674\n",
      "2025-12-07 13:47:59.490836: Pseudo dice [0.8785, 0.9207, 0.8934]\n",
      "2025-12-07 13:47:59.490836: Epoch time: 138.15 s\n",
      "2025-12-07 13:47:59.490836: Yayy! New best EMA pseudo Dice: 0.8748\n",
      "2025-12-07 13:48:00.418223: \n",
      "2025-12-07 13:48:00.418223: Epoch 45\n",
      "2025-12-07 13:48:00.420227: Current learning rate: 0.00959\n",
      "2025-12-07 13:50:18.262044: train_loss -0.7367\n",
      "2025-12-07 13:50:18.264782: val_loss -0.7574\n",
      "2025-12-07 13:50:18.265785: Pseudo dice [0.8693, 0.9164, 0.8974]\n",
      "2025-12-07 13:50:18.267915: Epoch time: 137.84 s\n",
      "2025-12-07 13:50:18.268916: Yayy! New best EMA pseudo Dice: 0.8767\n",
      "2025-12-07 13:50:19.171134: \n",
      "2025-12-07 13:50:19.171134: Epoch 46\n",
      "2025-12-07 13:50:19.173098: Current learning rate: 0.00959\n",
      "2025-12-07 13:52:37.202387: train_loss -0.7436\n",
      "2025-12-07 13:52:37.202387: val_loss -0.7717\n",
      "2025-12-07 13:52:37.202387: Pseudo dice [0.8737, 0.9235, 0.8935]\n",
      "2025-12-07 13:52:37.202387: Epoch time: 138.03 s\n",
      "2025-12-07 13:52:37.202387: Yayy! New best EMA pseudo Dice: 0.8788\n",
      "2025-12-07 13:52:38.061700: \n",
      "2025-12-07 13:52:38.061700: Epoch 47\n",
      "2025-12-07 13:52:38.077672: Current learning rate: 0.00958\n",
      "2025-12-07 13:54:56.026354: train_loss -0.7512\n",
      "2025-12-07 13:54:56.026354: val_loss -0.7578\n",
      "2025-12-07 13:54:56.026354: Pseudo dice [0.8669, 0.9171, 0.8977]\n",
      "2025-12-07 13:54:56.029906: Epoch time: 137.96 s\n",
      "2025-12-07 13:54:56.031908: Yayy! New best EMA pseudo Dice: 0.8803\n",
      "2025-12-07 13:54:57.078152: \n",
      "2025-12-07 13:54:57.078152: Epoch 48\n",
      "2025-12-07 13:54:57.078152: Current learning rate: 0.00957\n",
      "2025-12-07 13:57:15.120730: train_loss -0.7548\n",
      "2025-12-07 13:57:15.120730: val_loss -0.7752\n",
      "2025-12-07 13:57:15.122733: Pseudo dice [0.8817, 0.925, 0.8889]\n",
      "2025-12-07 13:57:15.124736: Epoch time: 138.04 s\n",
      "2025-12-07 13:57:15.124736: Yayy! New best EMA pseudo Dice: 0.8821\n",
      "2025-12-07 13:57:15.998859: \n",
      "2025-12-07 13:57:15.998859: Epoch 49\n",
      "2025-12-07 13:57:16.014501: Current learning rate: 0.00956\n",
      "2025-12-07 13:59:33.967801: train_loss -0.7621\n",
      "2025-12-07 13:59:33.967801: val_loss -0.7815\n",
      "2025-12-07 13:59:33.967801: Pseudo dice [0.882, 0.9293, 0.8922]\n",
      "2025-12-07 13:59:33.967801: Epoch time: 137.97 s\n",
      "2025-12-07 13:59:34.223984: Yayy! New best EMA pseudo Dice: 0.884\n",
      "2025-12-07 13:59:35.109556: \n",
      "2025-12-07 13:59:35.109556: Epoch 50\n",
      "2025-12-07 13:59:35.109556: Current learning rate: 0.00955\n",
      "2025-12-07 14:01:53.284263: train_loss -0.7651\n",
      "2025-12-07 14:01:53.284263: val_loss -0.7713\n",
      "2025-12-07 14:01:53.286264: Pseudo dice [0.8695, 0.9142, 0.9061]\n",
      "2025-12-07 14:01:53.288265: Epoch time: 138.17 s\n",
      "2025-12-07 14:01:53.289265: Yayy! New best EMA pseudo Dice: 0.8853\n",
      "2025-12-07 14:01:54.192549: \n",
      "2025-12-07 14:01:54.193554: Epoch 51\n",
      "2025-12-07 14:01:54.194742: Current learning rate: 0.00954\n",
      "2025-12-07 14:04:12.320916: train_loss -0.7568\n",
      "2025-12-07 14:04:12.323343: val_loss -0.7886\n",
      "2025-12-07 14:04:12.325345: Pseudo dice [0.8872, 0.9308, 0.8965]\n",
      "2025-12-07 14:04:12.325345: Epoch time: 138.13 s\n",
      "2025-12-07 14:04:12.327347: Yayy! New best EMA pseudo Dice: 0.8872\n",
      "2025-12-07 14:04:13.217274: \n",
      "2025-12-07 14:04:13.217274: Epoch 52\n",
      "2025-12-07 14:04:13.217274: Current learning rate: 0.00953\n",
      "2025-12-07 14:06:31.374332: train_loss -0.7599\n",
      "2025-12-07 14:06:31.376335: val_loss -0.7676\n",
      "2025-12-07 14:06:31.378337: Pseudo dice [0.8724, 0.9153, 0.897]\n",
      "2025-12-07 14:06:31.380339: Epoch time: 138.16 s\n",
      "2025-12-07 14:06:31.380339: Yayy! New best EMA pseudo Dice: 0.888\n",
      "2025-12-07 14:06:32.268285: \n",
      "2025-12-07 14:06:32.270287: Epoch 53\n",
      "2025-12-07 14:06:32.270287: Current learning rate: 0.00952\n",
      "2025-12-07 14:08:50.275688: train_loss -0.7644\n",
      "2025-12-07 14:08:50.277691: val_loss -0.7689\n",
      "2025-12-07 14:08:50.277691: Pseudo dice [0.867, 0.9207, 0.8894]\n",
      "2025-12-07 14:08:50.280437: Epoch time: 138.01 s\n",
      "2025-12-07 14:08:50.280437: Yayy! New best EMA pseudo Dice: 0.8884\n",
      "2025-12-07 14:08:51.337795: \n",
      "2025-12-07 14:08:51.337795: Epoch 54\n",
      "2025-12-07 14:08:51.339860: Current learning rate: 0.00951\n",
      "2025-12-07 14:11:09.496246: train_loss -0.742\n",
      "2025-12-07 14:11:09.497247: val_loss -0.7651\n",
      "2025-12-07 14:11:09.498261: Pseudo dice [0.8776, 0.92, 0.9047]\n",
      "2025-12-07 14:11:09.499264: Epoch time: 138.16 s\n",
      "2025-12-07 14:11:09.499264: Yayy! New best EMA pseudo Dice: 0.8897\n",
      "2025-12-07 14:11:10.379016: \n",
      "2025-12-07 14:11:10.379016: Epoch 55\n",
      "2025-12-07 14:11:10.381033: Current learning rate: 0.0095\n",
      "2025-12-07 14:13:28.727434: train_loss -0.7619\n",
      "2025-12-07 14:13:28.728439: val_loss -0.7768\n",
      "2025-12-07 14:13:28.729443: Pseudo dice [0.8759, 0.9226, 0.9032]\n",
      "2025-12-07 14:13:28.730887: Epoch time: 138.35 s\n",
      "2025-12-07 14:13:28.732020: Yayy! New best EMA pseudo Dice: 0.8908\n",
      "2025-12-07 14:13:29.632132: \n",
      "2025-12-07 14:13:29.634803: Epoch 56\n",
      "2025-12-07 14:13:29.634803: Current learning rate: 0.00949\n",
      "2025-12-07 14:15:47.516894: train_loss -0.7668\n",
      "2025-12-07 14:15:47.518897: val_loss -0.7955\n",
      "2025-12-07 14:15:47.522906: Pseudo dice [0.8852, 0.9302, 0.9092]\n",
      "2025-12-07 14:15:47.524910: Epoch time: 137.88 s\n",
      "2025-12-07 14:15:47.524910: Yayy! New best EMA pseudo Dice: 0.8925\n",
      "2025-12-07 14:15:48.431750: \n",
      "2025-12-07 14:15:48.431750: Epoch 57\n",
      "2025-12-07 14:15:48.431750: Current learning rate: 0.00949\n",
      "2025-12-07 14:18:06.530013: train_loss -0.7701\n",
      "2025-12-07 14:18:06.530013: val_loss -0.7885\n",
      "2025-12-07 14:18:06.533609: Pseudo dice [0.8838, 0.9245, 0.9017]\n",
      "2025-12-07 14:18:06.535612: Epoch time: 138.1 s\n",
      "2025-12-07 14:18:06.535612: Yayy! New best EMA pseudo Dice: 0.8936\n",
      "2025-12-07 14:18:07.414712: \n",
      "2025-12-07 14:18:07.416714: Epoch 58\n",
      "2025-12-07 14:18:07.416714: Current learning rate: 0.00948\n",
      "2025-12-07 14:20:25.404863: train_loss -0.7702\n",
      "2025-12-07 14:20:25.404863: val_loss -0.7962\n",
      "2025-12-07 14:20:25.411859: Pseudo dice [0.8844, 0.9327, 0.9039]\n",
      "2025-12-07 14:20:25.411859: Epoch time: 137.99 s\n",
      "2025-12-07 14:20:25.413862: Yayy! New best EMA pseudo Dice: 0.8949\n",
      "2025-12-07 14:20:26.309478: \n",
      "2025-12-07 14:20:26.309478: Epoch 59\n",
      "2025-12-07 14:20:26.311482: Current learning rate: 0.00947\n",
      "2025-12-07 14:22:44.464775: train_loss -0.7687\n",
      "2025-12-07 14:22:44.464775: val_loss -0.7907\n",
      "2025-12-07 14:22:44.466780: Pseudo dice [0.8877, 0.9297, 0.9076]\n",
      "2025-12-07 14:22:44.467783: Epoch time: 138.16 s\n",
      "2025-12-07 14:22:44.468822: Yayy! New best EMA pseudo Dice: 0.8963\n",
      "2025-12-07 14:22:45.529862: \n",
      "2025-12-07 14:22:45.529862: Epoch 60\n",
      "2025-12-07 14:22:45.529862: Current learning rate: 0.00946\n",
      "2025-12-07 14:25:03.547161: train_loss -0.7757\n",
      "2025-12-07 14:25:03.547161: val_loss -0.7721\n",
      "2025-12-07 14:25:03.563090: Pseudo dice [0.8716, 0.9133, 0.8963]\n",
      "2025-12-07 14:25:03.564569: Epoch time: 138.02 s\n",
      "2025-12-07 14:25:04.199976: \n",
      "2025-12-07 14:25:04.199976: Epoch 61\n",
      "2025-12-07 14:25:04.201984: Current learning rate: 0.00945\n",
      "2025-12-07 14:27:22.186749: train_loss -0.763\n",
      "2025-12-07 14:27:22.186749: val_loss -0.7915\n",
      "2025-12-07 14:27:22.186749: Pseudo dice [0.8848, 0.9282, 0.9078]\n",
      "2025-12-07 14:27:22.198249: Epoch time: 137.99 s\n",
      "2025-12-07 14:27:22.198249: Yayy! New best EMA pseudo Dice: 0.8971\n",
      "2025-12-07 14:27:23.119595: \n",
      "2025-12-07 14:27:23.120599: Epoch 62\n",
      "2025-12-07 14:27:23.122604: Current learning rate: 0.00944\n",
      "2025-12-07 14:29:41.109188: train_loss -0.7632\n",
      "2025-12-07 14:29:41.109188: val_loss -0.7862\n",
      "2025-12-07 14:29:41.113193: Pseudo dice [0.88, 0.9359, 0.8971]\n",
      "2025-12-07 14:29:41.113193: Epoch time: 137.99 s\n",
      "2025-12-07 14:29:41.113193: Yayy! New best EMA pseudo Dice: 0.8978\n",
      "2025-12-07 14:29:42.010019: \n",
      "2025-12-07 14:29:42.010019: Epoch 63\n",
      "2025-12-07 14:29:42.010019: Current learning rate: 0.00943\n",
      "2025-12-07 14:32:00.133271: train_loss -0.7658\n",
      "2025-12-07 14:32:00.133271: val_loss -0.7754\n",
      "2025-12-07 14:32:00.133271: Pseudo dice [0.8806, 0.9187, 0.8894]\n",
      "2025-12-07 14:32:00.133271: Epoch time: 138.12 s\n",
      "2025-12-07 14:32:00.773430: \n",
      "2025-12-07 14:32:00.773430: Epoch 64\n",
      "2025-12-07 14:32:00.775844: Current learning rate: 0.00942\n",
      "2025-12-07 14:34:18.834719: train_loss -0.77\n",
      "2025-12-07 14:34:18.836722: val_loss -0.7952\n",
      "2025-12-07 14:34:18.836722: Pseudo dice [0.883, 0.9313, 0.9046]\n",
      "2025-12-07 14:34:18.839950: Epoch time: 138.06 s\n",
      "2025-12-07 14:34:18.839950: Yayy! New best EMA pseudo Dice: 0.8985\n",
      "2025-12-07 14:34:19.749488: \n",
      "2025-12-07 14:34:19.749488: Epoch 65\n",
      "2025-12-07 14:34:19.749488: Current learning rate: 0.00941\n",
      "2025-12-07 14:36:37.855958: train_loss -0.77\n",
      "2025-12-07 14:36:37.856958: val_loss -0.8027\n",
      "2025-12-07 14:36:37.858999: Pseudo dice [0.8942, 0.9335, 0.9026]\n",
      "2025-12-07 14:36:37.860000: Epoch time: 138.12 s\n",
      "2025-12-07 14:36:37.861000: Yayy! New best EMA pseudo Dice: 0.8997\n",
      "2025-12-07 14:36:38.931678: \n",
      "2025-12-07 14:36:38.932681: Epoch 66\n",
      "2025-12-07 14:36:38.933684: Current learning rate: 0.0094\n",
      "2025-12-07 14:38:56.968285: train_loss -0.7727\n",
      "2025-12-07 14:38:56.968285: val_loss -0.7871\n",
      "2025-12-07 14:38:56.968285: Pseudo dice [0.8782, 0.9222, 0.9156]\n",
      "2025-12-07 14:38:56.968285: Epoch time: 138.04 s\n",
      "2025-12-07 14:38:56.968285: Yayy! New best EMA pseudo Dice: 0.9003\n",
      "2025-12-07 14:38:57.867096: \n",
      "2025-12-07 14:38:57.868098: Epoch 67\n",
      "2025-12-07 14:38:57.870107: Current learning rate: 0.00939\n",
      "2025-12-07 14:41:16.294418: train_loss -0.773\n",
      "2025-12-07 14:41:16.294418: val_loss -0.7866\n",
      "2025-12-07 14:41:16.296158: Pseudo dice [0.8861, 0.927, 0.9049]\n",
      "2025-12-07 14:41:16.298164: Epoch time: 138.43 s\n",
      "2025-12-07 14:41:16.300168: Yayy! New best EMA pseudo Dice: 0.9008\n",
      "2025-12-07 14:41:17.213991: \n",
      "2025-12-07 14:41:17.213991: Epoch 68\n",
      "2025-12-07 14:41:17.216001: Current learning rate: 0.00939\n",
      "2025-12-07 14:43:35.331965: train_loss -0.7749\n",
      "2025-12-07 14:43:35.332967: val_loss -0.7903\n",
      "2025-12-07 14:43:35.333970: Pseudo dice [0.8787, 0.9275, 0.8987]\n",
      "2025-12-07 14:43:35.335975: Epoch time: 138.12 s\n",
      "2025-12-07 14:43:35.336977: Yayy! New best EMA pseudo Dice: 0.9009\n",
      "2025-12-07 14:43:36.237662: \n",
      "2025-12-07 14:43:36.239664: Epoch 69\n",
      "2025-12-07 14:43:36.239664: Current learning rate: 0.00938\n",
      "2025-12-07 14:45:54.321488: train_loss -0.779\n",
      "2025-12-07 14:45:54.323490: val_loss -0.8105\n",
      "2025-12-07 14:45:54.325492: Pseudo dice [0.8885, 0.9362, 0.9259]\n",
      "2025-12-07 14:45:54.325492: Epoch time: 138.08 s\n",
      "2025-12-07 14:45:54.327494: Yayy! New best EMA pseudo Dice: 0.9025\n",
      "2025-12-07 14:45:55.405250: \n",
      "2025-12-07 14:45:55.407254: Epoch 70\n",
      "2025-12-07 14:45:55.407254: Current learning rate: 0.00937\n",
      "2025-12-07 14:48:13.466728: train_loss -0.7779\n",
      "2025-12-07 14:48:13.467728: val_loss -0.8034\n",
      "2025-12-07 14:48:13.467728: Pseudo dice [0.8929, 0.9339, 0.9079]\n",
      "2025-12-07 14:48:13.467728: Epoch time: 138.06 s\n",
      "2025-12-07 14:48:13.467728: Yayy! New best EMA pseudo Dice: 0.9034\n",
      "2025-12-07 14:48:14.372334: \n",
      "2025-12-07 14:48:14.372334: Epoch 71\n",
      "2025-12-07 14:48:14.372334: Current learning rate: 0.00936\n",
      "2025-12-07 14:50:32.549900: train_loss -0.7857\n",
      "2025-12-07 14:50:32.550900: val_loss -0.7929\n",
      "2025-12-07 14:50:32.552900: Pseudo dice [0.8838, 0.9297, 0.9097]\n",
      "2025-12-07 14:50:32.554215: Epoch time: 138.18 s\n",
      "2025-12-07 14:50:32.555215: Yayy! New best EMA pseudo Dice: 0.9038\n",
      "2025-12-07 14:50:33.647736: \n",
      "2025-12-07 14:50:33.647736: Epoch 72\n",
      "2025-12-07 14:50:33.650096: Current learning rate: 0.00935\n",
      "2025-12-07 14:52:51.641641: train_loss -0.7802\n",
      "2025-12-07 14:52:51.641641: val_loss -0.7876\n",
      "2025-12-07 14:52:51.641641: Pseudo dice [0.8753, 0.9276, 0.9093]\n",
      "2025-12-07 14:52:51.641641: Epoch time: 137.99 s\n",
      "2025-12-07 14:52:51.641641: Yayy! New best EMA pseudo Dice: 0.9039\n",
      "2025-12-07 14:52:52.662028: \n",
      "2025-12-07 14:52:52.662028: Epoch 73\n",
      "2025-12-07 14:52:52.664046: Current learning rate: 0.00934\n",
      "2025-12-07 14:55:10.733321: train_loss -0.7835\n",
      "2025-12-07 14:55:10.733321: val_loss -0.8145\n",
      "2025-12-07 14:55:10.733321: Pseudo dice [0.9013, 0.9313, 0.9134]\n",
      "2025-12-07 14:55:10.733321: Epoch time: 138.07 s\n",
      "2025-12-07 14:55:10.733321: Yayy! New best EMA pseudo Dice: 0.905\n",
      "2025-12-07 14:55:11.659639: \n",
      "2025-12-07 14:55:11.659639: Epoch 74\n",
      "2025-12-07 14:55:11.659639: Current learning rate: 0.00933\n",
      "2025-12-07 14:57:29.842820: train_loss -0.7836\n",
      "2025-12-07 14:57:29.842820: val_loss -0.8017\n",
      "2025-12-07 14:57:29.842820: Pseudo dice [0.8874, 0.9269, 0.9165]\n",
      "2025-12-07 14:57:29.842820: Epoch time: 138.19 s\n",
      "2025-12-07 14:57:29.842820: Yayy! New best EMA pseudo Dice: 0.9055\n",
      "2025-12-07 14:57:30.733202: \n",
      "2025-12-07 14:57:30.733202: Epoch 75\n",
      "2025-12-07 14:57:30.748980: Current learning rate: 0.00932\n",
      "2025-12-07 14:59:48.733162: train_loss -0.7836\n",
      "2025-12-07 14:59:48.733162: val_loss -0.8213\n",
      "2025-12-07 14:59:48.733162: Pseudo dice [0.9033, 0.9381, 0.9155]\n",
      "2025-12-07 14:59:48.738547: Epoch time: 138.0 s\n",
      "2025-12-07 14:59:48.738547: Yayy! New best EMA pseudo Dice: 0.9069\n",
      "2025-12-07 14:59:49.799754: \n",
      "2025-12-07 14:59:49.799754: Epoch 76\n",
      "2025-12-07 14:59:49.801580: Current learning rate: 0.00931\n",
      "2025-12-07 15:02:07.841862: train_loss -0.7839\n",
      "2025-12-07 15:02:07.841862: val_loss -0.8092\n",
      "2025-12-07 15:02:07.843864: Pseudo dice [0.8952, 0.9355, 0.9117]\n",
      "2025-12-07 15:02:07.843864: Epoch time: 138.04 s\n",
      "2025-12-07 15:02:07.843864: Yayy! New best EMA pseudo Dice: 0.9076\n",
      "2025-12-07 15:02:08.762301: \n",
      "2025-12-07 15:02:08.762301: Epoch 77\n",
      "2025-12-07 15:02:08.765147: Current learning rate: 0.0093\n",
      "2025-12-07 15:04:26.873888: train_loss -0.7768\n",
      "2025-12-07 15:04:26.873888: val_loss -0.7989\n",
      "2025-12-07 15:04:26.873888: Pseudo dice [0.8857, 0.9309, 0.9094]\n",
      "2025-12-07 15:04:26.873888: Epoch time: 138.11 s\n",
      "2025-12-07 15:04:26.873888: Yayy! New best EMA pseudo Dice: 0.9077\n",
      "2025-12-07 15:04:27.951750: \n",
      "2025-12-07 15:04:27.951750: Epoch 78\n",
      "2025-12-07 15:04:27.951750: Current learning rate: 0.0093\n",
      "2025-12-07 15:06:45.948339: train_loss -0.7869\n",
      "2025-12-07 15:06:45.950341: val_loss -0.7932\n",
      "2025-12-07 15:06:45.952081: Pseudo dice [0.887, 0.9312, 0.9065]\n",
      "2025-12-07 15:06:45.956087: Epoch time: 138.0 s\n",
      "2025-12-07 15:06:45.958091: Yayy! New best EMA pseudo Dice: 0.9078\n",
      "2025-12-07 15:06:47.059551: \n",
      "2025-12-07 15:06:47.059551: Epoch 79\n",
      "2025-12-07 15:06:47.061551: Current learning rate: 0.00929\n",
      "2025-12-07 15:09:05.310976: train_loss -0.7915\n",
      "2025-12-07 15:09:05.310976: val_loss -0.8168\n",
      "2025-12-07 15:09:05.310976: Pseudo dice [0.8971, 0.9438, 0.9092]\n",
      "2025-12-07 15:09:05.310976: Epoch time: 138.25 s\n",
      "2025-12-07 15:09:05.323732: Yayy! New best EMA pseudo Dice: 0.9087\n",
      "2025-12-07 15:09:06.234081: \n",
      "2025-12-07 15:09:06.234081: Epoch 80\n",
      "2025-12-07 15:09:06.234081: Current learning rate: 0.00928\n",
      "2025-12-07 15:11:25.072927: train_loss -0.7902\n",
      "2025-12-07 15:11:25.072927: val_loss -0.8031\n",
      "2025-12-07 15:11:25.074929: Pseudo dice [0.8901, 0.927, 0.9109]\n",
      "2025-12-07 15:11:25.076669: Epoch time: 138.84 s\n",
      "2025-12-07 15:11:25.078564: Yayy! New best EMA pseudo Dice: 0.9087\n",
      "2025-12-07 15:11:25.998330: \n",
      "2025-12-07 15:11:25.998330: Epoch 81\n",
      "2025-12-07 15:11:25.998330: Current learning rate: 0.00927\n",
      "2025-12-07 15:13:44.609713: train_loss -0.7953\n",
      "2025-12-07 15:13:44.609713: val_loss -0.8039\n",
      "2025-12-07 15:13:44.611716: Pseudo dice [0.8857, 0.9305, 0.9235]\n",
      "2025-12-07 15:13:44.613718: Epoch time: 138.61 s\n",
      "2025-12-07 15:13:44.615720: Yayy! New best EMA pseudo Dice: 0.9092\n",
      "2025-12-07 15:13:45.575633: \n",
      "2025-12-07 15:13:45.575633: Epoch 82\n",
      "2025-12-07 15:13:45.576768: Current learning rate: 0.00926\n",
      "2025-12-07 15:16:04.210169: train_loss -0.7891\n",
      "2025-12-07 15:16:04.210169: val_loss -0.8159\n",
      "2025-12-07 15:16:04.210169: Pseudo dice [0.8994, 0.9389, 0.9159]\n",
      "2025-12-07 15:16:04.210169: Epoch time: 138.64 s\n",
      "2025-12-07 15:16:04.210169: Yayy! New best EMA pseudo Dice: 0.9101\n",
      "2025-12-07 15:16:05.270648: \n",
      "2025-12-07 15:16:05.270648: Epoch 83\n",
      "2025-12-07 15:16:05.373798: Current learning rate: 0.00925\n",
      "2025-12-07 15:18:23.699862: train_loss -0.7844\n",
      "2025-12-07 15:18:23.699862: val_loss -0.8123\n",
      "2025-12-07 15:18:23.702919: Pseudo dice [0.896, 0.9363, 0.914]\n",
      "2025-12-07 15:18:23.703960: Epoch time: 138.43 s\n",
      "2025-12-07 15:18:23.705961: Yayy! New best EMA pseudo Dice: 0.9106\n",
      "2025-12-07 15:18:24.596686: \n",
      "2025-12-07 15:18:24.597991: Epoch 84\n",
      "2025-12-07 15:18:24.598994: Current learning rate: 0.00924\n",
      "2025-12-07 15:20:42.841209: train_loss -0.7959\n",
      "2025-12-07 15:20:42.841209: val_loss -0.8107\n",
      "2025-12-07 15:20:42.843213: Pseudo dice [0.8936, 0.9346, 0.9172]\n",
      "2025-12-07 15:20:42.843213: Epoch time: 138.24 s\n",
      "2025-12-07 15:20:42.843213: Yayy! New best EMA pseudo Dice: 0.9111\n",
      "2025-12-07 15:20:43.717208: \n",
      "2025-12-07 15:20:43.717208: Epoch 85\n",
      "2025-12-07 15:20:43.717208: Current learning rate: 0.00923\n",
      "2025-12-07 15:23:01.830331: train_loss -0.7809\n",
      "2025-12-07 15:23:01.832335: val_loss -0.8198\n",
      "2025-12-07 15:23:01.834337: Pseudo dice [0.9047, 0.9358, 0.9116]\n",
      "2025-12-07 15:23:01.834337: Epoch time: 138.11 s\n",
      "2025-12-07 15:23:01.836340: Yayy! New best EMA pseudo Dice: 0.9117\n",
      "2025-12-07 15:23:02.831616: \n",
      "2025-12-07 15:23:02.831616: Epoch 86\n",
      "2025-12-07 15:23:02.831616: Current learning rate: 0.00922\n",
      "2025-12-07 15:25:20.888710: train_loss -0.788\n",
      "2025-12-07 15:25:20.890451: val_loss -0.8091\n",
      "2025-12-07 15:25:20.892542: Pseudo dice [0.8888, 0.9323, 0.9229]\n",
      "2025-12-07 15:25:20.893543: Epoch time: 138.06 s\n",
      "2025-12-07 15:25:20.894543: Yayy! New best EMA pseudo Dice: 0.912\n",
      "2025-12-07 15:25:21.779858: \n",
      "2025-12-07 15:25:21.779858: Epoch 87\n",
      "2025-12-07 15:25:21.779858: Current learning rate: 0.00921\n",
      "2025-12-07 15:27:39.822057: train_loss -0.792\n",
      "2025-12-07 15:27:39.824059: val_loss -0.8135\n",
      "2025-12-07 15:27:39.826064: Pseudo dice [0.8935, 0.9328, 0.9186]\n",
      "2025-12-07 15:27:39.827804: Epoch time: 138.04 s\n",
      "2025-12-07 15:27:39.829807: Yayy! New best EMA pseudo Dice: 0.9123\n",
      "2025-12-07 15:27:40.729155: \n",
      "2025-12-07 15:27:40.729155: Epoch 88\n",
      "2025-12-07 15:27:40.729155: Current learning rate: 0.0092\n",
      "2025-12-07 15:29:58.702726: train_loss -0.7922\n",
      "2025-12-07 15:29:58.702726: val_loss -0.8049\n",
      "2025-12-07 15:29:58.716668: Pseudo dice [0.8953, 0.9353, 0.9104]\n",
      "2025-12-07 15:29:58.716668: Epoch time: 137.97 s\n",
      "2025-12-07 15:29:58.718410: Yayy! New best EMA pseudo Dice: 0.9124\n",
      "2025-12-07 15:29:59.877388: \n",
      "2025-12-07 15:29:59.877388: Epoch 89\n",
      "2025-12-07 15:29:59.877388: Current learning rate: 0.0092\n",
      "2025-12-07 15:32:17.845928: train_loss -0.7723\n",
      "2025-12-07 15:32:17.845928: val_loss -0.7628\n",
      "2025-12-07 15:32:17.847931: Pseudo dice [0.8638, 0.9116, 0.8958]\n",
      "2025-12-07 15:32:17.847931: Epoch time: 137.97 s\n",
      "2025-12-07 15:32:18.473625: \n",
      "2025-12-07 15:32:18.473625: Epoch 90\n",
      "2025-12-07 15:32:18.473625: Current learning rate: 0.00919\n",
      "2025-12-07 15:34:37.119003: train_loss -0.7698\n",
      "2025-12-07 15:34:37.121009: val_loss -0.8071\n",
      "2025-12-07 15:34:37.122013: Pseudo dice [0.8956, 0.9352, 0.9201]\n",
      "2025-12-07 15:34:37.123018: Epoch time: 138.65 s\n",
      "2025-12-07 15:34:37.742519: \n",
      "2025-12-07 15:34:37.742519: Epoch 91\n",
      "2025-12-07 15:34:37.742519: Current learning rate: 0.00918\n",
      "2025-12-07 15:36:56.155664: train_loss -0.7776\n",
      "2025-12-07 15:36:56.155664: val_loss -0.8155\n",
      "2025-12-07 15:36:56.157667: Pseudo dice [0.8978, 0.9313, 0.9096]\n",
      "2025-12-07 15:36:56.159669: Epoch time: 138.41 s\n",
      "2025-12-07 15:36:56.812711: \n",
      "2025-12-07 15:36:56.814675: Epoch 92\n",
      "2025-12-07 15:36:56.814675: Current learning rate: 0.00917\n",
      "2025-12-07 15:39:15.214092: train_loss -0.7966\n",
      "2025-12-07 15:39:15.216094: val_loss -0.8326\n",
      "2025-12-07 15:39:15.217835: Pseudo dice [0.9131, 0.9416, 0.9178]\n",
      "2025-12-07 15:39:15.220004: Epoch time: 138.4 s\n",
      "2025-12-07 15:39:15.843603: \n",
      "2025-12-07 15:39:15.843603: Epoch 93\n",
      "2025-12-07 15:39:15.843603: Current learning rate: 0.00916\n",
      "2025-12-07 15:41:34.076472: train_loss -0.7986\n",
      "2025-12-07 15:41:34.076472: val_loss -0.8028\n",
      "2025-12-07 15:41:34.078212: Pseudo dice [0.89, 0.9243, 0.9153]\n",
      "2025-12-07 15:41:34.078212: Epoch time: 138.23 s\n",
      "2025-12-07 15:41:34.695260: \n",
      "2025-12-07 15:41:34.697262: Epoch 94\n",
      "2025-12-07 15:41:34.697262: Current learning rate: 0.00915\n",
      "2025-12-07 15:43:53.047504: train_loss -0.7987\n",
      "2025-12-07 15:43:53.047504: val_loss -0.8202\n",
      "2025-12-07 15:43:53.049506: Pseudo dice [0.8975, 0.933, 0.9333]\n",
      "2025-12-07 15:43:53.051509: Epoch time: 138.35 s\n",
      "2025-12-07 15:43:53.053512: Yayy! New best EMA pseudo Dice: 0.9131\n",
      "2025-12-07 15:43:54.067632: \n",
      "2025-12-07 15:43:54.067632: Epoch 95\n",
      "2025-12-07 15:43:54.067632: Current learning rate: 0.00914\n",
      "2025-12-07 15:46:12.124059: train_loss -0.7989\n",
      "2025-12-07 15:46:12.126063: val_loss -0.8135\n",
      "2025-12-07 15:46:12.128068: Pseudo dice [0.8963, 0.9333, 0.9221]\n",
      "2025-12-07 15:46:12.130070: Epoch time: 138.06 s\n",
      "2025-12-07 15:46:12.132072: Yayy! New best EMA pseudo Dice: 0.9135\n",
      "2025-12-07 15:46:13.208620: \n",
      "2025-12-07 15:46:13.208620: Epoch 96\n",
      "2025-12-07 15:46:13.208620: Current learning rate: 0.00913\n",
      "2025-12-07 15:48:31.231319: train_loss -0.7992\n",
      "2025-12-07 15:48:31.233060: val_loss -0.8096\n",
      "2025-12-07 15:48:31.233060: Pseudo dice [0.8915, 0.936, 0.9051]\n",
      "2025-12-07 15:48:31.233060: Epoch time: 138.02 s\n",
      "2025-12-07 15:48:31.872406: \n",
      "2025-12-07 15:48:31.872406: Epoch 97\n",
      "2025-12-07 15:48:31.874149: Current learning rate: 0.00912\n",
      "2025-12-07 15:50:49.756040: train_loss -0.7994\n",
      "2025-12-07 15:50:49.758040: val_loss -0.8147\n",
      "2025-12-07 15:50:49.761040: Pseudo dice [0.896, 0.9368, 0.917]\n",
      "2025-12-07 15:50:49.763041: Epoch time: 137.88 s\n",
      "2025-12-07 15:50:49.764042: Yayy! New best EMA pseudo Dice: 0.9136\n",
      "2025-12-07 15:50:50.748638: \n",
      "2025-12-07 15:50:50.748638: Epoch 98\n",
      "2025-12-07 15:50:50.748638: Current learning rate: 0.00911\n",
      "2025-12-07 15:53:08.550105: train_loss -0.795\n",
      "2025-12-07 15:53:08.551105: val_loss -0.823\n",
      "2025-12-07 15:53:08.554105: Pseudo dice [0.8977, 0.9395, 0.9256]\n",
      "2025-12-07 15:53:08.555106: Epoch time: 137.8 s\n",
      "2025-12-07 15:53:08.557106: Yayy! New best EMA pseudo Dice: 0.9143\n",
      "2025-12-07 15:53:09.471509: \n",
      "2025-12-07 15:53:09.471509: Epoch 99\n",
      "2025-12-07 15:53:09.471509: Current learning rate: 0.0091\n",
      "2025-12-07 15:55:27.374797: train_loss -0.7977\n",
      "2025-12-07 15:55:27.375866: val_loss -0.8193\n",
      "2025-12-07 15:55:27.377870: Pseudo dice [0.8974, 0.9409, 0.918]\n",
      "2025-12-07 15:55:27.378870: Epoch time: 137.91 s\n",
      "2025-12-07 15:55:27.624727: Yayy! New best EMA pseudo Dice: 0.9147\n",
      "2025-12-07 15:55:28.519988: \n",
      "2025-12-07 15:55:28.519988: Epoch 100\n",
      "2025-12-07 15:55:28.519988: Current learning rate: 0.0091\n",
      "2025-12-07 15:57:46.514616: train_loss -0.7988\n",
      "2025-12-07 15:57:46.515687: val_loss -0.8197\n",
      "2025-12-07 15:57:46.517686: Pseudo dice [0.8954, 0.9328, 0.9284]\n",
      "2025-12-07 15:57:46.519687: Epoch time: 137.99 s\n",
      "2025-12-07 15:57:46.520687: Yayy! New best EMA pseudo Dice: 0.9152\n",
      "2025-12-07 15:57:47.581095: \n",
      "2025-12-07 15:57:47.582097: Epoch 101\n",
      "2025-12-07 15:57:47.584105: Current learning rate: 0.00909\n",
      "2025-12-07 16:00:05.891500: train_loss -0.7755\n",
      "2025-12-07 16:00:05.891500: val_loss -0.7492\n",
      "2025-12-07 16:00:05.908652: Pseudo dice [0.8527, 0.9113, 0.9045]\n",
      "2025-12-07 16:00:05.909656: Epoch time: 138.31 s\n",
      "2025-12-07 16:00:06.514517: \n",
      "2025-12-07 16:00:06.514517: Epoch 102\n",
      "2025-12-07 16:00:06.514517: Current learning rate: 0.00908\n",
      "2025-12-07 16:02:24.558972: train_loss -0.7171\n",
      "2025-12-07 16:02:24.558972: val_loss -0.7497\n",
      "2025-12-07 16:02:24.561977: Pseudo dice [0.8617, 0.9055, 0.8943]\n",
      "2025-12-07 16:02:24.563979: Epoch time: 138.04 s\n",
      "2025-12-07 16:02:25.186821: \n",
      "2025-12-07 16:02:25.186821: Epoch 103\n",
      "2025-12-07 16:02:25.186821: Current learning rate: 0.00907\n",
      "2025-12-07 16:04:43.179550: train_loss -0.7604\n",
      "2025-12-07 16:04:43.181552: val_loss -0.7805\n",
      "2025-12-07 16:04:43.185558: Pseudo dice [0.8799, 0.9247, 0.8995]\n",
      "2025-12-07 16:04:43.187300: Epoch time: 137.99 s\n",
      "2025-12-07 16:04:43.821460: \n",
      "2025-12-07 16:04:43.821460: Epoch 104\n",
      "2025-12-07 16:04:43.821460: Current learning rate: 0.00906\n",
      "2025-12-07 16:07:01.811475: train_loss -0.7787\n",
      "2025-12-07 16:07:01.811475: val_loss -0.8099\n",
      "2025-12-07 16:07:01.811475: Pseudo dice [0.8937, 0.9395, 0.9121]\n",
      "2025-12-07 16:07:01.816200: Epoch time: 137.99 s\n",
      "2025-12-07 16:07:02.448996: \n",
      "2025-12-07 16:07:02.448996: Epoch 105\n",
      "2025-12-07 16:07:02.451605: Current learning rate: 0.00905\n",
      "2025-12-07 16:09:20.368018: train_loss -0.7814\n",
      "2025-12-07 16:09:20.368018: val_loss -0.805\n",
      "2025-12-07 16:09:20.368018: Pseudo dice [0.8919, 0.9328, 0.9179]\n",
      "2025-12-07 16:09:20.368018: Epoch time: 137.92 s\n",
      "2025-12-07 16:09:20.983096: \n",
      "2025-12-07 16:09:20.983096: Epoch 106\n",
      "2025-12-07 16:09:20.983096: Current learning rate: 0.00904\n",
      "2025-12-07 16:11:38.946606: train_loss -0.7844\n",
      "2025-12-07 16:11:38.946606: val_loss -0.8045\n",
      "2025-12-07 16:11:38.949704: Pseudo dice [0.8915, 0.9313, 0.9177]\n",
      "2025-12-07 16:11:38.950704: Epoch time: 137.96 s\n",
      "2025-12-07 16:11:39.587979: \n",
      "2025-12-07 16:11:39.587979: Epoch 107\n",
      "2025-12-07 16:11:39.587979: Current learning rate: 0.00903\n",
      "2025-12-07 16:13:58.280770: train_loss -0.7941\n",
      "2025-12-07 16:13:58.280770: val_loss -0.8093\n",
      "2025-12-07 16:13:58.284748: Pseudo dice [0.8902, 0.9342, 0.925]\n",
      "2025-12-07 16:13:58.284748: Epoch time: 138.69 s\n",
      "2025-12-07 16:13:59.075996: \n",
      "2025-12-07 16:13:59.075996: Epoch 108\n",
      "2025-12-07 16:13:59.077999: Current learning rate: 0.00902\n",
      "2025-12-07 16:16:17.364313: train_loss -0.7948\n",
      "2025-12-07 16:16:17.364313: val_loss -0.8088\n",
      "2025-12-07 16:16:17.366314: Pseudo dice [0.8899, 0.9283, 0.9205]\n",
      "2025-12-07 16:16:17.368317: Epoch time: 138.29 s\n",
      "2025-12-07 16:16:17.999796: \n",
      "2025-12-07 16:16:17.999796: Epoch 109\n",
      "2025-12-07 16:16:17.999796: Current learning rate: 0.00901\n",
      "2025-12-07 16:18:36.327240: train_loss -0.7972\n",
      "2025-12-07 16:18:36.327240: val_loss -0.8378\n",
      "2025-12-07 16:18:36.343140: Pseudo dice [0.9119, 0.9463, 0.9225]\n",
      "2025-12-07 16:18:36.345294: Epoch time: 138.33 s\n",
      "2025-12-07 16:18:36.983928: \n",
      "2025-12-07 16:18:36.983928: Epoch 110\n",
      "2025-12-07 16:18:36.983928: Current learning rate: 0.009\n",
      "2025-12-07 16:20:55.467999: train_loss -0.7999\n",
      "2025-12-07 16:20:55.467999: val_loss -0.815\n",
      "2025-12-07 16:20:55.485145: Pseudo dice [0.8933, 0.9356, 0.9215]\n",
      "2025-12-07 16:20:55.486149: Epoch time: 138.48 s\n",
      "2025-12-07 16:20:56.092997: \n",
      "2025-12-07 16:20:56.092997: Epoch 111\n",
      "2025-12-07 16:20:56.108891: Current learning rate: 0.009\n",
      "2025-12-07 16:23:14.327790: train_loss -0.8044\n",
      "2025-12-07 16:23:14.327790: val_loss -0.8075\n",
      "2025-12-07 16:23:14.327790: Pseudo dice [0.8892, 0.9351, 0.9214]\n",
      "2025-12-07 16:23:14.327790: Epoch time: 138.23 s\n",
      "2025-12-07 16:23:14.967971: \n",
      "2025-12-07 16:23:14.967971: Epoch 112\n",
      "2025-12-07 16:23:14.967971: Current learning rate: 0.00899\n",
      "2025-12-07 16:25:33.175082: train_loss -0.7996\n",
      "2025-12-07 16:25:33.176084: val_loss -0.8305\n",
      "2025-12-07 16:25:33.178086: Pseudo dice [0.9082, 0.949, 0.9193]\n",
      "2025-12-07 16:25:33.179219: Epoch time: 138.21 s\n",
      "2025-12-07 16:25:33.796665: \n",
      "2025-12-07 16:25:33.796665: Epoch 113\n",
      "2025-12-07 16:25:33.796665: Current learning rate: 0.00898\n",
      "2025-12-07 16:27:51.807860: train_loss -0.8036\n",
      "2025-12-07 16:27:51.807860: val_loss -0.8201\n",
      "2025-12-07 16:27:51.810868: Pseudo dice [0.898, 0.9322, 0.9208]\n",
      "2025-12-07 16:27:51.810868: Epoch time: 138.01 s\n",
      "2025-12-07 16:27:52.609508: \n",
      "2025-12-07 16:27:52.609508: Epoch 114\n",
      "2025-12-07 16:27:52.609508: Current learning rate: 0.00897\n",
      "2025-12-07 16:30:10.639646: train_loss -0.7909\n",
      "2025-12-07 16:30:10.639646: val_loss -0.8089\n",
      "2025-12-07 16:30:10.639646: Pseudo dice [0.8893, 0.9352, 0.9184]\n",
      "2025-12-07 16:30:10.655335: Epoch time: 138.03 s\n",
      "2025-12-07 16:30:11.291274: \n",
      "2025-12-07 16:30:11.293277: Epoch 115\n",
      "2025-12-07 16:30:11.293277: Current learning rate: 0.00896\n",
      "2025-12-07 16:32:29.155187: train_loss -0.8004\n",
      "2025-12-07 16:32:29.155187: val_loss -0.825\n",
      "2025-12-07 16:32:29.155187: Pseudo dice [0.9033, 0.9392, 0.9234]\n",
      "2025-12-07 16:32:29.155187: Epoch time: 137.86 s\n",
      "2025-12-07 16:32:29.155187: Yayy! New best EMA pseudo Dice: 0.9156\n",
      "2025-12-07 16:32:30.061271: \n",
      "2025-12-07 16:32:30.061271: Epoch 116\n",
      "2025-12-07 16:32:30.061271: Current learning rate: 0.00895\n",
      "2025-12-07 16:34:48.156141: train_loss -0.7987\n",
      "2025-12-07 16:34:48.158144: val_loss -0.8332\n",
      "2025-12-07 16:34:48.160146: Pseudo dice [0.9103, 0.9425, 0.9241]\n",
      "2025-12-07 16:34:48.160146: Epoch time: 138.1 s\n",
      "2025-12-07 16:34:48.162148: Yayy! New best EMA pseudo Dice: 0.9166\n",
      "2025-12-07 16:34:49.068957: \n",
      "2025-12-07 16:34:49.068957: Epoch 117\n",
      "2025-12-07 16:34:49.068957: Current learning rate: 0.00894\n",
      "2025-12-07 16:37:06.967427: train_loss -0.806\n",
      "2025-12-07 16:37:06.967427: val_loss -0.828\n",
      "2025-12-07 16:37:06.969168: Pseudo dice [0.9015, 0.94, 0.9243]\n",
      "2025-12-07 16:37:06.969168: Epoch time: 137.9 s\n",
      "2025-12-07 16:37:06.969168: Yayy! New best EMA pseudo Dice: 0.9171\n",
      "2025-12-07 16:37:07.904633: \n",
      "2025-12-07 16:37:07.904633: Epoch 118\n",
      "2025-12-07 16:37:07.904633: Current learning rate: 0.00893\n",
      "2025-12-07 16:39:25.964246: train_loss -0.8013\n",
      "2025-12-07 16:39:25.965246: val_loss -0.8324\n",
      "2025-12-07 16:39:25.967246: Pseudo dice [0.9043, 0.943, 0.927]\n",
      "2025-12-07 16:39:25.969247: Epoch time: 138.06 s\n",
      "2025-12-07 16:39:25.971247: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2025-12-07 16:39:26.874269: \n",
      "2025-12-07 16:39:26.874269: Epoch 119\n",
      "2025-12-07 16:39:26.874269: Current learning rate: 0.00892\n",
      "2025-12-07 16:41:44.774969: train_loss -0.8063\n",
      "2025-12-07 16:41:44.775970: val_loss -0.8229\n",
      "2025-12-07 16:41:44.778970: Pseudo dice [0.8987, 0.933, 0.9229]\n",
      "2025-12-07 16:41:44.781009: Epoch time: 137.92 s\n",
      "2025-12-07 16:41:44.783009: Yayy! New best EMA pseudo Dice: 0.9179\n",
      "2025-12-07 16:41:45.869986: \n",
      "2025-12-07 16:41:45.869986: Epoch 120\n",
      "2025-12-07 16:41:45.872069: Current learning rate: 0.00891\n",
      "2025-12-07 16:44:04.077094: train_loss -0.8078\n",
      "2025-12-07 16:44:04.077094: val_loss -0.8139\n",
      "2025-12-07 16:44:04.077094: Pseudo dice [0.8876, 0.9318, 0.9276]\n",
      "2025-12-07 16:44:04.077094: Epoch time: 138.21 s\n",
      "2025-12-07 16:44:04.825780: \n",
      "2025-12-07 16:44:04.825780: Epoch 121\n",
      "2025-12-07 16:44:04.827784: Current learning rate: 0.0089\n",
      "2025-12-07 16:46:22.766015: train_loss -0.8088\n",
      "2025-12-07 16:46:22.766015: val_loss -0.8291\n",
      "2025-12-07 16:46:22.769016: Pseudo dice [0.9035, 0.9424, 0.922]\n",
      "2025-12-07 16:46:22.770015: Epoch time: 137.94 s\n",
      "2025-12-07 16:46:22.771016: Yayy! New best EMA pseudo Dice: 0.9182\n",
      "2025-12-07 16:46:23.655277: \n",
      "2025-12-07 16:46:23.655277: Epoch 122\n",
      "2025-12-07 16:46:23.655277: Current learning rate: 0.00889\n",
      "2025-12-07 16:48:41.561579: train_loss -0.8129\n",
      "2025-12-07 16:48:41.561579: val_loss -0.8455\n",
      "2025-12-07 16:48:41.561579: Pseudo dice [0.9133, 0.946, 0.9336]\n",
      "2025-12-07 16:48:41.561579: Epoch time: 137.91 s\n",
      "2025-12-07 16:48:41.569604: Yayy! New best EMA pseudo Dice: 0.9195\n",
      "2025-12-07 16:48:42.467548: \n",
      "2025-12-07 16:48:42.467548: Epoch 123\n",
      "2025-12-07 16:48:42.467548: Current learning rate: 0.00889\n",
      "2025-12-07 16:51:00.448020: train_loss -0.804\n",
      "2025-12-07 16:51:00.448020: val_loss -0.8282\n",
      "2025-12-07 16:51:00.448020: Pseudo dice [0.9026, 0.9379, 0.9197]\n",
      "2025-12-07 16:51:00.451008: Epoch time: 137.98 s\n",
      "2025-12-07 16:51:00.452510: Yayy! New best EMA pseudo Dice: 0.9195\n",
      "2025-12-07 16:51:01.457386: \n",
      "2025-12-07 16:51:01.457386: Epoch 124\n",
      "2025-12-07 16:51:01.462206: Current learning rate: 0.00888\n",
      "2025-12-07 16:53:19.457713: train_loss -0.8121\n",
      "2025-12-07 16:53:19.457713: val_loss -0.829\n",
      "2025-12-07 16:53:19.459715: Pseudo dice [0.9012, 0.938, 0.9278]\n",
      "2025-12-07 16:53:19.461717: Epoch time: 138.0 s\n",
      "2025-12-07 16:53:19.463719: Yayy! New best EMA pseudo Dice: 0.9198\n",
      "2025-12-07 16:53:20.359650: \n",
      "2025-12-07 16:53:20.359650: Epoch 125\n",
      "2025-12-07 16:53:20.359650: Current learning rate: 0.00887\n",
      "2025-12-07 16:55:38.324898: train_loss -0.8137\n",
      "2025-12-07 16:55:38.326901: val_loss -0.8415\n",
      "2025-12-07 16:55:38.328642: Pseudo dice [0.9095, 0.9396, 0.9289]\n",
      "2025-12-07 16:55:38.328642: Epoch time: 137.97 s\n",
      "2025-12-07 16:55:38.332147: Yayy! New best EMA pseudo Dice: 0.9204\n",
      "2025-12-07 16:55:39.406934: \n",
      "2025-12-07 16:55:39.406934: Epoch 126\n",
      "2025-12-07 16:55:39.406934: Current learning rate: 0.00886\n",
      "2025-12-07 16:57:57.317434: train_loss -0.8105\n",
      "2025-12-07 16:57:57.317434: val_loss -0.8386\n",
      "2025-12-07 16:57:57.319437: Pseudo dice [0.9063, 0.9453, 0.9321]\n",
      "2025-12-07 16:57:57.319437: Epoch time: 137.91 s\n",
      "2025-12-07 16:57:57.319437: Yayy! New best EMA pseudo Dice: 0.9212\n",
      "2025-12-07 16:57:58.311456: \n",
      "2025-12-07 16:57:58.311456: Epoch 127\n",
      "2025-12-07 16:57:58.311456: Current learning rate: 0.00885\n",
      "2025-12-07 17:00:17.405801: train_loss -0.8136\n",
      "2025-12-07 17:00:17.405801: val_loss -0.8313\n",
      "2025-12-07 17:00:17.405801: Pseudo dice [0.906, 0.9331, 0.9232]\n",
      "2025-12-07 17:00:17.421701: Epoch time: 139.09 s\n",
      "2025-12-07 17:00:18.045698: \n",
      "2025-12-07 17:00:18.045698: Epoch 128\n",
      "2025-12-07 17:00:18.045698: Current learning rate: 0.00884\n",
      "2025-12-07 17:02:36.500011: train_loss -0.8139\n",
      "2025-12-07 17:02:36.500011: val_loss -0.8176\n",
      "2025-12-07 17:02:36.515863: Pseudo dice [0.8942, 0.9302, 0.9295]\n",
      "2025-12-07 17:02:36.515863: Epoch time: 138.45 s\n",
      "2025-12-07 17:02:37.139918: \n",
      "2025-12-07 17:02:37.139918: Epoch 129\n",
      "2025-12-07 17:02:37.155756: Current learning rate: 0.00883\n",
      "2025-12-07 17:04:55.655053: train_loss -0.7855\n",
      "2025-12-07 17:04:55.656053: val_loss -0.7897\n",
      "2025-12-07 17:04:55.658070: Pseudo dice [0.891, 0.9279, 0.8986]\n",
      "2025-12-07 17:04:55.660072: Epoch time: 138.52 s\n",
      "2025-12-07 17:04:56.311609: \n",
      "2025-12-07 17:04:56.311609: Epoch 130\n",
      "2025-12-07 17:04:56.311609: Current learning rate: 0.00882\n",
      "2025-12-07 17:07:14.862648: train_loss -0.7718\n",
      "2025-12-07 17:07:14.862648: val_loss -0.7914\n",
      "2025-12-07 17:07:14.866652: Pseudo dice [0.882, 0.9234, 0.9139]\n",
      "2025-12-07 17:07:14.866652: Epoch time: 138.55 s\n",
      "2025-12-07 17:07:15.515313: \n",
      "2025-12-07 17:07:15.515313: Epoch 131\n",
      "2025-12-07 17:07:15.515313: Current learning rate: 0.00881\n",
      "2025-12-07 17:09:33.796189: train_loss -0.7823\n",
      "2025-12-07 17:09:33.796189: val_loss -0.8289\n",
      "2025-12-07 17:09:33.796189: Pseudo dice [0.9057, 0.9471, 0.9174]\n",
      "2025-12-07 17:09:33.796189: Epoch time: 138.28 s\n",
      "2025-12-07 17:09:34.623466: \n",
      "2025-12-07 17:09:34.623466: Epoch 132\n",
      "2025-12-07 17:09:34.623466: Current learning rate: 0.0088\n",
      "2025-12-07 17:11:52.643983: train_loss -0.8024\n",
      "2025-12-07 17:11:52.643983: val_loss -0.8341\n",
      "2025-12-07 17:11:52.647726: Pseudo dice [0.9091, 0.9439, 0.9227]\n",
      "2025-12-07 17:11:52.649852: Epoch time: 138.02 s\n",
      "2025-12-07 17:11:53.298155: \n",
      "2025-12-07 17:11:53.298155: Epoch 133\n",
      "2025-12-07 17:11:53.300162: Current learning rate: 0.00879\n",
      "2025-12-07 17:14:11.259985: train_loss -0.808\n",
      "2025-12-07 17:14:11.260984: val_loss -0.8228\n",
      "2025-12-07 17:14:11.263985: Pseudo dice [0.8961, 0.9345, 0.9263]\n",
      "2025-12-07 17:14:11.266268: Epoch time: 137.96 s\n",
      "2025-12-07 17:14:11.922715: \n",
      "2025-12-07 17:14:11.922715: Epoch 134\n",
      "2025-12-07 17:14:11.924720: Current learning rate: 0.00879\n",
      "2025-12-07 17:16:29.921141: train_loss -0.8093\n",
      "2025-12-07 17:16:29.921141: val_loss -0.8397\n",
      "2025-12-07 17:16:29.936005: Pseudo dice [0.9072, 0.9415, 0.9327]\n",
      "2025-12-07 17:16:29.937010: Epoch time: 138.0 s\n",
      "2025-12-07 17:16:30.561895: \n",
      "2025-12-07 17:16:30.577718: Epoch 135\n",
      "2025-12-07 17:16:30.577718: Current learning rate: 0.00878\n",
      "2025-12-07 17:18:48.555811: train_loss -0.8132\n",
      "2025-12-07 17:18:48.557814: val_loss -0.8256\n",
      "2025-12-07 17:18:48.557814: Pseudo dice [0.8993, 0.939, 0.9234]\n",
      "2025-12-07 17:18:48.557814: Epoch time: 137.99 s\n",
      "2025-12-07 17:18:49.202758: \n",
      "2025-12-07 17:18:49.202758: Epoch 136\n",
      "2025-12-07 17:18:49.210597: Current learning rate: 0.00877\n",
      "2025-12-07 17:21:07.139787: train_loss -0.8039\n",
      "2025-12-07 17:21:07.139787: val_loss -0.8217\n",
      "2025-12-07 17:21:07.155553: Pseudo dice [0.901, 0.9371, 0.9246]\n",
      "2025-12-07 17:21:07.155553: Epoch time: 137.94 s\n",
      "2025-12-07 17:21:07.811364: \n",
      "2025-12-07 17:21:07.811364: Epoch 137\n",
      "2025-12-07 17:21:07.811364: Current learning rate: 0.00876\n",
      "2025-12-07 17:23:25.937650: train_loss -0.7671\n",
      "2025-12-07 17:23:25.937650: val_loss -0.822\n",
      "2025-12-07 17:23:25.946115: Pseudo dice [0.9, 0.944, 0.9197]\n",
      "2025-12-07 17:23:25.946115: Epoch time: 138.14 s\n",
      "2025-12-07 17:23:26.773906: \n",
      "2025-12-07 17:23:26.774919: Epoch 138\n",
      "2025-12-07 17:23:26.776982: Current learning rate: 0.00875\n",
      "2025-12-07 17:25:44.794427: train_loss -0.7887\n",
      "2025-12-07 17:25:44.796169: val_loss -0.8158\n",
      "2025-12-07 17:25:44.798171: Pseudo dice [0.9024, 0.9362, 0.9011]\n",
      "2025-12-07 17:25:44.800174: Epoch time: 138.02 s\n",
      "2025-12-07 17:25:45.452436: \n",
      "2025-12-07 17:25:45.452436: Epoch 139\n",
      "2025-12-07 17:25:45.452436: Current learning rate: 0.00874\n",
      "2025-12-07 17:28:03.821480: train_loss -0.8007\n",
      "2025-12-07 17:28:03.823483: val_loss -0.8267\n",
      "2025-12-07 17:28:03.825486: Pseudo dice [0.9024, 0.9439, 0.9189]\n",
      "2025-12-07 17:28:03.827227: Epoch time: 138.37 s\n",
      "2025-12-07 17:28:04.483886: \n",
      "2025-12-07 17:28:04.483886: Epoch 140\n",
      "2025-12-07 17:28:04.483886: Current learning rate: 0.00873\n",
      "2025-12-07 17:30:24.088966: train_loss -0.8098\n",
      "2025-12-07 17:30:24.090969: val_loss -0.8107\n",
      "2025-12-07 17:30:24.093182: Pseudo dice [0.8943, 0.9312, 0.9172]\n",
      "2025-12-07 17:30:24.093182: Epoch time: 139.61 s\n",
      "2025-12-07 17:30:24.737122: \n",
      "2025-12-07 17:30:24.737122: Epoch 141\n",
      "2025-12-07 17:30:24.737122: Current learning rate: 0.00872\n",
      "2025-12-07 17:32:43.481237: train_loss -0.8128\n",
      "2025-12-07 17:32:43.482977: val_loss -0.8421\n",
      "2025-12-07 17:32:43.486983: Pseudo dice [0.9103, 0.9398, 0.9376]\n",
      "2025-12-07 17:32:43.488985: Epoch time: 138.75 s\n",
      "2025-12-07 17:32:44.151053: \n",
      "2025-12-07 17:32:44.151053: Epoch 142\n",
      "2025-12-07 17:32:44.153138: Current learning rate: 0.00871\n",
      "2025-12-07 17:35:02.927681: train_loss -0.8063\n",
      "2025-12-07 17:35:02.927681: val_loss -0.8428\n",
      "2025-12-07 17:35:02.930252: Pseudo dice [0.9111, 0.9524, 0.925]\n",
      "2025-12-07 17:35:02.932253: Epoch time: 138.78 s\n",
      "2025-12-07 17:35:03.686587: \n",
      "2025-12-07 17:35:03.686587: Epoch 143\n",
      "2025-12-07 17:35:03.702412: Current learning rate: 0.0087\n",
      "2025-12-07 17:37:22.536846: train_loss -0.8017\n",
      "2025-12-07 17:37:22.536846: val_loss -0.8295\n",
      "2025-12-07 17:37:22.538847: Pseudo dice [0.8999, 0.9379, 0.9332]\n",
      "2025-12-07 17:37:22.540849: Epoch time: 138.85 s\n",
      "2025-12-07 17:37:22.542851: Yayy! New best EMA pseudo Dice: 0.9214\n",
      "2025-12-07 17:37:23.659793: \n",
      "2025-12-07 17:37:23.659793: Epoch 144\n",
      "2025-12-07 17:37:23.659793: Current learning rate: 0.00869\n",
      "2025-12-07 17:39:42.089901: train_loss -0.8111\n",
      "2025-12-07 17:39:42.091906: val_loss -0.8318\n",
      "2025-12-07 17:39:42.097652: Pseudo dice [0.9071, 0.9429, 0.923]\n",
      "2025-12-07 17:39:42.099656: Epoch time: 138.43 s\n",
      "2025-12-07 17:39:42.101661: Yayy! New best EMA pseudo Dice: 0.9217\n",
      "2025-12-07 17:39:43.017161: \n",
      "2025-12-07 17:39:43.017161: Epoch 145\n",
      "2025-12-07 17:39:43.017161: Current learning rate: 0.00868\n",
      "2025-12-07 17:42:01.058166: train_loss -0.8121\n",
      "2025-12-07 17:42:01.058166: val_loss -0.8244\n",
      "2025-12-07 17:42:01.061909: Pseudo dice [0.898, 0.9388, 0.9324]\n",
      "2025-12-07 17:42:01.063912: Epoch time: 138.04 s\n",
      "2025-12-07 17:42:01.063912: Yayy! New best EMA pseudo Dice: 0.9218\n",
      "2025-12-07 17:42:02.014707: \n",
      "2025-12-07 17:42:02.014707: Epoch 146\n",
      "2025-12-07 17:42:02.014707: Current learning rate: 0.00868\n",
      "2025-12-07 17:44:19.997125: train_loss -0.8097\n",
      "2025-12-07 17:44:19.997125: val_loss -0.8341\n",
      "2025-12-07 17:44:20.000709: Pseudo dice [0.903, 0.9405, 0.9291]\n",
      "2025-12-07 17:44:20.002711: Epoch time: 137.98 s\n",
      "2025-12-07 17:44:20.004713: Yayy! New best EMA pseudo Dice: 0.9221\n",
      "2025-12-07 17:44:20.917750: \n",
      "2025-12-07 17:44:20.918749: Epoch 147\n",
      "2025-12-07 17:44:20.920387: Current learning rate: 0.00867\n",
      "2025-12-07 17:46:39.014913: train_loss -0.816\n",
      "2025-12-07 17:46:39.014913: val_loss -0.8485\n",
      "2025-12-07 17:46:39.017622: Pseudo dice [0.9127, 0.9456, 0.9335]\n",
      "2025-12-07 17:46:39.017622: Epoch time: 138.1 s\n",
      "2025-12-07 17:46:39.017622: Yayy! New best EMA pseudo Dice: 0.9229\n",
      "2025-12-07 17:46:39.930253: \n",
      "2025-12-07 17:46:39.930253: Epoch 148\n",
      "2025-12-07 17:46:39.932803: Current learning rate: 0.00866\n",
      "2025-12-07 17:48:57.969177: train_loss -0.8076\n",
      "2025-12-07 17:48:57.969177: val_loss -0.8384\n",
      "2025-12-07 17:48:57.976707: Pseudo dice [0.9076, 0.95, 0.9251]\n",
      "2025-12-07 17:48:57.978712: Epoch time: 138.04 s\n",
      "2025-12-07 17:48:57.978712: Yayy! New best EMA pseudo Dice: 0.9234\n",
      "2025-12-07 17:48:58.901242: \n",
      "2025-12-07 17:48:58.901242: Epoch 149\n",
      "2025-12-07 17:48:58.903246: Current learning rate: 0.00865\n",
      "2025-12-07 17:51:16.936489: train_loss -0.8105\n",
      "2025-12-07 17:51:16.938336: val_loss -0.8429\n",
      "2025-12-07 17:51:16.940338: Pseudo dice [0.9131, 0.9449, 0.9286]\n",
      "2025-12-07 17:51:16.942340: Epoch time: 138.04 s\n",
      "2025-12-07 17:51:17.192648: Yayy! New best EMA pseudo Dice: 0.9239\n",
      "2025-12-07 17:51:18.294205: \n",
      "2025-12-07 17:51:18.295945: Epoch 150\n",
      "2025-12-07 17:51:18.298096: Current learning rate: 0.00864\n",
      "2025-12-07 17:53:36.215772: train_loss -0.8147\n",
      "2025-12-07 17:53:36.216774: val_loss -0.8351\n",
      "2025-12-07 17:53:36.217776: Pseudo dice [0.905, 0.9453, 0.9263]\n",
      "2025-12-07 17:53:36.217776: Epoch time: 137.92 s\n",
      "2025-12-07 17:53:36.217776: Yayy! New best EMA pseudo Dice: 0.9241\n",
      "2025-12-07 17:53:37.148480: \n",
      "2025-12-07 17:53:37.148480: Epoch 151\n",
      "2025-12-07 17:53:37.150528: Current learning rate: 0.00863\n",
      "2025-12-07 17:55:55.216359: train_loss -0.821\n",
      "2025-12-07 17:55:55.217359: val_loss -0.8297\n",
      "2025-12-07 17:55:55.219403: Pseudo dice [0.9043, 0.9366, 0.9287]\n",
      "2025-12-07 17:55:55.221403: Epoch time: 138.07 s\n",
      "2025-12-07 17:55:55.883945: \n",
      "2025-12-07 17:55:55.883945: Epoch 152\n",
      "2025-12-07 17:55:55.886334: Current learning rate: 0.00862\n",
      "2025-12-07 17:58:13.901354: train_loss -0.8151\n",
      "2025-12-07 17:58:13.901354: val_loss -0.8464\n",
      "2025-12-07 17:58:13.903094: Pseudo dice [0.9128, 0.9509, 0.9303]\n",
      "2025-12-07 17:58:13.905096: Epoch time: 138.02 s\n",
      "2025-12-07 17:58:13.907099: Yayy! New best EMA pseudo Dice: 0.9247\n",
      "2025-12-07 17:58:14.812714: \n",
      "2025-12-07 17:58:14.812714: Epoch 153\n",
      "2025-12-07 17:58:14.824982: Current learning rate: 0.00861\n",
      "2025-12-07 18:00:32.796708: train_loss -0.8181\n",
      "2025-12-07 18:00:32.796708: val_loss -0.8428\n",
      "2025-12-07 18:00:32.805098: Pseudo dice [0.9102, 0.9464, 0.9244]\n",
      "2025-12-07 18:00:32.805098: Epoch time: 137.98 s\n",
      "2025-12-07 18:00:32.805098: Yayy! New best EMA pseudo Dice: 0.925\n",
      "2025-12-07 18:00:33.717705: \n",
      "2025-12-07 18:00:33.717705: Epoch 154\n",
      "2025-12-07 18:00:33.717705: Current learning rate: 0.0086\n",
      "2025-12-07 18:02:51.797120: train_loss -0.8096\n",
      "2025-12-07 18:02:51.799124: val_loss -0.8315\n",
      "2025-12-07 18:02:51.801126: Pseudo dice [0.9022, 0.9381, 0.9271]\n",
      "2025-12-07 18:02:51.801126: Epoch time: 138.08 s\n",
      "2025-12-07 18:02:52.686293: \n",
      "2025-12-07 18:02:52.687296: Epoch 155\n",
      "2025-12-07 18:02:52.689314: Current learning rate: 0.00859\n",
      "2025-12-07 18:05:10.515537: train_loss -0.8249\n",
      "2025-12-07 18:05:10.515537: val_loss -0.84\n",
      "2025-12-07 18:05:10.515537: Pseudo dice [0.908, 0.9443, 0.9323]\n",
      "2025-12-07 18:05:10.524654: Epoch time: 137.83 s\n",
      "2025-12-07 18:05:10.524654: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2025-12-07 18:05:11.445362: \n",
      "2025-12-07 18:05:11.445362: Epoch 156\n",
      "2025-12-07 18:05:11.445362: Current learning rate: 0.00858\n",
      "2025-12-07 18:07:29.468419: train_loss -0.8198\n",
      "2025-12-07 18:07:29.468419: val_loss -0.8294\n",
      "2025-12-07 18:07:29.468419: Pseudo dice [0.9018, 0.9448, 0.9306]\n",
      "2025-12-07 18:07:29.482199: Epoch time: 138.02 s\n",
      "2025-12-07 18:07:29.484202: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2025-12-07 18:07:30.390563: \n",
      "2025-12-07 18:07:30.390563: Epoch 157\n",
      "2025-12-07 18:07:30.406241: Current learning rate: 0.00858\n",
      "2025-12-07 18:09:48.483290: train_loss -0.8227\n",
      "2025-12-07 18:09:48.484290: val_loss -0.8407\n",
      "2025-12-07 18:09:48.487290: Pseudo dice [0.9129, 0.9473, 0.926]\n",
      "2025-12-07 18:09:48.489291: Epoch time: 138.09 s\n",
      "2025-12-07 18:09:48.492486: Yayy! New best EMA pseudo Dice: 0.9255\n",
      "2025-12-07 18:09:49.572466: \n",
      "2025-12-07 18:09:49.572466: Epoch 158\n",
      "2025-12-07 18:09:49.572466: Current learning rate: 0.00857\n",
      "2025-12-07 18:12:07.422258: train_loss -0.8202\n",
      "2025-12-07 18:12:07.423407: val_loss -0.8358\n",
      "2025-12-07 18:12:07.425417: Pseudo dice [0.9077, 0.9398, 0.9263]\n",
      "2025-12-07 18:12:07.428424: Epoch time: 137.86 s\n",
      "2025-12-07 18:12:08.084292: \n",
      "2025-12-07 18:12:08.084292: Epoch 159\n",
      "2025-12-07 18:12:08.087030: Current learning rate: 0.00856\n",
      "2025-12-07 18:14:26.050752: train_loss -0.8172\n",
      "2025-12-07 18:14:26.050752: val_loss -0.8353\n",
      "2025-12-07 18:14:26.062822: Pseudo dice [0.9036, 0.9388, 0.9309]\n",
      "2025-12-07 18:14:26.062822: Epoch time: 137.97 s\n",
      "2025-12-07 18:14:26.719042: \n",
      "2025-12-07 18:14:26.719042: Epoch 160\n",
      "2025-12-07 18:14:26.734987: Current learning rate: 0.00855\n",
      "2025-12-07 18:16:44.701471: train_loss -0.8114\n",
      "2025-12-07 18:16:44.701471: val_loss -0.8426\n",
      "2025-12-07 18:16:44.701471: Pseudo dice [0.9114, 0.9477, 0.927]\n",
      "2025-12-07 18:16:44.701471: Epoch time: 137.98 s\n",
      "2025-12-07 18:16:44.701471: Yayy! New best EMA pseudo Dice: 0.9256\n",
      "2025-12-07 18:16:45.951902: \n",
      "2025-12-07 18:16:45.951902: Epoch 161\n",
      "2025-12-07 18:16:45.969745: Current learning rate: 0.00854\n",
      "2025-12-07 18:19:03.939879: train_loss -0.821\n",
      "2025-12-07 18:19:03.939879: val_loss -0.8272\n",
      "2025-12-07 18:19:03.942886: Pseudo dice [0.8942, 0.9334, 0.9273]\n",
      "2025-12-07 18:19:03.944944: Epoch time: 137.99 s\n",
      "2025-12-07 18:19:04.607598: \n",
      "2025-12-07 18:19:04.607598: Epoch 162\n",
      "2025-12-07 18:19:04.608600: Current learning rate: 0.00853\n",
      "2025-12-07 18:21:22.592535: train_loss -0.8177\n",
      "2025-12-07 18:21:22.592535: val_loss -0.8321\n",
      "2025-12-07 18:21:22.592535: Pseudo dice [0.908, 0.9433, 0.9248]\n",
      "2025-12-07 18:21:22.598449: Epoch time: 137.99 s\n",
      "2025-12-07 18:21:23.233416: \n",
      "2025-12-07 18:21:23.233416: Epoch 163\n",
      "2025-12-07 18:21:23.249347: Current learning rate: 0.00852\n",
      "2025-12-07 18:23:41.187200: train_loss -0.8207\n",
      "2025-12-07 18:23:41.188203: val_loss -0.8483\n",
      "2025-12-07 18:23:41.190207: Pseudo dice [0.9087, 0.9471, 0.9393]\n",
      "2025-12-07 18:23:41.192462: Epoch time: 137.95 s\n",
      "2025-12-07 18:23:41.960069: \n",
      "2025-12-07 18:23:41.961072: Epoch 164\n",
      "2025-12-07 18:23:41.963077: Current learning rate: 0.00851\n",
      "2025-12-07 18:26:00.074785: train_loss -0.8141\n",
      "2025-12-07 18:26:00.074785: val_loss -0.8365\n",
      "2025-12-07 18:26:00.077102: Pseudo dice [0.9054, 0.9399, 0.9308]\n",
      "2025-12-07 18:26:00.079104: Epoch time: 138.12 s\n",
      "2025-12-07 18:26:00.733460: \n",
      "2025-12-07 18:26:00.733460: Epoch 165\n",
      "2025-12-07 18:26:00.733460: Current learning rate: 0.0085\n",
      "2025-12-07 18:28:18.824225: train_loss -0.8197\n",
      "2025-12-07 18:28:18.824225: val_loss -0.8272\n",
      "2025-12-07 18:28:18.827729: Pseudo dice [0.9012, 0.9337, 0.9263]\n",
      "2025-12-07 18:28:18.829807: Epoch time: 138.09 s\n",
      "2025-12-07 18:28:19.477584: \n",
      "2025-12-07 18:28:19.477584: Epoch 166\n",
      "2025-12-07 18:28:19.477584: Current learning rate: 0.00849\n",
      "2025-12-07 18:30:37.590889: train_loss -0.816\n",
      "2025-12-07 18:30:37.591889: val_loss -0.8428\n",
      "2025-12-07 18:30:37.595044: Pseudo dice [0.907, 0.9432, 0.9294]\n",
      "2025-12-07 18:30:37.596044: Epoch time: 138.12 s\n",
      "2025-12-07 18:30:38.498636: \n",
      "2025-12-07 18:30:38.498636: Epoch 167\n",
      "2025-12-07 18:30:38.505867: Current learning rate: 0.00848\n",
      "2025-12-07 18:32:56.404827: train_loss -0.8164\n",
      "2025-12-07 18:32:56.404827: val_loss -0.8412\n",
      "2025-12-07 18:32:56.404827: Pseudo dice [0.9132, 0.9438, 0.9343]\n",
      "2025-12-07 18:32:56.420504: Epoch time: 137.91 s\n",
      "2025-12-07 18:32:56.420504: Yayy! New best EMA pseudo Dice: 0.9257\n",
      "2025-12-07 18:32:57.351302: \n",
      "2025-12-07 18:32:57.353304: Epoch 168\n",
      "2025-12-07 18:32:57.353304: Current learning rate: 0.00847\n",
      "2025-12-07 18:35:15.244566: train_loss -0.8269\n",
      "2025-12-07 18:35:15.244566: val_loss -0.849\n",
      "2025-12-07 18:35:15.248570: Pseudo dice [0.9148, 0.9472, 0.9373]\n",
      "2025-12-07 18:35:15.250572: Epoch time: 137.89 s\n",
      "2025-12-07 18:35:15.252575: Yayy! New best EMA pseudo Dice: 0.9265\n",
      "2025-12-07 18:35:16.213330: \n",
      "2025-12-07 18:35:16.213330: Epoch 169\n",
      "2025-12-07 18:35:16.213330: Current learning rate: 0.00847\n",
      "2025-12-07 18:37:34.295702: train_loss -0.8229\n",
      "2025-12-07 18:37:34.295702: val_loss -0.8571\n",
      "2025-12-07 18:37:34.295702: Pseudo dice [0.9244, 0.9495, 0.9374]\n",
      "2025-12-07 18:37:34.295702: Epoch time: 138.08 s\n",
      "2025-12-07 18:37:34.311500: Yayy! New best EMA pseudo Dice: 0.9275\n",
      "2025-12-07 18:37:35.279917: \n",
      "2025-12-07 18:37:35.279917: Epoch 170\n",
      "2025-12-07 18:37:35.279917: Current learning rate: 0.00846\n",
      "2025-12-07 18:39:53.562235: train_loss -0.8204\n",
      "2025-12-07 18:39:53.578196: val_loss -0.8447\n",
      "2025-12-07 18:39:53.578196: Pseudo dice [0.9127, 0.9456, 0.9264]\n",
      "2025-12-07 18:39:53.578196: Epoch time: 138.28 s\n",
      "2025-12-07 18:39:53.578196: Yayy! New best EMA pseudo Dice: 0.9276\n",
      "2025-12-07 18:39:54.486213: \n",
      "2025-12-07 18:39:54.486213: Epoch 171\n",
      "2025-12-07 18:39:54.489742: Current learning rate: 0.00845\n",
      "2025-12-07 18:42:12.404204: train_loss -0.818\n",
      "2025-12-07 18:42:12.404204: val_loss -0.8347\n",
      "2025-12-07 18:42:12.407221: Pseudo dice [0.9028, 0.9419, 0.9343]\n",
      "2025-12-07 18:42:12.408319: Epoch time: 137.92 s\n",
      "2025-12-07 18:42:13.045727: \n",
      "2025-12-07 18:42:13.045727: Epoch 172\n",
      "2025-12-07 18:42:13.045727: Current learning rate: 0.00844\n",
      "2025-12-07 18:44:31.201588: train_loss -0.8208\n",
      "2025-12-07 18:44:31.203329: val_loss -0.8364\n",
      "2025-12-07 18:44:31.203329: Pseudo dice [0.9068, 0.9445, 0.9271]\n",
      "2025-12-07 18:44:31.203329: Epoch time: 138.16 s\n",
      "2025-12-07 18:44:32.171478: \n",
      "2025-12-07 18:44:32.171478: Epoch 173\n",
      "2025-12-07 18:44:32.171478: Current learning rate: 0.00843\n",
      "2025-12-07 18:46:50.074674: train_loss -0.8237\n",
      "2025-12-07 18:46:50.074674: val_loss -0.846\n",
      "2025-12-07 18:46:50.078722: Pseudo dice [0.9114, 0.9458, 0.9368]\n",
      "2025-12-07 18:46:50.080723: Epoch time: 137.9 s\n",
      "2025-12-07 18:46:50.083727: Yayy! New best EMA pseudo Dice: 0.9278\n",
      "2025-12-07 18:46:50.999908: \n",
      "2025-12-07 18:46:50.999908: Epoch 174\n",
      "2025-12-07 18:46:50.999908: Current learning rate: 0.00842\n",
      "2025-12-07 18:49:09.075900: train_loss -0.8222\n",
      "2025-12-07 18:49:09.077402: val_loss -0.8432\n",
      "2025-12-07 18:49:09.079404: Pseudo dice [0.9121, 0.9439, 0.9297]\n",
      "2025-12-07 18:49:09.079404: Epoch time: 138.08 s\n",
      "2025-12-07 18:49:09.082844: Yayy! New best EMA pseudo Dice: 0.9278\n",
      "2025-12-07 18:49:09.998808: \n",
      "2025-12-07 18:49:09.998808: Epoch 175\n",
      "2025-12-07 18:49:09.998808: Current learning rate: 0.00841\n",
      "2025-12-07 18:51:28.003251: train_loss -0.8225\n",
      "2025-12-07 18:51:28.003251: val_loss -0.8305\n",
      "2025-12-07 18:51:28.006258: Pseudo dice [0.9037, 0.9343, 0.9351]\n",
      "2025-12-07 18:51:28.008262: Epoch time: 138.01 s\n",
      "2025-12-07 18:51:28.777461: \n",
      "2025-12-07 18:51:28.777461: Epoch 176\n",
      "2025-12-07 18:51:28.780472: Current learning rate: 0.0084\n",
      "2025-12-07 18:53:46.807074: train_loss -0.82\n",
      "2025-12-07 18:53:46.808074: val_loss -0.8381\n",
      "2025-12-07 18:53:46.810096: Pseudo dice [0.9015, 0.9411, 0.9418]\n",
      "2025-12-07 18:53:46.811098: Epoch time: 138.03 s\n",
      "2025-12-07 18:53:47.467961: \n",
      "2025-12-07 18:53:47.467961: Epoch 177\n",
      "2025-12-07 18:53:47.467961: Current learning rate: 0.00839\n",
      "2025-12-07 18:56:05.474582: train_loss -0.8167\n",
      "2025-12-07 18:56:05.474582: val_loss -0.8488\n",
      "2025-12-07 18:56:05.477583: Pseudo dice [0.9109, 0.9509, 0.9309]\n",
      "2025-12-07 18:56:05.479593: Epoch time: 138.01 s\n",
      "2025-12-07 18:56:05.481598: Yayy! New best EMA pseudo Dice: 0.9279\n",
      "2025-12-07 18:56:06.577782: \n",
      "2025-12-07 18:56:06.577782: Epoch 178\n",
      "2025-12-07 18:56:06.577782: Current learning rate: 0.00838\n",
      "2025-12-07 18:58:24.343248: train_loss -0.8201\n",
      "2025-12-07 18:58:24.343248: val_loss -0.8374\n",
      "2025-12-07 18:58:24.347252: Pseudo dice [0.9085, 0.9379, 0.93]\n",
      "2025-12-07 18:58:24.347252: Epoch time: 137.77 s\n",
      "2025-12-07 18:58:25.117040: \n",
      "2025-12-07 18:58:25.117040: Epoch 179\n",
      "2025-12-07 18:58:25.117040: Current learning rate: 0.00837\n",
      "2025-12-07 19:00:43.133048: train_loss -0.8219\n",
      "2025-12-07 19:00:43.134050: val_loss -0.8527\n",
      "2025-12-07 19:00:43.137057: Pseudo dice [0.9169, 0.9428, 0.9412]\n",
      "2025-12-07 19:00:43.139068: Epoch time: 138.02 s\n",
      "2025-12-07 19:00:43.139068: Yayy! New best EMA pseudo Dice: 0.9282\n",
      "2025-12-07 19:00:44.030402: \n",
      "2025-12-07 19:00:44.030402: Epoch 180\n",
      "2025-12-07 19:00:44.046318: Current learning rate: 0.00836\n",
      "2025-12-07 19:03:02.109035: train_loss -0.8195\n",
      "2025-12-07 19:03:02.109035: val_loss -0.8392\n",
      "2025-12-07 19:03:02.109035: Pseudo dice [0.9078, 0.9451, 0.9347]\n",
      "2025-12-07 19:03:02.125158: Epoch time: 138.08 s\n",
      "2025-12-07 19:03:02.127405: Yayy! New best EMA pseudo Dice: 0.9283\n",
      "2025-12-07 19:03:03.071138: \n",
      "2025-12-07 19:03:03.071138: Epoch 181\n",
      "2025-12-07 19:03:03.071138: Current learning rate: 0.00836\n",
      "2025-12-07 19:05:20.875661: train_loss -0.8243\n",
      "2025-12-07 19:05:20.875661: val_loss -0.8422\n",
      "2025-12-07 19:05:20.881678: Pseudo dice [0.9062, 0.9414, 0.939]\n",
      "2025-12-07 19:05:20.885689: Epoch time: 137.81 s\n",
      "2025-12-07 19:05:20.887496: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2025-12-07 19:05:22.007797: \n",
      "2025-12-07 19:05:22.007797: Epoch 182\n",
      "2025-12-07 19:05:22.009802: Current learning rate: 0.00835\n",
      "2025-12-07 19:07:40.175119: train_loss -0.8224\n",
      "2025-12-07 19:07:40.175119: val_loss -0.8345\n",
      "2025-12-07 19:07:40.178126: Pseudo dice [0.9069, 0.9406, 0.9318]\n",
      "2025-12-07 19:07:40.180132: Epoch time: 138.17 s\n",
      "2025-12-07 19:07:40.811682: \n",
      "2025-12-07 19:07:40.811682: Epoch 183\n",
      "2025-12-07 19:07:40.811682: Current learning rate: 0.00834\n",
      "2025-12-07 19:09:58.655071: train_loss -0.8258\n",
      "2025-12-07 19:09:58.655071: val_loss -0.8466\n",
      "2025-12-07 19:09:58.655071: Pseudo dice [0.91, 0.9447, 0.9372]\n",
      "2025-12-07 19:09:58.655071: Epoch time: 137.84 s\n",
      "2025-12-07 19:09:58.655071: Yayy! New best EMA pseudo Dice: 0.9284\n",
      "2025-12-07 19:09:59.749969: \n",
      "2025-12-07 19:09:59.749969: Epoch 184\n",
      "2025-12-07 19:09:59.749969: Current learning rate: 0.00833\n",
      "2025-12-07 19:12:17.827423: train_loss -0.8188\n",
      "2025-12-07 19:12:17.833464: val_loss -0.8451\n",
      "2025-12-07 19:12:17.835465: Pseudo dice [0.9057, 0.9461, 0.9348]\n",
      "2025-12-07 19:12:17.837468: Epoch time: 138.08 s\n",
      "2025-12-07 19:12:17.839470: Yayy! New best EMA pseudo Dice: 0.9285\n",
      "2025-12-07 19:12:18.853768: \n",
      "2025-12-07 19:12:18.853768: Epoch 185\n",
      "2025-12-07 19:12:18.856777: Current learning rate: 0.00832\n",
      "2025-12-07 19:14:36.909649: train_loss -0.8167\n",
      "2025-12-07 19:14:36.909649: val_loss -0.8371\n",
      "2025-12-07 19:14:36.913653: Pseudo dice [0.9056, 0.9451, 0.9298]\n",
      "2025-12-07 19:14:36.915655: Epoch time: 138.06 s\n",
      "2025-12-07 19:14:37.566916: \n",
      "2025-12-07 19:14:37.568918: Epoch 186\n",
      "2025-12-07 19:14:37.568918: Current learning rate: 0.00831\n",
      "2025-12-07 19:16:55.717859: train_loss -0.816\n",
      "2025-12-07 19:16:55.717859: val_loss -0.8362\n",
      "2025-12-07 19:16:55.717859: Pseudo dice [0.9042, 0.9403, 0.9279]\n",
      "2025-12-07 19:16:55.717859: Epoch time: 138.15 s\n",
      "2025-12-07 19:16:56.367774: \n",
      "2025-12-07 19:16:56.367774: Epoch 187\n",
      "2025-12-07 19:16:56.367774: Current learning rate: 0.0083\n",
      "2025-12-07 19:19:14.427472: train_loss -0.8212\n",
      "2025-12-07 19:19:14.429475: val_loss -0.8554\n",
      "2025-12-07 19:19:14.431478: Pseudo dice [0.9188, 0.9529, 0.9318]\n",
      "2025-12-07 19:19:14.433481: Epoch time: 138.06 s\n",
      "2025-12-07 19:19:14.435483: Yayy! New best EMA pseudo Dice: 0.9286\n",
      "2025-12-07 19:19:15.373972: \n",
      "2025-12-07 19:19:15.373972: Epoch 188\n",
      "2025-12-07 19:19:15.377150: Current learning rate: 0.00829\n",
      "2025-12-07 19:21:33.502789: train_loss -0.8243\n",
      "2025-12-07 19:21:33.502789: val_loss -0.8516\n",
      "2025-12-07 19:21:33.506793: Pseudo dice [0.9168, 0.9495, 0.9336]\n",
      "2025-12-07 19:21:33.508794: Epoch time: 138.13 s\n",
      "2025-12-07 19:21:33.511840: Yayy! New best EMA pseudo Dice: 0.929\n",
      "2025-12-07 19:21:34.427148: \n",
      "2025-12-07 19:21:34.428889: Epoch 189\n",
      "2025-12-07 19:21:34.428889: Current learning rate: 0.00828\n",
      "2025-12-07 19:23:53.672543: train_loss -0.8254\n",
      "2025-12-07 19:23:53.673545: val_loss -0.8502\n",
      "2025-12-07 19:23:53.676548: Pseudo dice [0.9103, 0.9486, 0.9401]\n",
      "2025-12-07 19:23:53.678548: Epoch time: 139.25 s\n",
      "2025-12-07 19:23:53.680549: Yayy! New best EMA pseudo Dice: 0.9294\n",
      "2025-12-07 19:23:54.772289: \n",
      "2025-12-07 19:23:54.772289: Epoch 190\n",
      "2025-12-07 19:23:54.772289: Current learning rate: 0.00827\n",
      "2025-12-07 19:26:13.140501: train_loss -0.8251\n",
      "2025-12-07 19:26:13.140501: val_loss -0.8374\n",
      "2025-12-07 19:26:13.140501: Pseudo dice [0.9026, 0.9453, 0.9319]\n",
      "2025-12-07 19:26:13.148446: Epoch time: 138.37 s\n",
      "2025-12-07 19:26:13.967290: \n",
      "2025-12-07 19:26:13.967290: Epoch 191\n",
      "2025-12-07 19:26:13.972101: Current learning rate: 0.00826\n",
      "2025-12-07 19:28:32.276417: train_loss -0.8236\n",
      "2025-12-07 19:28:32.276417: val_loss -0.8456\n",
      "2025-12-07 19:28:32.282434: Pseudo dice [0.9068, 0.9443, 0.9371]\n",
      "2025-12-07 19:28:32.286445: Epoch time: 138.31 s\n",
      "2025-12-07 19:28:32.954730: \n",
      "2025-12-07 19:28:32.954730: Epoch 192\n",
      "2025-12-07 19:28:32.957001: Current learning rate: 0.00825\n",
      "2025-12-07 19:30:51.385888: train_loss -0.8112\n",
      "2025-12-07 19:30:51.387890: val_loss -0.8407\n",
      "2025-12-07 19:30:51.389392: Pseudo dice [0.9098, 0.9455, 0.929]\n",
      "2025-12-07 19:30:51.389392: Epoch time: 138.43 s\n",
      "2025-12-07 19:30:52.060239: \n",
      "2025-12-07 19:30:52.060239: Epoch 193\n",
      "2025-12-07 19:30:52.061743: Current learning rate: 0.00824\n",
      "2025-12-07 19:33:10.416735: train_loss -0.8228\n",
      "2025-12-07 19:33:10.416735: val_loss -0.8545\n",
      "2025-12-07 19:33:10.418738: Pseudo dice [0.9146, 0.9489, 0.9404]\n",
      "2025-12-07 19:33:10.420479: Epoch time: 138.36 s\n",
      "2025-12-07 19:33:10.420479: Yayy! New best EMA pseudo Dice: 0.9296\n",
      "2025-12-07 19:33:11.365000: \n",
      "2025-12-07 19:33:11.366005: Epoch 194\n",
      "2025-12-07 19:33:11.367747: Current learning rate: 0.00824\n",
      "2025-12-07 19:35:29.217067: train_loss -0.8256\n",
      "2025-12-07 19:35:29.217067: val_loss -0.833\n",
      "2025-12-07 19:35:29.234911: Pseudo dice [0.8962, 0.9376, 0.9317]\n",
      "2025-12-07 19:35:29.236913: Epoch time: 137.85 s\n",
      "2025-12-07 19:35:29.889876: \n",
      "2025-12-07 19:35:29.889876: Epoch 195\n",
      "2025-12-07 19:35:29.889876: Current learning rate: 0.00823\n",
      "2025-12-07 19:37:47.970228: train_loss -0.8265\n",
      "2025-12-07 19:37:47.970228: val_loss -0.8417\n",
      "2025-12-07 19:37:47.972231: Pseudo dice [0.9111, 0.9429, 0.9266]\n",
      "2025-12-07 19:37:47.972231: Epoch time: 138.08 s\n",
      "2025-12-07 19:37:48.889841: \n",
      "2025-12-07 19:37:48.889841: Epoch 196\n",
      "2025-12-07 19:37:48.889841: Current learning rate: 0.00822\n",
      "2025-12-07 19:40:07.002556: train_loss -0.8238\n",
      "2025-12-07 19:40:07.002556: val_loss -0.8395\n",
      "2025-12-07 19:40:07.005558: Pseudo dice [0.9061, 0.9477, 0.9338]\n",
      "2025-12-07 19:40:07.007559: Epoch time: 138.11 s\n",
      "2025-12-07 19:40:07.765582: \n",
      "2025-12-07 19:40:07.765582: Epoch 197\n",
      "2025-12-07 19:40:07.781512: Current learning rate: 0.00821\n",
      "2025-12-07 19:42:25.835589: train_loss -0.82\n",
      "2025-12-07 19:42:25.835589: val_loss -0.8436\n",
      "2025-12-07 19:42:25.845140: Pseudo dice [0.9152, 0.945, 0.9288]\n",
      "2025-12-07 19:42:25.847142: Epoch time: 138.07 s\n",
      "2025-12-07 19:42:26.499300: \n",
      "2025-12-07 19:42:26.499300: Epoch 198\n",
      "2025-12-07 19:42:26.514967: Current learning rate: 0.0082\n",
      "2025-12-07 19:44:44.458312: train_loss -0.8279\n",
      "2025-12-07 19:44:44.459312: val_loss -0.8525\n",
      "2025-12-07 19:44:44.461313: Pseudo dice [0.9124, 0.9574, 0.9283]\n",
      "2025-12-07 19:44:44.464313: Epoch time: 137.96 s\n",
      "2025-12-07 19:44:45.123506: \n",
      "2025-12-07 19:44:45.123506: Epoch 199\n",
      "2025-12-07 19:44:45.123506: Current learning rate: 0.00819\n",
      "2025-12-07 19:47:02.950369: train_loss -0.8179\n",
      "2025-12-07 19:47:02.950369: val_loss -0.847\n",
      "2025-12-07 19:47:02.952371: Pseudo dice [0.9144, 0.9478, 0.928]\n",
      "2025-12-07 19:47:02.955392: Epoch time: 137.83 s\n",
      "2025-12-07 19:47:04.034540: \n",
      "2025-12-07 19:47:04.034540: Epoch 200\n",
      "2025-12-07 19:47:04.034540: Current learning rate: 0.00818\n",
      "2025-12-07 19:49:22.014951: train_loss -0.8212\n",
      "2025-12-07 19:49:22.014951: val_loss -0.835\n",
      "2025-12-07 19:49:22.024478: Pseudo dice [0.901, 0.9364, 0.9307]\n",
      "2025-12-07 19:49:22.027309: Epoch time: 137.98 s\n",
      "2025-12-07 19:49:22.846105: \n",
      "2025-12-07 19:49:22.847108: Epoch 201\n",
      "2025-12-07 19:49:22.849117: Current learning rate: 0.00817\n",
      "2025-12-07 19:51:40.733444: train_loss -0.8258\n",
      "2025-12-07 19:51:40.733444: val_loss -0.8512\n",
      "2025-12-07 19:51:40.740831: Pseudo dice [0.915, 0.949, 0.9327]\n",
      "2025-12-07 19:51:40.742833: Epoch time: 137.89 s\n",
      "2025-12-07 19:51:41.404788: \n",
      "2025-12-07 19:51:41.404788: Epoch 202\n",
      "2025-12-07 19:51:41.408677: Current learning rate: 0.00816\n",
      "2025-12-07 19:53:59.282296: train_loss -0.8259\n",
      "2025-12-07 19:53:59.284299: val_loss -0.8431\n",
      "2025-12-07 19:53:59.286301: Pseudo dice [0.9097, 0.9404, 0.9382]\n",
      "2025-12-07 19:53:59.290306: Epoch time: 137.89 s\n",
      "2025-12-07 19:54:00.061769: \n",
      "2025-12-07 19:54:00.061769: Epoch 203\n",
      "2025-12-07 19:54:00.077420: Current learning rate: 0.00815\n",
      "2025-12-07 19:56:18.106199: train_loss -0.8202\n",
      "2025-12-07 19:56:18.106199: val_loss -0.845\n",
      "2025-12-07 19:56:18.108218: Pseudo dice [0.9073, 0.9494, 0.935]\n",
      "2025-12-07 19:56:18.108218: Epoch time: 138.04 s\n",
      "2025-12-07 19:56:18.764354: \n",
      "2025-12-07 19:56:18.764354: Epoch 204\n",
      "2025-12-07 19:56:18.764354: Current learning rate: 0.00814\n",
      "2025-12-07 19:58:36.915333: train_loss -0.8178\n",
      "2025-12-07 19:58:36.917335: val_loss -0.838\n",
      "2025-12-07 19:58:36.921079: Pseudo dice [0.9059, 0.9406, 0.9339]\n",
      "2025-12-07 19:58:36.923083: Epoch time: 138.15 s\n",
      "2025-12-07 19:58:37.592671: \n",
      "2025-12-07 19:58:37.592671: Epoch 205\n",
      "2025-12-07 19:58:37.592671: Current learning rate: 0.00813\n",
      "2025-12-07 20:00:55.452054: train_loss -0.8209\n",
      "2025-12-07 20:00:55.452054: val_loss -0.853\n",
      "2025-12-07 20:00:55.457753: Pseudo dice [0.9157, 0.9495, 0.9339]\n",
      "2025-12-07 20:00:55.457753: Epoch time: 137.86 s\n",
      "2025-12-07 20:00:56.078774: \n",
      "2025-12-07 20:00:56.078774: Epoch 206\n",
      "2025-12-07 20:00:56.081788: Current learning rate: 0.00813\n",
      "2025-12-07 20:03:14.140695: train_loss -0.8227\n",
      "2025-12-07 20:03:14.140695: val_loss -0.8243\n",
      "2025-12-07 20:03:14.140695: Pseudo dice [0.8941, 0.9312, 0.9341]\n",
      "2025-12-07 20:03:14.146950: Epoch time: 138.06 s\n",
      "2025-12-07 20:03:14.780078: \n",
      "2025-12-07 20:03:14.780078: Epoch 207\n",
      "2025-12-07 20:03:14.780078: Current learning rate: 0.00812\n",
      "2025-12-07 20:05:32.722013: train_loss -0.7927\n",
      "2025-12-07 20:05:32.722013: val_loss -0.8266\n",
      "2025-12-07 20:05:32.722013: Pseudo dice [0.9038, 0.9395, 0.92]\n",
      "2025-12-07 20:05:32.722013: Epoch time: 137.94 s\n",
      "2025-12-07 20:05:33.548093: \n",
      "2025-12-07 20:05:33.548093: Epoch 208\n",
      "2025-12-07 20:05:33.548093: Current learning rate: 0.00811\n",
      "2025-12-07 20:07:51.458176: train_loss -0.7879\n",
      "2025-12-07 20:07:51.458176: val_loss -0.8138\n",
      "2025-12-07 20:07:51.461197: Pseudo dice [0.8964, 0.9343, 0.9236]\n",
      "2025-12-07 20:07:51.463201: Epoch time: 137.91 s\n",
      "2025-12-07 20:07:52.217158: \n",
      "2025-12-07 20:07:52.232817: Epoch 209\n",
      "2025-12-07 20:07:52.232817: Current learning rate: 0.0081\n",
      "2025-12-07 20:10:10.092541: train_loss -0.8041\n",
      "2025-12-07 20:10:10.092541: val_loss -0.8375\n",
      "2025-12-07 20:10:10.092541: Pseudo dice [0.905, 0.941, 0.9282]\n",
      "2025-12-07 20:10:10.092541: Epoch time: 137.88 s\n",
      "2025-12-07 20:10:10.727145: \n",
      "2025-12-07 20:10:10.727145: Epoch 210\n",
      "2025-12-07 20:10:10.729157: Current learning rate: 0.00809\n",
      "2025-12-07 20:12:28.689175: train_loss -0.8133\n",
      "2025-12-07 20:12:28.689175: val_loss -0.8391\n",
      "2025-12-07 20:12:28.689175: Pseudo dice [0.9145, 0.9437, 0.9288]\n",
      "2025-12-07 20:12:28.689175: Epoch time: 137.96 s\n",
      "2025-12-07 20:12:29.327861: \n",
      "2025-12-07 20:12:29.327861: Epoch 211\n",
      "2025-12-07 20:12:29.327861: Current learning rate: 0.00808\n",
      "2025-12-07 20:14:47.327923: train_loss -0.812\n",
      "2025-12-07 20:14:47.327923: val_loss -0.8363\n",
      "2025-12-07 20:14:47.327923: Pseudo dice [0.9096, 0.945, 0.9256]\n",
      "2025-12-07 20:14:47.327923: Epoch time: 138.0 s\n",
      "2025-12-07 20:14:47.973922: \n",
      "2025-12-07 20:14:47.973922: Epoch 212\n",
      "2025-12-07 20:14:47.973922: Current learning rate: 0.00807\n",
      "2025-12-07 20:17:06.132528: train_loss -0.812\n",
      "2025-12-07 20:17:06.133530: val_loss -0.8395\n",
      "2025-12-07 20:17:06.136547: Pseudo dice [0.9105, 0.9498, 0.9238]\n",
      "2025-12-07 20:17:06.138554: Epoch time: 138.16 s\n",
      "2025-12-07 20:17:06.749240: \n",
      "2025-12-07 20:17:06.749240: Epoch 213\n",
      "2025-12-07 20:17:06.749240: Current learning rate: 0.00806\n",
      "2025-12-07 20:19:24.578490: train_loss -0.8107\n",
      "2025-12-07 20:19:24.578490: val_loss -0.8473\n",
      "2025-12-07 20:19:24.594319: Pseudo dice [0.915, 0.9518, 0.9315]\n",
      "2025-12-07 20:19:24.594319: Epoch time: 137.83 s\n",
      "2025-12-07 20:19:25.486346: \n",
      "2025-12-07 20:19:25.486346: Epoch 214\n",
      "2025-12-07 20:19:25.489359: Current learning rate: 0.00805\n",
      "2025-12-07 20:21:43.358577: train_loss -0.8168\n",
      "2025-12-07 20:21:43.358577: val_loss -0.8381\n",
      "2025-12-07 20:21:43.358577: Pseudo dice [0.9071, 0.9456, 0.9228]\n",
      "2025-12-07 20:21:43.358577: Epoch time: 137.87 s\n",
      "2025-12-07 20:21:43.983007: \n",
      "2025-12-07 20:21:43.983007: Epoch 215\n",
      "2025-12-07 20:21:43.983007: Current learning rate: 0.00804\n",
      "2025-12-07 20:24:01.857908: train_loss -0.8182\n",
      "2025-12-07 20:24:01.857908: val_loss -0.8425\n",
      "2025-12-07 20:24:01.863152: Pseudo dice [0.9107, 0.9429, 0.9345]\n",
      "2025-12-07 20:24:01.865155: Epoch time: 137.87 s\n",
      "2025-12-07 20:24:02.499253: \n",
      "2025-12-07 20:24:02.499253: Epoch 216\n",
      "2025-12-07 20:24:02.499253: Current learning rate: 0.00803\n",
      "2025-12-07 20:26:20.530557: train_loss -0.825\n",
      "2025-12-07 20:26:20.530557: val_loss -0.8476\n",
      "2025-12-07 20:26:20.530557: Pseudo dice [0.9153, 0.9458, 0.9377]\n",
      "2025-12-07 20:26:20.530557: Epoch time: 138.03 s\n",
      "2025-12-07 20:26:21.311542: \n",
      "2025-12-07 20:26:21.311542: Epoch 217\n",
      "2025-12-07 20:26:21.327261: Current learning rate: 0.00802\n",
      "2025-12-07 20:28:39.421726: train_loss -0.8189\n",
      "2025-12-07 20:28:39.421726: val_loss -0.8397\n",
      "2025-12-07 20:28:39.421726: Pseudo dice [0.906, 0.9436, 0.9337]\n",
      "2025-12-07 20:28:39.421726: Epoch time: 138.11 s\n",
      "2025-12-07 20:28:40.045655: \n",
      "2025-12-07 20:28:40.045655: Epoch 218\n",
      "2025-12-07 20:28:40.061619: Current learning rate: 0.00801\n",
      "2025-12-07 20:30:57.906782: train_loss -0.8248\n",
      "2025-12-07 20:30:57.906782: val_loss -0.8412\n",
      "2025-12-07 20:30:57.906782: Pseudo dice [0.9059, 0.9461, 0.9345]\n",
      "2025-12-07 20:30:57.906782: Epoch time: 137.86 s\n",
      "2025-12-07 20:30:58.562199: \n",
      "2025-12-07 20:30:58.562199: Epoch 219\n",
      "2025-12-07 20:30:58.562199: Current learning rate: 0.00801\n",
      "2025-12-07 20:33:16.500032: train_loss -0.8201\n",
      "2025-12-07 20:33:16.500032: val_loss -0.8524\n",
      "2025-12-07 20:33:16.500032: Pseudo dice [0.9192, 0.9524, 0.9255]\n",
      "2025-12-07 20:33:16.505838: Epoch time: 137.95 s\n",
      "2025-12-07 20:33:17.452533: \n",
      "2025-12-07 20:33:17.452533: Epoch 220\n",
      "2025-12-07 20:33:17.468335: Current learning rate: 0.008\n",
      "2025-12-07 20:35:35.382198: train_loss -0.8239\n",
      "2025-12-07 20:35:35.382198: val_loss -0.8395\n",
      "2025-12-07 20:35:35.386201: Pseudo dice [0.9031, 0.938, 0.9363]\n",
      "2025-12-07 20:35:35.388203: Epoch time: 137.93 s\n",
      "2025-12-07 20:35:36.018170: \n",
      "2025-12-07 20:35:36.018170: Epoch 221\n",
      "2025-12-07 20:35:36.018170: Current learning rate: 0.00799\n",
      "2025-12-07 20:37:53.940255: train_loss -0.8247\n",
      "2025-12-07 20:37:53.940255: val_loss -0.8469\n",
      "2025-12-07 20:37:53.943261: Pseudo dice [0.9104, 0.9463, 0.94]\n",
      "2025-12-07 20:37:53.945273: Epoch time: 137.92 s\n",
      "2025-12-07 20:37:54.592773: \n",
      "2025-12-07 20:37:54.592773: Epoch 222\n",
      "2025-12-07 20:37:54.608532: Current learning rate: 0.00798\n",
      "2025-12-07 20:40:12.644933: train_loss -0.8229\n",
      "2025-12-07 20:40:12.644933: val_loss -0.8444\n",
      "2025-12-07 20:40:12.648402: Pseudo dice [0.9086, 0.9472, 0.9347]\n",
      "2025-12-07 20:40:12.650404: Epoch time: 138.05 s\n",
      "2025-12-07 20:40:13.375329: \n",
      "2025-12-07 20:40:13.375329: Epoch 223\n",
      "2025-12-07 20:40:13.375329: Current learning rate: 0.00797\n",
      "2025-12-07 20:42:31.167648: train_loss -0.8253\n",
      "2025-12-07 20:42:31.169650: val_loss -0.8679\n",
      "2025-12-07 20:42:31.172509: Pseudo dice [0.9209, 0.9526, 0.9439]\n",
      "2025-12-07 20:42:31.175515: Epoch time: 137.79 s\n",
      "2025-12-07 20:42:31.177521: Yayy! New best EMA pseudo Dice: 0.9298\n",
      "2025-12-07 20:42:32.060571: \n",
      "2025-12-07 20:42:32.060571: Epoch 224\n",
      "2025-12-07 20:42:32.061657: Current learning rate: 0.00796\n",
      "2025-12-07 20:44:51.216740: train_loss -0.8328\n",
      "2025-12-07 20:44:51.216740: val_loss -0.8551\n",
      "2025-12-07 20:44:51.220483: Pseudo dice [0.9164, 0.9462, 0.9407]\n",
      "2025-12-07 20:44:51.222485: Epoch time: 139.16 s\n",
      "2025-12-07 20:44:51.224486: Yayy! New best EMA pseudo Dice: 0.9303\n",
      "2025-12-07 20:44:52.128910: \n",
      "2025-12-07 20:44:52.130913: Epoch 225\n",
      "2025-12-07 20:44:52.130913: Current learning rate: 0.00795\n",
      "2025-12-07 20:47:10.608327: train_loss -0.8276\n",
      "2025-12-07 20:47:10.608327: val_loss -0.8577\n",
      "2025-12-07 20:47:10.608327: Pseudo dice [0.9198, 0.9495, 0.9364]\n",
      "2025-12-07 20:47:10.608327: Epoch time: 138.48 s\n",
      "2025-12-07 20:47:10.608327: Yayy! New best EMA pseudo Dice: 0.9308\n",
      "2025-12-07 20:47:11.779070: \n",
      "2025-12-07 20:47:11.779070: Epoch 226\n",
      "2025-12-07 20:47:11.795063: Current learning rate: 0.00794\n",
      "2025-12-07 20:49:30.182772: train_loss -0.8235\n",
      "2025-12-07 20:49:30.182772: val_loss -0.8511\n",
      "2025-12-07 20:49:30.186777: Pseudo dice [0.9103, 0.9449, 0.9419]\n",
      "2025-12-07 20:49:30.188016: Epoch time: 138.4 s\n",
      "2025-12-07 20:49:30.191017: Yayy! New best EMA pseudo Dice: 0.9309\n",
      "2025-12-07 20:49:31.077151: \n",
      "2025-12-07 20:49:31.077151: Epoch 227\n",
      "2025-12-07 20:49:31.077151: Current learning rate: 0.00793\n",
      "2025-12-07 20:51:49.391018: train_loss -0.8273\n",
      "2025-12-07 20:51:49.391018: val_loss -0.8516\n",
      "2025-12-07 20:51:49.406980: Pseudo dice [0.9151, 0.9473, 0.9363]\n",
      "2025-12-07 20:51:49.409112: Epoch time: 138.31 s\n",
      "2025-12-07 20:51:49.410115: Yayy! New best EMA pseudo Dice: 0.9311\n",
      "2025-12-07 20:51:50.297872: \n",
      "2025-12-07 20:51:50.297872: Epoch 228\n",
      "2025-12-07 20:51:50.297872: Current learning rate: 0.00792\n",
      "2025-12-07 20:54:08.764357: train_loss -0.8291\n",
      "2025-12-07 20:54:08.764357: val_loss -0.8466\n",
      "2025-12-07 20:54:08.773907: Pseudo dice [0.912, 0.9508, 0.9384]\n",
      "2025-12-07 20:54:08.775910: Epoch time: 138.47 s\n",
      "2025-12-07 20:54:08.777912: Yayy! New best EMA pseudo Dice: 0.9314\n",
      "2025-12-07 20:54:09.670253: \n",
      "2025-12-07 20:54:09.670253: Epoch 229\n",
      "2025-12-07 20:54:09.672210: Current learning rate: 0.00791\n",
      "2025-12-07 20:56:27.608752: train_loss -0.8284\n",
      "2025-12-07 20:56:27.608752: val_loss -0.8492\n",
      "2025-12-07 20:56:27.614924: Pseudo dice [0.9098, 0.9493, 0.9386]\n",
      "2025-12-07 20:56:27.617476: Epoch time: 137.94 s\n",
      "2025-12-07 20:56:27.619478: Yayy! New best EMA pseudo Dice: 0.9315\n",
      "2025-12-07 20:56:28.498840: \n",
      "2025-12-07 20:56:28.498840: Epoch 230\n",
      "2025-12-07 20:56:28.498840: Current learning rate: 0.0079\n",
      "2025-12-07 20:58:46.662389: train_loss -0.831\n",
      "2025-12-07 20:58:46.664391: val_loss -0.8531\n",
      "2025-12-07 20:58:46.666393: Pseudo dice [0.9158, 0.9479, 0.9331]\n",
      "2025-12-07 20:58:46.668395: Epoch time: 138.16 s\n",
      "2025-12-07 20:58:46.670397: Yayy! New best EMA pseudo Dice: 0.9316\n",
      "2025-12-07 20:58:47.546303: \n",
      "2025-12-07 20:58:47.546303: Epoch 231\n",
      "2025-12-07 20:58:47.546303: Current learning rate: 0.00789\n",
      "2025-12-07 21:01:05.630060: train_loss -0.8247\n",
      "2025-12-07 21:01:05.630060: val_loss -0.8691\n",
      "2025-12-07 21:01:05.635821: Pseudo dice [0.9274, 0.9574, 0.9387]\n",
      "2025-12-07 21:01:05.637825: Epoch time: 138.08 s\n",
      "2025-12-07 21:01:05.639827: Yayy! New best EMA pseudo Dice: 0.9325\n",
      "2025-12-07 21:01:06.701637: \n",
      "2025-12-07 21:01:06.701637: Epoch 232\n",
      "2025-12-07 21:01:06.717332: Current learning rate: 0.00789\n",
      "2025-12-07 21:03:24.753635: train_loss -0.8231\n",
      "2025-12-07 21:03:24.753635: val_loss -0.8546\n",
      "2025-12-07 21:03:24.753635: Pseudo dice [0.9126, 0.9485, 0.9373]\n",
      "2025-12-07 21:03:24.764627: Epoch time: 138.05 s\n",
      "2025-12-07 21:03:24.766629: Yayy! New best EMA pseudo Dice: 0.9326\n",
      "2025-12-07 21:03:25.642739: \n",
      "2025-12-07 21:03:25.642739: Epoch 233\n",
      "2025-12-07 21:03:25.642739: Current learning rate: 0.00788\n",
      "2025-12-07 21:05:43.634435: train_loss -0.8312\n",
      "2025-12-07 21:05:43.635440: val_loss -0.8547\n",
      "2025-12-07 21:05:43.638450: Pseudo dice [0.9127, 0.9514, 0.94]\n",
      "2025-12-07 21:05:43.640517: Epoch time: 138.0 s\n",
      "2025-12-07 21:05:43.642525: Yayy! New best EMA pseudo Dice: 0.9328\n",
      "2025-12-07 21:05:44.499591: \n",
      "2025-12-07 21:05:44.499591: Epoch 234\n",
      "2025-12-07 21:05:44.516526: Current learning rate: 0.00787\n",
      "2025-12-07 21:08:02.615041: train_loss -0.8266\n",
      "2025-12-07 21:08:02.616041: val_loss -0.852\n",
      "2025-12-07 21:08:02.619042: Pseudo dice [0.916, 0.9464, 0.9354]\n",
      "2025-12-07 21:08:02.621043: Epoch time: 138.12 s\n",
      "2025-12-07 21:08:03.234133: \n",
      "2025-12-07 21:08:03.234133: Epoch 235\n",
      "2025-12-07 21:08:03.234133: Current learning rate: 0.00786\n",
      "2025-12-07 21:10:21.171710: train_loss -0.8297\n",
      "2025-12-07 21:10:21.171710: val_loss -0.8555\n",
      "2025-12-07 21:10:21.185844: Pseudo dice [0.9181, 0.9475, 0.9358]\n",
      "2025-12-07 21:10:21.187584: Epoch time: 137.94 s\n",
      "2025-12-07 21:10:21.187584: Yayy! New best EMA pseudo Dice: 0.9329\n",
      "2025-12-07 21:10:22.080126: \n",
      "2025-12-07 21:10:22.081125: Epoch 236\n",
      "2025-12-07 21:10:22.083573: Current learning rate: 0.00785\n",
      "2025-12-07 21:12:40.093121: train_loss -0.8291\n",
      "2025-12-07 21:12:40.093121: val_loss -0.8501\n",
      "2025-12-07 21:12:40.093121: Pseudo dice [0.9124, 0.9446, 0.9373]\n",
      "2025-12-07 21:12:40.093121: Epoch time: 138.01 s\n",
      "2025-12-07 21:12:40.868225: \n",
      "2025-12-07 21:12:40.868225: Epoch 237\n",
      "2025-12-07 21:12:40.868225: Current learning rate: 0.00784\n",
      "2025-12-07 21:14:58.995024: train_loss -0.832\n",
      "2025-12-07 21:14:58.995024: val_loss -0.8583\n",
      "2025-12-07 21:14:58.999028: Pseudo dice [0.919, 0.9506, 0.9406]\n",
      "2025-12-07 21:14:58.999028: Epoch time: 138.13 s\n",
      "2025-12-07 21:14:59.002529: Yayy! New best EMA pseudo Dice: 0.9331\n",
      "2025-12-07 21:14:59.922078: \n",
      "2025-12-07 21:14:59.923152: Epoch 238\n",
      "2025-12-07 21:14:59.925920: Current learning rate: 0.00783\n",
      "2025-12-07 21:17:17.795784: train_loss -0.8296\n",
      "2025-12-07 21:17:17.795784: val_loss -0.8522\n",
      "2025-12-07 21:17:17.795784: Pseudo dice [0.9124, 0.949, 0.9357]\n",
      "2025-12-07 21:17:17.795784: Epoch time: 137.89 s\n",
      "2025-12-07 21:17:18.406173: \n",
      "2025-12-07 21:17:18.406173: Epoch 239\n",
      "2025-12-07 21:17:18.406173: Current learning rate: 0.00782\n",
      "2025-12-07 21:19:36.430578: train_loss -0.8295\n",
      "2025-12-07 21:19:36.430578: val_loss -0.8465\n",
      "2025-12-07 21:19:36.434582: Pseudo dice [0.9149, 0.944, 0.9293]\n",
      "2025-12-07 21:19:36.436322: Epoch time: 138.02 s\n",
      "2025-12-07 21:19:37.072423: \n",
      "2025-12-07 21:19:37.073533: Epoch 240\n",
      "2025-12-07 21:19:37.075712: Current learning rate: 0.00781\n",
      "2025-12-07 21:21:55.078055: train_loss -0.8257\n",
      "2025-12-07 21:21:55.078055: val_loss -0.8577\n",
      "2025-12-07 21:21:55.094397: Pseudo dice [0.9146, 0.9535, 0.94]\n",
      "2025-12-07 21:21:55.096411: Epoch time: 138.01 s\n",
      "2025-12-07 21:21:55.889596: \n",
      "2025-12-07 21:21:55.889596: Epoch 241\n",
      "2025-12-07 21:21:55.889596: Current learning rate: 0.0078\n",
      "2025-12-07 21:24:13.749816: train_loss -0.8265\n",
      "2025-12-07 21:24:13.765867: val_loss -0.8666\n",
      "2025-12-07 21:24:13.768970: Pseudo dice [0.923, 0.9535, 0.9402]\n",
      "2025-12-07 21:24:13.771008: Epoch time: 137.86 s\n",
      "2025-12-07 21:24:13.773008: Yayy! New best EMA pseudo Dice: 0.9336\n",
      "2025-12-07 21:24:14.655297: \n",
      "2025-12-07 21:24:14.655297: Epoch 242\n",
      "2025-12-07 21:24:14.655297: Current learning rate: 0.00779\n",
      "2025-12-07 21:26:32.713045: train_loss -0.8304\n",
      "2025-12-07 21:26:32.715048: val_loss -0.8574\n",
      "2025-12-07 21:26:32.718052: Pseudo dice [0.9213, 0.9473, 0.9347]\n",
      "2025-12-07 21:26:32.720979: Epoch time: 138.06 s\n",
      "2025-12-07 21:26:32.722980: Yayy! New best EMA pseudo Dice: 0.9337\n",
      "2025-12-07 21:26:33.627991: \n",
      "2025-12-07 21:26:33.627991: Epoch 243\n",
      "2025-12-07 21:26:33.627991: Current learning rate: 0.00778\n",
      "2025-12-07 21:28:51.654844: train_loss -0.8316\n",
      "2025-12-07 21:28:51.654844: val_loss -0.8624\n",
      "2025-12-07 21:28:51.657147: Pseudo dice [0.9188, 0.9469, 0.9374]\n",
      "2025-12-07 21:28:51.660646: Epoch time: 138.03 s\n",
      "2025-12-07 21:28:51.662648: Yayy! New best EMA pseudo Dice: 0.9338\n",
      "2025-12-07 21:28:52.860897: \n",
      "2025-12-07 21:28:52.860897: Epoch 244\n",
      "2025-12-07 21:28:52.863908: Current learning rate: 0.00777\n",
      "2025-12-07 21:31:10.946009: train_loss -0.8266\n",
      "2025-12-07 21:31:10.948010: val_loss -0.8491\n",
      "2025-12-07 21:31:10.950012: Pseudo dice [0.913, 0.9513, 0.9363]\n",
      "2025-12-07 21:31:10.951753: Epoch time: 138.09 s\n",
      "2025-12-07 21:31:11.578771: \n",
      "2025-12-07 21:31:11.578771: Epoch 245\n",
      "2025-12-07 21:31:11.578771: Current learning rate: 0.00777\n",
      "2025-12-07 21:33:29.528265: train_loss -0.8295\n",
      "2025-12-07 21:33:29.530268: val_loss -0.8488\n",
      "2025-12-07 21:33:29.532270: Pseudo dice [0.9108, 0.95, 0.9295]\n",
      "2025-12-07 21:33:29.536274: Epoch time: 137.95 s\n",
      "2025-12-07 21:33:30.170769: \n",
      "2025-12-07 21:33:30.170769: Epoch 246\n",
      "2025-12-07 21:33:30.170769: Current learning rate: 0.00776\n",
      "2025-12-07 21:35:48.139231: train_loss -0.8264\n",
      "2025-12-07 21:35:48.139231: val_loss -0.8604\n",
      "2025-12-07 21:35:48.139231: Pseudo dice [0.9196, 0.953, 0.9347]\n",
      "2025-12-07 21:35:48.149452: Epoch time: 137.97 s\n",
      "2025-12-07 21:35:48.904662: \n",
      "2025-12-07 21:35:48.904662: Epoch 247\n",
      "2025-12-07 21:35:48.920391: Current learning rate: 0.00775\n",
      "2025-12-07 21:38:06.749270: train_loss -0.8339\n",
      "2025-12-07 21:38:06.749270: val_loss -0.8532\n",
      "2025-12-07 21:38:06.749270: Pseudo dice [0.9145, 0.9486, 0.9385]\n",
      "2025-12-07 21:38:06.766527: Epoch time: 137.84 s\n",
      "2025-12-07 21:38:07.390110: \n",
      "2025-12-07 21:38:07.390110: Epoch 248\n",
      "2025-12-07 21:38:07.390110: Current learning rate: 0.00774\n",
      "2025-12-07 21:40:25.536451: train_loss -0.8294\n",
      "2025-12-07 21:40:25.536451: val_loss -0.8607\n",
      "2025-12-07 21:40:25.536451: Pseudo dice [0.9158, 0.9571, 0.9426]\n",
      "2025-12-07 21:40:25.546232: Epoch time: 138.15 s\n",
      "2025-12-07 21:40:25.546232: Yayy! New best EMA pseudo Dice: 0.9341\n",
      "2025-12-07 21:40:26.421183: \n",
      "2025-12-07 21:40:26.421183: Epoch 249\n",
      "2025-12-07 21:40:26.436961: Current learning rate: 0.00773\n",
      "2025-12-07 21:42:44.547129: train_loss -0.8326\n",
      "2025-12-07 21:42:44.547129: val_loss -0.853\n",
      "2025-12-07 21:42:44.551135: Pseudo dice [0.9163, 0.9482, 0.9326]\n",
      "2025-12-07 21:42:44.555140: Epoch time: 138.13 s\n",
      "2025-12-07 21:42:45.749716: \n",
      "2025-12-07 21:42:45.749716: Epoch 250\n",
      "2025-12-07 21:42:45.753219: Current learning rate: 0.00772\n",
      "2025-12-07 21:45:03.689371: train_loss -0.8337\n",
      "2025-12-07 21:45:03.689371: val_loss -0.8532\n",
      "2025-12-07 21:45:03.692379: Pseudo dice [0.9143, 0.9474, 0.935]\n",
      "2025-12-07 21:45:03.694385: Epoch time: 137.94 s\n",
      "2025-12-07 21:45:04.310545: \n",
      "2025-12-07 21:45:04.310545: Epoch 251\n",
      "2025-12-07 21:45:04.310545: Current learning rate: 0.00771\n",
      "2025-12-07 21:47:22.239588: train_loss -0.8346\n",
      "2025-12-07 21:47:22.240591: val_loss -0.853\n",
      "2025-12-07 21:47:22.243597: Pseudo dice [0.9118, 0.9472, 0.9397]\n",
      "2025-12-07 21:47:22.246603: Epoch time: 137.93 s\n",
      "2025-12-07 21:47:22.874994: \n",
      "2025-12-07 21:47:22.874994: Epoch 252\n",
      "2025-12-07 21:47:22.874994: Current learning rate: 0.0077\n",
      "2025-12-07 21:49:40.702409: train_loss -0.8323\n",
      "2025-12-07 21:49:40.704412: val_loss -0.8576\n",
      "2025-12-07 21:49:40.708416: Pseudo dice [0.917, 0.9464, 0.9402]\n",
      "2025-12-07 21:49:40.712426: Epoch time: 137.83 s\n",
      "2025-12-07 21:49:41.448214: \n",
      "2025-12-07 21:49:41.448214: Epoch 253\n",
      "2025-12-07 21:49:41.451221: Current learning rate: 0.00769\n",
      "2025-12-07 21:51:59.733950: train_loss -0.8307\n",
      "2025-12-07 21:51:59.735953: val_loss -0.8629\n",
      "2025-12-07 21:51:59.738920: Pseudo dice [0.9205, 0.9529, 0.9382]\n",
      "2025-12-07 21:51:59.741266: Epoch time: 138.29 s\n",
      "2025-12-07 21:52:00.358394: \n",
      "2025-12-07 21:52:00.358394: Epoch 254\n",
      "2025-12-07 21:52:00.374187: Current learning rate: 0.00768\n",
      "2025-12-07 21:54:18.464573: train_loss -0.8334\n",
      "2025-12-07 21:54:18.466575: val_loss -0.8455\n",
      "2025-12-07 21:54:18.468315: Pseudo dice [0.9108, 0.9452, 0.9349]\n",
      "2025-12-07 21:54:18.472320: Epoch time: 138.11 s\n",
      "2025-12-07 21:54:19.093534: \n",
      "2025-12-07 21:54:19.093534: Epoch 255\n",
      "2025-12-07 21:54:19.093534: Current learning rate: 0.00767\n",
      "2025-12-07 21:56:37.170746: train_loss -0.8322\n",
      "2025-12-07 21:56:37.170746: val_loss -0.8575\n",
      "2025-12-07 21:56:37.186826: Pseudo dice [0.9148, 0.9506, 0.9424]\n",
      "2025-12-07 21:56:37.186826: Epoch time: 138.08 s\n",
      "2025-12-07 21:56:38.109541: \n",
      "2025-12-07 21:56:38.109541: Epoch 256\n",
      "2025-12-07 21:56:38.109541: Current learning rate: 0.00766\n",
      "2025-12-07 21:58:57.668011: train_loss -0.8373\n",
      "2025-12-07 21:58:57.669012: val_loss -0.8641\n",
      "2025-12-07 21:58:57.671012: Pseudo dice [0.9174, 0.9476, 0.9475]\n",
      "2025-12-07 21:58:57.674949: Epoch time: 139.56 s\n",
      "2025-12-07 21:58:57.676754: Yayy! New best EMA pseudo Dice: 0.9343\n",
      "2025-12-07 21:58:58.561255: \n",
      "2025-12-07 21:58:58.561255: Epoch 257\n",
      "2025-12-07 21:58:58.561255: Current learning rate: 0.00765\n",
      "2025-12-07 22:01:17.124300: train_loss -0.8242\n",
      "2025-12-07 22:01:17.126112: val_loss -0.8474\n",
      "2025-12-07 22:01:17.128115: Pseudo dice [0.9088, 0.948, 0.9426]\n",
      "2025-12-07 22:01:17.128115: Epoch time: 138.56 s\n",
      "2025-12-07 22:01:17.761024: \n",
      "2025-12-07 22:01:17.761024: Epoch 258\n",
      "2025-12-07 22:01:17.762027: Current learning rate: 0.00764\n",
      "2025-12-07 22:03:36.327931: train_loss -0.8316\n",
      "2025-12-07 22:03:36.327931: val_loss -0.856\n",
      "2025-12-07 22:03:36.327931: Pseudo dice [0.915, 0.947, 0.9394]\n",
      "2025-12-07 22:03:36.327931: Epoch time: 138.58 s\n",
      "2025-12-07 22:03:37.043583: \n",
      "2025-12-07 22:03:37.043583: Epoch 259\n",
      "2025-12-07 22:03:37.045591: Current learning rate: 0.00764\n",
      "2025-12-07 22:05:55.436077: train_loss -0.8351\n",
      "2025-12-07 22:05:55.436077: val_loss -0.8547\n",
      "2025-12-07 22:05:55.436077: Pseudo dice [0.9109, 0.9485, 0.9398]\n",
      "2025-12-07 22:05:55.436077: Epoch time: 138.39 s\n",
      "2025-12-07 22:05:56.083239: \n",
      "2025-12-07 22:05:56.083239: Epoch 260\n",
      "2025-12-07 22:05:56.086251: Current learning rate: 0.00763\n",
      "2025-12-07 22:08:14.575604: train_loss -0.8283\n",
      "2025-12-07 22:08:14.577606: val_loss -0.8558\n",
      "2025-12-07 22:08:14.580910: Pseudo dice [0.9217, 0.9509, 0.9339]\n",
      "2025-12-07 22:08:14.582910: Epoch time: 138.49 s\n",
      "2025-12-07 22:08:15.226619: \n",
      "2025-12-07 22:08:15.226619: Epoch 261\n",
      "2025-12-07 22:08:15.229722: Current learning rate: 0.00762\n",
      "2025-12-07 22:10:33.435554: train_loss -0.8382\n",
      "2025-12-07 22:10:33.437295: val_loss -0.8504\n",
      "2025-12-07 22:10:33.441546: Pseudo dice [0.9148, 0.9483, 0.9308]\n",
      "2025-12-07 22:10:33.444547: Epoch time: 138.21 s\n",
      "2025-12-07 22:10:34.298403: \n",
      "2025-12-07 22:10:34.298403: Epoch 262\n",
      "2025-12-07 22:10:34.298403: Current learning rate: 0.00761\n",
      "2025-12-07 22:12:52.277247: train_loss -0.8311\n",
      "2025-12-07 22:12:52.277247: val_loss -0.8636\n",
      "2025-12-07 22:12:52.281252: Pseudo dice [0.9198, 0.9548, 0.9429]\n",
      "2025-12-07 22:12:52.285256: Epoch time: 137.98 s\n",
      "2025-12-07 22:12:52.287258: Yayy! New best EMA pseudo Dice: 0.9344\n",
      "2025-12-07 22:12:53.187024: \n",
      "2025-12-07 22:12:53.187024: Epoch 263\n",
      "2025-12-07 22:12:53.190109: Current learning rate: 0.0076\n",
      "2025-12-07 22:15:11.218149: train_loss -0.831\n",
      "2025-12-07 22:15:11.218149: val_loss -0.86\n",
      "2025-12-07 22:15:11.233134: Pseudo dice [0.9179, 0.9498, 0.9436]\n",
      "2025-12-07 22:15:11.234136: Epoch time: 138.03 s\n",
      "2025-12-07 22:15:11.238141: Yayy! New best EMA pseudo Dice: 0.9347\n",
      "2025-12-07 22:15:12.124793: \n",
      "2025-12-07 22:15:12.124793: Epoch 264\n",
      "2025-12-07 22:15:12.124793: Current learning rate: 0.00759\n",
      "2025-12-07 22:17:30.340743: train_loss -0.8348\n",
      "2025-12-07 22:17:30.340743: val_loss -0.86\n",
      "2025-12-07 22:17:30.342484: Pseudo dice [0.9157, 0.9523, 0.9416]\n",
      "2025-12-07 22:17:30.347994: Epoch time: 138.22 s\n",
      "2025-12-07 22:17:30.349996: Yayy! New best EMA pseudo Dice: 0.9349\n",
      "2025-12-07 22:17:31.298331: \n",
      "2025-12-07 22:17:31.299333: Epoch 265\n",
      "2025-12-07 22:17:31.301639: Current learning rate: 0.00758\n",
      "2025-12-07 22:19:49.372313: train_loss -0.8284\n",
      "2025-12-07 22:19:49.372313: val_loss -0.8534\n",
      "2025-12-07 22:19:49.374053: Pseudo dice [0.9147, 0.945, 0.9351]\n",
      "2025-12-07 22:19:49.379553: Epoch time: 138.08 s\n",
      "2025-12-07 22:19:50.022130: \n",
      "2025-12-07 22:19:50.023871: Epoch 266\n",
      "2025-12-07 22:19:50.023871: Current learning rate: 0.00757\n",
      "2025-12-07 22:22:08.108613: train_loss -0.8332\n",
      "2025-12-07 22:22:08.108613: val_loss -0.8654\n",
      "2025-12-07 22:22:08.108613: Pseudo dice [0.9203, 0.9525, 0.9378]\n",
      "2025-12-07 22:22:08.124251: Epoch time: 138.09 s\n",
      "2025-12-07 22:22:08.758091: \n",
      "2025-12-07 22:22:08.760094: Epoch 267\n",
      "2025-12-07 22:22:08.760094: Current learning rate: 0.00756\n",
      "2025-12-07 22:24:26.763957: train_loss -0.828\n",
      "2025-12-07 22:24:26.765960: val_loss -0.8594\n",
      "2025-12-07 22:24:26.769966: Pseudo dice [0.9202, 0.95, 0.9408]\n",
      "2025-12-07 22:24:26.773970: Epoch time: 138.01 s\n",
      "2025-12-07 22:24:26.775710: Yayy! New best EMA pseudo Dice: 0.935\n",
      "2025-12-07 22:24:27.892126: \n",
      "2025-12-07 22:24:27.892126: Epoch 268\n",
      "2025-12-07 22:24:27.895138: Current learning rate: 0.00755\n",
      "2025-12-07 22:26:45.950401: train_loss -0.831\n",
      "2025-12-07 22:26:45.950401: val_loss -0.8697\n",
      "2025-12-07 22:26:45.952403: Pseudo dice [0.9244, 0.9561, 0.9409]\n",
      "2025-12-07 22:26:45.952403: Epoch time: 138.06 s\n",
      "2025-12-07 22:26:45.959905: Yayy! New best EMA pseudo Dice: 0.9356\n",
      "2025-12-07 22:26:46.889719: \n",
      "2025-12-07 22:26:46.889719: Epoch 269\n",
      "2025-12-07 22:26:46.905557: Current learning rate: 0.00754\n",
      "2025-12-07 22:29:04.827124: train_loss -0.8334\n",
      "2025-12-07 22:29:04.827124: val_loss -0.8494\n",
      "2025-12-07 22:29:04.834346: Pseudo dice [0.91, 0.9448, 0.9418]\n",
      "2025-12-07 22:29:04.836348: Epoch time: 137.94 s\n",
      "2025-12-07 22:29:05.467885: \n",
      "2025-12-07 22:29:05.467885: Epoch 270\n",
      "2025-12-07 22:29:05.478964: Current learning rate: 0.00753\n",
      "2025-12-07 22:31:23.547784: train_loss -0.8289\n",
      "2025-12-07 22:31:23.547784: val_loss -0.8627\n",
      "2025-12-07 22:31:23.565903: Pseudo dice [0.9171, 0.9534, 0.9406]\n",
      "2025-12-07 22:31:23.568177: Epoch time: 138.08 s\n",
      "2025-12-07 22:31:24.271743: \n",
      "2025-12-07 22:31:24.272745: Epoch 271\n",
      "2025-12-07 22:31:24.275752: Current learning rate: 0.00752\n",
      "2025-12-07 22:33:42.220912: train_loss -0.8294\n",
      "2025-12-07 22:33:42.220912: val_loss -0.8453\n",
      "2025-12-07 22:33:42.227914: Pseudo dice [0.9106, 0.9449, 0.929]\n",
      "2025-12-07 22:33:42.231915: Epoch time: 137.95 s\n",
      "2025-12-07 22:33:42.869613: \n",
      "2025-12-07 22:33:42.869613: Epoch 272\n",
      "2025-12-07 22:33:42.872625: Current learning rate: 0.00751\n",
      "2025-12-07 22:36:00.936425: train_loss -0.8321\n",
      "2025-12-07 22:36:00.936425: val_loss -0.8582\n",
      "2025-12-07 22:36:00.940253: Pseudo dice [0.9156, 0.9509, 0.9411]\n",
      "2025-12-07 22:36:00.942255: Epoch time: 138.07 s\n",
      "2025-12-07 22:36:01.584116: \n",
      "2025-12-07 22:36:01.584116: Epoch 273\n",
      "2025-12-07 22:36:01.584116: Current learning rate: 0.00751\n",
      "2025-12-07 22:38:19.890408: train_loss -0.8368\n",
      "2025-12-07 22:38:19.892410: val_loss -0.8643\n",
      "2025-12-07 22:38:19.894412: Pseudo dice [0.9183, 0.9543, 0.9437]\n",
      "2025-12-07 22:38:19.896414: Epoch time: 138.31 s\n",
      "2025-12-07 22:38:20.858843: \n",
      "2025-12-07 22:38:20.858843: Epoch 274\n",
      "2025-12-07 22:38:20.874658: Current learning rate: 0.0075\n",
      "2025-12-07 22:40:38.846540: train_loss -0.8326\n",
      "2025-12-07 22:40:38.846540: val_loss -0.8559\n",
      "2025-12-07 22:40:38.850545: Pseudo dice [0.9172, 0.9488, 0.9393]\n",
      "2025-12-07 22:40:38.852547: Epoch time: 137.99 s\n",
      "2025-12-07 22:40:39.485692: \n",
      "2025-12-07 22:40:39.485692: Epoch 275\n",
      "2025-12-07 22:40:39.485692: Current learning rate: 0.00749\n",
      "2025-12-07 22:42:57.607576: train_loss -0.8336\n",
      "2025-12-07 22:42:57.607576: val_loss -0.8494\n",
      "2025-12-07 22:42:57.610728: Pseudo dice [0.9128, 0.9499, 0.9327]\n",
      "2025-12-07 22:42:57.610728: Epoch time: 138.12 s\n",
      "2025-12-07 22:42:58.259645: \n",
      "2025-12-07 22:42:58.259645: Epoch 276\n",
      "2025-12-07 22:42:58.259645: Current learning rate: 0.00748\n",
      "2025-12-07 22:45:16.326764: train_loss -0.8303\n",
      "2025-12-07 22:45:16.326764: val_loss -0.8492\n",
      "2025-12-07 22:45:16.330818: Pseudo dice [0.9085, 0.9477, 0.9375]\n",
      "2025-12-07 22:45:16.332820: Epoch time: 138.07 s\n",
      "2025-12-07 22:45:17.086702: \n",
      "2025-12-07 22:45:17.087704: Epoch 277\n",
      "2025-12-07 22:45:17.090713: Current learning rate: 0.00747\n",
      "2025-12-07 22:47:35.171386: train_loss -0.8287\n",
      "2025-12-07 22:47:35.171386: val_loss -0.8554\n",
      "2025-12-07 22:47:35.171386: Pseudo dice [0.917, 0.9456, 0.9389]\n",
      "2025-12-07 22:47:35.171386: Epoch time: 138.09 s\n",
      "2025-12-07 22:47:35.810192: \n",
      "2025-12-07 22:47:35.810192: Epoch 278\n",
      "2025-12-07 22:47:35.810192: Current learning rate: 0.00746\n",
      "2025-12-07 22:49:53.717565: train_loss -0.8351\n",
      "2025-12-07 22:49:53.717565: val_loss -0.853\n",
      "2025-12-07 22:49:53.717565: Pseudo dice [0.9133, 0.944, 0.9385]\n",
      "2025-12-07 22:49:53.717565: Epoch time: 137.91 s\n",
      "2025-12-07 22:49:54.345709: \n",
      "2025-12-07 22:49:54.346714: Epoch 279\n",
      "2025-12-07 22:49:54.349069: Current learning rate: 0.00745\n",
      "2025-12-07 22:52:12.326843: train_loss -0.8296\n",
      "2025-12-07 22:52:12.326843: val_loss -0.857\n",
      "2025-12-07 22:52:12.331196: Pseudo dice [0.9143, 0.9534, 0.9365]\n",
      "2025-12-07 22:52:12.331196: Epoch time: 137.98 s\n",
      "2025-12-07 22:52:13.076559: \n",
      "2025-12-07 22:52:13.076559: Epoch 280\n",
      "2025-12-07 22:52:13.092209: Current learning rate: 0.00744\n",
      "2025-12-07 22:54:31.216544: train_loss -0.8194\n",
      "2025-12-07 22:54:31.217548: val_loss -0.8273\n",
      "2025-12-07 22:54:31.217548: Pseudo dice [0.9017, 0.9382, 0.9275]\n",
      "2025-12-07 22:54:31.217548: Epoch time: 138.14 s\n",
      "2025-12-07 22:54:32.024543: \n",
      "2025-12-07 22:54:32.024543: Epoch 281\n",
      "2025-12-07 22:54:32.024543: Current learning rate: 0.00743\n",
      "2025-12-07 22:56:50.005874: train_loss -0.7999\n",
      "2025-12-07 22:56:50.005874: val_loss -0.821\n",
      "2025-12-07 22:56:50.009616: Pseudo dice [0.8964, 0.9362, 0.9307]\n",
      "2025-12-07 22:56:50.013621: Epoch time: 137.98 s\n",
      "2025-12-07 22:56:50.655148: \n",
      "2025-12-07 22:56:50.655148: Epoch 282\n",
      "2025-12-07 22:56:50.655148: Current learning rate: 0.00742\n",
      "2025-12-07 22:59:08.811145: train_loss -0.8116\n",
      "2025-12-07 22:59:08.811145: val_loss -0.8453\n",
      "2025-12-07 22:59:08.815166: Pseudo dice [0.9179, 0.9498, 0.9184]\n",
      "2025-12-07 22:59:08.819172: Epoch time: 138.16 s\n",
      "2025-12-07 22:59:09.467377: \n",
      "2025-12-07 22:59:09.467377: Epoch 283\n",
      "2025-12-07 22:59:09.483089: Current learning rate: 0.00741\n",
      "2025-12-07 23:01:27.449691: train_loss -0.7979\n",
      "2025-12-07 23:01:27.449691: val_loss -0.8431\n",
      "2025-12-07 23:01:27.453728: Pseudo dice [0.9139, 0.9472, 0.9318]\n",
      "2025-12-07 23:01:27.456089: Epoch time: 137.98 s\n",
      "2025-12-07 23:01:28.077409: \n",
      "2025-12-07 23:01:28.077409: Epoch 284\n",
      "2025-12-07 23:01:28.088246: Current learning rate: 0.0074\n",
      "2025-12-07 23:03:46.029778: train_loss -0.8125\n",
      "2025-12-07 23:03:46.031811: val_loss -0.8368\n",
      "2025-12-07 23:03:46.037825: Pseudo dice [0.9048, 0.9467, 0.9344]\n",
      "2025-12-07 23:03:46.039828: Epoch time: 137.95 s\n",
      "2025-12-07 23:03:46.710685: \n",
      "2025-12-07 23:03:46.710685: Epoch 285\n",
      "2025-12-07 23:03:46.710685: Current learning rate: 0.00739\n",
      "2025-12-07 23:06:04.741943: train_loss -0.7908\n",
      "2025-12-07 23:06:04.741943: val_loss -0.8135\n",
      "2025-12-07 23:06:04.745947: Pseudo dice [0.8932, 0.9365, 0.9235]\n",
      "2025-12-07 23:06:04.747948: Epoch time: 138.03 s\n",
      "2025-12-07 23:06:05.386195: \n",
      "2025-12-07 23:06:05.386195: Epoch 286\n",
      "2025-12-07 23:06:05.390004: Current learning rate: 0.00738\n",
      "2025-12-07 23:08:23.308720: train_loss -0.8036\n",
      "2025-12-07 23:08:23.308720: val_loss -0.8378\n",
      "2025-12-07 23:08:23.312462: Pseudo dice [0.9066, 0.9452, 0.9297]\n",
      "2025-12-07 23:08:23.315462: Epoch time: 137.92 s\n",
      "2025-12-07 23:08:24.122929: \n",
      "2025-12-07 23:08:24.123934: Epoch 287\n",
      "2025-12-07 23:08:24.127041: Current learning rate: 0.00738\n",
      "2025-12-07 23:10:42.148884: train_loss -0.8064\n",
      "2025-12-07 23:10:42.150887: val_loss -0.8391\n",
      "2025-12-07 23:10:42.152889: Pseudo dice [0.9063, 0.9414, 0.9304]\n",
      "2025-12-07 23:10:42.154891: Epoch time: 138.03 s\n",
      "2025-12-07 23:10:42.805406: \n",
      "2025-12-07 23:10:42.805406: Epoch 288\n",
      "2025-12-07 23:10:42.805406: Current learning rate: 0.00737\n",
      "2025-12-07 23:13:00.671267: train_loss -0.822\n",
      "2025-12-07 23:13:00.671267: val_loss -0.8407\n",
      "2025-12-07 23:13:00.687845: Pseudo dice [0.9054, 0.9432, 0.9372]\n",
      "2025-12-07 23:13:00.690884: Epoch time: 137.87 s\n",
      "2025-12-07 23:13:01.312269: \n",
      "2025-12-07 23:13:01.312269: Epoch 289\n",
      "2025-12-07 23:13:01.328250: Current learning rate: 0.00736\n",
      "2025-12-07 23:15:19.280317: train_loss -0.8263\n",
      "2025-12-07 23:15:19.280317: val_loss -0.8572\n",
      "2025-12-07 23:15:19.280317: Pseudo dice [0.9187, 0.9522, 0.9345]\n",
      "2025-12-07 23:15:19.289331: Epoch time: 137.97 s\n",
      "2025-12-07 23:15:19.920658: \n",
      "2025-12-07 23:15:19.920658: Epoch 290\n",
      "2025-12-07 23:15:19.920658: Current learning rate: 0.00735\n",
      "2025-12-07 23:17:37.874197: train_loss -0.8236\n",
      "2025-12-07 23:17:37.890186: val_loss -0.8547\n",
      "2025-12-07 23:17:37.890186: Pseudo dice [0.9185, 0.9516, 0.9294]\n",
      "2025-12-07 23:17:37.890186: Epoch time: 137.95 s\n",
      "2025-12-07 23:17:38.514464: \n",
      "2025-12-07 23:17:38.514464: Epoch 291\n",
      "2025-12-07 23:17:38.514464: Current learning rate: 0.00734\n",
      "2025-12-07 23:19:56.467934: train_loss -0.8308\n",
      "2025-12-07 23:19:56.467934: val_loss -0.8706\n",
      "2025-12-07 23:19:56.483610: Pseudo dice [0.9255, 0.955, 0.9424]\n",
      "2025-12-07 23:19:56.483610: Epoch time: 137.95 s\n",
      "2025-12-07 23:19:57.107977: \n",
      "2025-12-07 23:19:57.107977: Epoch 292\n",
      "2025-12-07 23:19:57.107977: Current learning rate: 0.00733\n",
      "2025-12-07 23:22:15.014388: train_loss -0.8315\n",
      "2025-12-07 23:22:15.014388: val_loss -0.8504\n",
      "2025-12-07 23:22:15.014388: Pseudo dice [0.912, 0.9476, 0.9364]\n",
      "2025-12-07 23:22:15.014388: Epoch time: 137.91 s\n",
      "2025-12-07 23:22:15.827208: \n",
      "2025-12-07 23:22:15.827208: Epoch 293\n",
      "2025-12-07 23:22:15.838271: Current learning rate: 0.00732\n",
      "2025-12-07 23:24:33.743375: train_loss -0.8335\n",
      "2025-12-07 23:24:33.743375: val_loss -0.8525\n",
      "2025-12-07 23:24:33.749382: Pseudo dice [0.913, 0.9529, 0.939]\n",
      "2025-12-07 23:24:33.751097: Epoch time: 137.92 s\n",
      "2025-12-07 23:24:34.503320: \n",
      "2025-12-07 23:24:34.503320: Epoch 294\n",
      "2025-12-07 23:24:34.503320: Current learning rate: 0.00731\n",
      "2025-12-07 23:26:52.497139: train_loss -0.8301\n",
      "2025-12-07 23:26:52.497139: val_loss -0.8635\n",
      "2025-12-07 23:26:52.501012: Pseudo dice [0.922, 0.9537, 0.9401]\n",
      "2025-12-07 23:26:52.504754: Epoch time: 138.0 s\n",
      "2025-12-07 23:26:53.140279: \n",
      "2025-12-07 23:26:53.140279: Epoch 295\n",
      "2025-12-07 23:26:53.151899: Current learning rate: 0.0073\n",
      "2025-12-07 23:29:11.140428: train_loss -0.8316\n",
      "2025-12-07 23:29:11.140428: val_loss -0.8479\n",
      "2025-12-07 23:29:11.140428: Pseudo dice [0.9073, 0.945, 0.9451]\n",
      "2025-12-07 23:29:11.155829: Epoch time: 138.0 s\n",
      "2025-12-07 23:29:11.795320: \n",
      "2025-12-07 23:29:11.795320: Epoch 296\n",
      "2025-12-07 23:29:11.795320: Current learning rate: 0.00729\n",
      "2025-12-07 23:31:29.940754: train_loss -0.8346\n",
      "2025-12-07 23:31:29.941757: val_loss -0.8632\n",
      "2025-12-07 23:31:29.944775: Pseudo dice [0.9189, 0.9516, 0.9378]\n",
      "2025-12-07 23:31:29.946787: Epoch time: 138.15 s\n",
      "2025-12-07 23:31:30.687677: \n",
      "2025-12-07 23:31:30.687677: Epoch 297\n",
      "2025-12-07 23:31:30.687677: Current learning rate: 0.00728\n",
      "2025-12-07 23:33:49.086060: train_loss -0.8342\n",
      "2025-12-07 23:33:49.087062: val_loss -0.8636\n",
      "2025-12-07 23:33:49.091071: Pseudo dice [0.918, 0.9513, 0.9409]\n",
      "2025-12-07 23:33:49.093225: Epoch time: 138.4 s\n",
      "2025-12-07 23:33:49.728059: \n",
      "2025-12-07 23:33:49.728059: Epoch 298\n",
      "2025-12-07 23:33:49.744141: Current learning rate: 0.00727\n",
      "2025-12-07 23:36:08.140214: train_loss -0.8339\n",
      "2025-12-07 23:36:08.140214: val_loss -0.8646\n",
      "2025-12-07 23:36:08.140214: Pseudo dice [0.9159, 0.9496, 0.9452]\n",
      "2025-12-07 23:36:08.140214: Epoch time: 138.41 s\n",
      "2025-12-07 23:36:08.775344: \n",
      "2025-12-07 23:36:08.791089: Epoch 299\n",
      "2025-12-07 23:36:08.791089: Current learning rate: 0.00726\n",
      "2025-12-07 23:38:27.121349: train_loss -0.8343\n",
      "2025-12-07 23:38:27.123352: val_loss -0.8627\n",
      "2025-12-07 23:38:27.126856: Pseudo dice [0.919, 0.9496, 0.941]\n",
      "2025-12-07 23:38:27.128858: Epoch time: 138.35 s\n",
      "2025-12-07 23:38:28.327419: \n",
      "2025-12-07 23:38:28.327419: Epoch 300\n",
      "2025-12-07 23:38:28.343538: Current learning rate: 0.00725\n",
      "2025-12-07 23:40:46.722750: train_loss -0.8356\n",
      "2025-12-07 23:40:46.726755: val_loss -0.8571\n",
      "2025-12-07 23:40:46.731763: Pseudo dice [0.9096, 0.9502, 0.9478]\n",
      "2025-12-07 23:40:46.735768: Epoch time: 138.4 s\n",
      "2025-12-07 23:40:47.406730: \n",
      "2025-12-07 23:40:47.406730: Epoch 301\n",
      "2025-12-07 23:40:47.406730: Current learning rate: 0.00724\n",
      "2025-12-07 23:43:05.609263: train_loss -0.8345\n",
      "2025-12-07 23:43:05.609263: val_loss -0.8481\n",
      "2025-12-07 23:43:05.624980: Pseudo dice [0.9071, 0.9469, 0.9434]\n",
      "2025-12-07 23:43:05.626984: Epoch time: 138.2 s\n",
      "2025-12-07 23:43:06.265054: \n",
      "2025-12-07 23:43:06.265054: Epoch 302\n",
      "2025-12-07 23:43:06.265054: Current learning rate: 0.00724\n",
      "2025-12-07 23:45:24.298235: train_loss -0.832\n",
      "2025-12-07 23:45:24.298235: val_loss -0.8744\n",
      "2025-12-07 23:45:24.302242: Pseudo dice [0.9302, 0.9586, 0.9383]\n",
      "2025-12-07 23:45:24.304245: Epoch time: 138.03 s\n",
      "2025-12-07 23:45:24.998133: \n",
      "2025-12-07 23:45:24.998133: Epoch 303\n",
      "2025-12-07 23:45:24.998133: Current learning rate: 0.00723\n",
      "2025-12-07 23:47:43.064955: train_loss -0.8346\n",
      "2025-12-07 23:47:43.064955: val_loss -0.8548\n",
      "2025-12-07 23:47:43.068966: Pseudo dice [0.9112, 0.9494, 0.9437]\n",
      "2025-12-07 23:47:43.070993: Epoch time: 138.07 s\n",
      "2025-12-07 23:47:43.712336: \n",
      "2025-12-07 23:47:43.713340: Epoch 304\n",
      "2025-12-07 23:47:43.713340: Current learning rate: 0.00722\n",
      "2025-12-07 23:50:01.772634: train_loss -0.8341\n",
      "2025-12-07 23:50:01.772634: val_loss -0.8737\n",
      "2025-12-07 23:50:01.778641: Pseudo dice [0.9283, 0.9579, 0.9399]\n",
      "2025-12-07 23:50:01.780382: Epoch time: 138.07 s\n",
      "2025-12-07 23:50:02.422159: \n",
      "2025-12-07 23:50:02.422159: Epoch 305\n",
      "2025-12-07 23:50:02.422159: Current learning rate: 0.00721\n",
      "2025-12-07 23:52:20.515662: train_loss -0.8364\n",
      "2025-12-07 23:52:20.515662: val_loss -0.8719\n",
      "2025-12-07 23:52:20.515662: Pseudo dice [0.9265, 0.9583, 0.9424]\n",
      "2025-12-07 23:52:20.515662: Epoch time: 138.11 s\n",
      "2025-12-07 23:52:20.515662: Yayy! New best EMA pseudo Dice: 0.9361\n",
      "2025-12-07 23:52:21.790479: \n",
      "2025-12-07 23:52:21.790479: Epoch 306\n",
      "2025-12-07 23:52:21.794497: Current learning rate: 0.0072\n",
      "2025-12-07 23:54:39.765142: train_loss -0.8362\n",
      "2025-12-07 23:54:39.781145: val_loss -0.8619\n",
      "2025-12-07 23:54:39.784337: Pseudo dice [0.9201, 0.9503, 0.9313]\n",
      "2025-12-07 23:54:39.787501: Epoch time: 137.98 s\n",
      "2025-12-07 23:54:40.432220: \n",
      "2025-12-07 23:54:40.432220: Epoch 307\n",
      "2025-12-07 23:54:40.435117: Current learning rate: 0.00719\n",
      "2025-12-07 23:56:58.405448: train_loss -0.8356\n",
      "2025-12-07 23:56:58.407451: val_loss -0.8661\n",
      "2025-12-07 23:56:58.409453: Pseudo dice [0.9197, 0.9477, 0.9462]\n",
      "2025-12-07 23:56:58.412003: Epoch time: 137.97 s\n",
      "2025-12-07 23:56:59.045540: \n",
      "2025-12-07 23:56:59.045540: Epoch 308\n",
      "2025-12-07 23:56:59.045540: Current learning rate: 0.00718\n",
      "2025-12-07 23:59:17.092239: train_loss -0.8374\n",
      "2025-12-07 23:59:17.092239: val_loss -0.859\n",
      "2025-12-07 23:59:17.092239: Pseudo dice [0.9183, 0.9487, 0.9375]\n",
      "2025-12-07 23:59:17.092239: Epoch time: 138.05 s\n",
      "2025-12-07 23:59:17.830390: \n",
      "2025-12-07 23:59:17.830390: Epoch 309\n",
      "2025-12-07 23:59:17.832393: Current learning rate: 0.00717\n",
      "2025-12-08 00:01:35.780961: train_loss -0.8362\n",
      "2025-12-08 00:01:35.780961: val_loss -0.858\n",
      "2025-12-08 00:01:35.780961: Pseudo dice [0.9163, 0.9501, 0.9401]\n",
      "2025-12-08 00:01:35.780961: Epoch time: 137.95 s\n",
      "2025-12-08 00:01:36.422058: \n",
      "2025-12-08 00:01:36.422058: Epoch 310\n",
      "2025-12-08 00:01:36.437803: Current learning rate: 0.00716\n",
      "2025-12-08 00:03:54.439849: train_loss -0.8386\n",
      "2025-12-08 00:03:54.439849: val_loss -0.8592\n",
      "2025-12-08 00:03:54.445861: Pseudo dice [0.9155, 0.9502, 0.9394]\n",
      "2025-12-08 00:03:54.449874: Epoch time: 138.02 s\n",
      "2025-12-08 00:03:55.252268: \n",
      "2025-12-08 00:03:55.253270: Epoch 311\n",
      "2025-12-08 00:03:55.256277: Current learning rate: 0.00715\n",
      "2025-12-08 00:06:13.127785: train_loss -0.8401\n",
      "2025-12-08 00:06:13.128788: val_loss -0.8644\n",
      "2025-12-08 00:06:13.131797: Pseudo dice [0.92, 0.956, 0.9395]\n",
      "2025-12-08 00:06:13.133798: Epoch time: 137.88 s\n",
      "2025-12-08 00:06:13.843223: \n",
      "2025-12-08 00:06:13.843223: Epoch 312\n",
      "2025-12-08 00:06:13.856093: Current learning rate: 0.00714\n",
      "2025-12-08 00:08:32.048893: train_loss -0.8357\n",
      "2025-12-08 00:08:32.049895: val_loss -0.8647\n",
      "2025-12-08 00:08:32.052901: Pseudo dice [0.921, 0.9542, 0.9366]\n",
      "2025-12-08 00:08:32.055914: Epoch time: 138.21 s\n",
      "2025-12-08 00:08:32.057919: Yayy! New best EMA pseudo Dice: 0.9362\n",
      "2025-12-08 00:08:32.952033: \n",
      "2025-12-08 00:08:32.952033: Epoch 313\n",
      "2025-12-08 00:08:32.952033: Current learning rate: 0.00713\n",
      "2025-12-08 00:10:50.890639: train_loss -0.8333\n",
      "2025-12-08 00:10:50.890639: val_loss -0.8605\n",
      "2025-12-08 00:10:50.905008: Pseudo dice [0.9164, 0.9481, 0.9416]\n",
      "2025-12-08 00:10:50.906748: Epoch time: 137.94 s\n",
      "2025-12-08 00:10:51.586398: \n",
      "2025-12-08 00:10:51.586398: Epoch 314\n",
      "2025-12-08 00:10:51.590629: Current learning rate: 0.00712\n",
      "2025-12-08 00:13:09.561850: train_loss -0.838\n",
      "2025-12-08 00:13:09.561850: val_loss -0.87\n",
      "2025-12-08 00:13:09.577534: Pseudo dice [0.9191, 0.9573, 0.941]\n",
      "2025-12-08 00:13:09.579538: Epoch time: 137.98 s\n",
      "2025-12-08 00:13:09.581540: Yayy! New best EMA pseudo Dice: 0.9364\n",
      "2025-12-08 00:13:10.619074: \n",
      "2025-12-08 00:13:10.619074: Epoch 315\n",
      "2025-12-08 00:13:10.623226: Current learning rate: 0.00711\n",
      "2025-12-08 00:15:28.621215: train_loss -0.8367\n",
      "2025-12-08 00:15:28.621215: val_loss -0.8613\n",
      "2025-12-08 00:15:28.626116: Pseudo dice [0.916, 0.9518, 0.9412]\n",
      "2025-12-08 00:15:28.629124: Epoch time: 138.0 s\n",
      "2025-12-08 00:15:29.263845: \n",
      "2025-12-08 00:15:29.263845: Epoch 316\n",
      "2025-12-08 00:15:29.279652: Current learning rate: 0.0071\n",
      "2025-12-08 00:17:47.399035: train_loss -0.8347\n",
      "2025-12-08 00:17:47.400036: val_loss -0.8572\n",
      "2025-12-08 00:17:47.403522: Pseudo dice [0.9126, 0.9486, 0.9409]\n",
      "2025-12-08 00:17:47.406523: Epoch time: 138.14 s\n",
      "2025-12-08 00:17:48.217999: \n",
      "2025-12-08 00:17:48.217999: Epoch 317\n",
      "2025-12-08 00:17:48.217999: Current learning rate: 0.0071\n",
      "2025-12-08 00:20:06.287145: train_loss -0.8373\n",
      "2025-12-08 00:20:06.289148: val_loss -0.8614\n",
      "2025-12-08 00:20:06.295159: Pseudo dice [0.9197, 0.9511, 0.9445]\n",
      "2025-12-08 00:20:06.297162: Epoch time: 138.07 s\n",
      "2025-12-08 00:20:06.983799: \n",
      "2025-12-08 00:20:06.983799: Epoch 318\n",
      "2025-12-08 00:20:06.994499: Current learning rate: 0.00709\n",
      "2025-12-08 00:22:25.061189: train_loss -0.8318\n",
      "2025-12-08 00:22:25.061189: val_loss -0.8417\n",
      "2025-12-08 00:22:25.061189: Pseudo dice [0.9085, 0.9388, 0.9351]\n",
      "2025-12-08 00:22:25.061189: Epoch time: 138.08 s\n",
      "2025-12-08 00:22:25.718481: \n",
      "2025-12-08 00:22:25.718481: Epoch 319\n",
      "2025-12-08 00:22:25.718481: Current learning rate: 0.00708\n",
      "2025-12-08 00:24:43.704688: train_loss -0.8333\n",
      "2025-12-08 00:24:43.704688: val_loss -0.8489\n",
      "2025-12-08 00:24:43.722122: Pseudo dice [0.916, 0.9487, 0.9296]\n",
      "2025-12-08 00:24:43.724128: Epoch time: 138.0 s\n",
      "2025-12-08 00:24:44.358932: \n",
      "2025-12-08 00:24:44.358932: Epoch 320\n",
      "2025-12-08 00:24:44.358932: Current learning rate: 0.00707\n",
      "2025-12-08 00:27:03.726955: train_loss -0.8356\n",
      "2025-12-08 00:27:03.726955: val_loss -0.8494\n",
      "2025-12-08 00:27:03.726955: Pseudo dice [0.9102, 0.9473, 0.9368]\n",
      "2025-12-08 00:27:03.726955: Epoch time: 139.37 s\n",
      "2025-12-08 00:27:04.498457: \n",
      "2025-12-08 00:27:04.498457: Epoch 321\n",
      "2025-12-08 00:27:04.498457: Current learning rate: 0.00706\n",
      "2025-12-08 00:29:22.833615: train_loss -0.8397\n",
      "2025-12-08 00:29:22.833615: val_loss -0.8613\n",
      "2025-12-08 00:29:22.837620: Pseudo dice [0.9178, 0.9528, 0.9403]\n",
      "2025-12-08 00:29:22.839621: Epoch time: 138.34 s\n",
      "2025-12-08 00:29:23.500575: \n",
      "2025-12-08 00:29:23.500575: Epoch 322\n",
      "2025-12-08 00:29:23.504044: Current learning rate: 0.00705\n",
      "2025-12-08 00:31:42.093463: train_loss -0.8394\n",
      "2025-12-08 00:31:42.093463: val_loss -0.874\n",
      "2025-12-08 00:31:42.093463: Pseudo dice [0.9276, 0.9602, 0.9324]\n",
      "2025-12-08 00:31:42.110623: Epoch time: 138.59 s\n",
      "2025-12-08 00:31:42.751926: \n",
      "2025-12-08 00:31:42.751926: Epoch 323\n",
      "2025-12-08 00:31:42.751926: Current learning rate: 0.00704\n",
      "2025-12-08 00:34:01.202296: train_loss -0.8385\n",
      "2025-12-08 00:34:01.202296: val_loss -0.8637\n",
      "2025-12-08 00:34:01.202296: Pseudo dice [0.9195, 0.9504, 0.9429]\n",
      "2025-12-08 00:34:01.202296: Epoch time: 138.45 s\n",
      "2025-12-08 00:34:02.003758: \n",
      "2025-12-08 00:34:02.004763: Epoch 324\n",
      "2025-12-08 00:34:02.007787: Current learning rate: 0.00703\n",
      "2025-12-08 00:36:20.543963: train_loss -0.8371\n",
      "2025-12-08 00:36:20.545966: val_loss -0.8595\n",
      "2025-12-08 00:36:20.545966: Pseudo dice [0.9157, 0.9512, 0.942]\n",
      "2025-12-08 00:36:20.545966: Epoch time: 138.54 s\n",
      "2025-12-08 00:36:21.186123: \n",
      "2025-12-08 00:36:21.186123: Epoch 325\n",
      "2025-12-08 00:36:21.186123: Current learning rate: 0.00702\n",
      "2025-12-08 00:38:39.265363: train_loss -0.8381\n",
      "2025-12-08 00:38:39.266566: val_loss -0.8671\n",
      "2025-12-08 00:38:39.272568: Pseudo dice [0.921, 0.9508, 0.9439]\n",
      "2025-12-08 00:38:39.275568: Epoch time: 138.08 s\n",
      "2025-12-08 00:38:39.925719: \n",
      "2025-12-08 00:38:39.925719: Epoch 326\n",
      "2025-12-08 00:38:39.925719: Current learning rate: 0.00701\n",
      "2025-12-08 00:40:58.031832: train_loss -0.8404\n",
      "2025-12-08 00:40:58.031832: val_loss -0.8662\n",
      "2025-12-08 00:40:58.035837: Pseudo dice [0.9192, 0.9491, 0.9433]\n",
      "2025-12-08 00:40:58.039842: Epoch time: 138.11 s\n",
      "2025-12-08 00:40:58.811100: \n",
      "2025-12-08 00:40:58.811100: Epoch 327\n",
      "2025-12-08 00:40:58.811100: Current learning rate: 0.007\n",
      "2025-12-08 00:43:16.749996: train_loss -0.8433\n",
      "2025-12-08 00:43:16.749996: val_loss -0.8638\n",
      "2025-12-08 00:43:16.754000: Pseudo dice [0.9191, 0.9468, 0.9456]\n",
      "2025-12-08 00:43:16.758004: Epoch time: 137.95 s\n",
      "2025-12-08 00:43:17.413989: \n",
      "2025-12-08 00:43:17.413989: Epoch 328\n",
      "2025-12-08 00:43:17.413989: Current learning rate: 0.00699\n",
      "2025-12-08 00:45:35.344790: train_loss -0.8344\n",
      "2025-12-08 00:45:35.346792: val_loss -0.8561\n",
      "2025-12-08 00:45:35.350796: Pseudo dice [0.9144, 0.9511, 0.9426]\n",
      "2025-12-08 00:45:35.352798: Epoch time: 137.93 s\n",
      "2025-12-08 00:45:36.154520: \n",
      "2025-12-08 00:45:36.154520: Epoch 329\n",
      "2025-12-08 00:45:36.154520: Current learning rate: 0.00698\n",
      "2025-12-08 00:47:54.211820: train_loss -0.8371\n",
      "2025-12-08 00:47:54.213823: val_loss -0.8753\n",
      "2025-12-08 00:47:54.217566: Pseudo dice [0.9261, 0.956, 0.9473]\n",
      "2025-12-08 00:47:54.217566: Epoch time: 138.06 s\n",
      "2025-12-08 00:47:54.223063: Yayy! New best EMA pseudo Dice: 0.9369\n",
      "2025-12-08 00:47:55.242570: \n",
      "2025-12-08 00:47:55.242570: Epoch 330\n",
      "2025-12-08 00:47:55.246795: Current learning rate: 0.00697\n",
      "2025-12-08 00:50:13.170966: train_loss -0.8371\n",
      "2025-12-08 00:50:13.170966: val_loss -0.8652\n",
      "2025-12-08 00:50:13.176441: Pseudo dice [0.9231, 0.9542, 0.9376]\n",
      "2025-12-08 00:50:13.178443: Epoch time: 137.93 s\n",
      "2025-12-08 00:50:13.182447: Yayy! New best EMA pseudo Dice: 0.9371\n",
      "2025-12-08 00:50:14.093762: \n",
      "2025-12-08 00:50:14.093762: Epoch 331\n",
      "2025-12-08 00:50:14.093762: Current learning rate: 0.00696\n",
      "2025-12-08 00:52:32.236237: train_loss -0.8348\n",
      "2025-12-08 00:52:32.237237: val_loss -0.8538\n",
      "2025-12-08 00:52:32.241239: Pseudo dice [0.9123, 0.9469, 0.9399]\n",
      "2025-12-08 00:52:32.243239: Epoch time: 138.14 s\n",
      "2025-12-08 00:52:32.874624: \n",
      "2025-12-08 00:52:32.874624: Epoch 332\n",
      "2025-12-08 00:52:32.874624: Current learning rate: 0.00696\n",
      "2025-12-08 00:54:51.070688: train_loss -0.8361\n",
      "2025-12-08 00:54:51.070688: val_loss -0.8587\n",
      "2025-12-08 00:54:51.076432: Pseudo dice [0.9177, 0.9523, 0.9439]\n",
      "2025-12-08 00:54:51.080440: Epoch time: 138.2 s\n",
      "2025-12-08 00:54:51.827390: \n",
      "2025-12-08 00:54:51.827390: Epoch 333\n",
      "2025-12-08 00:54:51.827390: Current learning rate: 0.00695\n",
      "2025-12-08 00:57:10.046482: train_loss -0.8443\n",
      "2025-12-08 00:57:10.046482: val_loss -0.8656\n",
      "2025-12-08 00:57:10.062560: Pseudo dice [0.9211, 0.9543, 0.9408]\n",
      "2025-12-08 00:57:10.062560: Epoch time: 138.22 s\n",
      "2025-12-08 00:57:10.703147: \n",
      "2025-12-08 00:57:10.703147: Epoch 334\n",
      "2025-12-08 00:57:10.705663: Current learning rate: 0.00694\n",
      "2025-12-08 00:59:28.825096: train_loss -0.8399\n",
      "2025-12-08 00:59:28.825096: val_loss -0.8616\n",
      "2025-12-08 00:59:28.829157: Pseudo dice [0.9123, 0.948, 0.9473]\n",
      "2025-12-08 00:59:28.832157: Epoch time: 138.12 s\n",
      "2025-12-08 00:59:29.639856: \n",
      "2025-12-08 00:59:29.639856: Epoch 335\n",
      "2025-12-08 00:59:29.655511: Current learning rate: 0.00693\n",
      "2025-12-08 01:01:47.720448: train_loss -0.8444\n",
      "2025-12-08 01:01:47.720448: val_loss -0.8728\n",
      "2025-12-08 01:01:47.724528: Pseudo dice [0.9268, 0.9564, 0.9423]\n",
      "2025-12-08 01:01:47.727539: Epoch time: 138.08 s\n",
      "2025-12-08 01:01:47.729544: Yayy! New best EMA pseudo Dice: 0.9374\n",
      "2025-12-08 01:01:48.796588: \n",
      "2025-12-08 01:01:48.796588: Epoch 336\n",
      "2025-12-08 01:01:48.796588: Current learning rate: 0.00692\n",
      "2025-12-08 01:04:06.895915: train_loss -0.8412\n",
      "2025-12-08 01:04:06.895915: val_loss -0.8622\n",
      "2025-12-08 01:04:06.899920: Pseudo dice [0.9196, 0.9497, 0.9404]\n",
      "2025-12-08 01:04:06.901922: Epoch time: 138.1 s\n",
      "2025-12-08 01:04:07.555324: \n",
      "2025-12-08 01:04:07.555324: Epoch 337\n",
      "2025-12-08 01:04:07.559338: Current learning rate: 0.00691\n",
      "2025-12-08 01:06:25.592855: train_loss -0.8339\n",
      "2025-12-08 01:06:25.592855: val_loss -0.8533\n",
      "2025-12-08 01:06:25.592855: Pseudo dice [0.9088, 0.9481, 0.9409]\n",
      "2025-12-08 01:06:25.592855: Epoch time: 138.04 s\n",
      "2025-12-08 01:06:26.245044: \n",
      "2025-12-08 01:06:26.245044: Epoch 338\n",
      "2025-12-08 01:06:26.245044: Current learning rate: 0.0069\n",
      "2025-12-08 01:08:44.328248: train_loss -0.832\n",
      "2025-12-08 01:08:44.343976: val_loss -0.8684\n",
      "2025-12-08 01:08:44.343976: Pseudo dice [0.9254, 0.961, 0.9362]\n",
      "2025-12-08 01:08:44.343976: Epoch time: 138.08 s\n",
      "2025-12-08 01:08:45.040061: \n",
      "2025-12-08 01:08:45.041065: Epoch 339\n",
      "2025-12-08 01:08:45.044077: Current learning rate: 0.00689\n",
      "2025-12-08 01:11:03.092904: train_loss -0.8407\n",
      "2025-12-08 01:11:03.092904: val_loss -0.8619\n",
      "2025-12-08 01:11:03.098425: Pseudo dice [0.9176, 0.945, 0.9456]\n",
      "2025-12-08 01:11:03.100427: Epoch time: 138.05 s\n",
      "2025-12-08 01:11:03.748774: \n",
      "2025-12-08 01:11:03.748774: Epoch 340\n",
      "2025-12-08 01:11:03.748774: Current learning rate: 0.00688\n",
      "2025-12-08 01:13:21.807560: train_loss -0.8421\n",
      "2025-12-08 01:13:21.807560: val_loss -0.8628\n",
      "2025-12-08 01:13:21.811566: Pseudo dice [0.9191, 0.9501, 0.9396]\n",
      "2025-12-08 01:13:21.815308: Epoch time: 138.06 s\n",
      "2025-12-08 01:13:22.651364: \n",
      "2025-12-08 01:13:22.651364: Epoch 341\n",
      "2025-12-08 01:13:22.655158: Current learning rate: 0.00687\n",
      "2025-12-08 01:15:40.764422: train_loss -0.8392\n",
      "2025-12-08 01:15:40.764422: val_loss -0.8556\n",
      "2025-12-08 01:15:40.780231: Pseudo dice [0.9124, 0.9458, 0.9458]\n",
      "2025-12-08 01:15:40.780231: Epoch time: 138.11 s\n",
      "2025-12-08 01:15:41.421033: \n",
      "2025-12-08 01:15:41.421033: Epoch 342\n",
      "2025-12-08 01:15:41.421033: Current learning rate: 0.00686\n",
      "2025-12-08 01:17:59.605619: train_loss -0.7995\n",
      "2025-12-08 01:17:59.607621: val_loss -0.7946\n",
      "2025-12-08 01:17:59.609636: Pseudo dice [0.8933, 0.9287, 0.9201]\n",
      "2025-12-08 01:17:59.612760: Epoch time: 138.18 s\n",
      "2025-12-08 01:18:00.250533: \n",
      "2025-12-08 01:18:00.250533: Epoch 343\n",
      "2025-12-08 01:18:00.250533: Current learning rate: 0.00685\n",
      "2025-12-08 01:20:18.326825: train_loss -0.7811\n",
      "2025-12-08 01:20:18.326825: val_loss -0.8226\n",
      "2025-12-08 01:20:18.331953: Pseudo dice [0.8995, 0.9402, 0.9269]\n",
      "2025-12-08 01:20:18.335957: Epoch time: 138.08 s\n",
      "2025-12-08 01:20:18.984692: \n",
      "2025-12-08 01:20:18.984692: Epoch 344\n",
      "2025-12-08 01:20:18.984692: Current learning rate: 0.00684\n",
      "2025-12-08 01:22:36.951534: train_loss -0.8034\n",
      "2025-12-08 01:22:36.951534: val_loss -0.838\n",
      "2025-12-08 01:22:36.951534: Pseudo dice [0.9067, 0.947, 0.9338]\n",
      "2025-12-08 01:22:36.958352: Epoch time: 137.97 s\n",
      "2025-12-08 01:22:37.650197: \n",
      "2025-12-08 01:22:37.650197: Epoch 345\n",
      "2025-12-08 01:22:37.653610: Current learning rate: 0.00683\n",
      "2025-12-08 01:24:55.852500: train_loss -0.8204\n",
      "2025-12-08 01:24:55.852500: val_loss -0.8533\n",
      "2025-12-08 01:24:55.858245: Pseudo dice [0.9125, 0.9479, 0.9425]\n",
      "2025-12-08 01:24:55.860904: Epoch time: 138.2 s\n",
      "2025-12-08 01:24:56.514814: \n",
      "2025-12-08 01:24:56.514814: Epoch 346\n",
      "2025-12-08 01:24:56.514814: Current learning rate: 0.00682\n",
      "2025-12-08 01:27:14.725862: train_loss -0.8247\n",
      "2025-12-08 01:27:14.725862: val_loss -0.8462\n",
      "2025-12-08 01:27:14.731606: Pseudo dice [0.9152, 0.948, 0.9259]\n",
      "2025-12-08 01:27:14.733609: Epoch time: 138.21 s\n",
      "2025-12-08 01:27:15.577980: \n",
      "2025-12-08 01:27:15.577980: Epoch 347\n",
      "2025-12-08 01:27:15.577980: Current learning rate: 0.00681\n",
      "2025-12-08 01:29:33.735149: train_loss -0.8248\n",
      "2025-12-08 01:29:33.735149: val_loss -0.8482\n",
      "2025-12-08 01:29:33.738709: Pseudo dice [0.9112, 0.9427, 0.9371]\n",
      "2025-12-08 01:29:33.740711: Epoch time: 138.16 s\n",
      "2025-12-08 01:29:34.392215: \n",
      "2025-12-08 01:29:34.392215: Epoch 348\n",
      "2025-12-08 01:29:34.392215: Current learning rate: 0.0068\n",
      "2025-12-08 01:31:52.558987: train_loss -0.8316\n",
      "2025-12-08 01:31:52.560989: val_loss -0.8572\n",
      "2025-12-08 01:31:52.564088: Pseudo dice [0.9129, 0.948, 0.9471]\n",
      "2025-12-08 01:31:52.567147: Epoch time: 138.17 s\n",
      "2025-12-08 01:31:53.251940: \n",
      "2025-12-08 01:31:53.251940: Epoch 349\n",
      "2025-12-08 01:31:53.251940: Current learning rate: 0.0068\n",
      "2025-12-08 01:34:11.312023: train_loss -0.8365\n",
      "2025-12-08 01:34:11.312023: val_loss -0.8603\n",
      "2025-12-08 01:34:11.317610: Pseudo dice [0.9209, 0.9527, 0.9323]\n",
      "2025-12-08 01:34:11.319613: Epoch time: 138.06 s\n",
      "2025-12-08 01:34:12.230705: \n",
      "2025-12-08 01:34:12.230705: Epoch 350\n",
      "2025-12-08 01:34:12.232922: Current learning rate: 0.00679\n",
      "2025-12-08 01:36:30.561439: train_loss -0.8304\n",
      "2025-12-08 01:36:30.561439: val_loss -0.8519\n",
      "2025-12-08 01:36:30.577500: Pseudo dice [0.9085, 0.9462, 0.9415]\n",
      "2025-12-08 01:36:30.577500: Epoch time: 138.33 s\n",
      "2025-12-08 01:36:31.218225: \n",
      "2025-12-08 01:36:31.218225: Epoch 351\n",
      "2025-12-08 01:36:31.234223: Current learning rate: 0.00678\n",
      "2025-12-08 01:38:49.354671: train_loss -0.8347\n",
      "2025-12-08 01:38:49.354671: val_loss -0.8685\n",
      "2025-12-08 01:38:49.360415: Pseudo dice [0.9259, 0.9539, 0.9387]\n",
      "2025-12-08 01:38:49.362975: Epoch time: 138.14 s\n",
      "2025-12-08 01:38:50.015067: \n",
      "2025-12-08 01:38:50.015067: Epoch 352\n",
      "2025-12-08 01:38:50.015067: Current learning rate: 0.00677\n",
      "2025-12-08 01:41:08.108850: train_loss -0.8333\n",
      "2025-12-08 01:41:08.108850: val_loss -0.8684\n",
      "2025-12-08 01:41:08.124717: Pseudo dice [0.9172, 0.9557, 0.9453]\n",
      "2025-12-08 01:41:08.124717: Epoch time: 138.09 s\n",
      "2025-12-08 01:41:08.945081: \n",
      "2025-12-08 01:41:08.945081: Epoch 353\n",
      "2025-12-08 01:41:08.945081: Current learning rate: 0.00676\n",
      "2025-12-08 01:43:26.968684: train_loss -0.8279\n",
      "2025-12-08 01:43:26.968684: val_loss -0.8627\n",
      "2025-12-08 01:43:26.968684: Pseudo dice [0.9183, 0.953, 0.935]\n",
      "2025-12-08 01:43:26.984771: Epoch time: 138.03 s\n",
      "2025-12-08 01:43:27.639426: \n",
      "2025-12-08 01:43:27.639426: Epoch 354\n",
      "2025-12-08 01:43:27.639426: Current learning rate: 0.00675\n",
      "2025-12-08 01:45:45.717525: train_loss -0.8356\n",
      "2025-12-08 01:45:45.717525: val_loss -0.8698\n",
      "2025-12-08 01:45:45.733621: Pseudo dice [0.9221, 0.9576, 0.946]\n",
      "2025-12-08 01:45:45.733621: Epoch time: 138.09 s\n",
      "2025-12-08 01:45:46.389168: \n",
      "2025-12-08 01:45:46.389168: Epoch 355\n",
      "2025-12-08 01:45:46.389168: Current learning rate: 0.00674\n",
      "2025-12-08 01:48:04.371220: train_loss -0.8378\n",
      "2025-12-08 01:48:04.371220: val_loss -0.8699\n",
      "2025-12-08 01:48:04.376262: Pseudo dice [0.9244, 0.9558, 0.9416]\n",
      "2025-12-08 01:48:04.379269: Epoch time: 137.98 s\n",
      "2025-12-08 01:48:05.165694: \n",
      "2025-12-08 01:48:05.165694: Epoch 356\n",
      "2025-12-08 01:48:05.169703: Current learning rate: 0.00673\n",
      "2025-12-08 01:50:23.312317: train_loss -0.8368\n",
      "2025-12-08 01:50:23.312317: val_loss -0.8604\n",
      "2025-12-08 01:50:23.312317: Pseudo dice [0.9171, 0.9495, 0.9422]\n",
      "2025-12-08 01:50:23.328018: Epoch time: 138.15 s\n",
      "2025-12-08 01:50:23.952336: \n",
      "2025-12-08 01:50:23.952336: Epoch 357\n",
      "2025-12-08 01:50:23.968353: Current learning rate: 0.00672\n",
      "2025-12-08 01:52:42.171222: train_loss -0.8397\n",
      "2025-12-08 01:52:42.171222: val_loss -0.8647\n",
      "2025-12-08 01:52:42.176645: Pseudo dice [0.9231, 0.9532, 0.9369]\n",
      "2025-12-08 01:52:42.176645: Epoch time: 138.22 s\n",
      "2025-12-08 01:52:42.827331: \n",
      "2025-12-08 01:52:42.827331: Epoch 358\n",
      "2025-12-08 01:52:42.827331: Current learning rate: 0.00671\n",
      "2025-12-08 01:55:01.013851: train_loss -0.8443\n",
      "2025-12-08 01:55:01.014851: val_loss -0.8666\n",
      "2025-12-08 01:55:01.019182: Pseudo dice [0.9196, 0.954, 0.944]\n",
      "2025-12-08 01:55:01.022182: Epoch time: 138.19 s\n",
      "2025-12-08 01:55:02.041869: \n",
      "2025-12-08 01:55:02.041869: Epoch 359\n",
      "2025-12-08 01:55:02.046123: Current learning rate: 0.0067\n",
      "2025-12-08 01:57:20.124151: train_loss -0.8382\n",
      "2025-12-08 01:57:20.127131: val_loss -0.8574\n",
      "2025-12-08 01:57:20.130574: Pseudo dice [0.9165, 0.9488, 0.9435]\n",
      "2025-12-08 01:57:20.130574: Epoch time: 138.08 s\n",
      "2025-12-08 01:57:20.780443: \n",
      "2025-12-08 01:57:20.780443: Epoch 360\n",
      "2025-12-08 01:57:20.780443: Current learning rate: 0.00669\n",
      "2025-12-08 01:59:38.846765: train_loss -0.8385\n",
      "2025-12-08 01:59:38.847765: val_loss -0.8686\n",
      "2025-12-08 01:59:38.851766: Pseudo dice [0.9226, 0.9551, 0.9394]\n",
      "2025-12-08 01:59:38.854766: Epoch time: 138.07 s\n",
      "2025-12-08 01:59:39.499969: \n",
      "2025-12-08 01:59:39.499969: Epoch 361\n",
      "2025-12-08 01:59:39.518119: Current learning rate: 0.00668\n",
      "2025-12-08 02:01:57.582834: train_loss -0.8362\n",
      "2025-12-08 02:01:57.582834: val_loss -0.8644\n",
      "2025-12-08 02:01:57.586839: Pseudo dice [0.9205, 0.9573, 0.9392]\n",
      "2025-12-08 02:01:57.586839: Epoch time: 138.08 s\n",
      "2025-12-08 02:01:58.388101: \n",
      "2025-12-08 02:01:58.389105: Epoch 362\n",
      "2025-12-08 02:01:58.390108: Current learning rate: 0.00667\n",
      "2025-12-08 02:04:16.454195: train_loss -0.8386\n",
      "2025-12-08 02:04:16.455198: val_loss -0.8648\n",
      "2025-12-08 02:04:16.459213: Pseudo dice [0.9212, 0.952, 0.9433]\n",
      "2025-12-08 02:04:16.462224: Epoch time: 138.07 s\n",
      "2025-12-08 02:04:17.092880: \n",
      "2025-12-08 02:04:17.092880: Epoch 363\n",
      "2025-12-08 02:04:17.111848: Current learning rate: 0.00666\n",
      "2025-12-08 02:06:35.204291: train_loss -0.8296\n",
      "2025-12-08 02:06:35.204291: val_loss -0.8566\n",
      "2025-12-08 02:06:35.204291: Pseudo dice [0.9153, 0.9502, 0.9389]\n",
      "2025-12-08 02:06:35.204291: Epoch time: 138.11 s\n",
      "2025-12-08 02:06:35.842953: \n",
      "2025-12-08 02:06:35.842953: Epoch 364\n",
      "2025-12-08 02:06:35.858902: Current learning rate: 0.00665\n",
      "2025-12-08 02:08:54.124501: train_loss -0.8394\n",
      "2025-12-08 02:08:54.124501: val_loss -0.8642\n",
      "2025-12-08 02:08:54.128347: Pseudo dice [0.916, 0.9535, 0.945]\n",
      "2025-12-08 02:08:54.131843: Epoch time: 138.28 s\n",
      "2025-12-08 02:08:55.108622: \n",
      "2025-12-08 02:08:55.108622: Epoch 365\n",
      "2025-12-08 02:08:55.114011: Current learning rate: 0.00665\n",
      "2025-12-08 02:11:13.242517: train_loss -0.8414\n",
      "2025-12-08 02:11:13.244520: val_loss -0.8582\n",
      "2025-12-08 02:11:13.248523: Pseudo dice [0.9141, 0.9472, 0.9443]\n",
      "2025-12-08 02:11:13.252028: Epoch time: 138.13 s\n",
      "2025-12-08 02:11:13.951745: \n",
      "2025-12-08 02:11:13.951745: Epoch 366\n",
      "2025-12-08 02:11:13.967403: Current learning rate: 0.00664\n",
      "2025-12-08 02:13:32.140447: train_loss -0.8401\n",
      "2025-12-08 02:13:32.140447: val_loss -0.8699\n",
      "2025-12-08 02:13:32.158419: Pseudo dice [0.9207, 0.9599, 0.9378]\n",
      "2025-12-08 02:13:32.162423: Epoch time: 138.19 s\n",
      "2025-12-08 02:13:32.813192: \n",
      "2025-12-08 02:13:32.813192: Epoch 367\n",
      "2025-12-08 02:13:32.813192: Current learning rate: 0.00663\n",
      "2025-12-08 02:15:50.979984: train_loss -0.839\n",
      "2025-12-08 02:15:50.981987: val_loss -0.8521\n",
      "2025-12-08 02:15:50.985732: Pseudo dice [0.9093, 0.9418, 0.9429]\n",
      "2025-12-08 02:15:50.987735: Epoch time: 138.17 s\n",
      "2025-12-08 02:15:51.758043: \n",
      "2025-12-08 02:15:51.758043: Epoch 368\n",
      "2025-12-08 02:15:51.760046: Current learning rate: 0.00662\n",
      "2025-12-08 02:18:09.824323: train_loss -0.8444\n",
      "2025-12-08 02:18:09.824323: val_loss -0.8748\n",
      "2025-12-08 02:18:09.827139: Pseudo dice [0.9248, 0.9614, 0.9434]\n",
      "2025-12-08 02:18:09.827139: Epoch time: 138.07 s\n",
      "2025-12-08 02:18:10.483166: \n",
      "2025-12-08 02:18:10.483166: Epoch 369\n",
      "2025-12-08 02:18:10.483166: Current learning rate: 0.00661\n",
      "2025-12-08 02:20:28.647552: train_loss -0.8394\n",
      "2025-12-08 02:20:28.649554: val_loss -0.8763\n",
      "2025-12-08 02:20:28.651985: Pseudo dice [0.9246, 0.9604, 0.945]\n",
      "2025-12-08 02:20:28.655488: Epoch time: 138.16 s\n",
      "2025-12-08 02:20:28.659087: Yayy! New best EMA pseudo Dice: 0.9377\n",
      "2025-12-08 02:20:29.564861: \n",
      "2025-12-08 02:20:29.564861: Epoch 370\n",
      "2025-12-08 02:20:29.564861: Current learning rate: 0.0066\n",
      "2025-12-08 02:22:47.487225: train_loss -0.8282\n",
      "2025-12-08 02:22:47.488229: val_loss -0.8508\n",
      "2025-12-08 02:22:47.492233: Pseudo dice [0.9151, 0.9466, 0.9348]\n",
      "2025-12-08 02:22:47.495234: Epoch time: 137.92 s\n",
      "2025-12-08 02:22:48.405642: \n",
      "2025-12-08 02:22:48.405642: Epoch 371\n",
      "2025-12-08 02:22:48.405642: Current learning rate: 0.00659\n",
      "2025-12-08 02:25:06.483941: train_loss -0.8183\n",
      "2025-12-08 02:25:06.483941: val_loss -0.8434\n",
      "2025-12-08 02:25:06.491540: Pseudo dice [0.9136, 0.9454, 0.9262]\n",
      "2025-12-08 02:25:06.493543: Epoch time: 138.08 s\n",
      "2025-12-08 02:25:07.151245: \n",
      "2025-12-08 02:25:07.151245: Epoch 372\n",
      "2025-12-08 02:25:07.154933: Current learning rate: 0.00658\n",
      "2025-12-08 02:27:25.075832: train_loss -0.825\n",
      "2025-12-08 02:27:25.075832: val_loss -0.8476\n",
      "2025-12-08 02:27:25.081002: Pseudo dice [0.9109, 0.9462, 0.9405]\n",
      "2025-12-08 02:27:25.084002: Epoch time: 137.93 s\n",
      "2025-12-08 02:27:25.764031: \n",
      "2025-12-08 02:27:25.764031: Epoch 373\n",
      "2025-12-08 02:27:25.779832: Current learning rate: 0.00657\n",
      "2025-12-08 02:29:43.936963: train_loss -0.8283\n",
      "2025-12-08 02:29:43.938966: val_loss -0.873\n",
      "2025-12-08 02:29:43.942971: Pseudo dice [0.9268, 0.9564, 0.9432]\n",
      "2025-12-08 02:29:43.946713: Epoch time: 138.17 s\n",
      "2025-12-08 02:29:44.670276: \n",
      "2025-12-08 02:29:44.670276: Epoch 374\n",
      "2025-12-08 02:29:44.681874: Current learning rate: 0.00656\n",
      "2025-12-08 02:32:02.868458: train_loss -0.835\n",
      "2025-12-08 02:32:02.870461: val_loss -0.8624\n",
      "2025-12-08 02:32:02.874205: Pseudo dice [0.919, 0.9534, 0.9409]\n",
      "2025-12-08 02:32:02.874205: Epoch time: 138.2 s\n",
      "2025-12-08 02:32:03.530175: \n",
      "2025-12-08 02:32:03.530175: Epoch 375\n",
      "2025-12-08 02:32:03.530175: Current learning rate: 0.00655\n",
      "2025-12-08 02:34:21.592253: train_loss -0.8421\n",
      "2025-12-08 02:34:21.592253: val_loss -0.8704\n",
      "2025-12-08 02:34:21.608191: Pseudo dice [0.9219, 0.9569, 0.9439]\n",
      "2025-12-08 02:34:21.608191: Epoch time: 138.08 s\n",
      "2025-12-08 02:34:22.264355: \n",
      "2025-12-08 02:34:22.264355: Epoch 376\n",
      "2025-12-08 02:34:22.264355: Current learning rate: 0.00654\n",
      "2025-12-08 02:36:40.390229: train_loss -0.841\n",
      "2025-12-08 02:36:40.390229: val_loss -0.8527\n",
      "2025-12-08 02:36:40.399731: Pseudo dice [0.9141, 0.9436, 0.9419]\n",
      "2025-12-08 02:36:40.401734: Epoch time: 138.13 s\n",
      "2025-12-08 02:36:41.232527: \n",
      "2025-12-08 02:36:41.233535: Epoch 377\n",
      "2025-12-08 02:36:41.233535: Current learning rate: 0.00653\n",
      "2025-12-08 02:38:59.583930: train_loss -0.8316\n",
      "2025-12-08 02:38:59.587930: val_loss -0.8659\n",
      "2025-12-08 02:38:59.591931: Pseudo dice [0.92, 0.9551, 0.9401]\n",
      "2025-12-08 02:38:59.594999: Epoch time: 138.35 s\n",
      "2025-12-08 02:39:00.481476: \n",
      "2025-12-08 02:39:00.483479: Epoch 378\n",
      "2025-12-08 02:39:00.486238: Current learning rate: 0.00652\n",
      "2025-12-08 02:41:18.677762: train_loss -0.8364\n",
      "2025-12-08 02:41:18.678762: val_loss -0.8603\n",
      "2025-12-08 02:41:18.682194: Pseudo dice [0.9148, 0.9507, 0.9418]\n",
      "2025-12-08 02:41:18.685194: Epoch time: 138.2 s\n",
      "2025-12-08 02:41:19.346641: \n",
      "2025-12-08 02:41:19.347642: Epoch 379\n",
      "2025-12-08 02:41:19.350807: Current learning rate: 0.00651\n",
      "2025-12-08 02:43:37.487444: train_loss -0.8377\n",
      "2025-12-08 02:43:37.487444: val_loss -0.8735\n",
      "2025-12-08 02:43:37.487444: Pseudo dice [0.9245, 0.9589, 0.9465]\n",
      "2025-12-08 02:43:37.487444: Epoch time: 138.14 s\n",
      "2025-12-08 02:43:38.139395: \n",
      "2025-12-08 02:43:38.139395: Epoch 380\n",
      "2025-12-08 02:43:38.155071: Current learning rate: 0.0065\n",
      "2025-12-08 02:45:56.295943: train_loss -0.8448\n",
      "2025-12-08 02:45:56.295943: val_loss -0.8577\n",
      "2025-12-08 02:45:56.295943: Pseudo dice [0.913, 0.9456, 0.9423]\n",
      "2025-12-08 02:45:56.295943: Epoch time: 138.16 s\n",
      "2025-12-08 02:45:56.951688: \n",
      "2025-12-08 02:45:56.951688: Epoch 381\n",
      "2025-12-08 02:45:56.951688: Current learning rate: 0.00649\n",
      "2025-12-08 02:48:15.217619: train_loss -0.8397\n",
      "2025-12-08 02:48:15.217619: val_loss -0.872\n",
      "2025-12-08 02:48:15.221240: Pseudo dice [0.922, 0.9535, 0.9402]\n",
      "2025-12-08 02:48:15.225722: Epoch time: 138.27 s\n",
      "2025-12-08 02:48:15.873787: \n",
      "2025-12-08 02:48:15.873787: Epoch 382\n",
      "2025-12-08 02:48:15.873787: Current learning rate: 0.00648\n",
      "2025-12-08 02:50:34.107256: train_loss -0.8444\n",
      "2025-12-08 02:50:34.109001: val_loss -0.8674\n",
      "2025-12-08 02:50:34.113009: Pseudo dice [0.9207, 0.9543, 0.9361]\n",
      "2025-12-08 02:50:34.117013: Epoch time: 138.23 s\n",
      "2025-12-08 02:50:35.030485: \n",
      "2025-12-08 02:50:35.030485: Epoch 383\n",
      "2025-12-08 02:50:35.046127: Current learning rate: 0.00648\n",
      "2025-12-08 02:52:53.181716: train_loss -0.8495\n",
      "2025-12-08 02:52:53.181716: val_loss -0.86\n",
      "2025-12-08 02:52:53.187564: Pseudo dice [0.9168, 0.9484, 0.9478]\n",
      "2025-12-08 02:52:53.190568: Epoch time: 138.15 s\n",
      "2025-12-08 02:52:53.855164: \n",
      "2025-12-08 02:52:53.855164: Epoch 384\n",
      "2025-12-08 02:52:53.858153: Current learning rate: 0.00647\n",
      "2025-12-08 02:55:12.061866: train_loss -0.8437\n",
      "2025-12-08 02:55:12.061866: val_loss -0.8639\n",
      "2025-12-08 02:55:12.077723: Pseudo dice [0.9171, 0.9476, 0.9466]\n",
      "2025-12-08 02:55:12.077723: Epoch time: 138.21 s\n",
      "2025-12-08 02:55:12.733779: \n",
      "2025-12-08 02:55:12.733779: Epoch 385\n",
      "2025-12-08 02:55:12.749818: Current learning rate: 0.00646\n",
      "2025-12-08 02:57:30.735100: train_loss -0.8416\n",
      "2025-12-08 02:57:30.735100: val_loss -0.8721\n",
      "2025-12-08 02:57:30.735100: Pseudo dice [0.9287, 0.9572, 0.9451]\n",
      "2025-12-08 02:57:30.735100: Epoch time: 138.0 s\n",
      "2025-12-08 02:57:30.735100: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2025-12-08 02:57:31.786474: \n",
      "2025-12-08 02:57:31.788477: Epoch 386\n",
      "2025-12-08 02:57:31.792483: Current learning rate: 0.00645\n",
      "2025-12-08 02:59:49.942345: train_loss -0.8341\n",
      "2025-12-08 02:59:49.943347: val_loss -0.8669\n",
      "2025-12-08 02:59:49.947356: Pseudo dice [0.9189, 0.9476, 0.949]\n",
      "2025-12-08 02:59:49.950357: Epoch time: 138.16 s\n",
      "2025-12-08 02:59:49.954278: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2025-12-08 02:59:50.921900: \n",
      "2025-12-08 02:59:50.922902: Epoch 387\n",
      "2025-12-08 02:59:50.925962: Current learning rate: 0.00644\n",
      "2025-12-08 03:02:09.124265: train_loss -0.8449\n",
      "2025-12-08 03:02:09.124265: val_loss -0.8562\n",
      "2025-12-08 03:02:09.142216: Pseudo dice [0.9161, 0.9476, 0.9387]\n",
      "2025-12-08 03:02:09.144219: Epoch time: 138.2 s\n",
      "2025-12-08 03:02:09.808990: \n",
      "2025-12-08 03:02:09.808990: Epoch 388\n",
      "2025-12-08 03:02:09.813210: Current learning rate: 0.00643\n",
      "2025-12-08 03:04:27.968601: train_loss -0.8448\n",
      "2025-12-08 03:04:27.968601: val_loss -0.8572\n",
      "2025-12-08 03:04:27.968601: Pseudo dice [0.9146, 0.947, 0.937]\n",
      "2025-12-08 03:04:27.983935: Epoch time: 138.16 s\n",
      "2025-12-08 03:04:28.952362: \n",
      "2025-12-08 03:04:28.952362: Epoch 389\n",
      "2025-12-08 03:04:28.952362: Current learning rate: 0.00642\n",
      "2025-12-08 03:06:47.077363: train_loss -0.8412\n",
      "2025-12-08 03:06:47.077363: val_loss -0.8699\n",
      "2025-12-08 03:06:47.093205: Pseudo dice [0.9214, 0.9549, 0.9484]\n",
      "2025-12-08 03:06:47.093205: Epoch time: 138.14 s\n",
      "2025-12-08 03:06:47.784683: \n",
      "2025-12-08 03:06:47.785685: Epoch 390\n",
      "2025-12-08 03:06:47.788692: Current learning rate: 0.00641\n",
      "2025-12-08 03:09:06.078344: train_loss -0.8399\n",
      "2025-12-08 03:09:06.078344: val_loss -0.8639\n",
      "2025-12-08 03:09:06.083585: Pseudo dice [0.9156, 0.9525, 0.9453]\n",
      "2025-12-08 03:09:06.083585: Epoch time: 138.29 s\n",
      "2025-12-08 03:09:06.733888: \n",
      "2025-12-08 03:09:06.733888: Epoch 391\n",
      "2025-12-08 03:09:06.733888: Current learning rate: 0.0064\n",
      "2025-12-08 03:11:24.836907: train_loss -0.8387\n",
      "2025-12-08 03:11:24.836907: val_loss -0.8686\n",
      "2025-12-08 03:11:24.840911: Pseudo dice [0.9233, 0.954, 0.9407]\n",
      "2025-12-08 03:11:24.842912: Epoch time: 138.1 s\n",
      "2025-12-08 03:11:25.509063: \n",
      "2025-12-08 03:11:25.509063: Epoch 392\n",
      "2025-12-08 03:11:25.509063: Current learning rate: 0.00639\n",
      "2025-12-08 03:13:43.632042: train_loss -0.8453\n",
      "2025-12-08 03:13:43.632042: val_loss -0.8676\n",
      "2025-12-08 03:13:43.636046: Pseudo dice [0.9191, 0.956, 0.947]\n",
      "2025-12-08 03:13:43.641791: Epoch time: 138.12 s\n",
      "2025-12-08 03:13:43.643794: Yayy! New best EMA pseudo Dice: 0.938\n",
      "2025-12-08 03:13:44.609759: \n",
      "2025-12-08 03:13:44.609759: Epoch 393\n",
      "2025-12-08 03:13:44.609759: Current learning rate: 0.00638\n",
      "2025-12-08 03:16:02.657092: train_loss -0.843\n",
      "2025-12-08 03:16:02.657092: val_loss -0.8646\n",
      "2025-12-08 03:16:02.661096: Pseudo dice [0.9182, 0.9535, 0.947]\n",
      "2025-12-08 03:16:02.665100: Epoch time: 138.05 s\n",
      "2025-12-08 03:16:02.667102: Yayy! New best EMA pseudo Dice: 0.9382\n",
      "2025-12-08 03:16:03.612159: \n",
      "2025-12-08 03:16:03.612159: Epoch 394\n",
      "2025-12-08 03:16:03.616231: Current learning rate: 0.00637\n",
      "2025-12-08 03:18:21.594995: train_loss -0.8362\n",
      "2025-12-08 03:18:21.594995: val_loss -0.8691\n",
      "2025-12-08 03:18:21.599014: Pseudo dice [0.9263, 0.9552, 0.9373]\n",
      "2025-12-08 03:18:21.602023: Epoch time: 137.98 s\n",
      "2025-12-08 03:18:21.605032: Yayy! New best EMA pseudo Dice: 0.9383\n",
      "2025-12-08 03:18:22.515700: \n",
      "2025-12-08 03:18:22.515700: Epoch 395\n",
      "2025-12-08 03:18:22.530218: Current learning rate: 0.00636\n",
      "2025-12-08 03:20:40.606604: train_loss -0.8373\n",
      "2025-12-08 03:20:40.608344: val_loss -0.8663\n",
      "2025-12-08 03:20:40.614354: Pseudo dice [0.9227, 0.9539, 0.9458]\n",
      "2025-12-08 03:20:40.618360: Epoch time: 138.09 s\n",
      "2025-12-08 03:20:40.622364: Yayy! New best EMA pseudo Dice: 0.9386\n",
      "2025-12-08 03:20:41.578708: \n",
      "2025-12-08 03:20:41.578708: Epoch 396\n",
      "2025-12-08 03:20:41.578708: Current learning rate: 0.00635\n",
      "2025-12-08 03:22:59.704244: train_loss -0.842\n",
      "2025-12-08 03:22:59.704244: val_loss -0.8651\n",
      "2025-12-08 03:22:59.708248: Pseudo dice [0.9164, 0.9501, 0.9515]\n",
      "2025-12-08 03:22:59.710250: Epoch time: 138.13 s\n",
      "2025-12-08 03:22:59.714256: Yayy! New best EMA pseudo Dice: 0.9386\n",
      "2025-12-08 03:23:00.651760: \n",
      "2025-12-08 03:23:00.653500: Epoch 397\n",
      "2025-12-08 03:23:00.657289: Current learning rate: 0.00634\n",
      "2025-12-08 03:25:18.843692: train_loss -0.8387\n",
      "2025-12-08 03:25:18.843692: val_loss -0.8817\n",
      "2025-12-08 03:25:18.848744: Pseudo dice [0.93, 0.9595, 0.9471]\n",
      "2025-12-08 03:25:18.853226: Epoch time: 138.19 s\n",
      "2025-12-08 03:25:18.856241: Yayy! New best EMA pseudo Dice: 0.9393\n",
      "2025-12-08 03:25:19.796421: \n",
      "2025-12-08 03:25:19.796421: Epoch 398\n",
      "2025-12-08 03:25:19.796421: Current learning rate: 0.00633\n",
      "2025-12-08 03:27:37.898034: train_loss -0.8438\n",
      "2025-12-08 03:27:37.899036: val_loss -0.8692\n",
      "2025-12-08 03:27:37.902038: Pseudo dice [0.9198, 0.9604, 0.9428]\n",
      "2025-12-08 03:27:37.905398: Epoch time: 138.1 s\n",
      "2025-12-08 03:27:37.905398: Yayy! New best EMA pseudo Dice: 0.9395\n",
      "2025-12-08 03:27:38.844100: \n",
      "2025-12-08 03:27:38.844100: Epoch 399\n",
      "2025-12-08 03:27:38.855774: Current learning rate: 0.00632\n",
      "2025-12-08 03:29:56.905093: train_loss -0.8466\n",
      "2025-12-08 03:29:56.907132: val_loss -0.856\n",
      "2025-12-08 03:29:56.909134: Pseudo dice [0.9121, 0.9491, 0.9414]\n",
      "2025-12-08 03:29:56.914598: Epoch time: 138.06 s\n",
      "2025-12-08 03:29:57.997496: \n",
      "2025-12-08 03:29:57.998498: Epoch 400\n",
      "2025-12-08 03:29:57.999501: Current learning rate: 0.00631\n",
      "2025-12-08 03:32:15.973520: train_loss -0.8398\n",
      "2025-12-08 03:32:15.973520: val_loss -0.8713\n",
      "2025-12-08 03:32:15.975522: Pseudo dice [0.9202, 0.9549, 0.9446]\n",
      "2025-12-08 03:32:15.981361: Epoch time: 137.98 s\n",
      "2025-12-08 03:32:16.678240: \n",
      "2025-12-08 03:32:16.678240: Epoch 401\n",
      "2025-12-08 03:32:16.680244: Current learning rate: 0.0063\n",
      "2025-12-08 03:34:34.480013: train_loss -0.8459\n",
      "2025-12-08 03:34:34.480013: val_loss -0.8769\n",
      "2025-12-08 03:34:34.485519: Pseudo dice [0.9247, 0.9573, 0.9474]\n",
      "2025-12-08 03:34:34.489523: Epoch time: 137.81 s\n",
      "2025-12-08 03:34:35.139940: \n",
      "2025-12-08 03:34:35.139940: Epoch 402\n",
      "2025-12-08 03:34:35.158858: Current learning rate: 0.0063\n",
      "2025-12-08 03:36:53.233999: train_loss -0.8394\n",
      "2025-12-08 03:36:53.233999: val_loss -0.8556\n",
      "2025-12-08 03:36:53.233999: Pseudo dice [0.9148, 0.9464, 0.9412]\n",
      "2025-12-08 03:36:53.241812: Epoch time: 138.09 s\n",
      "2025-12-08 03:36:53.890372: \n",
      "2025-12-08 03:36:53.890372: Epoch 403\n",
      "2025-12-08 03:36:53.890372: Current learning rate: 0.00629\n",
      "2025-12-08 03:39:12.027005: train_loss -0.8364\n",
      "2025-12-08 03:39:12.028007: val_loss -0.8705\n",
      "2025-12-08 03:39:12.032205: Pseudo dice [0.9221, 0.955, 0.9471]\n",
      "2025-12-08 03:39:12.032205: Epoch time: 138.14 s\n",
      "2025-12-08 03:39:12.718785: \n",
      "2025-12-08 03:39:12.718785: Epoch 404\n",
      "2025-12-08 03:39:12.718785: Current learning rate: 0.00628\n",
      "2025-12-08 03:41:31.069227: train_loss -0.8408\n",
      "2025-12-08 03:41:31.071230: val_loss -0.8509\n",
      "2025-12-08 03:41:31.075236: Pseudo dice [0.9118, 0.9502, 0.9361]\n",
      "2025-12-08 03:41:31.077238: Epoch time: 138.35 s\n",
      "2025-12-08 03:41:31.904927: \n",
      "2025-12-08 03:41:31.904927: Epoch 405\n",
      "2025-12-08 03:41:31.904927: Current learning rate: 0.00627\n",
      "2025-12-08 03:43:50.056937: train_loss -0.8406\n",
      "2025-12-08 03:43:50.057938: val_loss -0.8555\n",
      "2025-12-08 03:43:50.060939: Pseudo dice [0.9133, 0.951, 0.9398]\n",
      "2025-12-08 03:43:50.066652: Epoch time: 138.15 s\n",
      "2025-12-08 03:43:50.719065: \n",
      "2025-12-08 03:43:50.719065: Epoch 406\n",
      "2025-12-08 03:43:50.719065: Current learning rate: 0.00626\n",
      "2025-12-08 03:46:08.916267: train_loss -0.8316\n",
      "2025-12-08 03:46:08.916267: val_loss -0.8566\n",
      "2025-12-08 03:46:08.920272: Pseudo dice [0.9154, 0.9492, 0.9412]\n",
      "2025-12-08 03:46:08.924170: Epoch time: 138.2 s\n",
      "2025-12-08 03:46:09.584572: \n",
      "2025-12-08 03:46:09.585577: Epoch 407\n",
      "2025-12-08 03:46:09.588320: Current learning rate: 0.00625\n",
      "2025-12-08 03:48:27.671250: train_loss -0.8335\n",
      "2025-12-08 03:48:27.671250: val_loss -0.8674\n",
      "2025-12-08 03:48:27.682055: Pseudo dice [0.923, 0.9537, 0.9422]\n",
      "2025-12-08 03:48:27.682055: Epoch time: 138.09 s\n",
      "2025-12-08 03:48:28.327754: \n",
      "2025-12-08 03:48:28.327754: Epoch 408\n",
      "2025-12-08 03:48:28.337674: Current learning rate: 0.00624\n",
      "2025-12-08 03:50:46.390258: train_loss -0.8379\n",
      "2025-12-08 03:50:46.390258: val_loss -0.8678\n",
      "2025-12-08 03:50:46.397342: Pseudo dice [0.9205, 0.9591, 0.9444]\n",
      "2025-12-08 03:50:46.397342: Epoch time: 138.06 s\n",
      "2025-12-08 03:50:47.055384: \n",
      "2025-12-08 03:50:47.056397: Epoch 409\n",
      "2025-12-08 03:50:47.056397: Current learning rate: 0.00623\n",
      "2025-12-08 03:53:05.265937: train_loss -0.8405\n",
      "2025-12-08 03:53:05.265937: val_loss -0.8636\n",
      "2025-12-08 03:53:05.269941: Pseudo dice [0.9153, 0.955, 0.9437]\n",
      "2025-12-08 03:53:05.273945: Epoch time: 138.22 s\n",
      "2025-12-08 03:53:05.937830: \n",
      "2025-12-08 03:53:05.937830: Epoch 410\n",
      "2025-12-08 03:53:05.937830: Current learning rate: 0.00622\n",
      "2025-12-08 03:55:24.264482: train_loss -0.8416\n",
      "2025-12-08 03:55:24.264482: val_loss -0.8626\n",
      "2025-12-08 03:55:24.280422: Pseudo dice [0.915, 0.9494, 0.9406]\n",
      "2025-12-08 03:55:24.280422: Epoch time: 138.33 s\n",
      "2025-12-08 03:55:25.078295: \n",
      "2025-12-08 03:55:25.078295: Epoch 411\n",
      "2025-12-08 03:55:25.078295: Current learning rate: 0.00621\n",
      "2025-12-08 03:57:43.248443: train_loss -0.8467\n",
      "2025-12-08 03:57:43.248443: val_loss -0.877\n",
      "2025-12-08 03:57:43.248443: Pseudo dice [0.926, 0.9551, 0.9504]\n",
      "2025-12-08 03:57:43.248443: Epoch time: 138.19 s\n",
      "2025-12-08 03:57:43.889479: \n",
      "2025-12-08 03:57:43.889479: Epoch 412\n",
      "2025-12-08 03:57:43.889479: Current learning rate: 0.0062\n",
      "2025-12-08 04:00:02.046167: train_loss -0.8395\n",
      "2025-12-08 04:00:02.046167: val_loss -0.8635\n",
      "2025-12-08 04:00:02.053979: Pseudo dice [0.922, 0.9522, 0.9394]\n",
      "2025-12-08 04:00:02.053979: Epoch time: 138.16 s\n",
      "2025-12-08 04:00:02.687552: \n",
      "2025-12-08 04:00:02.687552: Epoch 413\n",
      "2025-12-08 04:00:02.687552: Current learning rate: 0.00619\n",
      "2025-12-08 04:02:21.028466: train_loss -0.8401\n",
      "2025-12-08 04:02:21.030207: val_loss -0.8649\n",
      "2025-12-08 04:02:21.030207: Pseudo dice [0.9202, 0.9539, 0.9393]\n",
      "2025-12-08 04:02:21.036333: Epoch time: 138.34 s\n",
      "2025-12-08 04:02:21.654998: \n",
      "2025-12-08 04:02:21.654998: Epoch 414\n",
      "2025-12-08 04:02:21.670675: Current learning rate: 0.00618\n",
      "2025-12-08 04:04:39.889251: train_loss -0.8442\n",
      "2025-12-08 04:04:39.889251: val_loss -0.8683\n",
      "2025-12-08 04:04:39.895257: Pseudo dice [0.9224, 0.9538, 0.9472]\n",
      "2025-12-08 04:04:39.898999: Epoch time: 138.23 s\n",
      "2025-12-08 04:04:40.530149: \n",
      "2025-12-08 04:04:40.530149: Epoch 415\n",
      "2025-12-08 04:04:40.530149: Current learning rate: 0.00617\n",
      "2025-12-08 04:06:58.717176: train_loss -0.8432\n",
      "2025-12-08 04:06:58.718917: val_loss -0.8669\n",
      "2025-12-08 04:06:58.724041: Pseudo dice [0.9213, 0.9531, 0.9443]\n",
      "2025-12-08 04:06:58.728042: Epoch time: 138.19 s\n",
      "2025-12-08 04:06:59.367639: \n",
      "2025-12-08 04:06:59.367639: Epoch 416\n",
      "2025-12-08 04:06:59.371012: Current learning rate: 0.00616\n",
      "2025-12-08 04:09:17.525694: train_loss -0.8453\n",
      "2025-12-08 04:09:17.527696: val_loss -0.8649\n",
      "2025-12-08 04:09:17.530699: Pseudo dice [0.9157, 0.9489, 0.9474]\n",
      "2025-12-08 04:09:17.530699: Epoch time: 138.16 s\n",
      "2025-12-08 04:09:18.171477: \n",
      "2025-12-08 04:09:18.171477: Epoch 417\n",
      "2025-12-08 04:09:18.171477: Current learning rate: 0.00615\n",
      "2025-12-08 04:11:36.343222: train_loss -0.8472\n",
      "2025-12-08 04:11:36.343222: val_loss -0.8651\n",
      "2025-12-08 04:11:36.343222: Pseudo dice [0.9154, 0.9528, 0.9426]\n",
      "2025-12-08 04:11:36.343222: Epoch time: 138.17 s\n",
      "2025-12-08 04:11:37.159213: \n",
      "2025-12-08 04:11:37.159213: Epoch 418\n",
      "2025-12-08 04:11:37.159213: Current learning rate: 0.00614\n",
      "2025-12-08 04:13:55.369683: train_loss -0.8429\n",
      "2025-12-08 04:13:55.369683: val_loss -0.8663\n",
      "2025-12-08 04:13:55.373686: Pseudo dice [0.9221, 0.9541, 0.9398]\n",
      "2025-12-08 04:13:55.377691: Epoch time: 138.21 s\n",
      "2025-12-08 04:13:55.999895: \n",
      "2025-12-08 04:13:55.999895: Epoch 419\n",
      "2025-12-08 04:13:56.015930: Current learning rate: 0.00613\n",
      "2025-12-08 04:16:14.213716: train_loss -0.845\n",
      "2025-12-08 04:16:14.215718: val_loss -0.8661\n",
      "2025-12-08 04:16:14.219723: Pseudo dice [0.9187, 0.9508, 0.9463]\n",
      "2025-12-08 04:16:14.221725: Epoch time: 138.21 s\n",
      "2025-12-08 04:16:14.842891: \n",
      "2025-12-08 04:16:14.842891: Epoch 420\n",
      "2025-12-08 04:16:14.842891: Current learning rate: 0.00612\n",
      "2025-12-08 04:18:33.217798: train_loss -0.8374\n",
      "2025-12-08 04:18:33.217798: val_loss -0.8743\n",
      "2025-12-08 04:18:33.217798: Pseudo dice [0.9224, 0.9592, 0.9456]\n",
      "2025-12-08 04:18:33.217798: Epoch time: 138.37 s\n",
      "2025-12-08 04:18:33.874552: \n",
      "2025-12-08 04:18:33.874552: Epoch 421\n",
      "2025-12-08 04:18:33.874552: Current learning rate: 0.00612\n",
      "2025-12-08 04:20:52.074597: train_loss -0.8418\n",
      "2025-12-08 04:20:52.076601: val_loss -0.8718\n",
      "2025-12-08 04:20:52.082350: Pseudo dice [0.9258, 0.9589, 0.9427]\n",
      "2025-12-08 04:20:52.086355: Epoch time: 138.2 s\n",
      "2025-12-08 04:20:52.723272: \n",
      "2025-12-08 04:20:52.723272: Epoch 422\n",
      "2025-12-08 04:20:52.723272: Current learning rate: 0.00611\n",
      "2025-12-08 04:23:10.843787: train_loss -0.8362\n",
      "2025-12-08 04:23:10.843787: val_loss -0.8568\n",
      "2025-12-08 04:23:10.859681: Pseudo dice [0.9111, 0.9537, 0.9432]\n",
      "2025-12-08 04:23:10.862787: Epoch time: 138.12 s\n",
      "2025-12-08 04:23:11.483188: \n",
      "2025-12-08 04:23:11.483188: Epoch 423\n",
      "2025-12-08 04:23:11.483188: Current learning rate: 0.0061\n",
      "2025-12-08 04:25:29.608536: train_loss -0.8438\n",
      "2025-12-08 04:25:29.608536: val_loss -0.8714\n",
      "2025-12-08 04:25:29.608536: Pseudo dice [0.919, 0.9572, 0.9447]\n",
      "2025-12-08 04:25:29.608536: Epoch time: 138.14 s\n",
      "2025-12-08 04:25:30.493695: \n",
      "2025-12-08 04:25:30.494741: Epoch 424\n",
      "2025-12-08 04:25:30.497749: Current learning rate: 0.00609\n",
      "2025-12-08 04:27:48.656250: train_loss -0.8406\n",
      "2025-12-08 04:27:48.656250: val_loss -0.8562\n",
      "2025-12-08 04:27:48.656250: Pseudo dice [0.9143, 0.9465, 0.9446]\n",
      "2025-12-08 04:27:48.656250: Epoch time: 138.16 s\n",
      "2025-12-08 04:27:49.280776: \n",
      "2025-12-08 04:27:49.280776: Epoch 425\n",
      "2025-12-08 04:27:49.298748: Current learning rate: 0.00608\n",
      "2025-12-08 04:30:07.378622: train_loss -0.8444\n",
      "2025-12-08 04:30:07.378622: val_loss -0.8733\n",
      "2025-12-08 04:30:07.384628: Pseudo dice [0.9237, 0.9506, 0.9455]\n",
      "2025-12-08 04:30:07.388632: Epoch time: 138.1 s\n",
      "2025-12-08 04:30:08.026947: \n",
      "2025-12-08 04:30:08.026947: Epoch 426\n",
      "2025-12-08 04:30:08.030803: Current learning rate: 0.00607\n",
      "2025-12-08 04:32:26.185552: train_loss -0.8421\n",
      "2025-12-08 04:32:26.185552: val_loss -0.8702\n",
      "2025-12-08 04:32:26.186553: Pseudo dice [0.9218, 0.9546, 0.9471]\n",
      "2025-12-08 04:32:26.186553: Epoch time: 138.16 s\n",
      "2025-12-08 04:32:26.931798: \n",
      "2025-12-08 04:32:26.933800: Epoch 427\n",
      "2025-12-08 04:32:26.937544: Current learning rate: 0.00606\n",
      "2025-12-08 04:34:44.992664: train_loss -0.8372\n",
      "2025-12-08 04:34:44.993666: val_loss -0.867\n",
      "2025-12-08 04:34:44.997669: Pseudo dice [0.9162, 0.9517, 0.9483]\n",
      "2025-12-08 04:34:44.998670: Epoch time: 138.06 s\n",
      "2025-12-08 04:34:45.640112: \n",
      "2025-12-08 04:34:45.640112: Epoch 428\n",
      "2025-12-08 04:34:45.644178: Current learning rate: 0.00605\n",
      "2025-12-08 04:37:03.704918: train_loss -0.8415\n",
      "2025-12-08 04:37:03.705920: val_loss -0.8692\n",
      "2025-12-08 04:37:03.710933: Pseudo dice [0.9189, 0.9557, 0.9385]\n",
      "2025-12-08 04:37:03.713934: Epoch time: 138.07 s\n",
      "2025-12-08 04:37:04.385855: \n",
      "2025-12-08 04:37:04.386861: Epoch 429\n",
      "2025-12-08 04:37:04.390876: Current learning rate: 0.00604\n",
      "2025-12-08 04:39:22.468029: train_loss -0.8413\n",
      "2025-12-08 04:39:22.468029: val_loss -0.8531\n",
      "2025-12-08 04:39:22.484149: Pseudo dice [0.9128, 0.9491, 0.9399]\n",
      "2025-12-08 04:39:22.484149: Epoch time: 138.08 s\n",
      "2025-12-08 04:39:23.436585: \n",
      "2025-12-08 04:39:23.436585: Epoch 430\n",
      "2025-12-08 04:39:23.452533: Current learning rate: 0.00603\n",
      "2025-12-08 04:41:41.512358: train_loss -0.8445\n",
      "2025-12-08 04:41:41.514361: val_loss -0.8753\n",
      "2025-12-08 04:41:41.514361: Pseudo dice [0.9251, 0.9545, 0.9447]\n",
      "2025-12-08 04:41:41.521347: Epoch time: 138.08 s\n",
      "2025-12-08 04:41:42.167205: \n",
      "2025-12-08 04:41:42.167205: Epoch 431\n",
      "2025-12-08 04:41:42.171210: Current learning rate: 0.00602\n",
      "2025-12-08 04:44:00.317755: train_loss -0.8366\n",
      "2025-12-08 04:44:00.317755: val_loss -0.8517\n",
      "2025-12-08 04:44:00.323760: Pseudo dice [0.913, 0.9486, 0.9351]\n",
      "2025-12-08 04:44:00.325762: Epoch time: 138.15 s\n",
      "2025-12-08 04:44:00.952704: \n",
      "2025-12-08 04:44:00.952704: Epoch 432\n",
      "2025-12-08 04:44:00.968697: Current learning rate: 0.00601\n",
      "2025-12-08 04:46:19.030457: train_loss -0.8366\n",
      "2025-12-08 04:46:19.032427: val_loss -0.8595\n",
      "2025-12-08 04:46:19.036431: Pseudo dice [0.918, 0.955, 0.9379]\n",
      "2025-12-08 04:46:19.038433: Epoch time: 138.08 s\n",
      "2025-12-08 04:46:19.670521: \n",
      "2025-12-08 04:46:19.670521: Epoch 433\n",
      "2025-12-08 04:46:19.686603: Current learning rate: 0.006\n",
      "2025-12-08 04:48:37.870573: train_loss -0.8408\n",
      "2025-12-08 04:48:37.872576: val_loss -0.8628\n",
      "2025-12-08 04:48:37.878082: Pseudo dice [0.921, 0.9465, 0.942]\n",
      "2025-12-08 04:48:37.880085: Epoch time: 138.2 s\n",
      "2025-12-08 04:48:38.523712: \n",
      "2025-12-08 04:48:38.525715: Epoch 434\n",
      "2025-12-08 04:48:38.525715: Current learning rate: 0.00599\n",
      "2025-12-08 04:50:56.573106: train_loss -0.8452\n",
      "2025-12-08 04:50:56.573106: val_loss -0.8651\n",
      "2025-12-08 04:50:56.576848: Pseudo dice [0.9183, 0.9529, 0.9389]\n",
      "2025-12-08 04:50:56.580853: Epoch time: 138.05 s\n",
      "2025-12-08 04:50:57.217012: \n",
      "2025-12-08 04:50:57.217012: Epoch 435\n",
      "2025-12-08 04:50:57.217012: Current learning rate: 0.00598\n",
      "2025-12-08 04:53:15.324461: train_loss -0.843\n",
      "2025-12-08 04:53:15.324461: val_loss -0.8665\n",
      "2025-12-08 04:53:15.328343: Pseudo dice [0.9173, 0.9523, 0.944]\n",
      "2025-12-08 04:53:15.330345: Epoch time: 138.11 s\n",
      "2025-12-08 04:53:15.984052: \n",
      "2025-12-08 04:53:15.984052: Epoch 436\n",
      "2025-12-08 04:53:16.000031: Current learning rate: 0.00597\n",
      "2025-12-08 04:55:34.293838: train_loss -0.843\n",
      "2025-12-08 04:55:34.293838: val_loss -0.8758\n",
      "2025-12-08 04:55:34.296382: Pseudo dice [0.9248, 0.9541, 0.9483]\n",
      "2025-12-08 04:55:34.296382: Epoch time: 138.31 s\n",
      "2025-12-08 04:55:35.108673: \n",
      "2025-12-08 04:55:35.108673: Epoch 437\n",
      "2025-12-08 04:55:35.108673: Current learning rate: 0.00596\n",
      "2025-12-08 04:57:53.266028: train_loss -0.8441\n",
      "2025-12-08 04:57:53.266028: val_loss -0.8596\n",
      "2025-12-08 04:57:53.271029: Pseudo dice [0.9133, 0.9522, 0.9433]\n",
      "2025-12-08 04:57:53.275030: Epoch time: 138.16 s\n",
      "2025-12-08 04:57:53.952893: \n",
      "2025-12-08 04:57:53.968717: Epoch 438\n",
      "2025-12-08 04:57:53.968717: Current learning rate: 0.00595\n",
      "2025-12-08 05:00:12.168419: train_loss -0.8436\n",
      "2025-12-08 05:00:12.168419: val_loss -0.8756\n",
      "2025-12-08 05:00:12.177450: Pseudo dice [0.9279, 0.9539, 0.945]\n",
      "2025-12-08 05:00:12.181454: Epoch time: 138.22 s\n",
      "2025-12-08 05:00:12.813382: \n",
      "2025-12-08 05:00:12.813382: Epoch 439\n",
      "2025-12-08 05:00:12.827391: Current learning rate: 0.00594\n",
      "2025-12-08 05:02:31.155185: train_loss -0.8374\n",
      "2025-12-08 05:02:31.155185: val_loss -0.8682\n",
      "2025-12-08 05:02:31.160370: Pseudo dice [0.9187, 0.9539, 0.9439]\n",
      "2025-12-08 05:02:31.164088: Epoch time: 138.34 s\n",
      "2025-12-08 05:02:31.796096: \n",
      "2025-12-08 05:02:31.796096: Epoch 440\n",
      "2025-12-08 05:02:31.812061: Current learning rate: 0.00593\n",
      "2025-12-08 05:04:49.875850: train_loss -0.835\n",
      "2025-12-08 05:04:49.875850: val_loss -0.8644\n",
      "2025-12-08 05:04:49.875850: Pseudo dice [0.9222, 0.9531, 0.9357]\n",
      "2025-12-08 05:04:49.875850: Epoch time: 138.08 s\n",
      "2025-12-08 05:04:50.515215: \n",
      "2025-12-08 05:04:50.515215: Epoch 441\n",
      "2025-12-08 05:04:50.515215: Current learning rate: 0.00592\n",
      "2025-12-08 05:07:08.734112: train_loss -0.8409\n",
      "2025-12-08 05:07:08.734112: val_loss -0.8717\n",
      "2025-12-08 05:07:08.734112: Pseudo dice [0.9227, 0.9551, 0.946]\n",
      "2025-12-08 05:07:08.750076: Epoch time: 138.22 s\n",
      "2025-12-08 05:07:09.374291: \n",
      "2025-12-08 05:07:09.374291: Epoch 442\n",
      "2025-12-08 05:07:09.374291: Current learning rate: 0.00592\n",
      "2025-12-08 05:09:27.461574: train_loss -0.8416\n",
      "2025-12-08 05:09:27.462574: val_loss -0.8728\n",
      "2025-12-08 05:09:27.466580: Pseudo dice [0.9261, 0.9541, 0.9421]\n",
      "2025-12-08 05:09:27.467584: Epoch time: 138.09 s\n",
      "2025-12-08 05:09:28.296798: \n",
      "2025-12-08 05:09:28.296798: Epoch 443\n",
      "2025-12-08 05:09:28.310115: Current learning rate: 0.00591\n",
      "2025-12-08 05:11:46.414550: train_loss -0.8451\n",
      "2025-12-08 05:11:46.415550: val_loss -0.8632\n",
      "2025-12-08 05:11:46.419036: Pseudo dice [0.9197, 0.9546, 0.9444]\n",
      "2025-12-08 05:11:46.421163: Epoch time: 138.12 s\n",
      "2025-12-08 05:11:47.037868: \n",
      "2025-12-08 05:11:47.037868: Epoch 444\n",
      "2025-12-08 05:11:47.037868: Current learning rate: 0.0059\n",
      "2025-12-08 05:14:05.123485: train_loss -0.8437\n",
      "2025-12-08 05:14:05.123485: val_loss -0.8681\n",
      "2025-12-08 05:14:05.143210: Pseudo dice [0.9224, 0.9509, 0.9461]\n",
      "2025-12-08 05:14:05.145212: Epoch time: 138.09 s\n",
      "2025-12-08 05:14:05.775430: \n",
      "2025-12-08 05:14:05.775430: Epoch 445\n",
      "2025-12-08 05:14:05.779784: Current learning rate: 0.00589\n",
      "2025-12-08 05:16:23.951935: train_loss -0.844\n",
      "2025-12-08 05:16:23.951935: val_loss -0.8732\n",
      "2025-12-08 05:16:23.956809: Pseudo dice [0.9267, 0.9552, 0.9406]\n",
      "2025-12-08 05:16:23.960813: Epoch time: 138.18 s\n",
      "2025-12-08 05:16:24.584570: \n",
      "2025-12-08 05:16:24.585582: Epoch 446\n",
      "2025-12-08 05:16:24.588773: Current learning rate: 0.00588\n",
      "2025-12-08 05:18:42.721341: train_loss -0.8414\n",
      "2025-12-08 05:18:42.722341: val_loss -0.8724\n",
      "2025-12-08 05:18:42.726342: Pseudo dice [0.9241, 0.9585, 0.9441]\n",
      "2025-12-08 05:18:42.729343: Epoch time: 138.14 s\n",
      "2025-12-08 05:18:43.352829: \n",
      "2025-12-08 05:18:43.352829: Epoch 447\n",
      "2025-12-08 05:18:43.352829: Current learning rate: 0.00587\n",
      "2025-12-08 05:21:01.456614: train_loss -0.8425\n",
      "2025-12-08 05:21:01.456614: val_loss -0.8703\n",
      "2025-12-08 05:21:01.464511: Pseudo dice [0.9232, 0.9546, 0.9442]\n",
      "2025-12-08 05:21:01.470256: Epoch time: 138.11 s\n",
      "2025-12-08 05:21:01.472258: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-08 05:21:02.378389: \n",
      "2025-12-08 05:21:02.378389: Epoch 448\n",
      "2025-12-08 05:21:02.382407: Current learning rate: 0.00586\n",
      "2025-12-08 05:23:20.515336: train_loss -0.8461\n",
      "2025-12-08 05:23:20.515336: val_loss -0.8625\n",
      "2025-12-08 05:23:20.531023: Pseudo dice [0.9192, 0.9487, 0.94]\n",
      "2025-12-08 05:23:20.535611: Epoch time: 138.14 s\n",
      "2025-12-08 05:23:21.326668: \n",
      "2025-12-08 05:23:21.326668: Epoch 449\n",
      "2025-12-08 05:23:21.333194: Current learning rate: 0.00585\n",
      "2025-12-08 05:25:39.483210: train_loss -0.8384\n",
      "2025-12-08 05:25:39.483210: val_loss -0.8619\n",
      "2025-12-08 05:25:39.483210: Pseudo dice [0.9188, 0.9482, 0.9355]\n",
      "2025-12-08 05:25:39.490305: Epoch time: 138.16 s\n",
      "2025-12-08 05:25:40.374905: \n",
      "2025-12-08 05:25:40.374905: Epoch 450\n",
      "2025-12-08 05:25:40.384761: Current learning rate: 0.00584\n",
      "2025-12-08 05:27:58.421380: train_loss -0.8453\n",
      "2025-12-08 05:27:58.421380: val_loss -0.8689\n",
      "2025-12-08 05:27:58.421380: Pseudo dice [0.9262, 0.9558, 0.9365]\n",
      "2025-12-08 05:27:58.437293: Epoch time: 138.05 s\n",
      "2025-12-08 05:27:59.047946: \n",
      "2025-12-08 05:27:59.047946: Epoch 451\n",
      "2025-12-08 05:27:59.053955: Current learning rate: 0.00583\n",
      "2025-12-08 05:30:17.216994: train_loss -0.8334\n",
      "2025-12-08 05:30:17.216994: val_loss -0.8222\n",
      "2025-12-08 05:30:17.221119: Pseudo dice [0.8941, 0.943, 0.9254]\n",
      "2025-12-08 05:30:17.223811: Epoch time: 138.17 s\n",
      "2025-12-08 05:30:17.847966: \n",
      "2025-12-08 05:30:17.849149: Epoch 452\n",
      "2025-12-08 05:30:17.853165: Current learning rate: 0.00582\n",
      "2025-12-08 05:32:36.025360: train_loss -0.8021\n",
      "2025-12-08 05:32:36.026364: val_loss -0.8591\n",
      "2025-12-08 05:32:36.030375: Pseudo dice [0.9191, 0.9532, 0.9347]\n",
      "2025-12-08 05:32:36.030375: Epoch time: 138.18 s\n",
      "2025-12-08 05:32:36.639990: \n",
      "2025-12-08 05:32:36.639990: Epoch 453\n",
      "2025-12-08 05:32:36.655903: Current learning rate: 0.00581\n",
      "2025-12-08 05:34:54.808411: train_loss -0.8176\n",
      "2025-12-08 05:34:54.810413: val_loss -0.8524\n",
      "2025-12-08 05:34:54.814417: Pseudo dice [0.9154, 0.9497, 0.9387]\n",
      "2025-12-08 05:34:54.816404: Epoch time: 138.17 s\n",
      "2025-12-08 05:34:55.436534: \n",
      "2025-12-08 05:34:55.436534: Epoch 454\n",
      "2025-12-08 05:34:55.436534: Current learning rate: 0.0058\n",
      "2025-12-08 05:37:13.704451: train_loss -0.8302\n",
      "2025-12-08 05:37:13.706456: val_loss -0.8671\n",
      "2025-12-08 05:37:13.712477: Pseudo dice [0.9236, 0.9541, 0.9434]\n",
      "2025-12-08 05:37:13.717852: Epoch time: 138.27 s\n",
      "2025-12-08 05:37:14.545344: \n",
      "2025-12-08 05:37:14.545344: Epoch 455\n",
      "2025-12-08 05:37:14.561262: Current learning rate: 0.00579\n",
      "2025-12-08 05:39:32.745559: train_loss -0.8217\n",
      "2025-12-08 05:39:32.747562: val_loss -0.837\n",
      "2025-12-08 05:39:32.751306: Pseudo dice [0.9056, 0.9423, 0.9373]\n",
      "2025-12-08 05:39:32.756998: Epoch time: 138.2 s\n",
      "2025-12-08 05:39:33.373315: \n",
      "2025-12-08 05:39:33.373315: Epoch 456\n",
      "2025-12-08 05:39:33.389080: Current learning rate: 0.00578\n",
      "2025-12-08 05:41:51.714415: train_loss -0.7944\n",
      "2025-12-08 05:41:51.715417: val_loss -0.8499\n",
      "2025-12-08 05:41:51.719455: Pseudo dice [0.9154, 0.9536, 0.9351]\n",
      "2025-12-08 05:41:51.723460: Epoch time: 138.34 s\n",
      "2025-12-08 05:41:52.373981: \n",
      "2025-12-08 05:41:52.373981: Epoch 457\n",
      "2025-12-08 05:41:52.373981: Current learning rate: 0.00577\n",
      "2025-12-08 05:44:10.577240: train_loss -0.8114\n",
      "2025-12-08 05:44:10.579049: val_loss -0.8515\n",
      "2025-12-08 05:44:10.583055: Pseudo dice [0.9157, 0.9498, 0.9387]\n",
      "2025-12-08 05:44:10.587059: Epoch time: 138.2 s\n",
      "2025-12-08 05:44:11.218941: \n",
      "2025-12-08 05:44:11.218941: Epoch 458\n",
      "2025-12-08 05:44:11.218941: Current learning rate: 0.00576\n",
      "2025-12-08 05:46:29.300604: train_loss -0.8203\n",
      "2025-12-08 05:46:29.300604: val_loss -0.854\n",
      "2025-12-08 05:46:29.306612: Pseudo dice [0.9133, 0.9518, 0.9377]\n",
      "2025-12-08 05:46:29.308615: Epoch time: 138.08 s\n",
      "2025-12-08 05:46:29.936109: \n",
      "2025-12-08 05:46:29.936109: Epoch 459\n",
      "2025-12-08 05:46:29.936109: Current learning rate: 0.00575\n",
      "2025-12-08 05:48:48.109591: train_loss -0.8259\n",
      "2025-12-08 05:48:48.109591: val_loss -0.854\n",
      "2025-12-08 05:48:48.109591: Pseudo dice [0.9186, 0.9495, 0.9323]\n",
      "2025-12-08 05:48:48.109591: Epoch time: 138.17 s\n",
      "2025-12-08 05:48:48.733154: \n",
      "2025-12-08 05:48:48.733154: Epoch 460\n",
      "2025-12-08 05:48:48.733154: Current learning rate: 0.00574\n",
      "2025-12-08 05:51:06.782135: train_loss -0.8338\n",
      "2025-12-08 05:51:06.783139: val_loss -0.8543\n",
      "2025-12-08 05:51:06.787152: Pseudo dice [0.9138, 0.942, 0.9431]\n",
      "2025-12-08 05:51:06.790167: Epoch time: 138.05 s\n",
      "2025-12-08 05:51:07.405884: \n",
      "2025-12-08 05:51:07.405884: Epoch 461\n",
      "2025-12-08 05:51:07.405884: Current learning rate: 0.00573\n",
      "2025-12-08 05:53:25.325693: train_loss -0.8348\n",
      "2025-12-08 05:53:25.325693: val_loss -0.8574\n",
      "2025-12-08 05:53:25.326696: Pseudo dice [0.9181, 0.9502, 0.9418]\n",
      "2025-12-08 05:53:25.326696: Epoch time: 137.92 s\n",
      "2025-12-08 05:53:26.186282: \n",
      "2025-12-08 05:53:26.186282: Epoch 462\n",
      "2025-12-08 05:53:26.186282: Current learning rate: 0.00572\n",
      "2025-12-08 05:55:44.358374: train_loss -0.8347\n",
      "2025-12-08 05:55:44.358374: val_loss -0.863\n",
      "2025-12-08 05:55:44.374285: Pseudo dice [0.9197, 0.9519, 0.9441]\n",
      "2025-12-08 05:55:44.374285: Epoch time: 138.17 s\n",
      "2025-12-08 05:55:44.984594: \n",
      "2025-12-08 05:55:44.984594: Epoch 463\n",
      "2025-12-08 05:55:44.984594: Current learning rate: 0.00571\n",
      "2025-12-08 05:58:03.045187: train_loss -0.8398\n",
      "2025-12-08 05:58:03.045187: val_loss -0.8624\n",
      "2025-12-08 05:58:03.060919: Pseudo dice [0.9161, 0.9533, 0.948]\n",
      "2025-12-08 05:58:03.064924: Epoch time: 138.06 s\n",
      "2025-12-08 05:58:03.686740: \n",
      "2025-12-08 05:58:03.686740: Epoch 464\n",
      "2025-12-08 05:58:03.686740: Current learning rate: 0.0057\n",
      "2025-12-08 06:00:21.726662: train_loss -0.8429\n",
      "2025-12-08 06:00:21.728664: val_loss -0.8709\n",
      "2025-12-08 06:00:21.734409: Pseudo dice [0.921, 0.9527, 0.9518]\n",
      "2025-12-08 06:00:21.741626: Epoch time: 138.04 s\n",
      "2025-12-08 06:00:22.483474: \n",
      "2025-12-08 06:00:22.483474: Epoch 465\n",
      "2025-12-08 06:00:22.483474: Current learning rate: 0.0057\n",
      "2025-12-08 06:02:40.717708: train_loss -0.8372\n",
      "2025-12-08 06:02:40.717708: val_loss -0.871\n",
      "2025-12-08 06:02:40.717708: Pseudo dice [0.9236, 0.9544, 0.941]\n",
      "2025-12-08 06:02:40.727214: Epoch time: 138.23 s\n",
      "2025-12-08 06:02:41.359803: \n",
      "2025-12-08 06:02:41.359803: Epoch 466\n",
      "2025-12-08 06:02:41.359803: Current learning rate: 0.00569\n",
      "2025-12-08 06:04:59.577625: train_loss -0.8429\n",
      "2025-12-08 06:04:59.577625: val_loss -0.8647\n",
      "2025-12-08 06:04:59.593708: Pseudo dice [0.9191, 0.952, 0.9496]\n",
      "2025-12-08 06:04:59.593708: Epoch time: 138.22 s\n",
      "2025-12-08 06:05:00.202204: \n",
      "2025-12-08 06:05:00.202204: Epoch 467\n",
      "2025-12-08 06:05:00.202204: Current learning rate: 0.00568\n",
      "2025-12-08 06:07:18.271718: train_loss -0.8435\n",
      "2025-12-08 06:07:18.271718: val_loss -0.8632\n",
      "2025-12-08 06:07:18.276719: Pseudo dice [0.9203, 0.9548, 0.937]\n",
      "2025-12-08 06:07:18.279719: Epoch time: 138.07 s\n",
      "2025-12-08 06:07:19.046460: \n",
      "2025-12-08 06:07:19.046460: Epoch 468\n",
      "2025-12-08 06:07:19.061726: Current learning rate: 0.00567\n",
      "2025-12-08 06:09:37.204686: train_loss -0.8402\n",
      "2025-12-08 06:09:37.204686: val_loss -0.8657\n",
      "2025-12-08 06:09:37.218350: Pseudo dice [0.9212, 0.952, 0.9402]\n",
      "2025-12-08 06:09:37.223823: Epoch time: 138.16 s\n",
      "2025-12-08 06:09:38.014571: \n",
      "2025-12-08 06:09:38.014571: Epoch 469\n",
      "2025-12-08 06:09:38.014571: Current learning rate: 0.00566\n",
      "2025-12-08 06:11:56.101418: train_loss -0.846\n",
      "2025-12-08 06:11:56.101418: val_loss -0.8722\n",
      "2025-12-08 06:11:56.105421: Pseudo dice [0.9215, 0.9557, 0.9436]\n",
      "2025-12-08 06:11:56.109164: Epoch time: 138.09 s\n",
      "2025-12-08 06:11:56.737579: \n",
      "2025-12-08 06:11:56.737579: Epoch 470\n",
      "2025-12-08 06:11:56.737579: Current learning rate: 0.00565\n",
      "2025-12-08 06:14:14.915096: train_loss -0.846\n",
      "2025-12-08 06:14:14.916097: val_loss -0.857\n",
      "2025-12-08 06:14:14.919597: Pseudo dice [0.9136, 0.9494, 0.9413]\n",
      "2025-12-08 06:14:14.923831: Epoch time: 138.18 s\n",
      "2025-12-08 06:14:15.712318: \n",
      "2025-12-08 06:14:15.712318: Epoch 471\n",
      "2025-12-08 06:14:15.712318: Current learning rate: 0.00564\n",
      "2025-12-08 06:16:33.792442: train_loss -0.8427\n",
      "2025-12-08 06:16:33.794444: val_loss -0.8784\n",
      "2025-12-08 06:16:33.796445: Pseudo dice [0.9264, 0.9567, 0.9465]\n",
      "2025-12-08 06:16:33.802008: Epoch time: 138.08 s\n",
      "2025-12-08 06:16:34.422307: \n",
      "2025-12-08 06:16:34.422307: Epoch 472\n",
      "2025-12-08 06:16:34.424309: Current learning rate: 0.00563\n",
      "2025-12-08 06:18:52.473480: train_loss -0.8443\n",
      "2025-12-08 06:18:52.473480: val_loss -0.8626\n",
      "2025-12-08 06:18:52.477484: Pseudo dice [0.9185, 0.9497, 0.9421]\n",
      "2025-12-08 06:18:52.481488: Epoch time: 138.05 s\n",
      "2025-12-08 06:18:53.155477: \n",
      "2025-12-08 06:18:53.155477: Epoch 473\n",
      "2025-12-08 06:18:53.155477: Current learning rate: 0.00562\n",
      "2025-12-08 06:21:11.404872: train_loss -0.8417\n",
      "2025-12-08 06:21:11.404872: val_loss -0.8683\n",
      "2025-12-08 06:21:11.415123: Pseudo dice [0.9225, 0.9544, 0.9409]\n",
      "2025-12-08 06:21:11.417126: Epoch time: 138.27 s\n",
      "2025-12-08 06:21:12.092856: \n",
      "2025-12-08 06:21:12.092856: Epoch 474\n",
      "2025-12-08 06:21:12.110533: Current learning rate: 0.00561\n",
      "2025-12-08 06:23:30.216406: train_loss -0.8417\n",
      "2025-12-08 06:23:30.218147: val_loss -0.8659\n",
      "2025-12-08 06:23:30.222709: Pseudo dice [0.9174, 0.9484, 0.9467]\n",
      "2025-12-08 06:23:30.226713: Epoch time: 138.12 s\n",
      "2025-12-08 06:23:31.027474: \n",
      "2025-12-08 06:23:31.027474: Epoch 475\n",
      "2025-12-08 06:23:31.029976: Current learning rate: 0.0056\n",
      "2025-12-08 06:25:49.178159: train_loss -0.8383\n",
      "2025-12-08 06:25:49.180162: val_loss -0.8611\n",
      "2025-12-08 06:25:49.180162: Pseudo dice [0.9191, 0.9528, 0.9384]\n",
      "2025-12-08 06:25:49.185809: Epoch time: 138.15 s\n",
      "2025-12-08 06:25:49.795794: \n",
      "2025-12-08 06:25:49.795794: Epoch 476\n",
      "2025-12-08 06:25:49.795794: Current learning rate: 0.00559\n",
      "2025-12-08 06:28:07.895010: train_loss -0.8392\n",
      "2025-12-08 06:28:07.895010: val_loss -0.8686\n",
      "2025-12-08 06:28:07.901016: Pseudo dice [0.9272, 0.9527, 0.9345]\n",
      "2025-12-08 06:28:07.905760: Epoch time: 138.1 s\n",
      "2025-12-08 06:28:08.592438: \n",
      "2025-12-08 06:28:08.592438: Epoch 477\n",
      "2025-12-08 06:28:08.599945: Current learning rate: 0.00558\n",
      "2025-12-08 06:30:26.872214: train_loss -0.8427\n",
      "2025-12-08 06:30:26.872214: val_loss -0.8686\n",
      "2025-12-08 06:30:26.878296: Pseudo dice [0.9218, 0.9541, 0.9457]\n",
      "2025-12-08 06:30:26.882296: Epoch time: 138.28 s\n",
      "2025-12-08 06:30:27.522088: \n",
      "2025-12-08 06:30:27.522088: Epoch 478\n",
      "2025-12-08 06:30:27.522088: Current learning rate: 0.00557\n",
      "2025-12-08 06:32:45.859542: train_loss -0.8424\n",
      "2025-12-08 06:32:45.860628: val_loss -0.8639\n",
      "2025-12-08 06:32:45.864631: Pseudo dice [0.9198, 0.9506, 0.9447]\n",
      "2025-12-08 06:32:45.867632: Epoch time: 138.34 s\n",
      "2025-12-08 06:32:46.509107: \n",
      "2025-12-08 06:32:46.509107: Epoch 479\n",
      "2025-12-08 06:32:46.514996: Current learning rate: 0.00556\n",
      "2025-12-08 06:35:04.559544: train_loss -0.8449\n",
      "2025-12-08 06:35:04.560546: val_loss -0.8659\n",
      "2025-12-08 06:35:04.561548: Pseudo dice [0.9217, 0.9558, 0.9369]\n",
      "2025-12-08 06:35:04.567582: Epoch time: 138.05 s\n",
      "2025-12-08 06:35:05.265440: \n",
      "2025-12-08 06:35:05.265440: Epoch 480\n",
      "2025-12-08 06:35:05.268877: Current learning rate: 0.00555\n",
      "2025-12-08 06:37:23.378318: train_loss -0.8454\n",
      "2025-12-08 06:37:23.380320: val_loss -0.8694\n",
      "2025-12-08 06:37:23.384324: Pseudo dice [0.9238, 0.9522, 0.9495]\n",
      "2025-12-08 06:37:23.388327: Epoch time: 138.13 s\n",
      "2025-12-08 06:37:24.014509: \n",
      "2025-12-08 06:37:24.014509: Epoch 481\n",
      "2025-12-08 06:37:24.014509: Current learning rate: 0.00554\n",
      "2025-12-08 06:39:42.528482: train_loss -0.8376\n",
      "2025-12-08 06:39:42.533823: val_loss -0.8647\n",
      "2025-12-08 06:39:42.535825: Pseudo dice [0.9162, 0.9492, 0.9422]\n",
      "2025-12-08 06:39:42.539829: Epoch time: 138.51 s\n",
      "2025-12-08 06:39:43.352092: \n",
      "2025-12-08 06:39:43.352092: Epoch 482\n",
      "2025-12-08 06:39:43.355803: Current learning rate: 0.00553\n",
      "2025-12-08 06:42:01.220183: train_loss -0.8476\n",
      "2025-12-08 06:42:01.220183: val_loss -0.8692\n",
      "2025-12-08 06:42:01.226191: Pseudo dice [0.9234, 0.9573, 0.9431]\n",
      "2025-12-08 06:42:01.226191: Epoch time: 137.87 s\n",
      "2025-12-08 06:42:01.958833: \n",
      "2025-12-08 06:42:01.959835: Epoch 483\n",
      "2025-12-08 06:42:01.963845: Current learning rate: 0.00552\n",
      "2025-12-08 06:44:20.088468: train_loss -0.847\n",
      "2025-12-08 06:44:20.088468: val_loss -0.8694\n",
      "2025-12-08 06:44:20.093950: Pseudo dice [0.9198, 0.9549, 0.9477]\n",
      "2025-12-08 06:44:20.097954: Epoch time: 138.13 s\n",
      "2025-12-08 06:44:20.739543: \n",
      "2025-12-08 06:44:20.740547: Epoch 484\n",
      "2025-12-08 06:44:20.740547: Current learning rate: 0.00551\n",
      "2025-12-08 06:46:38.985071: train_loss -0.8453\n",
      "2025-12-08 06:46:38.985071: val_loss -0.8621\n",
      "2025-12-08 06:46:38.989077: Pseudo dice [0.9197, 0.9498, 0.9411]\n",
      "2025-12-08 06:46:38.993916: Epoch time: 138.25 s\n",
      "2025-12-08 06:46:39.625150: \n",
      "2025-12-08 06:46:39.625150: Epoch 485\n",
      "2025-12-08 06:46:39.625150: Current learning rate: 0.0055\n",
      "2025-12-08 06:48:57.811582: train_loss -0.8416\n",
      "2025-12-08 06:48:57.811582: val_loss -0.8657\n",
      "2025-12-08 06:48:57.811582: Pseudo dice [0.9184, 0.9534, 0.9436]\n",
      "2025-12-08 06:48:57.820384: Epoch time: 138.19 s\n",
      "2025-12-08 06:48:58.468880: \n",
      "2025-12-08 06:48:58.484672: Epoch 486\n",
      "2025-12-08 06:48:58.488679: Current learning rate: 0.00549\n",
      "2025-12-08 06:51:16.515791: train_loss -0.843\n",
      "2025-12-08 06:51:16.515791: val_loss -0.8694\n",
      "2025-12-08 06:51:16.515791: Pseudo dice [0.9235, 0.954, 0.9458]\n",
      "2025-12-08 06:51:16.531629: Epoch time: 138.05 s\n",
      "2025-12-08 06:51:17.139551: \n",
      "2025-12-08 06:51:17.139551: Epoch 487\n",
      "2025-12-08 06:51:17.155187: Current learning rate: 0.00548\n",
      "2025-12-08 06:53:35.373274: train_loss -0.8436\n",
      "2025-12-08 06:53:35.373274: val_loss -0.8714\n",
      "2025-12-08 06:53:35.375014: Pseudo dice [0.9236, 0.9561, 0.9455]\n",
      "2025-12-08 06:53:35.375014: Epoch time: 138.23 s\n",
      "2025-12-08 06:53:36.172153: \n",
      "2025-12-08 06:53:36.172153: Epoch 488\n",
      "2025-12-08 06:53:36.172153: Current learning rate: 0.00547\n",
      "2025-12-08 06:55:54.233932: train_loss -0.8435\n",
      "2025-12-08 06:55:54.233932: val_loss -0.861\n",
      "2025-12-08 06:55:54.233932: Pseudo dice [0.9175, 0.9514, 0.9412]\n",
      "2025-12-08 06:55:54.233932: Epoch time: 138.06 s\n",
      "2025-12-08 06:55:54.889833: \n",
      "2025-12-08 06:55:54.889833: Epoch 489\n",
      "2025-12-08 06:55:54.889833: Current learning rate: 0.00546\n",
      "2025-12-08 06:58:12.873818: train_loss -0.8478\n",
      "2025-12-08 06:58:12.873818: val_loss -0.8691\n",
      "2025-12-08 06:58:12.877962: Pseudo dice [0.922, 0.9524, 0.9447]\n",
      "2025-12-08 06:58:12.881966: Epoch time: 137.98 s\n",
      "2025-12-08 06:58:13.499788: \n",
      "2025-12-08 06:58:13.499788: Epoch 490\n",
      "2025-12-08 06:58:13.515759: Current learning rate: 0.00546\n",
      "2025-12-08 07:00:31.623969: train_loss -0.8445\n",
      "2025-12-08 07:00:31.623969: val_loss -0.8664\n",
      "2025-12-08 07:00:31.628327: Pseudo dice [0.9177, 0.9508, 0.9473]\n",
      "2025-12-08 07:00:31.632342: Epoch time: 138.12 s\n",
      "2025-12-08 07:00:32.265008: \n",
      "2025-12-08 07:00:32.265008: Epoch 491\n",
      "2025-12-08 07:00:32.265008: Current learning rate: 0.00545\n",
      "2025-12-08 07:02:50.344769: train_loss -0.8465\n",
      "2025-12-08 07:02:50.344769: val_loss -0.8664\n",
      "2025-12-08 07:02:50.358777: Pseudo dice [0.9205, 0.953, 0.9441]\n",
      "2025-12-08 07:02:50.360553: Epoch time: 138.08 s\n",
      "2025-12-08 07:02:51.015721: \n",
      "2025-12-08 07:02:51.015721: Epoch 492\n",
      "2025-12-08 07:02:51.015721: Current learning rate: 0.00544\n",
      "2025-12-08 07:05:09.149130: train_loss -0.844\n",
      "2025-12-08 07:05:09.149130: val_loss -0.8725\n",
      "2025-12-08 07:05:09.155099: Pseudo dice [0.9214, 0.9526, 0.9451]\n",
      "2025-12-08 07:05:09.156913: Epoch time: 138.14 s\n",
      "2025-12-08 07:05:09.779927: \n",
      "2025-12-08 07:05:09.779927: Epoch 493\n",
      "2025-12-08 07:05:09.779927: Current learning rate: 0.00543\n",
      "2025-12-08 07:07:27.968527: train_loss -0.8448\n",
      "2025-12-08 07:07:27.968527: val_loss -0.8745\n",
      "2025-12-08 07:07:27.986614: Pseudo dice [0.9254, 0.9525, 0.9489]\n",
      "2025-12-08 07:07:27.986614: Epoch time: 138.19 s\n",
      "2025-12-08 07:07:28.624251: \n",
      "2025-12-08 07:07:28.624251: Epoch 494\n",
      "2025-12-08 07:07:28.624251: Current learning rate: 0.00542\n",
      "2025-12-08 07:09:46.668461: train_loss -0.8442\n",
      "2025-12-08 07:09:46.670464: val_loss -0.8701\n",
      "2025-12-08 07:09:46.673487: Pseudo dice [0.9238, 0.9571, 0.9448]\n",
      "2025-12-08 07:09:46.673487: Epoch time: 138.04 s\n",
      "2025-12-08 07:09:46.673487: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-08 07:09:47.765152: \n",
      "2025-12-08 07:09:47.765152: Epoch 495\n",
      "2025-12-08 07:09:47.774718: Current learning rate: 0.00541\n",
      "2025-12-08 07:12:05.827110: train_loss -0.8456\n",
      "2025-12-08 07:12:05.827110: val_loss -0.8703\n",
      "2025-12-08 07:12:05.838676: Pseudo dice [0.9206, 0.9487, 0.9497]\n",
      "2025-12-08 07:12:05.838676: Epoch time: 138.06 s\n",
      "2025-12-08 07:12:05.843058: Yayy! New best EMA pseudo Dice: 0.9396\n",
      "2025-12-08 07:12:06.734819: \n",
      "2025-12-08 07:12:06.734819: Epoch 496\n",
      "2025-12-08 07:12:06.746399: Current learning rate: 0.0054\n",
      "2025-12-08 07:14:24.749332: train_loss -0.845\n",
      "2025-12-08 07:14:24.749332: val_loss -0.8747\n",
      "2025-12-08 07:14:24.765076: Pseudo dice [0.9291, 0.9561, 0.9413]\n",
      "2025-12-08 07:14:24.769814: Epoch time: 138.01 s\n",
      "2025-12-08 07:14:24.769814: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-08 07:14:25.703366: \n",
      "2025-12-08 07:14:25.703366: Epoch 497\n",
      "2025-12-08 07:14:25.703366: Current learning rate: 0.00539\n",
      "2025-12-08 07:16:43.809098: train_loss -0.8427\n",
      "2025-12-08 07:16:43.809098: val_loss -0.8734\n",
      "2025-12-08 07:16:43.811103: Pseudo dice [0.9235, 0.9532, 0.9454]\n",
      "2025-12-08 07:16:43.811103: Epoch time: 138.11 s\n",
      "2025-12-08 07:16:43.811103: Yayy! New best EMA pseudo Dice: 0.9399\n",
      "2025-12-08 07:16:44.723553: \n",
      "2025-12-08 07:16:44.725555: Epoch 498\n",
      "2025-12-08 07:16:44.725555: Current learning rate: 0.00538\n",
      "2025-12-08 07:19:02.798606: train_loss -0.839\n",
      "2025-12-08 07:19:02.798606: val_loss -0.8727\n",
      "2025-12-08 07:19:02.804612: Pseudo dice [0.9214, 0.9554, 0.9484]\n",
      "2025-12-08 07:19:02.808616: Epoch time: 138.08 s\n",
      "2025-12-08 07:19:02.814360: Yayy! New best EMA pseudo Dice: 0.9401\n",
      "2025-12-08 07:19:03.732311: \n",
      "2025-12-08 07:19:03.732311: Epoch 499\n",
      "2025-12-08 07:19:03.732311: Current learning rate: 0.00537\n",
      "2025-12-08 07:21:21.779516: train_loss -0.8455\n",
      "2025-12-08 07:21:21.779516: val_loss -0.8595\n",
      "2025-12-08 07:21:21.785523: Pseudo dice [0.9145, 0.9464, 0.9446]\n",
      "2025-12-08 07:21:21.789438: Epoch time: 138.05 s\n",
      "2025-12-08 07:21:22.867194: \n",
      "2025-12-08 07:21:22.868194: Epoch 500\n",
      "2025-12-08 07:21:22.872492: Current learning rate: 0.00536\n",
      "2025-12-08 07:23:40.958572: train_loss -0.8457\n",
      "2025-12-08 07:23:40.958572: val_loss -0.8658\n",
      "2025-12-08 07:23:40.963588: Pseudo dice [0.9197, 0.9511, 0.9519]\n",
      "2025-12-08 07:23:40.967606: Epoch time: 138.09 s\n",
      "2025-12-08 07:23:41.627552: \n",
      "2025-12-08 07:23:41.627552: Epoch 501\n",
      "2025-12-08 07:23:41.629555: Current learning rate: 0.00535\n",
      "2025-12-08 07:25:59.903878: train_loss -0.8442\n",
      "2025-12-08 07:25:59.903878: val_loss -0.8784\n",
      "2025-12-08 07:25:59.907884: Pseudo dice [0.9237, 0.9609, 0.9474]\n",
      "2025-12-08 07:25:59.911888: Epoch time: 138.28 s\n",
      "2025-12-08 07:25:59.917894: Yayy! New best EMA pseudo Dice: 0.9402\n",
      "2025-12-08 07:26:00.821723: \n",
      "2025-12-08 07:26:00.821723: Epoch 502\n",
      "2025-12-08 07:26:00.826736: Current learning rate: 0.00534\n",
      "2025-12-08 07:28:18.991899: train_loss -0.8359\n",
      "2025-12-08 07:28:18.991899: val_loss -0.8652\n",
      "2025-12-08 07:28:18.997905: Pseudo dice [0.9205, 0.9521, 0.9424]\n",
      "2025-12-08 07:28:19.001648: Epoch time: 138.17 s\n",
      "2025-12-08 07:28:19.639965: \n",
      "2025-12-08 07:28:19.639965: Epoch 503\n",
      "2025-12-08 07:28:19.639965: Current learning rate: 0.00533\n",
      "2025-12-08 07:30:37.744917: train_loss -0.8401\n",
      "2025-12-08 07:30:37.745919: val_loss -0.8718\n",
      "2025-12-08 07:30:37.748929: Pseudo dice [0.9215, 0.9538, 0.9484]\n",
      "2025-12-08 07:30:37.748929: Epoch time: 138.1 s\n",
      "2025-12-08 07:30:38.404575: \n",
      "2025-12-08 07:30:38.405580: Epoch 504\n",
      "2025-12-08 07:30:38.405580: Current learning rate: 0.00532\n",
      "2025-12-08 07:32:56.519256: train_loss -0.8461\n",
      "2025-12-08 07:32:56.519256: val_loss -0.869\n",
      "2025-12-08 07:32:56.525262: Pseudo dice [0.9205, 0.9538, 0.946]\n",
      "2025-12-08 07:32:56.531327: Epoch time: 138.12 s\n",
      "2025-12-08 07:32:57.155596: \n",
      "2025-12-08 07:32:57.155596: Epoch 505\n",
      "2025-12-08 07:32:57.164142: Current learning rate: 0.00531\n",
      "2025-12-08 07:35:15.372190: train_loss -0.8441\n",
      "2025-12-08 07:35:15.372190: val_loss -0.8717\n",
      "2025-12-08 07:35:15.380202: Pseudo dice [0.9211, 0.9522, 0.9502]\n",
      "2025-12-08 07:35:15.383271: Epoch time: 138.22 s\n",
      "2025-12-08 07:35:15.387139: Yayy! New best EMA pseudo Dice: 0.9402\n",
      "2025-12-08 07:35:16.477893: \n",
      "2025-12-08 07:35:16.479896: Epoch 506\n",
      "2025-12-08 07:35:16.483655: Current learning rate: 0.0053\n",
      "2025-12-08 07:37:34.585011: train_loss -0.8465\n",
      "2025-12-08 07:37:34.586011: val_loss -0.8751\n",
      "2025-12-08 07:37:34.592012: Pseudo dice [0.9259, 0.9573, 0.9471]\n",
      "2025-12-08 07:37:34.595326: Epoch time: 138.11 s\n",
      "2025-12-08 07:37:34.600327: Yayy! New best EMA pseudo Dice: 0.9405\n",
      "2025-12-08 07:37:35.656071: \n",
      "2025-12-08 07:37:35.656071: Epoch 507\n",
      "2025-12-08 07:37:35.656071: Current learning rate: 0.00529\n",
      "2025-12-08 07:39:53.605713: train_loss -0.8516\n",
      "2025-12-08 07:39:53.605713: val_loss -0.8725\n",
      "2025-12-08 07:39:53.609458: Pseudo dice [0.9216, 0.9571, 0.9455]\n",
      "2025-12-08 07:39:53.614959: Epoch time: 137.95 s\n",
      "2025-12-08 07:39:53.618963: Yayy! New best EMA pseudo Dice: 0.9406\n",
      "2025-12-08 07:39:54.530602: \n",
      "2025-12-08 07:39:54.530602: Epoch 508\n",
      "2025-12-08 07:39:54.530602: Current learning rate: 0.00528\n",
      "2025-12-08 07:42:12.578354: train_loss -0.8471\n",
      "2025-12-08 07:42:12.578354: val_loss -0.8683\n",
      "2025-12-08 07:42:12.593941: Pseudo dice [0.9149, 0.953, 0.9488]\n",
      "2025-12-08 07:42:12.593941: Epoch time: 138.06 s\n",
      "2025-12-08 07:42:13.233741: \n",
      "2025-12-08 07:42:13.233741: Epoch 509\n",
      "2025-12-08 07:42:13.233741: Current learning rate: 0.00527\n",
      "2025-12-08 07:44:31.203125: train_loss -0.8462\n",
      "2025-12-08 07:44:31.203125: val_loss -0.884\n",
      "2025-12-08 07:44:31.208144: Pseudo dice [0.9321, 0.957, 0.9488]\n",
      "2025-12-08 07:44:31.212155: Epoch time: 137.97 s\n",
      "2025-12-08 07:44:31.216165: Yayy! New best EMA pseudo Dice: 0.941\n",
      "2025-12-08 07:44:32.220291: \n",
      "2025-12-08 07:44:32.221303: Epoch 510\n",
      "2025-12-08 07:44:32.225307: Current learning rate: 0.00526\n",
      "2025-12-08 07:46:50.330163: train_loss -0.8476\n",
      "2025-12-08 07:46:50.330163: val_loss -0.8743\n",
      "2025-12-08 07:46:50.342403: Pseudo dice [0.9234, 0.956, 0.9503]\n",
      "2025-12-08 07:46:50.342403: Epoch time: 138.11 s\n",
      "2025-12-08 07:46:50.342403: Yayy! New best EMA pseudo Dice: 0.9412\n",
      "2025-12-08 07:46:51.249294: \n",
      "2025-12-08 07:46:51.249294: Epoch 511\n",
      "2025-12-08 07:46:51.249294: Current learning rate: 0.00525\n",
      "2025-12-08 07:49:09.315890: train_loss -0.8325\n",
      "2025-12-08 07:49:09.316895: val_loss -0.849\n",
      "2025-12-08 07:49:09.321898: Pseudo dice [0.913, 0.9424, 0.9373]\n",
      "2025-12-08 07:49:09.325898: Epoch time: 138.07 s\n",
      "2025-12-08 07:49:10.124212: \n",
      "2025-12-08 07:49:10.124212: Epoch 512\n",
      "2025-12-08 07:49:10.124212: Current learning rate: 0.00524\n",
      "2025-12-08 07:51:28.293964: train_loss -0.829\n",
      "2025-12-08 07:51:28.295705: val_loss -0.8596\n",
      "2025-12-08 07:51:28.301713: Pseudo dice [0.9124, 0.948, 0.9465]\n",
      "2025-12-08 07:51:28.305718: Epoch time: 138.17 s\n",
      "2025-12-08 07:51:29.046477: \n",
      "2025-12-08 07:51:29.046477: Epoch 513\n",
      "2025-12-08 07:51:29.046477: Current learning rate: 0.00523\n",
      "2025-12-08 07:53:47.156377: train_loss -0.8393\n",
      "2025-12-08 07:53:47.156377: val_loss -0.8636\n",
      "2025-12-08 07:53:47.156377: Pseudo dice [0.9218, 0.9518, 0.9449]\n",
      "2025-12-08 07:53:47.172035: Epoch time: 138.11 s\n",
      "2025-12-08 07:53:47.781952: \n",
      "2025-12-08 07:53:47.781952: Epoch 514\n",
      "2025-12-08 07:53:47.801920: Current learning rate: 0.00522\n",
      "2025-12-08 07:56:05.780976: train_loss -0.8378\n",
      "2025-12-08 07:56:05.780976: val_loss -0.8594\n",
      "2025-12-08 07:56:05.796868: Pseudo dice [0.9169, 0.9455, 0.9451]\n",
      "2025-12-08 07:56:05.796868: Epoch time: 138.0 s\n",
      "2025-12-08 07:56:06.428741: \n",
      "2025-12-08 07:56:06.428741: Epoch 515\n",
      "2025-12-08 07:56:06.433755: Current learning rate: 0.00521\n",
      "2025-12-08 07:58:24.617564: train_loss -0.8354\n",
      "2025-12-08 07:58:24.617564: val_loss -0.8599\n",
      "2025-12-08 07:58:24.623571: Pseudo dice [0.9194, 0.9554, 0.9325]\n",
      "2025-12-08 07:58:24.625312: Epoch time: 138.19 s\n",
      "2025-12-08 07:58:25.388320: \n",
      "2025-12-08 07:58:25.388320: Epoch 516\n",
      "2025-12-08 07:58:25.393124: Current learning rate: 0.0052\n",
      "2025-12-08 08:00:43.592191: train_loss -0.8393\n",
      "2025-12-08 08:00:43.592191: val_loss -0.8647\n",
      "2025-12-08 08:00:43.592191: Pseudo dice [0.9184, 0.9501, 0.9459]\n",
      "2025-12-08 08:00:43.592191: Epoch time: 138.21 s\n",
      "2025-12-08 08:00:44.266498: \n",
      "2025-12-08 08:00:44.266498: Epoch 517\n",
      "2025-12-08 08:00:44.266498: Current learning rate: 0.00519\n",
      "2025-12-08 08:03:02.467193: train_loss -0.8408\n",
      "2025-12-08 08:03:02.467193: val_loss -0.8686\n",
      "2025-12-08 08:03:02.482886: Pseudo dice [0.9202, 0.9492, 0.9459]\n",
      "2025-12-08 08:03:02.482886: Epoch time: 138.2 s\n",
      "2025-12-08 08:03:03.279865: \n",
      "2025-12-08 08:03:03.279865: Epoch 518\n",
      "2025-12-08 08:03:03.279865: Current learning rate: 0.00518\n",
      "2025-12-08 08:05:21.437251: train_loss -0.8422\n",
      "2025-12-08 08:05:21.437251: val_loss -0.8669\n",
      "2025-12-08 08:05:21.457336: Pseudo dice [0.9184, 0.9501, 0.9461]\n",
      "2025-12-08 08:05:21.461340: Epoch time: 138.16 s\n",
      "2025-12-08 08:05:22.187038: \n",
      "2025-12-08 08:05:22.187038: Epoch 519\n",
      "2025-12-08 08:05:22.187038: Current learning rate: 0.00518\n",
      "2025-12-08 08:07:40.173646: train_loss -0.8441\n",
      "2025-12-08 08:07:40.175648: val_loss -0.8673\n",
      "2025-12-08 08:07:40.179652: Pseudo dice [0.922, 0.9541, 0.9421]\n",
      "2025-12-08 08:07:40.183656: Epoch time: 137.99 s\n",
      "2025-12-08 08:07:40.811400: \n",
      "2025-12-08 08:07:40.811400: Epoch 520\n",
      "2025-12-08 08:07:40.811400: Current learning rate: 0.00517\n",
      "2025-12-08 08:09:59.019669: train_loss -0.8366\n",
      "2025-12-08 08:09:59.021672: val_loss -0.8625\n",
      "2025-12-08 08:09:59.026502: Pseudo dice [0.9174, 0.9522, 0.9386]\n",
      "2025-12-08 08:09:59.028505: Epoch time: 138.21 s\n",
      "2025-12-08 08:09:59.655467: \n",
      "2025-12-08 08:09:59.655467: Epoch 521\n",
      "2025-12-08 08:09:59.671438: Current learning rate: 0.00516\n",
      "2025-12-08 08:12:17.592239: train_loss -0.8467\n",
      "2025-12-08 08:12:17.592239: val_loss -0.8745\n",
      "2025-12-08 08:12:17.593979: Pseudo dice [0.9233, 0.9556, 0.9477]\n",
      "2025-12-08 08:12:17.593979: Epoch time: 137.94 s\n",
      "2025-12-08 08:12:18.233467: \n",
      "2025-12-08 08:12:18.233467: Epoch 522\n",
      "2025-12-08 08:12:18.233467: Current learning rate: 0.00515\n",
      "2025-12-08 08:14:36.499630: train_loss -0.844\n",
      "2025-12-08 08:14:36.499630: val_loss -0.8603\n",
      "2025-12-08 08:14:36.505805: Pseudo dice [0.9164, 0.9517, 0.9364]\n",
      "2025-12-08 08:14:36.509806: Epoch time: 138.27 s\n",
      "2025-12-08 08:14:37.140701: \n",
      "2025-12-08 08:14:37.140701: Epoch 523\n",
      "2025-12-08 08:14:37.140701: Current learning rate: 0.00514\n",
      "2025-12-08 08:16:55.165982: train_loss -0.8401\n",
      "2025-12-08 08:16:55.166982: val_loss -0.8511\n",
      "2025-12-08 08:16:55.170983: Pseudo dice [0.9099, 0.9452, 0.9417]\n",
      "2025-12-08 08:16:55.170983: Epoch time: 138.03 s\n",
      "2025-12-08 08:16:55.983569: \n",
      "2025-12-08 08:16:55.983569: Epoch 524\n",
      "2025-12-08 08:16:55.999230: Current learning rate: 0.00513\n",
      "2025-12-08 08:19:14.028756: train_loss -0.8465\n",
      "2025-12-08 08:19:14.030498: val_loss -0.8726\n",
      "2025-12-08 08:19:14.035598: Pseudo dice [0.9234, 0.9576, 0.9457]\n",
      "2025-12-08 08:19:14.039614: Epoch time: 138.05 s\n",
      "2025-12-08 08:19:14.701984: \n",
      "2025-12-08 08:19:14.701984: Epoch 525\n",
      "2025-12-08 08:19:14.701984: Current learning rate: 0.00512\n",
      "2025-12-08 08:21:32.860739: train_loss -0.8464\n",
      "2025-12-08 08:21:32.860739: val_loss -0.8563\n",
      "2025-12-08 08:21:32.870755: Pseudo dice [0.91, 0.9483, 0.9471]\n",
      "2025-12-08 08:21:32.874529: Epoch time: 138.16 s\n",
      "2025-12-08 08:21:33.514123: \n",
      "2025-12-08 08:21:33.514123: Epoch 526\n",
      "2025-12-08 08:21:33.514123: Current learning rate: 0.00511\n",
      "2025-12-08 08:23:51.810933: train_loss -0.8451\n",
      "2025-12-08 08:23:51.810933: val_loss -0.8715\n",
      "2025-12-08 08:23:51.810933: Pseudo dice [0.9232, 0.9525, 0.948]\n",
      "2025-12-08 08:23:51.810933: Epoch time: 138.3 s\n",
      "2025-12-08 08:23:52.448523: \n",
      "2025-12-08 08:23:52.448523: Epoch 527\n",
      "2025-12-08 08:23:52.451529: Current learning rate: 0.0051\n",
      "2025-12-08 08:26:10.549318: train_loss -0.8452\n",
      "2025-12-08 08:26:10.551320: val_loss -0.8641\n",
      "2025-12-08 08:26:10.559069: Pseudo dice [0.9162, 0.9493, 0.9437]\n",
      "2025-12-08 08:26:10.565076: Epoch time: 138.1 s\n",
      "2025-12-08 08:26:11.203157: \n",
      "2025-12-08 08:26:11.203157: Epoch 528\n",
      "2025-12-08 08:26:11.203157: Current learning rate: 0.00509\n",
      "2025-12-08 08:28:29.396263: train_loss -0.8446\n",
      "2025-12-08 08:28:29.396263: val_loss -0.8686\n",
      "2025-12-08 08:28:29.400267: Pseudo dice [0.922, 0.9567, 0.941]\n",
      "2025-12-08 08:28:29.406011: Epoch time: 138.19 s\n",
      "2025-12-08 08:28:30.031121: \n",
      "2025-12-08 08:28:30.031121: Epoch 529\n",
      "2025-12-08 08:28:30.031121: Current learning rate: 0.00508\n",
      "2025-12-08 08:30:48.144091: train_loss -0.845\n",
      "2025-12-08 08:30:48.144091: val_loss -0.8708\n",
      "2025-12-08 08:30:48.150098: Pseudo dice [0.923, 0.9556, 0.944]\n",
      "2025-12-08 08:30:48.154102: Epoch time: 138.11 s\n",
      "2025-12-08 08:30:49.000961: \n",
      "2025-12-08 08:30:49.000961: Epoch 530\n",
      "2025-12-08 08:30:49.000961: Current learning rate: 0.00507\n",
      "2025-12-08 08:33:07.213083: train_loss -0.8424\n",
      "2025-12-08 08:33:07.217677: val_loss -0.876\n",
      "2025-12-08 08:33:07.221500: Pseudo dice [0.9265, 0.9565, 0.9507]\n",
      "2025-12-08 08:33:07.226982: Epoch time: 138.21 s\n",
      "2025-12-08 08:33:07.857776: \n",
      "2025-12-08 08:33:07.857776: Epoch 531\n",
      "2025-12-08 08:33:07.873434: Current learning rate: 0.00506\n",
      "2025-12-08 08:35:25.929753: train_loss -0.8516\n",
      "2025-12-08 08:35:25.929753: val_loss -0.8754\n",
      "2025-12-08 08:35:25.934765: Pseudo dice [0.9246, 0.9568, 0.9467]\n",
      "2025-12-08 08:35:25.936770: Epoch time: 138.07 s\n",
      "2025-12-08 08:35:26.608288: \n",
      "2025-12-08 08:35:26.608288: Epoch 532\n",
      "2025-12-08 08:35:26.623954: Current learning rate: 0.00505\n",
      "2025-12-08 08:37:44.748628: train_loss -0.8498\n",
      "2025-12-08 08:37:44.748628: val_loss -0.8617\n",
      "2025-12-08 08:37:44.764560: Pseudo dice [0.915, 0.9501, 0.9444]\n",
      "2025-12-08 08:37:44.764560: Epoch time: 138.14 s\n",
      "2025-12-08 08:37:45.543654: \n",
      "2025-12-08 08:37:45.545041: Epoch 533\n",
      "2025-12-08 08:37:45.546044: Current learning rate: 0.00504\n",
      "2025-12-08 08:40:03.717402: train_loss -0.8433\n",
      "2025-12-08 08:40:03.717402: val_loss -0.8642\n",
      "2025-12-08 08:40:03.723543: Pseudo dice [0.915, 0.9496, 0.9428]\n",
      "2025-12-08 08:40:03.729116: Epoch time: 138.17 s\n",
      "2025-12-08 08:40:04.393683: \n",
      "2025-12-08 08:40:04.395101: Epoch 534\n",
      "2025-12-08 08:40:04.399103: Current learning rate: 0.00503\n",
      "2025-12-08 08:42:22.577264: train_loss -0.8446\n",
      "2025-12-08 08:42:22.577264: val_loss -0.8727\n",
      "2025-12-08 08:42:22.582350: Pseudo dice [0.9249, 0.9577, 0.9439]\n",
      "2025-12-08 08:42:22.588140: Epoch time: 138.18 s\n",
      "2025-12-08 08:42:23.220554: \n",
      "2025-12-08 08:42:23.220554: Epoch 535\n",
      "2025-12-08 08:42:23.224564: Current learning rate: 0.00502\n",
      "2025-12-08 08:44:41.353803: train_loss -0.8449\n",
      "2025-12-08 08:44:41.353803: val_loss -0.8812\n",
      "2025-12-08 08:44:41.359549: Pseudo dice [0.9295, 0.9612, 0.9465]\n",
      "2025-12-08 08:44:41.369564: Epoch time: 138.13 s\n",
      "2025-12-08 08:44:42.342937: \n",
      "2025-12-08 08:44:42.342937: Epoch 536\n",
      "2025-12-08 08:44:42.342937: Current learning rate: 0.00501\n",
      "2025-12-08 08:47:00.470068: train_loss -0.851\n",
      "2025-12-08 08:47:00.472069: val_loss -0.8726\n",
      "2025-12-08 08:47:00.478954: Pseudo dice [0.9217, 0.9528, 0.944]\n",
      "2025-12-08 08:47:00.481955: Epoch time: 138.13 s\n",
      "2025-12-08 08:47:01.157351: \n",
      "2025-12-08 08:47:01.157351: Epoch 537\n",
      "2025-12-08 08:47:01.157351: Current learning rate: 0.005\n",
      "2025-12-08 08:49:19.275853: train_loss -0.8485\n",
      "2025-12-08 08:49:19.275853: val_loss -0.8703\n",
      "2025-12-08 08:49:19.280433: Pseudo dice [0.9223, 0.951, 0.9459]\n",
      "2025-12-08 08:49:19.280433: Epoch time: 138.12 s\n",
      "2025-12-08 08:49:19.915541: \n",
      "2025-12-08 08:49:19.915541: Epoch 538\n",
      "2025-12-08 08:49:19.921118: Current learning rate: 0.00499\n",
      "2025-12-08 08:51:38.108196: train_loss -0.8477\n",
      "2025-12-08 08:51:38.108196: val_loss -0.8689\n",
      "2025-12-08 08:51:38.114202: Pseudo dice [0.9203, 0.9553, 0.9462]\n",
      "2025-12-08 08:51:38.120212: Epoch time: 138.19 s\n",
      "2025-12-08 08:51:38.936848: \n",
      "2025-12-08 08:51:38.936848: Epoch 539\n",
      "2025-12-08 08:51:38.942354: Current learning rate: 0.00498\n",
      "2025-12-08 08:53:57.042389: train_loss -0.8452\n",
      "2025-12-08 08:53:57.042389: val_loss -0.8769\n",
      "2025-12-08 08:53:57.049382: Pseudo dice [0.9255, 0.9529, 0.9519]\n",
      "2025-12-08 08:53:57.058384: Epoch time: 138.11 s\n",
      "2025-12-08 08:53:57.697227: \n",
      "2025-12-08 08:53:57.697227: Epoch 540\n",
      "2025-12-08 08:53:57.702594: Current learning rate: 0.00497\n",
      "2025-12-08 08:56:15.749310: train_loss -0.85\n",
      "2025-12-08 08:56:15.749310: val_loss -0.8761\n",
      "2025-12-08 08:56:15.765157: Pseudo dice [0.9259, 0.9586, 0.9458]\n",
      "2025-12-08 08:56:15.765157: Epoch time: 138.05 s\n",
      "2025-12-08 08:56:16.395381: \n",
      "2025-12-08 08:56:16.395381: Epoch 541\n",
      "2025-12-08 08:56:16.399394: Current learning rate: 0.00496\n",
      "2025-12-08 08:58:34.612226: train_loss -0.8521\n",
      "2025-12-08 08:58:34.612226: val_loss -0.8785\n",
      "2025-12-08 08:58:34.618232: Pseudo dice [0.9304, 0.9574, 0.9481]\n",
      "2025-12-08 08:58:34.623977: Epoch time: 138.22 s\n",
      "2025-12-08 08:58:35.286827: \n",
      "2025-12-08 08:58:35.287832: Epoch 542\n",
      "2025-12-08 08:58:35.291848: Current learning rate: 0.00495\n",
      "2025-12-08 09:00:53.295754: train_loss -0.8471\n",
      "2025-12-08 09:00:53.295754: val_loss -0.8598\n",
      "2025-12-08 09:00:53.308195: Pseudo dice [0.9145, 0.9441, 0.951]\n",
      "2025-12-08 09:00:53.313702: Epoch time: 138.01 s\n",
      "2025-12-08 09:00:54.109309: \n",
      "2025-12-08 09:00:54.109309: Epoch 543\n",
      "2025-12-08 09:00:54.109309: Current learning rate: 0.00494\n",
      "2025-12-08 09:03:12.248769: train_loss -0.8503\n",
      "2025-12-08 09:03:12.251712: val_loss -0.8711\n",
      "2025-12-08 09:03:12.256013: Pseudo dice [0.9229, 0.9557, 0.9423]\n",
      "2025-12-08 09:03:12.260017: Epoch time: 138.14 s\n",
      "2025-12-08 09:03:12.903055: \n",
      "2025-12-08 09:03:12.904058: Epoch 544\n",
      "2025-12-08 09:03:12.905076: Current learning rate: 0.00493\n",
      "2025-12-08 09:05:30.829271: train_loss -0.8484\n",
      "2025-12-08 09:05:30.829271: val_loss -0.866\n",
      "2025-12-08 09:05:30.835278: Pseudo dice [0.9203, 0.9514, 0.9394]\n",
      "2025-12-08 09:05:30.841283: Epoch time: 137.93 s\n",
      "2025-12-08 09:05:31.468428: \n",
      "2025-12-08 09:05:31.468428: Epoch 545\n",
      "2025-12-08 09:05:31.484177: Current learning rate: 0.00492\n",
      "2025-12-08 09:07:49.634703: train_loss -0.8436\n",
      "2025-12-08 09:07:49.635703: val_loss -0.8623\n",
      "2025-12-08 09:07:49.640704: Pseudo dice [0.9184, 0.9551, 0.9356]\n",
      "2025-12-08 09:07:49.644815: Epoch time: 138.17 s\n",
      "2025-12-08 09:07:50.428660: \n",
      "2025-12-08 09:07:50.428660: Epoch 546\n",
      "2025-12-08 09:07:50.433683: Current learning rate: 0.00491\n",
      "2025-12-08 09:10:08.622172: train_loss -0.842\n",
      "2025-12-08 09:10:08.622172: val_loss -0.8668\n",
      "2025-12-08 09:10:08.627962: Pseudo dice [0.919, 0.9554, 0.9496]\n",
      "2025-12-08 09:10:08.631966: Epoch time: 138.2 s\n",
      "2025-12-08 09:10:09.271993: \n",
      "2025-12-08 09:10:09.271993: Epoch 547\n",
      "2025-12-08 09:10:09.271993: Current learning rate: 0.0049\n",
      "2025-12-08 09:12:27.376120: train_loss -0.8478\n",
      "2025-12-08 09:12:27.378122: val_loss -0.8828\n",
      "2025-12-08 09:12:27.384128: Pseudo dice [0.9293, 0.9601, 0.9446]\n",
      "2025-12-08 09:12:27.388132: Epoch time: 138.11 s\n",
      "2025-12-08 09:12:28.025664: \n",
      "2025-12-08 09:12:28.026669: Epoch 548\n",
      "2025-12-08 09:12:28.030237: Current learning rate: 0.00489\n",
      "2025-12-08 09:14:46.187159: train_loss -0.8482\n",
      "2025-12-08 09:14:46.187159: val_loss -0.8685\n",
      "2025-12-08 09:14:46.202865: Pseudo dice [0.9207, 0.9523, 0.9438]\n",
      "2025-12-08 09:14:46.207516: Epoch time: 138.16 s\n",
      "2025-12-08 09:14:47.000050: \n",
      "2025-12-08 09:14:47.000050: Epoch 549\n",
      "2025-12-08 09:14:47.005127: Current learning rate: 0.00488\n",
      "2025-12-08 09:17:05.342887: train_loss -0.848\n",
      "2025-12-08 09:17:05.342887: val_loss -0.8684\n",
      "2025-12-08 09:17:05.342887: Pseudo dice [0.9206, 0.9549, 0.9471]\n",
      "2025-12-08 09:17:05.358592: Epoch time: 138.36 s\n",
      "2025-12-08 09:17:06.291917: \n",
      "2025-12-08 09:17:06.291917: Epoch 550\n",
      "2025-12-08 09:17:06.296480: Current learning rate: 0.00487\n",
      "2025-12-08 09:19:24.514457: train_loss -0.8459\n",
      "2025-12-08 09:19:24.516198: val_loss -0.8704\n",
      "2025-12-08 09:19:24.522381: Pseudo dice [0.9229, 0.9532, 0.9429]\n",
      "2025-12-08 09:19:24.527382: Epoch time: 138.22 s\n",
      "2025-12-08 09:19:25.173270: \n",
      "2025-12-08 09:19:25.173270: Epoch 551\n",
      "2025-12-08 09:19:25.177623: Current learning rate: 0.00486\n",
      "2025-12-08 09:21:43.368073: train_loss -0.848\n",
      "2025-12-08 09:21:43.368073: val_loss -0.8708\n",
      "2025-12-08 09:21:43.374151: Pseudo dice [0.9203, 0.9533, 0.9471]\n",
      "2025-12-08 09:21:43.378160: Epoch time: 138.2 s\n",
      "2025-12-08 09:21:44.082054: \n",
      "2025-12-08 09:21:44.082054: Epoch 552\n",
      "2025-12-08 09:21:44.087073: Current learning rate: 0.00485\n",
      "2025-12-08 09:24:02.249671: train_loss -0.8507\n",
      "2025-12-08 09:24:02.249671: val_loss -0.864\n",
      "2025-12-08 09:24:02.249671: Pseudo dice [0.9162, 0.9477, 0.9472]\n",
      "2025-12-08 09:24:02.265309: Epoch time: 138.17 s\n",
      "2025-12-08 09:24:02.890312: \n",
      "2025-12-08 09:24:02.890312: Epoch 553\n",
      "2025-12-08 09:24:02.890312: Current learning rate: 0.00484\n",
      "2025-12-08 09:26:21.017901: train_loss -0.8475\n",
      "2025-12-08 09:26:21.018901: val_loss -0.8783\n",
      "2025-12-08 09:26:21.024902: Pseudo dice [0.9284, 0.9552, 0.9477]\n",
      "2025-12-08 09:26:21.029904: Epoch time: 138.13 s\n",
      "2025-12-08 09:26:21.671298: \n",
      "2025-12-08 09:26:21.671298: Epoch 554\n",
      "2025-12-08 09:26:21.676367: Current learning rate: 0.00484\n",
      "2025-12-08 09:28:39.882823: train_loss -0.8444\n",
      "2025-12-08 09:28:39.884825: val_loss -0.8764\n",
      "2025-12-08 09:28:39.893783: Pseudo dice [0.9267, 0.9519, 0.9484]\n",
      "2025-12-08 09:28:39.897784: Epoch time: 138.23 s\n",
      "2025-12-08 09:28:40.696256: \n",
      "2025-12-08 09:28:40.697262: Epoch 555\n",
      "2025-12-08 09:28:40.700921: Current learning rate: 0.00483\n",
      "2025-12-08 09:30:58.788129: train_loss -0.8485\n",
      "2025-12-08 09:30:58.788129: val_loss -0.8703\n",
      "2025-12-08 09:30:58.795877: Pseudo dice [0.9208, 0.9535, 0.9437]\n",
      "2025-12-08 09:30:58.801897: Epoch time: 138.09 s\n",
      "2025-12-08 09:30:59.436984: \n",
      "2025-12-08 09:30:59.436984: Epoch 556\n",
      "2025-12-08 09:30:59.454298: Current learning rate: 0.00482\n",
      "2025-12-08 09:33:17.633950: train_loss -0.8497\n",
      "2025-12-08 09:33:17.633950: val_loss -0.8678\n",
      "2025-12-08 09:33:17.639956: Pseudo dice [0.9168, 0.9538, 0.9464]\n",
      "2025-12-08 09:33:17.643548: Epoch time: 138.2 s\n",
      "2025-12-08 09:33:18.285843: \n",
      "2025-12-08 09:33:18.285843: Epoch 557\n",
      "2025-12-08 09:33:18.285843: Current learning rate: 0.00481\n",
      "2025-12-08 09:35:36.608333: train_loss -0.8452\n",
      "2025-12-08 09:35:36.608333: val_loss -0.8697\n",
      "2025-12-08 09:35:36.613949: Pseudo dice [0.9212, 0.9504, 0.9456]\n",
      "2025-12-08 09:35:36.617953: Epoch time: 138.32 s\n",
      "2025-12-08 09:35:37.308359: \n",
      "2025-12-08 09:35:37.308359: Epoch 558\n",
      "2025-12-08 09:35:37.312446: Current learning rate: 0.0048\n",
      "2025-12-08 09:37:55.438079: train_loss -0.853\n",
      "2025-12-08 09:37:55.438079: val_loss -0.8748\n",
      "2025-12-08 09:37:55.451584: Pseudo dice [0.9281, 0.9572, 0.9429]\n",
      "2025-12-08 09:37:55.456745: Epoch time: 138.13 s\n",
      "2025-12-08 09:37:56.092868: \n",
      "2025-12-08 09:37:56.094871: Epoch 559\n",
      "2025-12-08 09:37:56.094871: Current learning rate: 0.00479\n",
      "2025-12-08 09:40:14.254556: train_loss -0.8392\n",
      "2025-12-08 09:40:14.256562: val_loss -0.8709\n",
      "2025-12-08 09:40:14.262572: Pseudo dice [0.9207, 0.9505, 0.9477]\n",
      "2025-12-08 09:40:14.270086: Epoch time: 138.16 s\n",
      "2025-12-08 09:40:14.910752: \n",
      "2025-12-08 09:40:14.910752: Epoch 560\n",
      "2025-12-08 09:40:14.910752: Current learning rate: 0.00478\n",
      "2025-12-08 09:42:33.014987: train_loss -0.8504\n",
      "2025-12-08 09:42:33.014987: val_loss -0.8782\n",
      "2025-12-08 09:42:33.024507: Pseudo dice [0.9286, 0.9575, 0.9415]\n",
      "2025-12-08 09:42:33.030513: Epoch time: 138.1 s\n",
      "2025-12-08 09:42:33.838246: \n",
      "2025-12-08 09:42:33.838246: Epoch 561\n",
      "2025-12-08 09:42:33.842721: Current learning rate: 0.00477\n",
      "2025-12-08 09:44:52.078267: train_loss -0.8453\n",
      "2025-12-08 09:44:52.078267: val_loss -0.8744\n",
      "2025-12-08 09:44:52.085787: Pseudo dice [0.9226, 0.9567, 0.949]\n",
      "2025-12-08 09:44:52.089792: Epoch time: 138.24 s\n",
      "2025-12-08 09:44:52.718794: \n",
      "2025-12-08 09:44:52.718794: Epoch 562\n",
      "2025-12-08 09:44:52.718794: Current learning rate: 0.00476\n",
      "2025-12-08 09:47:10.977910: train_loss -0.8442\n",
      "2025-12-08 09:47:10.979913: val_loss -0.8766\n",
      "2025-12-08 09:47:10.985657: Pseudo dice [0.9275, 0.9545, 0.9461]\n",
      "2025-12-08 09:47:10.993668: Epoch time: 138.26 s\n",
      "2025-12-08 09:47:11.640536: \n",
      "2025-12-08 09:47:11.640536: Epoch 563\n",
      "2025-12-08 09:47:11.640536: Current learning rate: 0.00475\n",
      "2025-12-08 09:49:29.864729: train_loss -0.8462\n",
      "2025-12-08 09:49:29.864729: val_loss -0.876\n",
      "2025-12-08 09:49:29.869737: Pseudo dice [0.926, 0.9516, 0.9466]\n",
      "2025-12-08 09:49:29.873739: Epoch time: 138.22 s\n",
      "2025-12-08 09:49:30.500667: \n",
      "2025-12-08 09:49:30.500667: Epoch 564\n",
      "2025-12-08 09:49:30.500667: Current learning rate: 0.00474\n",
      "2025-12-08 09:51:48.814432: train_loss -0.8394\n",
      "2025-12-08 09:51:48.814432: val_loss -0.8628\n",
      "2025-12-08 09:51:48.820441: Pseudo dice [0.9178, 0.9504, 0.9421]\n",
      "2025-12-08 09:51:48.824448: Epoch time: 138.31 s\n",
      "2025-12-08 09:51:49.640462: \n",
      "2025-12-08 09:51:49.640462: Epoch 565\n",
      "2025-12-08 09:51:49.644622: Current learning rate: 0.00473\n",
      "2025-12-08 09:54:07.780750: train_loss -0.8501\n",
      "2025-12-08 09:54:07.780750: val_loss -0.8792\n",
      "2025-12-08 09:54:07.796628: Pseudo dice [0.9316, 0.9565, 0.9423]\n",
      "2025-12-08 09:54:07.800516: Epoch time: 138.16 s\n",
      "2025-12-08 09:54:08.420606: \n",
      "2025-12-08 09:54:08.420606: Epoch 566\n",
      "2025-12-08 09:54:08.436584: Current learning rate: 0.00472\n",
      "2025-12-08 09:56:26.640449: train_loss -0.8461\n",
      "2025-12-08 09:56:26.640449: val_loss -0.8702\n",
      "2025-12-08 09:56:26.646455: Pseudo dice [0.9221, 0.9525, 0.9493]\n",
      "2025-12-08 09:56:26.650459: Epoch time: 138.22 s\n",
      "2025-12-08 09:56:27.297971: \n",
      "2025-12-08 09:56:27.297971: Epoch 567\n",
      "2025-12-08 09:56:27.297971: Current learning rate: 0.00471\n",
      "2025-12-08 09:58:45.562676: train_loss -0.8514\n",
      "2025-12-08 09:58:45.562676: val_loss -0.8766\n",
      "2025-12-08 09:58:45.571089: Pseudo dice [0.9264, 0.9567, 0.947]\n",
      "2025-12-08 09:58:45.575096: Epoch time: 138.27 s\n",
      "2025-12-08 09:58:46.546638: \n",
      "2025-12-08 09:58:46.546638: Epoch 568\n",
      "2025-12-08 09:58:46.559893: Current learning rate: 0.0047\n",
      "2025-12-08 10:01:04.826621: train_loss -0.8472\n",
      "2025-12-08 10:01:04.826621: val_loss -0.8567\n",
      "2025-12-08 10:01:04.837887: Pseudo dice [0.9144, 0.9494, 0.9393]\n",
      "2025-12-08 10:01:04.841891: Epoch time: 138.28 s\n",
      "2025-12-08 10:01:05.490356: \n",
      "2025-12-08 10:01:05.490356: Epoch 569\n",
      "2025-12-08 10:01:05.490356: Current learning rate: 0.00469\n",
      "2025-12-08 10:03:23.641619: train_loss -0.845\n",
      "2025-12-08 10:03:23.642621: val_loss -0.8753\n",
      "2025-12-08 10:03:23.647629: Pseudo dice [0.925, 0.9572, 0.9455]\n",
      "2025-12-08 10:03:23.651780: Epoch time: 138.15 s\n",
      "2025-12-08 10:03:24.297075: \n",
      "2025-12-08 10:03:24.298077: Epoch 570\n",
      "2025-12-08 10:03:24.302111: Current learning rate: 0.00468\n",
      "2025-12-08 10:05:42.391172: train_loss -0.8483\n",
      "2025-12-08 10:05:42.392175: val_loss -0.8776\n",
      "2025-12-08 10:05:42.396192: Pseudo dice [0.9238, 0.9568, 0.9465]\n",
      "2025-12-08 10:05:42.400202: Epoch time: 138.1 s\n",
      "2025-12-08 10:05:43.116364: \n",
      "2025-12-08 10:05:43.116364: Epoch 571\n",
      "2025-12-08 10:05:43.121372: Current learning rate: 0.00467\n",
      "2025-12-08 10:08:01.177659: train_loss -0.848\n",
      "2025-12-08 10:08:01.178661: val_loss -0.8739\n",
      "2025-12-08 10:08:01.182663: Pseudo dice [0.9229, 0.9532, 0.949]\n",
      "2025-12-08 10:08:01.185816: Epoch time: 138.06 s\n",
      "2025-12-08 10:08:01.814438: \n",
      "2025-12-08 10:08:01.815843: Epoch 572\n",
      "2025-12-08 10:08:01.819846: Current learning rate: 0.00466\n",
      "2025-12-08 10:10:19.952151: train_loss -0.8474\n",
      "2025-12-08 10:10:19.952151: val_loss -0.8706\n",
      "2025-12-08 10:10:19.968020: Pseudo dice [0.9229, 0.9553, 0.9489]\n",
      "2025-12-08 10:10:19.973167: Epoch time: 138.14 s\n",
      "2025-12-08 10:10:20.641559: \n",
      "2025-12-08 10:10:20.641559: Epoch 573\n",
      "2025-12-08 10:10:20.655317: Current learning rate: 0.00465\n",
      "2025-12-08 10:12:39.846951: train_loss -0.8478\n",
      "2025-12-08 10:12:39.848953: val_loss -0.8766\n",
      "2025-12-08 10:12:39.852958: Pseudo dice [0.9261, 0.9588, 0.9524]\n",
      "2025-12-08 10:12:39.856962: Epoch time: 139.21 s\n",
      "2025-12-08 10:12:39.860966: Yayy! New best EMA pseudo Dice: 0.9416\n",
      "2025-12-08 10:12:41.063480: \n",
      "2025-12-08 10:12:41.063480: Epoch 574\n",
      "2025-12-08 10:12:41.063480: Current learning rate: 0.00464\n",
      "2025-12-08 10:14:59.530400: train_loss -0.8469\n",
      "2025-12-08 10:14:59.530400: val_loss -0.8769\n",
      "2025-12-08 10:14:59.530400: Pseudo dice [0.9277, 0.9591, 0.9459]\n",
      "2025-12-08 10:14:59.530400: Epoch time: 138.47 s\n",
      "2025-12-08 10:14:59.547438: Yayy! New best EMA pseudo Dice: 0.9418\n",
      "2025-12-08 10:15:00.526584: \n",
      "2025-12-08 10:15:00.526584: Epoch 575\n",
      "2025-12-08 10:15:00.531934: Current learning rate: 0.00463\n",
      "2025-12-08 10:17:19.085349: train_loss -0.8524\n",
      "2025-12-08 10:17:19.087351: val_loss -0.8638\n",
      "2025-12-08 10:17:19.093356: Pseudo dice [0.9153, 0.9499, 0.9423]\n",
      "2025-12-08 10:17:19.095359: Epoch time: 138.56 s\n",
      "2025-12-08 10:17:19.744732: \n",
      "2025-12-08 10:17:19.744732: Epoch 576\n",
      "2025-12-08 10:17:19.744732: Current learning rate: 0.00462\n",
      "2025-12-08 10:19:38.315783: train_loss -0.8497\n",
      "2025-12-08 10:19:38.315783: val_loss -0.8752\n",
      "2025-12-08 10:19:38.321789: Pseudo dice [0.9279, 0.9562, 0.9412]\n",
      "2025-12-08 10:19:38.329535: Epoch time: 138.57 s\n",
      "2025-12-08 10:19:38.983898: \n",
      "2025-12-08 10:19:38.983898: Epoch 577\n",
      "2025-12-08 10:19:38.983898: Current learning rate: 0.00461\n",
      "2025-12-08 10:21:57.825167: train_loss -0.847\n",
      "2025-12-08 10:21:57.825167: val_loss -0.866\n",
      "2025-12-08 10:21:57.830132: Pseudo dice [0.9236, 0.9508, 0.945]\n",
      "2025-12-08 10:21:57.834136: Epoch time: 138.84 s\n",
      "2025-12-08 10:21:58.478226: \n",
      "2025-12-08 10:21:58.478226: Epoch 578\n",
      "2025-12-08 10:21:58.478226: Current learning rate: 0.0046\n",
      "2025-12-08 10:24:17.143631: train_loss -0.8426\n",
      "2025-12-08 10:24:17.143631: val_loss -0.8783\n",
      "2025-12-08 10:24:17.149376: Pseudo dice [0.927, 0.9572, 0.9486]\n",
      "2025-12-08 10:24:17.151378: Epoch time: 138.67 s\n",
      "2025-12-08 10:24:17.983595: \n",
      "2025-12-08 10:24:17.983595: Epoch 579\n",
      "2025-12-08 10:24:17.983595: Current learning rate: 0.00459\n",
      "2025-12-08 10:26:37.526812: train_loss -0.8486\n",
      "2025-12-08 10:26:37.526812: val_loss -0.8645\n",
      "2025-12-08 10:26:37.532819: Pseudo dice [0.9163, 0.9509, 0.9461]\n",
      "2025-12-08 10:26:37.536824: Epoch time: 139.54 s\n",
      "2025-12-08 10:26:38.181467: \n",
      "2025-12-08 10:26:38.181467: Epoch 580\n",
      "2025-12-08 10:26:38.185361: Current learning rate: 0.00458\n",
      "2025-12-08 10:28:56.716027: train_loss -0.8542\n",
      "2025-12-08 10:28:56.716027: val_loss -0.8691\n",
      "2025-12-08 10:28:56.730884: Pseudo dice [0.9194, 0.9507, 0.9463]\n",
      "2025-12-08 10:28:56.730884: Epoch time: 138.54 s\n",
      "2025-12-08 10:28:57.517035: \n",
      "2025-12-08 10:28:57.517035: Epoch 581\n",
      "2025-12-08 10:28:57.517035: Current learning rate: 0.00457\n",
      "2025-12-08 10:31:16.076910: train_loss -0.8498\n",
      "2025-12-08 10:31:16.076910: val_loss -0.8729\n",
      "2025-12-08 10:31:16.076910: Pseudo dice [0.9215, 0.9533, 0.951]\n",
      "2025-12-08 10:31:16.076910: Epoch time: 138.56 s\n",
      "2025-12-08 10:31:16.715948: \n",
      "2025-12-08 10:31:16.715948: Epoch 582\n",
      "2025-12-08 10:31:16.715948: Current learning rate: 0.00456\n",
      "2025-12-08 10:33:35.483987: train_loss -0.849\n",
      "2025-12-08 10:33:35.483987: val_loss -0.8757\n",
      "2025-12-08 10:33:35.489042: Pseudo dice [0.9231, 0.9559, 0.9491]\n",
      "2025-12-08 10:33:35.493053: Epoch time: 138.77 s\n",
      "2025-12-08 10:33:36.124057: \n",
      "2025-12-08 10:33:36.124057: Epoch 583\n",
      "2025-12-08 10:33:36.124057: Current learning rate: 0.00455\n",
      "2025-12-08 10:35:54.629456: train_loss -0.8506\n",
      "2025-12-08 10:35:54.629456: val_loss -0.8728\n",
      "2025-12-08 10:35:54.636457: Pseudo dice [0.9224, 0.9571, 0.9434]\n",
      "2025-12-08 10:35:54.643549: Epoch time: 138.51 s\n",
      "2025-12-08 10:35:55.373566: \n",
      "2025-12-08 10:35:55.373566: Epoch 584\n",
      "2025-12-08 10:35:55.389345: Current learning rate: 0.00454\n",
      "2025-12-08 10:38:13.646049: train_loss -0.8495\n",
      "2025-12-08 10:38:13.646049: val_loss -0.8739\n",
      "2025-12-08 10:38:13.652055: Pseudo dice [0.9226, 0.9544, 0.944]\n",
      "2025-12-08 10:38:13.655797: Epoch time: 138.27 s\n",
      "2025-12-08 10:38:14.478320: \n",
      "2025-12-08 10:38:14.478320: Epoch 585\n",
      "2025-12-08 10:38:14.483825: Current learning rate: 0.00453\n",
      "2025-12-08 10:40:32.619751: train_loss -0.8479\n",
      "2025-12-08 10:40:32.619751: val_loss -0.8789\n",
      "2025-12-08 10:40:32.627430: Pseudo dice [0.9293, 0.9587, 0.9473]\n",
      "2025-12-08 10:40:32.631435: Epoch time: 138.14 s\n",
      "2025-12-08 10:40:33.294771: \n",
      "2025-12-08 10:40:33.295876: Epoch 586\n",
      "2025-12-08 10:40:33.295876: Current learning rate: 0.00452\n",
      "2025-12-08 10:42:51.405753: train_loss -0.8431\n",
      "2025-12-08 10:42:51.405753: val_loss -0.8771\n",
      "2025-12-08 10:42:51.405753: Pseudo dice [0.9239, 0.955, 0.9463]\n",
      "2025-12-08 10:42:51.416932: Epoch time: 138.11 s\n",
      "2025-12-08 10:42:52.213477: \n",
      "2025-12-08 10:42:52.213477: Epoch 587\n",
      "2025-12-08 10:42:52.218479: Current learning rate: 0.00451\n",
      "2025-12-08 10:45:10.513573: train_loss -0.8486\n",
      "2025-12-08 10:45:10.515314: val_loss -0.8802\n",
      "2025-12-08 10:45:10.519178: Pseudo dice [0.927, 0.9517, 0.9465]\n",
      "2025-12-08 10:45:10.523182: Epoch time: 138.3 s\n",
      "2025-12-08 10:45:11.166549: \n",
      "2025-12-08 10:45:11.166549: Epoch 588\n",
      "2025-12-08 10:45:11.166549: Current learning rate: 0.0045\n",
      "2025-12-08 10:47:29.525609: train_loss -0.8468\n",
      "2025-12-08 10:47:29.525609: val_loss -0.8767\n",
      "2025-12-08 10:47:29.531540: Pseudo dice [0.9271, 0.9539, 0.9453]\n",
      "2025-12-08 10:47:29.534649: Epoch time: 138.36 s\n",
      "2025-12-08 10:47:30.172115: \n",
      "2025-12-08 10:47:30.172115: Epoch 589\n",
      "2025-12-08 10:47:30.190361: Current learning rate: 0.00449\n",
      "2025-12-08 10:49:48.393316: train_loss -0.8497\n",
      "2025-12-08 10:49:48.393316: val_loss -0.8744\n",
      "2025-12-08 10:49:48.398319: Pseudo dice [0.9228, 0.9575, 0.9441]\n",
      "2025-12-08 10:49:48.402782: Epoch time: 138.22 s\n",
      "2025-12-08 10:49:49.171736: \n",
      "2025-12-08 10:49:49.171736: Epoch 590\n",
      "2025-12-08 10:49:49.190457: Current learning rate: 0.00448\n",
      "2025-12-08 10:52:07.562145: train_loss -0.8444\n",
      "2025-12-08 10:52:07.562145: val_loss -0.8665\n",
      "2025-12-08 10:52:07.583274: Pseudo dice [0.9191, 0.9512, 0.949]\n",
      "2025-12-08 10:52:07.588285: Epoch time: 138.39 s\n",
      "2025-12-08 10:52:08.220322: \n",
      "2025-12-08 10:52:08.220322: Epoch 591\n",
      "2025-12-08 10:52:08.220322: Current learning rate: 0.00447\n",
      "2025-12-08 10:54:26.444059: train_loss -0.8449\n",
      "2025-12-08 10:54:26.444059: val_loss -0.8641\n",
      "2025-12-08 10:54:26.450069: Pseudo dice [0.9206, 0.9503, 0.9404]\n",
      "2025-12-08 10:54:26.454211: Epoch time: 138.22 s\n",
      "2025-12-08 10:54:27.267821: \n",
      "2025-12-08 10:54:27.268823: Epoch 592\n",
      "2025-12-08 10:54:27.273608: Current learning rate: 0.00446\n",
      "2025-12-08 10:56:45.425279: train_loss -0.8493\n",
      "2025-12-08 10:56:45.425279: val_loss -0.8589\n",
      "2025-12-08 10:56:45.427282: Pseudo dice [0.9133, 0.9529, 0.9455]\n",
      "2025-12-08 10:56:45.427282: Epoch time: 138.16 s\n",
      "2025-12-08 10:56:46.198904: \n",
      "2025-12-08 10:56:46.198904: Epoch 593\n",
      "2025-12-08 10:56:46.204860: Current learning rate: 0.00445\n",
      "2025-12-08 10:59:04.405008: train_loss -0.8498\n",
      "2025-12-08 10:59:04.405008: val_loss -0.86\n",
      "2025-12-08 10:59:04.422963: Pseudo dice [0.9125, 0.9496, 0.9448]\n",
      "2025-12-08 10:59:04.422963: Epoch time: 138.21 s\n",
      "2025-12-08 10:59:05.061343: \n",
      "2025-12-08 10:59:05.061343: Epoch 594\n",
      "2025-12-08 10:59:05.061343: Current learning rate: 0.00444\n",
      "2025-12-08 11:01:23.142632: train_loss -0.8531\n",
      "2025-12-08 11:01:23.142632: val_loss -0.8716\n",
      "2025-12-08 11:01:23.150194: Pseudo dice [0.9204, 0.9523, 0.9498]\n",
      "2025-12-08 11:01:23.155939: Epoch time: 138.08 s\n",
      "2025-12-08 11:01:23.795926: \n",
      "2025-12-08 11:01:23.795926: Epoch 595\n",
      "2025-12-08 11:01:23.811869: Current learning rate: 0.00443\n",
      "2025-12-08 11:03:42.064077: train_loss -0.8506\n",
      "2025-12-08 11:03:42.066079: val_loss -0.8724\n",
      "2025-12-08 11:03:42.074091: Pseudo dice [0.9214, 0.9482, 0.9457]\n",
      "2025-12-08 11:03:42.077834: Epoch time: 138.27 s\n",
      "2025-12-08 11:03:42.831482: \n",
      "2025-12-08 11:03:42.833485: Epoch 596\n",
      "2025-12-08 11:03:42.833485: Current learning rate: 0.00442\n",
      "2025-12-08 11:06:01.109091: train_loss -0.846\n",
      "2025-12-08 11:06:01.109091: val_loss -0.8724\n",
      "2025-12-08 11:06:01.123594: Pseudo dice [0.9225, 0.9553, 0.9458]\n",
      "2025-12-08 11:06:01.129100: Epoch time: 138.28 s\n",
      "2025-12-08 11:06:01.766088: \n",
      "2025-12-08 11:06:01.766088: Epoch 597\n",
      "2025-12-08 11:06:01.766088: Current learning rate: 0.00441\n",
      "2025-12-08 11:08:20.093657: train_loss -0.8482\n",
      "2025-12-08 11:08:20.093657: val_loss -0.8759\n",
      "2025-12-08 11:08:20.093657: Pseudo dice [0.9246, 0.9557, 0.9481]\n",
      "2025-12-08 11:08:20.093657: Epoch time: 138.33 s\n",
      "2025-12-08 11:08:20.905095: \n",
      "2025-12-08 11:08:20.905095: Epoch 598\n",
      "2025-12-08 11:08:20.905095: Current learning rate: 0.0044\n",
      "2025-12-08 11:10:39.106498: train_loss -0.8515\n",
      "2025-12-08 11:10:39.107498: val_loss -0.884\n",
      "2025-12-08 11:10:39.108545: Pseudo dice [0.9292, 0.9564, 0.9508]\n",
      "2025-12-08 11:10:39.108545: Epoch time: 138.2 s\n",
      "2025-12-08 11:10:39.873326: \n",
      "2025-12-08 11:10:39.873326: Epoch 599\n",
      "2025-12-08 11:10:39.880598: Current learning rate: 0.00439\n",
      "2025-12-08 11:12:58.171264: train_loss -0.8531\n",
      "2025-12-08 11:12:58.171264: val_loss -0.8754\n",
      "2025-12-08 11:12:58.171264: Pseudo dice [0.9233, 0.9544, 0.9503]\n",
      "2025-12-08 11:12:58.187120: Epoch time: 138.3 s\n",
      "2025-12-08 11:12:59.084806: \n",
      "2025-12-08 11:12:59.085807: Epoch 600\n",
      "2025-12-08 11:12:59.090566: Current learning rate: 0.00438\n",
      "2025-12-08 11:15:17.406298: train_loss -0.853\n",
      "2025-12-08 11:15:17.406298: val_loss -0.8797\n",
      "2025-12-08 11:15:17.415494: Pseudo dice [0.9254, 0.9604, 0.9476]\n",
      "2025-12-08 11:15:17.421541: Epoch time: 138.32 s\n",
      "2025-12-08 11:15:18.071161: \n",
      "2025-12-08 11:15:18.071161: Epoch 601\n",
      "2025-12-08 11:15:18.075681: Current learning rate: 0.00437\n",
      "2025-12-08 11:17:36.300196: train_loss -0.8521\n",
      "2025-12-08 11:17:36.300196: val_loss -0.8828\n",
      "2025-12-08 11:17:36.308204: Pseudo dice [0.9303, 0.961, 0.9459]\n",
      "2025-12-08 11:17:36.313950: Epoch time: 138.23 s\n",
      "2025-12-08 11:17:36.319957: Yayy! New best EMA pseudo Dice: 0.9418\n",
      "2025-12-08 11:17:37.389695: \n",
      "2025-12-08 11:17:37.389695: Epoch 602\n",
      "2025-12-08 11:17:37.405398: Current learning rate: 0.00436\n",
      "2025-12-08 11:19:55.645820: train_loss -0.8494\n",
      "2025-12-08 11:19:55.645820: val_loss -0.8765\n",
      "2025-12-08 11:19:55.653833: Pseudo dice [0.9259, 0.9538, 0.9489]\n",
      "2025-12-08 11:19:55.658716: Epoch time: 138.26 s\n",
      "2025-12-08 11:19:55.663717: Yayy! New best EMA pseudo Dice: 0.9419\n",
      "2025-12-08 11:19:56.799054: \n",
      "2025-12-08 11:19:56.799054: Epoch 603\n",
      "2025-12-08 11:19:56.799054: Current learning rate: 0.00435\n",
      "2025-12-08 11:22:15.030008: train_loss -0.848\n",
      "2025-12-08 11:22:15.045730: val_loss -0.88\n",
      "2025-12-08 11:22:15.045730: Pseudo dice [0.9304, 0.953, 0.9444]\n",
      "2025-12-08 11:22:15.045730: Epoch time: 138.23 s\n",
      "2025-12-08 11:22:15.045730: Yayy! New best EMA pseudo Dice: 0.942\n",
      "2025-12-08 11:22:15.960745: \n",
      "2025-12-08 11:22:15.960745: Epoch 604\n",
      "2025-12-08 11:22:15.960745: Current learning rate: 0.00434\n",
      "2025-12-08 11:24:34.213158: train_loss -0.8511\n",
      "2025-12-08 11:24:34.213158: val_loss -0.8815\n",
      "2025-12-08 11:24:34.218165: Pseudo dice [0.9276, 0.9613, 0.9505]\n",
      "2025-12-08 11:24:34.222170: Epoch time: 138.25 s\n",
      "2025-12-08 11:24:34.228178: Yayy! New best EMA pseudo Dice: 0.9424\n",
      "2025-12-08 11:24:35.218259: \n",
      "2025-12-08 11:24:35.218259: Epoch 605\n",
      "2025-12-08 11:24:35.218259: Current learning rate: 0.00433\n",
      "2025-12-08 11:26:53.442507: train_loss -0.8567\n",
      "2025-12-08 11:26:53.442507: val_loss -0.8681\n",
      "2025-12-08 11:26:53.448514: Pseudo dice [0.9155, 0.951, 0.9507]\n",
      "2025-12-08 11:26:53.452256: Epoch time: 138.22 s\n",
      "2025-12-08 11:26:54.092959: \n",
      "2025-12-08 11:26:54.092959: Epoch 606\n",
      "2025-12-08 11:26:54.092959: Current learning rate: 0.00432\n",
      "2025-12-08 11:29:12.498939: train_loss -0.8538\n",
      "2025-12-08 11:29:12.498939: val_loss -0.8762\n",
      "2025-12-08 11:29:12.504945: Pseudo dice [0.9261, 0.9541, 0.9506]\n",
      "2025-12-08 11:29:12.508949: Epoch time: 138.41 s\n",
      "2025-12-08 11:29:13.155500: \n",
      "2025-12-08 11:29:13.155500: Epoch 607\n",
      "2025-12-08 11:29:13.155500: Current learning rate: 0.00431\n",
      "2025-12-08 11:31:31.405615: train_loss -0.8496\n",
      "2025-12-08 11:31:31.405615: val_loss -0.8689\n",
      "2025-12-08 11:31:31.410798: Pseudo dice [0.9201, 0.9512, 0.9483]\n",
      "2025-12-08 11:31:31.415800: Epoch time: 138.25 s\n",
      "2025-12-08 11:31:32.149358: \n",
      "2025-12-08 11:31:32.149358: Epoch 608\n",
      "2025-12-08 11:31:32.149358: Current learning rate: 0.0043\n",
      "2025-12-08 11:33:50.372574: train_loss -0.8567\n",
      "2025-12-08 11:33:50.372574: val_loss -0.8746\n",
      "2025-12-08 11:33:50.378319: Pseudo dice [0.923, 0.9542, 0.9453]\n",
      "2025-12-08 11:33:50.382323: Epoch time: 138.22 s\n",
      "2025-12-08 11:33:51.202030: \n",
      "2025-12-08 11:33:51.202030: Epoch 609\n",
      "2025-12-08 11:33:51.202030: Current learning rate: 0.00429\n",
      "2025-12-08 11:36:09.369503: train_loss -0.8505\n",
      "2025-12-08 11:36:09.370504: val_loss -0.8751\n",
      "2025-12-08 11:36:09.375585: Pseudo dice [0.9249, 0.955, 0.9463]\n",
      "2025-12-08 11:36:09.379595: Epoch time: 138.17 s\n",
      "2025-12-08 11:36:10.014740: \n",
      "2025-12-08 11:36:10.014740: Epoch 610\n",
      "2025-12-08 11:36:10.014740: Current learning rate: 0.00429\n",
      "2025-12-08 11:38:28.210448: train_loss -0.8535\n",
      "2025-12-08 11:38:28.212450: val_loss -0.8797\n",
      "2025-12-08 11:38:28.217956: Pseudo dice [0.9259, 0.9536, 0.9475]\n",
      "2025-12-08 11:38:28.221831: Epoch time: 138.2 s\n",
      "2025-12-08 11:38:28.967634: \n",
      "2025-12-08 11:38:28.967634: Epoch 611\n",
      "2025-12-08 11:38:28.967634: Current learning rate: 0.00428\n",
      "2025-12-08 11:40:47.357665: train_loss -0.8515\n",
      "2025-12-08 11:40:47.365667: val_loss -0.8643\n",
      "2025-12-08 11:40:47.372669: Pseudo dice [0.9208, 0.9481, 0.9379]\n",
      "2025-12-08 11:40:47.377718: Epoch time: 138.39 s\n",
      "2025-12-08 11:40:48.061474: \n",
      "2025-12-08 11:40:48.061474: Epoch 612\n",
      "2025-12-08 11:40:48.061474: Current learning rate: 0.00427\n",
      "2025-12-08 11:43:06.157527: train_loss -0.8461\n",
      "2025-12-08 11:43:06.157527: val_loss -0.8812\n",
      "2025-12-08 11:43:06.163271: Pseudo dice [0.9301, 0.9566, 0.9473]\n",
      "2025-12-08 11:43:06.171264: Epoch time: 138.1 s\n",
      "2025-12-08 11:43:06.824162: \n",
      "2025-12-08 11:43:06.826164: Epoch 613\n",
      "2025-12-08 11:43:06.826164: Current learning rate: 0.00426\n",
      "2025-12-08 11:45:25.061231: train_loss -0.8476\n",
      "2025-12-08 11:45:25.061231: val_loss -0.8658\n",
      "2025-12-08 11:45:25.061231: Pseudo dice [0.9197, 0.9497, 0.9478]\n",
      "2025-12-08 11:45:25.061231: Epoch time: 138.24 s\n",
      "2025-12-08 11:45:25.786443: \n",
      "2025-12-08 11:45:25.787443: Epoch 614\n",
      "2025-12-08 11:45:25.792643: Current learning rate: 0.00425\n",
      "2025-12-08 11:47:43.687563: train_loss -0.8536\n",
      "2025-12-08 11:47:43.687563: val_loss -0.8762\n",
      "2025-12-08 11:47:43.693070: Pseudo dice [0.9253, 0.9549, 0.9466]\n",
      "2025-12-08 11:47:43.698550: Epoch time: 137.9 s\n",
      "2025-12-08 11:47:44.512382: \n",
      "2025-12-08 11:47:44.513387: Epoch 615\n",
      "2025-12-08 11:47:44.517420: Current learning rate: 0.00424\n",
      "2025-12-08 11:50:02.546664: train_loss -0.8489\n",
      "2025-12-08 11:50:02.546664: val_loss -0.8593\n",
      "2025-12-08 11:50:02.562541: Pseudo dice [0.9153, 0.9482, 0.9395]\n",
      "2025-12-08 11:50:02.562541: Epoch time: 138.04 s\n",
      "2025-12-08 11:50:03.201932: \n",
      "2025-12-08 11:50:03.201932: Epoch 616\n",
      "2025-12-08 11:50:03.201932: Current learning rate: 0.00423\n",
      "2025-12-08 11:52:21.484701: train_loss -0.8529\n",
      "2025-12-08 11:52:21.485701: val_loss -0.876\n",
      "2025-12-08 11:52:21.491930: Pseudo dice [0.9263, 0.955, 0.9422]\n",
      "2025-12-08 11:52:21.495931: Epoch time: 138.28 s\n",
      "2025-12-08 11:52:22.305726: \n",
      "2025-12-08 11:52:22.305726: Epoch 617\n",
      "2025-12-08 11:52:22.305726: Current learning rate: 0.00422\n",
      "2025-12-08 11:54:40.500879: train_loss -0.8566\n",
      "2025-12-08 11:54:40.500879: val_loss -0.8682\n",
      "2025-12-08 11:54:40.508887: Pseudo dice [0.9223, 0.9517, 0.943]\n",
      "2025-12-08 11:54:40.510889: Epoch time: 138.2 s\n",
      "2025-12-08 11:54:41.155396: \n",
      "2025-12-08 11:54:41.155396: Epoch 618\n",
      "2025-12-08 11:54:41.155396: Current learning rate: 0.00421\n",
      "2025-12-08 11:56:59.420792: train_loss -0.8489\n",
      "2025-12-08 11:56:59.420792: val_loss -0.8719\n",
      "2025-12-08 11:56:59.426293: Pseudo dice [0.9223, 0.9521, 0.9448]\n",
      "2025-12-08 11:56:59.430297: Epoch time: 138.27 s\n",
      "2025-12-08 11:57:00.081447: \n",
      "2025-12-08 11:57:00.083450: Epoch 619\n",
      "2025-12-08 11:57:00.083450: Current learning rate: 0.0042\n",
      "2025-12-08 11:59:18.498548: train_loss -0.8464\n",
      "2025-12-08 11:59:18.498548: val_loss -0.8766\n",
      "2025-12-08 11:59:18.511791: Pseudo dice [0.9255, 0.9569, 0.9496]\n",
      "2025-12-08 11:59:18.513794: Epoch time: 138.42 s\n",
      "2025-12-08 11:59:19.358364: \n",
      "2025-12-08 11:59:19.358364: Epoch 620\n",
      "2025-12-08 11:59:19.370532: Current learning rate: 0.00419\n",
      "2025-12-08 12:01:37.622609: train_loss -0.853\n",
      "2025-12-08 12:01:37.622609: val_loss -0.8695\n",
      "2025-12-08 12:01:37.630357: Pseudo dice [0.9193, 0.9518, 0.951]\n",
      "2025-12-08 12:01:37.636363: Epoch time: 138.26 s\n",
      "2025-12-08 12:01:38.466359: \n",
      "2025-12-08 12:01:38.467361: Epoch 621\n",
      "2025-12-08 12:01:38.468364: Current learning rate: 0.00418\n",
      "2025-12-08 12:03:56.608423: train_loss -0.8483\n",
      "2025-12-08 12:03:56.608423: val_loss -0.8784\n",
      "2025-12-08 12:03:56.608423: Pseudo dice [0.9259, 0.9558, 0.9461]\n",
      "2025-12-08 12:03:56.608423: Epoch time: 138.14 s\n",
      "2025-12-08 12:03:57.295645: \n",
      "2025-12-08 12:03:57.299144: Epoch 622\n",
      "2025-12-08 12:03:57.299144: Current learning rate: 0.00417\n",
      "2025-12-08 12:06:15.187115: train_loss -0.8468\n",
      "2025-12-08 12:06:15.187115: val_loss -0.8822\n",
      "2025-12-08 12:06:15.206437: Pseudo dice [0.9315, 0.9595, 0.9436]\n",
      "2025-12-08 12:06:15.210437: Epoch time: 137.89 s\n",
      "2025-12-08 12:06:15.930506: \n",
      "2025-12-08 12:06:15.930506: Epoch 623\n",
      "2025-12-08 12:06:15.936160: Current learning rate: 0.00416\n",
      "2025-12-08 12:08:34.107363: train_loss -0.8467\n",
      "2025-12-08 12:08:34.107363: val_loss -0.8829\n",
      "2025-12-08 12:08:34.114259: Pseudo dice [0.9279, 0.9582, 0.9516]\n",
      "2025-12-08 12:08:34.118259: Epoch time: 138.18 s\n",
      "2025-12-08 12:08:34.763387: \n",
      "2025-12-08 12:08:34.763387: Epoch 624\n",
      "2025-12-08 12:08:34.768507: Current learning rate: 0.00415\n",
      "2025-12-08 12:10:52.787512: train_loss -0.8506\n",
      "2025-12-08 12:10:52.789515: val_loss -0.8746\n",
      "2025-12-08 12:10:52.794888: Pseudo dice [0.923, 0.9542, 0.9492]\n",
      "2025-12-08 12:10:52.799895: Epoch time: 138.03 s\n",
      "2025-12-08 12:10:53.496061: \n",
      "2025-12-08 12:10:53.497066: Epoch 625\n",
      "2025-12-08 12:10:53.502118: Current learning rate: 0.00414\n",
      "2025-12-08 12:13:11.530668: train_loss -0.8549\n",
      "2025-12-08 12:13:11.530668: val_loss -0.8735\n",
      "2025-12-08 12:13:11.534675: Pseudo dice [0.9218, 0.9529, 0.9462]\n",
      "2025-12-08 12:13:11.538681: Epoch time: 138.04 s\n",
      "2025-12-08 12:13:12.256603: \n",
      "2025-12-08 12:13:12.257605: Epoch 626\n",
      "2025-12-08 12:13:12.262615: Current learning rate: 0.00413\n",
      "2025-12-08 12:15:30.682299: train_loss -0.8499\n",
      "2025-12-08 12:15:30.683300: val_loss -0.8703\n",
      "2025-12-08 12:15:30.690130: Pseudo dice [0.9204, 0.9533, 0.9473]\n",
      "2025-12-08 12:15:30.696136: Epoch time: 138.43 s\n",
      "2025-12-08 12:15:31.390268: \n",
      "2025-12-08 12:15:31.390268: Epoch 627\n",
      "2025-12-08 12:15:31.390268: Current learning rate: 0.00412\n",
      "2025-12-08 12:17:49.390009: train_loss -0.8502\n",
      "2025-12-08 12:17:49.390009: val_loss -0.8609\n",
      "2025-12-08 12:17:49.390009: Pseudo dice [0.9157, 0.9459, 0.9449]\n",
      "2025-12-08 12:17:49.405677: Epoch time: 138.0 s\n",
      "2025-12-08 12:17:50.218261: \n",
      "2025-12-08 12:17:50.218261: Epoch 628\n",
      "2025-12-08 12:17:50.234213: Current learning rate: 0.00411\n",
      "2025-12-08 12:20:08.339077: train_loss -0.8468\n",
      "2025-12-08 12:20:08.339077: val_loss -0.8728\n",
      "2025-12-08 12:20:08.345078: Pseudo dice [0.9238, 0.9495, 0.9431]\n",
      "2025-12-08 12:20:08.350079: Epoch time: 138.12 s\n",
      "2025-12-08 12:20:09.104075: \n",
      "2025-12-08 12:20:09.104075: Epoch 629\n",
      "2025-12-08 12:20:09.109162: Current learning rate: 0.0041\n",
      "2025-12-08 12:22:27.361389: train_loss -0.8479\n",
      "2025-12-08 12:22:27.362390: val_loss -0.8784\n",
      "2025-12-08 12:22:27.368390: Pseudo dice [0.9241, 0.9594, 0.9469]\n",
      "2025-12-08 12:22:27.373391: Epoch time: 138.26 s\n",
      "2025-12-08 12:22:28.015139: \n",
      "2025-12-08 12:22:28.015139: Epoch 630\n",
      "2025-12-08 12:22:28.030999: Current learning rate: 0.00409\n",
      "2025-12-08 12:24:46.295503: train_loss -0.8451\n",
      "2025-12-08 12:24:46.295503: val_loss -0.8629\n",
      "2025-12-08 12:24:46.315543: Pseudo dice [0.92, 0.9529, 0.9399]\n",
      "2025-12-08 12:24:46.321549: Epoch time: 138.28 s\n",
      "2025-12-08 12:24:46.952246: \n",
      "2025-12-08 12:24:46.952246: Epoch 631\n",
      "2025-12-08 12:24:46.968308: Current learning rate: 0.00408\n",
      "2025-12-08 12:27:05.041759: train_loss -0.847\n",
      "2025-12-08 12:27:05.041759: val_loss -0.874\n",
      "2025-12-08 12:27:05.045763: Pseudo dice [0.9228, 0.9557, 0.9474]\n",
      "2025-12-08 12:27:05.052637: Epoch time: 138.09 s\n",
      "2025-12-08 12:27:05.858378: \n",
      "2025-12-08 12:27:05.858378: Epoch 632\n",
      "2025-12-08 12:27:05.863616: Current learning rate: 0.00407\n",
      "2025-12-08 12:29:24.063162: train_loss -0.8465\n",
      "2025-12-08 12:29:24.065165: val_loss -0.8755\n",
      "2025-12-08 12:29:24.071172: Pseudo dice [0.9219, 0.9567, 0.9515]\n",
      "2025-12-08 12:29:24.075176: Epoch time: 138.2 s\n",
      "2025-12-08 12:29:24.702618: \n",
      "2025-12-08 12:29:24.702618: Epoch 633\n",
      "2025-12-08 12:29:24.718655: Current learning rate: 0.00406\n",
      "2025-12-08 12:31:42.971310: train_loss -0.8462\n",
      "2025-12-08 12:31:42.971310: val_loss -0.8716\n",
      "2025-12-08 12:31:42.976793: Pseudo dice [0.9229, 0.9559, 0.9501]\n",
      "2025-12-08 12:31:42.980803: Epoch time: 138.27 s\n",
      "2025-12-08 12:31:43.621922: \n",
      "2025-12-08 12:31:43.621922: Epoch 634\n",
      "2025-12-08 12:31:43.629534: Current learning rate: 0.00405\n",
      "2025-12-08 12:34:01.682093: train_loss -0.8533\n",
      "2025-12-08 12:34:01.682093: val_loss -0.8695\n",
      "2025-12-08 12:34:01.685836: Pseudo dice [0.9213, 0.9488, 0.9485]\n",
      "2025-12-08 12:34:01.693342: Epoch time: 138.06 s\n",
      "2025-12-08 12:34:02.479089: \n",
      "2025-12-08 12:34:02.479089: Epoch 635\n",
      "2025-12-08 12:34:02.484558: Current learning rate: 0.00404\n",
      "2025-12-08 12:36:20.505781: train_loss -0.8467\n",
      "2025-12-08 12:36:20.505781: val_loss -0.8792\n",
      "2025-12-08 12:36:20.511789: Pseudo dice [0.9274, 0.9588, 0.9447]\n",
      "2025-12-08 12:36:20.517536: Epoch time: 138.03 s\n",
      "2025-12-08 12:36:21.161222: \n",
      "2025-12-08 12:36:21.161222: Epoch 636\n",
      "2025-12-08 12:36:21.166617: Current learning rate: 0.00403\n",
      "2025-12-08 12:38:39.384805: train_loss -0.8521\n",
      "2025-12-08 12:38:39.386807: val_loss -0.8772\n",
      "2025-12-08 12:38:39.392551: Pseudo dice [0.9242, 0.9548, 0.949]\n",
      "2025-12-08 12:38:39.398557: Epoch time: 138.22 s\n",
      "2025-12-08 12:38:40.071875: \n",
      "2025-12-08 12:38:40.071875: Epoch 637\n",
      "2025-12-08 12:38:40.076886: Current learning rate: 0.00402\n",
      "2025-12-08 12:40:58.201819: train_loss -0.8494\n",
      "2025-12-08 12:40:58.202852: val_loss -0.8639\n",
      "2025-12-08 12:40:58.209970: Pseudo dice [0.9165, 0.9454, 0.9463]\n",
      "2025-12-08 12:40:58.213978: Epoch time: 138.13 s\n",
      "2025-12-08 12:40:58.952604: \n",
      "2025-12-08 12:40:58.952604: Epoch 638\n",
      "2025-12-08 12:40:58.968668: Current learning rate: 0.00401\n",
      "2025-12-08 12:43:17.202959: train_loss -0.8538\n",
      "2025-12-08 12:43:17.202959: val_loss -0.8772\n",
      "2025-12-08 12:43:17.202959: Pseudo dice [0.9259, 0.958, 0.948]\n",
      "2025-12-08 12:43:17.218887: Epoch time: 138.25 s\n",
      "2025-12-08 12:43:18.061367: \n",
      "2025-12-08 12:43:18.061367: Epoch 639\n",
      "2025-12-08 12:43:18.061367: Current learning rate: 0.004\n",
      "2025-12-08 12:45:36.186646: train_loss -0.8497\n",
      "2025-12-08 12:45:36.186646: val_loss -0.8796\n",
      "2025-12-08 12:45:36.197527: Pseudo dice [0.9307, 0.9575, 0.9476]\n",
      "2025-12-08 12:45:36.204708: Epoch time: 138.13 s\n",
      "2025-12-08 12:45:36.836503: \n",
      "2025-12-08 12:45:36.837781: Epoch 640\n",
      "2025-12-08 12:45:36.842793: Current learning rate: 0.00399\n",
      "2025-12-08 12:47:55.055684: train_loss -0.8514\n",
      "2025-12-08 12:47:55.055684: val_loss -0.8796\n",
      "2025-12-08 12:47:55.061689: Pseudo dice [0.9272, 0.9547, 0.9491]\n",
      "2025-12-08 12:47:55.066866: Epoch time: 138.22 s\n",
      "2025-12-08 12:47:55.732917: \n",
      "2025-12-08 12:47:55.732917: Epoch 641\n",
      "2025-12-08 12:47:55.732917: Current learning rate: 0.00398\n",
      "2025-12-08 12:50:13.796073: train_loss -0.8495\n",
      "2025-12-08 12:50:13.796073: val_loss -0.8814\n",
      "2025-12-08 12:50:13.796073: Pseudo dice [0.9294, 0.9582, 0.9482]\n",
      "2025-12-08 12:50:13.811918: Epoch time: 138.06 s\n",
      "2025-12-08 12:50:14.451980: \n",
      "2025-12-08 12:50:14.451980: Epoch 642\n",
      "2025-12-08 12:50:14.451980: Current learning rate: 0.00397\n",
      "2025-12-08 12:52:32.466749: train_loss -0.8481\n",
      "2025-12-08 12:52:32.467750: val_loss -0.8626\n",
      "2025-12-08 12:52:32.473797: Pseudo dice [0.9166, 0.9483, 0.9405]\n",
      "2025-12-08 12:52:32.479798: Epoch time: 138.01 s\n",
      "2025-12-08 12:52:33.130265: \n",
      "2025-12-08 12:52:33.130265: Epoch 643\n",
      "2025-12-08 12:52:33.135283: Current learning rate: 0.00396\n",
      "2025-12-08 12:54:51.125425: train_loss -0.8495\n",
      "2025-12-08 12:54:51.125425: val_loss -0.8736\n",
      "2025-12-08 12:54:51.141081: Pseudo dice [0.9265, 0.9588, 0.9412]\n",
      "2025-12-08 12:54:51.141081: Epoch time: 138.0 s\n",
      "2025-12-08 12:54:51.780529: \n",
      "2025-12-08 12:54:51.780529: Epoch 644\n",
      "2025-12-08 12:54:51.780529: Current learning rate: 0.00395\n",
      "2025-12-08 12:57:09.844167: train_loss -0.8462\n",
      "2025-12-08 12:57:09.844167: val_loss -0.8747\n",
      "2025-12-08 12:57:09.851676: Pseudo dice [0.9247, 0.9585, 0.9422]\n",
      "2025-12-08 12:57:09.851676: Epoch time: 138.06 s\n",
      "2025-12-08 12:57:10.554338: \n",
      "2025-12-08 12:57:10.554338: Epoch 645\n",
      "2025-12-08 12:57:10.559352: Current learning rate: 0.00394\n",
      "2025-12-08 12:59:28.682719: train_loss -0.8491\n",
      "2025-12-08 12:59:28.682719: val_loss -0.8805\n",
      "2025-12-08 12:59:28.686916: Pseudo dice [0.9306, 0.9603, 0.9405]\n",
      "2025-12-08 12:59:28.692058: Epoch time: 138.13 s\n",
      "2025-12-08 12:59:29.511787: \n",
      "2025-12-08 12:59:29.511787: Epoch 646\n",
      "2025-12-08 12:59:29.515794: Current learning rate: 0.00393\n",
      "2025-12-08 13:01:47.577433: train_loss -0.8503\n",
      "2025-12-08 13:01:47.577433: val_loss -0.8708\n",
      "2025-12-08 13:01:47.595304: Pseudo dice [0.9182, 0.9563, 0.9436]\n",
      "2025-12-08 13:01:47.601310: Epoch time: 138.07 s\n",
      "2025-12-08 13:01:48.246366: \n",
      "2025-12-08 13:01:48.246366: Epoch 647\n",
      "2025-12-08 13:01:48.248692: Current learning rate: 0.00392\n",
      "2025-12-08 13:04:06.263833: train_loss -0.8515\n",
      "2025-12-08 13:04:06.263833: val_loss -0.8723\n",
      "2025-12-08 13:04:06.270432: Pseudo dice [0.921, 0.954, 0.9481]\n",
      "2025-12-08 13:04:06.274436: Epoch time: 138.02 s\n",
      "2025-12-08 13:04:06.923144: \n",
      "2025-12-08 13:04:06.923144: Epoch 648\n",
      "2025-12-08 13:04:06.923144: Current learning rate: 0.00391\n",
      "2025-12-08 13:06:25.076009: train_loss -0.8512\n",
      "2025-12-08 13:06:25.076009: val_loss -0.8826\n",
      "2025-12-08 13:06:25.081069: Pseudo dice [0.927, 0.9625, 0.9515]\n",
      "2025-12-08 13:06:25.086075: Epoch time: 138.15 s\n",
      "2025-12-08 13:06:25.736226: \n",
      "2025-12-08 13:06:25.736226: Epoch 649\n",
      "2025-12-08 13:06:25.741611: Current learning rate: 0.0039\n",
      "2025-12-08 13:08:43.779920: train_loss -0.8528\n",
      "2025-12-08 13:08:43.795563: val_loss -0.8786\n",
      "2025-12-08 13:08:43.799568: Pseudo dice [0.9274, 0.956, 0.9487]\n",
      "2025-12-08 13:08:43.803572: Epoch time: 138.04 s\n",
      "2025-12-08 13:08:44.805733: \n",
      "2025-12-08 13:08:44.805733: Epoch 650\n",
      "2025-12-08 13:08:44.811478: Current learning rate: 0.00389\n",
      "2025-12-08 13:11:03.074291: train_loss -0.8493\n",
      "2025-12-08 13:11:03.075291: val_loss -0.8761\n",
      "2025-12-08 13:11:03.081501: Pseudo dice [0.9271, 0.9593, 0.9403]\n",
      "2025-12-08 13:11:03.087502: Epoch time: 138.27 s\n",
      "2025-12-08 13:11:03.737605: \n",
      "2025-12-08 13:11:03.739607: Epoch 651\n",
      "2025-12-08 13:11:03.739607: Current learning rate: 0.00388\n",
      "2025-12-08 13:13:22.036629: train_loss -0.8576\n",
      "2025-12-08 13:13:22.036629: val_loss -0.876\n",
      "2025-12-08 13:13:22.041649: Pseudo dice [0.9249, 0.9596, 0.9437]\n",
      "2025-12-08 13:13:22.045661: Epoch time: 138.3 s\n",
      "2025-12-08 13:13:22.687684: \n",
      "2025-12-08 13:13:22.688865: Epoch 652\n",
      "2025-12-08 13:13:22.692799: Current learning rate: 0.00387\n",
      "2025-12-08 13:15:40.876158: train_loss -0.8509\n",
      "2025-12-08 13:15:40.878163: val_loss -0.873\n",
      "2025-12-08 13:15:40.884170: Pseudo dice [0.923, 0.9541, 0.9468]\n",
      "2025-12-08 13:15:40.889914: Epoch time: 138.2 s\n",
      "2025-12-08 13:15:41.530931: \n",
      "2025-12-08 13:15:41.530931: Epoch 653\n",
      "2025-12-08 13:15:41.546314: Current learning rate: 0.00386\n",
      "2025-12-08 13:17:59.666878: train_loss -0.8518\n",
      "2025-12-08 13:17:59.666878: val_loss -0.874\n",
      "2025-12-08 13:17:59.670907: Pseudo dice [0.9242, 0.9545, 0.9501]\n",
      "2025-12-08 13:17:59.670907: Epoch time: 138.14 s\n",
      "2025-12-08 13:18:00.317849: \n",
      "2025-12-08 13:18:00.318849: Epoch 654\n",
      "2025-12-08 13:18:00.323081: Current learning rate: 0.00385\n",
      "2025-12-08 13:20:18.284275: train_loss -0.852\n",
      "2025-12-08 13:20:18.284275: val_loss -0.8768\n",
      "2025-12-08 13:20:18.290285: Pseudo dice [0.924, 0.9546, 0.9491]\n",
      "2025-12-08 13:20:18.294290: Epoch time: 137.97 s\n",
      "2025-12-08 13:20:18.957392: \n",
      "2025-12-08 13:20:18.957392: Epoch 655\n",
      "2025-12-08 13:20:18.957392: Current learning rate: 0.00384\n",
      "2025-12-08 13:22:37.208921: train_loss -0.8519\n",
      "2025-12-08 13:22:37.210923: val_loss -0.8775\n",
      "2025-12-08 13:22:37.214926: Pseudo dice [0.9246, 0.9557, 0.9454]\n",
      "2025-12-08 13:22:37.220747: Epoch time: 138.25 s\n",
      "2025-12-08 13:22:37.920427: \n",
      "2025-12-08 13:22:37.920427: Epoch 656\n",
      "2025-12-08 13:22:37.936234: Current learning rate: 0.00383\n",
      "2025-12-08 13:24:56.140154: train_loss -0.8506\n",
      "2025-12-08 13:24:56.140154: val_loss -0.8784\n",
      "2025-12-08 13:24:56.148244: Pseudo dice [0.9237, 0.955, 0.952]\n",
      "2025-12-08 13:24:56.154905: Epoch time: 138.22 s\n",
      "2025-12-08 13:24:56.952526: \n",
      "2025-12-08 13:24:56.952526: Epoch 657\n",
      "2025-12-08 13:24:56.968234: Current learning rate: 0.00382\n",
      "2025-12-08 13:27:15.185112: train_loss -0.851\n",
      "2025-12-08 13:27:15.186114: val_loss -0.8845\n",
      "2025-12-08 13:27:15.186114: Pseudo dice [0.9315, 0.9533, 0.9538]\n",
      "2025-12-08 13:27:15.195626: Epoch time: 138.23 s\n",
      "2025-12-08 13:27:15.199631: Yayy! New best EMA pseudo Dice: 0.9428\n",
      "2025-12-08 13:27:16.156150: \n",
      "2025-12-08 13:27:16.156150: Epoch 658\n",
      "2025-12-08 13:27:16.156150: Current learning rate: 0.00381\n",
      "2025-12-08 13:29:34.123275: train_loss -0.8551\n",
      "2025-12-08 13:29:34.125064: val_loss -0.8738\n",
      "2025-12-08 13:29:34.127066: Pseudo dice [0.9215, 0.9526, 0.95]\n",
      "2025-12-08 13:29:34.134416: Epoch time: 137.97 s\n",
      "2025-12-08 13:29:34.779781: \n",
      "2025-12-08 13:29:34.779781: Epoch 659\n",
      "2025-12-08 13:29:34.779781: Current learning rate: 0.0038\n",
      "2025-12-08 13:31:52.920936: train_loss -0.8375\n",
      "2025-12-08 13:31:52.920936: val_loss -0.8666\n",
      "2025-12-08 13:31:52.920936: Pseudo dice [0.9132, 0.9472, 0.953]\n",
      "2025-12-08 13:31:52.920936: Epoch time: 138.14 s\n",
      "2025-12-08 13:31:53.561152: \n",
      "2025-12-08 13:31:53.561152: Epoch 660\n",
      "2025-12-08 13:31:53.561152: Current learning rate: 0.00379\n",
      "2025-12-08 13:34:11.655358: train_loss -0.8367\n",
      "2025-12-08 13:34:11.655358: val_loss -0.8659\n",
      "2025-12-08 13:34:11.655358: Pseudo dice [0.921, 0.9563, 0.9431]\n",
      "2025-12-08 13:34:11.655358: Epoch time: 138.09 s\n",
      "2025-12-08 13:34:12.416150: \n",
      "2025-12-08 13:34:12.417152: Epoch 661\n",
      "2025-12-08 13:34:12.422212: Current learning rate: 0.00378\n",
      "2025-12-08 13:36:30.378357: train_loss -0.8428\n",
      "2025-12-08 13:36:30.379359: val_loss -0.8678\n",
      "2025-12-08 13:36:30.384370: Pseudo dice [0.9195, 0.9521, 0.9471]\n",
      "2025-12-08 13:36:30.388379: Epoch time: 137.96 s\n",
      "2025-12-08 13:36:31.015163: \n",
      "2025-12-08 13:36:31.015163: Epoch 662\n",
      "2025-12-08 13:36:31.015163: Current learning rate: 0.00377\n",
      "2025-12-08 13:38:48.950665: train_loss -0.8474\n",
      "2025-12-08 13:38:48.952406: val_loss -0.8791\n",
      "2025-12-08 13:38:48.959559: Pseudo dice [0.927, 0.9598, 0.9507]\n",
      "2025-12-08 13:38:48.964560: Epoch time: 137.94 s\n",
      "2025-12-08 13:38:49.639631: \n",
      "2025-12-08 13:38:49.639631: Epoch 663\n",
      "2025-12-08 13:38:49.639631: Current learning rate: 0.00376\n",
      "2025-12-08 13:41:07.859092: train_loss -0.8485\n",
      "2025-12-08 13:41:07.859092: val_loss -0.8763\n",
      "2025-12-08 13:41:07.859092: Pseudo dice [0.9261, 0.9571, 0.9499]\n",
      "2025-12-08 13:41:07.859092: Epoch time: 138.22 s\n",
      "2025-12-08 13:41:08.878801: \n",
      "2025-12-08 13:41:08.879804: Epoch 664\n",
      "2025-12-08 13:41:08.885817: Current learning rate: 0.00375\n",
      "2025-12-08 13:43:26.907921: train_loss -0.8461\n",
      "2025-12-08 13:43:26.908922: val_loss -0.8768\n",
      "2025-12-08 13:43:26.915942: Pseudo dice [0.9259, 0.9593, 0.953]\n",
      "2025-12-08 13:43:26.921955: Epoch time: 138.03 s\n",
      "2025-12-08 13:43:27.562299: \n",
      "2025-12-08 13:43:27.562299: Epoch 665\n",
      "2025-12-08 13:43:27.562299: Current learning rate: 0.00374\n",
      "2025-12-08 13:45:45.720987: train_loss -0.8485\n",
      "2025-12-08 13:45:45.721988: val_loss -0.8729\n",
      "2025-12-08 13:45:45.727014: Pseudo dice [0.9237, 0.9541, 0.9462]\n",
      "2025-12-08 13:45:45.732029: Epoch time: 138.16 s\n",
      "2025-12-08 13:45:46.374135: \n",
      "2025-12-08 13:45:46.374135: Epoch 666\n",
      "2025-12-08 13:45:46.390042: Current learning rate: 0.00373\n",
      "2025-12-08 13:48:04.329099: train_loss -0.8539\n",
      "2025-12-08 13:48:04.330099: val_loss -0.8764\n",
      "2025-12-08 13:48:04.335121: Pseudo dice [0.9244, 0.9555, 0.9459]\n",
      "2025-12-08 13:48:04.339128: Epoch time: 137.95 s\n",
      "2025-12-08 13:48:05.093156: \n",
      "2025-12-08 13:48:05.093156: Epoch 667\n",
      "2025-12-08 13:48:05.093156: Current learning rate: 0.00372\n",
      "2025-12-08 13:50:23.187428: train_loss -0.8466\n",
      "2025-12-08 13:50:23.187428: val_loss -0.8746\n",
      "2025-12-08 13:50:23.191364: Pseudo dice [0.9231, 0.9572, 0.9523]\n",
      "2025-12-08 13:50:23.197370: Epoch time: 138.09 s\n",
      "2025-12-08 13:50:23.859921: \n",
      "2025-12-08 13:50:23.859921: Epoch 668\n",
      "2025-12-08 13:50:23.859921: Current learning rate: 0.00371\n",
      "2025-12-08 13:52:41.640111: train_loss -0.8551\n",
      "2025-12-08 13:52:41.640111: val_loss -0.885\n",
      "2025-12-08 13:52:41.646566: Pseudo dice [0.9316, 0.9633, 0.9467]\n",
      "2025-12-08 13:52:41.650570: Epoch time: 137.8 s\n",
      "2025-12-08 13:52:41.654574: Yayy! New best EMA pseudo Dice: 0.9431\n",
      "2025-12-08 13:52:42.733728: \n",
      "2025-12-08 13:52:42.739226: Epoch 669\n",
      "2025-12-08 13:52:42.739226: Current learning rate: 0.0037\n",
      "2025-12-08 13:55:00.776456: train_loss -0.8571\n",
      "2025-12-08 13:55:00.778459: val_loss -0.879\n",
      "2025-12-08 13:55:00.788223: Pseudo dice [0.9281, 0.9551, 0.95]\n",
      "2025-12-08 13:55:00.794233: Epoch time: 138.04 s\n",
      "2025-12-08 13:55:00.801980: Yayy! New best EMA pseudo Dice: 0.9433\n",
      "2025-12-08 13:55:01.791488: \n",
      "2025-12-08 13:55:01.791488: Epoch 670\n",
      "2025-12-08 13:55:01.798554: Current learning rate: 0.00369\n",
      "2025-12-08 13:57:19.883137: train_loss -0.8569\n",
      "2025-12-08 13:57:19.883137: val_loss -0.8703\n",
      "2025-12-08 13:57:19.889146: Pseudo dice [0.9218, 0.9539, 0.9454]\n",
      "2025-12-08 13:57:19.895155: Epoch time: 138.09 s\n",
      "2025-12-08 13:57:20.547247: \n",
      "2025-12-08 13:57:20.547247: Epoch 671\n",
      "2025-12-08 13:57:20.563160: Current learning rate: 0.00368\n",
      "2025-12-08 13:59:38.639825: train_loss -0.851\n",
      "2025-12-08 13:59:38.639825: val_loss -0.8778\n",
      "2025-12-08 13:59:38.651332: Pseudo dice [0.9268, 0.9564, 0.948]\n",
      "2025-12-08 13:59:38.657338: Epoch time: 138.09 s\n",
      "2025-12-08 13:59:39.311259: \n",
      "2025-12-08 13:59:39.311259: Epoch 672\n",
      "2025-12-08 13:59:39.311259: Current learning rate: 0.00367\n",
      "2025-12-08 14:01:57.503399: train_loss -0.8495\n",
      "2025-12-08 14:01:57.505403: val_loss -0.8764\n",
      "2025-12-08 14:01:57.515160: Pseudo dice [0.9246, 0.9556, 0.9492]\n",
      "2025-12-08 14:01:57.521169: Epoch time: 138.19 s\n",
      "2025-12-08 14:01:58.226948: \n",
      "2025-12-08 14:01:58.226948: Epoch 673\n",
      "2025-12-08 14:01:58.232206: Current learning rate: 0.00366\n",
      "2025-12-08 14:04:16.421546: train_loss -0.8544\n",
      "2025-12-08 14:04:16.421546: val_loss -0.8823\n",
      "2025-12-08 14:04:16.428849: Pseudo dice [0.9272, 0.9575, 0.9527]\n",
      "2025-12-08 14:04:16.433204: Epoch time: 138.2 s\n",
      "2025-12-08 14:04:16.437432: Yayy! New best EMA pseudo Dice: 0.9433\n",
      "2025-12-08 14:04:17.368157: \n",
      "2025-12-08 14:04:17.369160: Epoch 674\n",
      "2025-12-08 14:04:17.373160: Current learning rate: 0.00365\n",
      "2025-12-08 14:06:35.379320: train_loss -0.8573\n",
      "2025-12-08 14:06:35.380321: val_loss -0.8849\n",
      "2025-12-08 14:06:35.385486: Pseudo dice [0.9324, 0.9622, 0.9481]\n",
      "2025-12-08 14:06:35.389491: Epoch time: 138.01 s\n",
      "2025-12-08 14:06:35.389491: Yayy! New best EMA pseudo Dice: 0.9438\n",
      "2025-12-08 14:06:36.522148: \n",
      "2025-12-08 14:06:36.522148: Epoch 675\n",
      "2025-12-08 14:06:36.527151: Current learning rate: 0.00364\n",
      "2025-12-08 14:08:54.505797: train_loss -0.854\n",
      "2025-12-08 14:08:54.505797: val_loss -0.8808\n",
      "2025-12-08 14:08:54.513805: Pseudo dice [0.9264, 0.9564, 0.9494]\n",
      "2025-12-08 14:08:54.519752: Epoch time: 137.98 s\n",
      "2025-12-08 14:08:54.523753: Yayy! New best EMA pseudo Dice: 0.9438\n",
      "2025-12-08 14:08:55.623903: \n",
      "2025-12-08 14:08:55.623903: Epoch 676\n",
      "2025-12-08 14:08:55.641427: Current learning rate: 0.00363\n",
      "2025-12-08 14:11:13.809203: train_loss -0.8492\n",
      "2025-12-08 14:11:13.809203: val_loss -0.8823\n",
      "2025-12-08 14:11:13.816720: Pseudo dice [0.929, 0.9605, 0.9456]\n",
      "2025-12-08 14:11:13.820724: Epoch time: 138.19 s\n",
      "2025-12-08 14:11:13.826730: Yayy! New best EMA pseudo Dice: 0.9439\n",
      "2025-12-08 14:11:14.767010: \n",
      "2025-12-08 14:11:14.767010: Epoch 677\n",
      "2025-12-08 14:11:14.767010: Current learning rate: 0.00362\n",
      "2025-12-08 14:13:32.906380: train_loss -0.8514\n",
      "2025-12-08 14:13:32.906380: val_loss -0.871\n",
      "2025-12-08 14:13:32.906380: Pseudo dice [0.9194, 0.9498, 0.9495]\n",
      "2025-12-08 14:13:32.924267: Epoch time: 138.14 s\n",
      "2025-12-08 14:13:33.577213: \n",
      "2025-12-08 14:13:33.577213: Epoch 678\n",
      "2025-12-08 14:13:33.577213: Current learning rate: 0.00361\n",
      "2025-12-08 14:15:51.697707: train_loss -0.8552\n",
      "2025-12-08 14:15:51.697707: val_loss -0.8754\n",
      "2025-12-08 14:15:51.704954: Pseudo dice [0.9245, 0.9542, 0.9522]\n",
      "2025-12-08 14:15:51.706955: Epoch time: 138.12 s\n",
      "2025-12-08 14:15:52.390827: \n",
      "2025-12-08 14:15:52.390827: Epoch 679\n",
      "2025-12-08 14:15:52.409650: Current learning rate: 0.0036\n",
      "2025-12-08 14:18:10.436878: train_loss -0.8514\n",
      "2025-12-08 14:18:10.436878: val_loss -0.8764\n",
      "2025-12-08 14:18:10.455440: Pseudo dice [0.9282, 0.9522, 0.94]\n",
      "2025-12-08 14:18:10.461185: Epoch time: 138.05 s\n",
      "2025-12-08 14:18:11.107991: \n",
      "2025-12-08 14:18:11.109029: Epoch 680\n",
      "2025-12-08 14:18:11.113734: Current learning rate: 0.00359\n",
      "2025-12-08 14:20:29.183959: train_loss -0.8499\n",
      "2025-12-08 14:20:29.183959: val_loss -0.8855\n",
      "2025-12-08 14:20:29.186929: Pseudo dice [0.931, 0.9633, 0.9524]\n",
      "2025-12-08 14:20:29.194530: Epoch time: 138.08 s\n",
      "2025-12-08 14:20:29.876988: \n",
      "2025-12-08 14:20:29.876988: Epoch 681\n",
      "2025-12-08 14:20:29.882746: Current learning rate: 0.00358\n",
      "2025-12-08 14:22:47.878353: train_loss -0.855\n",
      "2025-12-08 14:22:47.880356: val_loss -0.8866\n",
      "2025-12-08 14:22:47.886361: Pseudo dice [0.9307, 0.9586, 0.9464]\n",
      "2025-12-08 14:22:47.893872: Epoch time: 138.0 s\n",
      "2025-12-08 14:22:48.545602: \n",
      "2025-12-08 14:22:48.545602: Epoch 682\n",
      "2025-12-08 14:22:48.545602: Current learning rate: 0.00357\n",
      "2025-12-08 14:25:06.606699: train_loss -0.8555\n",
      "2025-12-08 14:25:06.608439: val_loss -0.8752\n",
      "2025-12-08 14:25:06.613319: Pseudo dice [0.9195, 0.9583, 0.952]\n",
      "2025-12-08 14:25:06.617322: Epoch time: 138.06 s\n",
      "2025-12-08 14:25:07.312084: \n",
      "2025-12-08 14:25:07.312084: Epoch 683\n",
      "2025-12-08 14:25:07.312084: Current learning rate: 0.00356\n",
      "2025-12-08 14:27:25.361518: train_loss -0.8478\n",
      "2025-12-08 14:27:25.363520: val_loss -0.8766\n",
      "2025-12-08 14:27:25.369264: Pseudo dice [0.9272, 0.9574, 0.9429]\n",
      "2025-12-08 14:27:25.373268: Epoch time: 138.05 s\n",
      "2025-12-08 14:27:26.014367: \n",
      "2025-12-08 14:27:26.014367: Epoch 684\n",
      "2025-12-08 14:27:26.030026: Current learning rate: 0.00355\n",
      "2025-12-08 14:29:45.748864: train_loss -0.8596\n",
      "2025-12-08 14:29:45.748864: val_loss -0.8849\n",
      "2025-12-08 14:29:45.756612: Pseudo dice [0.9335, 0.963, 0.9442]\n",
      "2025-12-08 14:29:45.764621: Epoch time: 139.73 s\n",
      "2025-12-08 14:29:45.770629: Yayy! New best EMA pseudo Dice: 0.944\n",
      "2025-12-08 14:29:46.799966: \n",
      "2025-12-08 14:29:46.799966: Epoch 685\n",
      "2025-12-08 14:29:46.811840: Current learning rate: 0.00354\n",
      "2025-12-08 14:32:05.424462: train_loss -0.8565\n",
      "2025-12-08 14:32:05.426464: val_loss -0.8789\n",
      "2025-12-08 14:32:05.430469: Pseudo dice [0.9272, 0.9539, 0.949]\n",
      "2025-12-08 14:32:05.438172: Epoch time: 138.62 s\n",
      "2025-12-08 14:32:06.265668: \n",
      "2025-12-08 14:32:06.265668: Epoch 686\n",
      "2025-12-08 14:32:06.272129: Current learning rate: 0.00353\n",
      "2025-12-08 14:34:24.760601: train_loss -0.8516\n",
      "2025-12-08 14:34:24.760601: val_loss -0.8749\n",
      "2025-12-08 14:34:24.768873: Pseudo dice [0.9254, 0.9566, 0.9446]\n",
      "2025-12-08 14:34:24.774616: Epoch time: 138.49 s\n",
      "2025-12-08 14:34:25.433776: \n",
      "2025-12-08 14:34:25.434779: Epoch 687\n",
      "2025-12-08 14:34:25.439952: Current learning rate: 0.00352\n",
      "2025-12-08 14:36:44.061196: train_loss -0.8514\n",
      "2025-12-08 14:36:44.061196: val_loss -0.8857\n",
      "2025-12-08 14:36:44.077057: Pseudo dice [0.9297, 0.961, 0.9521]\n",
      "2025-12-08 14:36:44.077057: Epoch time: 138.63 s\n",
      "2025-12-08 14:36:44.077057: Yayy! New best EMA pseudo Dice: 0.9442\n",
      "2025-12-08 14:36:45.045638: \n",
      "2025-12-08 14:36:45.045638: Epoch 688\n",
      "2025-12-08 14:36:45.061318: Current learning rate: 0.00351\n",
      "2025-12-08 14:39:03.537052: train_loss -0.8495\n",
      "2025-12-08 14:39:03.543052: val_loss -0.8803\n",
      "2025-12-08 14:39:03.548106: Pseudo dice [0.928, 0.9555, 0.9497]\n",
      "2025-12-08 14:39:03.553108: Epoch time: 138.49 s\n",
      "2025-12-08 14:39:03.557119: Yayy! New best EMA pseudo Dice: 0.9442\n",
      "2025-12-08 14:39:04.748383: \n",
      "2025-12-08 14:39:04.748383: Epoch 689\n",
      "2025-12-08 14:39:04.748383: Current learning rate: 0.0035\n",
      "2025-12-08 14:41:23.075969: train_loss -0.8569\n",
      "2025-12-08 14:41:23.078872: val_loss -0.884\n",
      "2025-12-08 14:41:23.087086: Pseudo dice [0.9301, 0.9611, 0.9493]\n",
      "2025-12-08 14:41:23.095119: Epoch time: 138.33 s\n",
      "2025-12-08 14:41:23.100863: Yayy! New best EMA pseudo Dice: 0.9444\n",
      "2025-12-08 14:41:24.030869: \n",
      "2025-12-08 14:41:24.030869: Epoch 690\n",
      "2025-12-08 14:41:24.046531: Current learning rate: 0.00349\n",
      "2025-12-08 14:43:42.276862: train_loss -0.8538\n",
      "2025-12-08 14:43:42.278863: val_loss -0.8714\n",
      "2025-12-08 14:43:42.280604: Pseudo dice [0.9199, 0.9554, 0.9489]\n",
      "2025-12-08 14:43:42.280604: Epoch time: 138.25 s\n",
      "2025-12-08 14:43:43.157835: \n",
      "2025-12-08 14:43:43.157835: Epoch 691\n",
      "2025-12-08 14:43:43.157835: Current learning rate: 0.00348\n",
      "2025-12-08 14:46:01.244288: train_loss -0.8508\n",
      "2025-12-08 14:46:01.246290: val_loss -0.8931\n",
      "2025-12-08 14:46:01.251446: Pseudo dice [0.9371, 0.9627, 0.9511]\n",
      "2025-12-08 14:46:01.256457: Epoch time: 138.09 s\n",
      "2025-12-08 14:46:01.260458: Yayy! New best EMA pseudo Dice: 0.9448\n",
      "2025-12-08 14:46:02.187280: \n",
      "2025-12-08 14:46:02.187280: Epoch 692\n",
      "2025-12-08 14:46:02.187280: Current learning rate: 0.00346\n",
      "2025-12-08 14:48:20.374037: train_loss -0.8538\n",
      "2025-12-08 14:48:20.374037: val_loss -0.8776\n",
      "2025-12-08 14:48:20.374037: Pseudo dice [0.9246, 0.9564, 0.9469]\n",
      "2025-12-08 14:48:20.389688: Epoch time: 138.19 s\n",
      "2025-12-08 14:48:21.033115: \n",
      "2025-12-08 14:48:21.034119: Epoch 693\n",
      "2025-12-08 14:48:21.034119: Current learning rate: 0.00345\n",
      "2025-12-08 14:50:39.214491: train_loss -0.8524\n",
      "2025-12-08 14:50:39.216493: val_loss -0.8894\n",
      "2025-12-08 14:50:39.222499: Pseudo dice [0.9321, 0.9608, 0.9519]\n",
      "2025-12-08 14:50:39.226503: Epoch time: 138.18 s\n",
      "2025-12-08 14:50:39.230507: Yayy! New best EMA pseudo Dice: 0.9449\n",
      "2025-12-08 14:50:40.170980: \n",
      "2025-12-08 14:50:40.170980: Epoch 694\n",
      "2025-12-08 14:50:40.186890: Current learning rate: 0.00344\n",
      "2025-12-08 14:52:58.237664: train_loss -0.8514\n",
      "2025-12-08 14:52:58.237664: val_loss -0.8758\n",
      "2025-12-08 14:52:58.237664: Pseudo dice [0.9237, 0.9585, 0.9446]\n",
      "2025-12-08 14:52:58.249638: Epoch time: 138.07 s\n",
      "2025-12-08 14:52:58.889695: \n",
      "2025-12-08 14:52:58.889695: Epoch 695\n",
      "2025-12-08 14:52:58.905593: Current learning rate: 0.00343\n",
      "2025-12-08 14:55:16.933279: train_loss -0.8574\n",
      "2025-12-08 14:55:16.933279: val_loss -0.8717\n",
      "2025-12-08 14:55:16.939348: Pseudo dice [0.9204, 0.9502, 0.9472]\n",
      "2025-12-08 14:55:16.944362: Epoch time: 138.04 s\n",
      "2025-12-08 14:55:17.592209: \n",
      "2025-12-08 14:55:17.592209: Epoch 696\n",
      "2025-12-08 14:55:17.592209: Current learning rate: 0.00342\n",
      "2025-12-08 14:57:35.687188: train_loss -0.853\n",
      "2025-12-08 14:57:35.687188: val_loss -0.8822\n",
      "2025-12-08 14:57:35.703288: Pseudo dice [0.9273, 0.9569, 0.9478]\n",
      "2025-12-08 14:57:35.703288: Epoch time: 138.09 s\n",
      "2025-12-08 14:57:36.530059: \n",
      "2025-12-08 14:57:36.530059: Epoch 697\n",
      "2025-12-08 14:57:36.530059: Current learning rate: 0.00341\n",
      "2025-12-08 14:59:54.594146: train_loss -0.8521\n",
      "2025-12-08 14:59:54.596151: val_loss -0.8788\n",
      "2025-12-08 14:59:54.604166: Pseudo dice [0.9252, 0.9563, 0.9469]\n",
      "2025-12-08 14:59:54.609910: Epoch time: 138.06 s\n",
      "2025-12-08 14:59:55.265494: \n",
      "2025-12-08 14:59:55.265494: Epoch 698\n",
      "2025-12-08 14:59:55.265494: Current learning rate: 0.0034\n",
      "2025-12-08 15:02:13.295466: train_loss -0.8512\n",
      "2025-12-08 15:02:13.295466: val_loss -0.8742\n",
      "2025-12-08 15:02:13.298509: Pseudo dice [0.9231, 0.9529, 0.9486]\n",
      "2025-12-08 15:02:13.308017: Epoch time: 138.03 s\n",
      "2025-12-08 15:02:13.984174: \n",
      "2025-12-08 15:02:13.984174: Epoch 699\n",
      "2025-12-08 15:02:13.984174: Current learning rate: 0.00339\n",
      "2025-12-08 15:04:32.133641: train_loss -0.8561\n",
      "2025-12-08 15:04:32.134644: val_loss -0.8834\n",
      "2025-12-08 15:04:32.139721: Pseudo dice [0.9295, 0.958, 0.9492]\n",
      "2025-12-08 15:04:32.144183: Epoch time: 138.15 s\n",
      "2025-12-08 15:04:33.060843: \n",
      "2025-12-08 15:04:33.060843: Epoch 700\n",
      "2025-12-08 15:04:33.060843: Current learning rate: 0.00338\n",
      "2025-12-08 15:06:51.173586: train_loss -0.8585\n",
      "2025-12-08 15:06:51.174588: val_loss -0.8767\n",
      "2025-12-08 15:06:51.182713: Pseudo dice [0.925, 0.9527, 0.9491]\n",
      "2025-12-08 15:06:51.186730: Epoch time: 138.11 s\n",
      "2025-12-08 15:06:51.842302: \n",
      "2025-12-08 15:06:51.842302: Epoch 701\n",
      "2025-12-08 15:06:51.858160: Current learning rate: 0.00337\n",
      "2025-12-08 15:09:10.185847: train_loss -0.8553\n",
      "2025-12-08 15:09:10.187885: val_loss -0.8771\n",
      "2025-12-08 15:09:10.195895: Pseudo dice [0.9245, 0.9546, 0.9494]\n",
      "2025-12-08 15:09:10.201640: Epoch time: 138.34 s\n",
      "2025-12-08 15:09:10.862355: \n",
      "2025-12-08 15:09:10.862355: Epoch 702\n",
      "2025-12-08 15:09:10.868383: Current learning rate: 0.00336\n",
      "2025-12-08 15:11:28.861055: train_loss -0.8542\n",
      "2025-12-08 15:11:28.861055: val_loss -0.8747\n",
      "2025-12-08 15:11:28.869066: Pseudo dice [0.9218, 0.955, 0.9458]\n",
      "2025-12-08 15:11:28.876076: Epoch time: 138.0 s\n",
      "2025-12-08 15:11:29.764257: \n",
      "2025-12-08 15:11:29.764257: Epoch 703\n",
      "2025-12-08 15:11:29.764257: Current learning rate: 0.00335\n",
      "2025-12-08 15:13:47.735636: train_loss -0.8498\n",
      "2025-12-08 15:13:47.737639: val_loss -0.8784\n",
      "2025-12-08 15:13:47.739642: Pseudo dice [0.9266, 0.9552, 0.9504]\n",
      "2025-12-08 15:13:47.749339: Epoch time: 137.97 s\n",
      "2025-12-08 15:13:48.394916: \n",
      "2025-12-08 15:13:48.394916: Epoch 704\n",
      "2025-12-08 15:13:48.405310: Current learning rate: 0.00334\n",
      "2025-12-08 15:16:06.511687: train_loss -0.852\n",
      "2025-12-08 15:16:06.512689: val_loss -0.8772\n",
      "2025-12-08 15:16:06.518733: Pseudo dice [0.9224, 0.9592, 0.9519]\n",
      "2025-12-08 15:16:06.523748: Epoch time: 138.12 s\n",
      "2025-12-08 15:16:07.171134: \n",
      "2025-12-08 15:16:07.171134: Epoch 705\n",
      "2025-12-08 15:16:07.187208: Current learning rate: 0.00333\n",
      "2025-12-08 15:18:25.311920: train_loss -0.8554\n",
      "2025-12-08 15:18:25.311920: val_loss -0.8725\n",
      "2025-12-08 15:18:25.311920: Pseudo dice [0.9208, 0.9474, 0.9516]\n",
      "2025-12-08 15:18:25.311920: Epoch time: 138.14 s\n",
      "2025-12-08 15:18:26.077391: \n",
      "2025-12-08 15:18:26.093311: Epoch 706\n",
      "2025-12-08 15:18:26.096830: Current learning rate: 0.00332\n",
      "2025-12-08 15:20:43.994702: train_loss -0.8558\n",
      "2025-12-08 15:20:43.994702: val_loss -0.8742\n",
      "2025-12-08 15:20:43.998703: Pseudo dice [0.923, 0.9553, 0.943]\n",
      "2025-12-08 15:20:44.007097: Epoch time: 137.92 s\n",
      "2025-12-08 15:20:44.654953: \n",
      "2025-12-08 15:20:44.654953: Epoch 707\n",
      "2025-12-08 15:20:44.670748: Current learning rate: 0.00331\n",
      "2025-12-08 15:23:02.835400: train_loss -0.8521\n",
      "2025-12-08 15:23:02.836401: val_loss -0.8735\n",
      "2025-12-08 15:23:02.842675: Pseudo dice [0.9225, 0.956, 0.9477]\n",
      "2025-12-08 15:23:02.847931: Epoch time: 138.18 s\n",
      "2025-12-08 15:23:03.499043: \n",
      "2025-12-08 15:23:03.499043: Epoch 708\n",
      "2025-12-08 15:23:03.499043: Current learning rate: 0.0033\n",
      "2025-12-08 15:25:21.555880: train_loss -0.8567\n",
      "2025-12-08 15:25:21.555880: val_loss -0.8848\n",
      "2025-12-08 15:25:21.561628: Pseudo dice [0.9306, 0.9597, 0.9505]\n",
      "2025-12-08 15:25:21.566023: Epoch time: 138.06 s\n",
      "2025-12-08 15:25:22.484155: \n",
      "2025-12-08 15:25:22.484155: Epoch 709\n",
      "2025-12-08 15:25:22.504245: Current learning rate: 0.00329\n",
      "2025-12-08 15:27:40.560529: train_loss -0.8537\n",
      "2025-12-08 15:27:40.560529: val_loss -0.884\n",
      "2025-12-08 15:27:40.565109: Pseudo dice [0.9298, 0.9567, 0.9552]\n",
      "2025-12-08 15:27:40.570751: Epoch time: 138.08 s\n",
      "2025-12-08 15:27:41.220109: \n",
      "2025-12-08 15:27:41.234573: Epoch 710\n",
      "2025-12-08 15:27:41.234573: Current learning rate: 0.00328\n",
      "2025-12-08 15:29:59.371914: train_loss -0.8539\n",
      "2025-12-08 15:29:59.372916: val_loss -0.882\n",
      "2025-12-08 15:29:59.373924: Pseudo dice [0.9301, 0.9606, 0.9455]\n",
      "2025-12-08 15:29:59.373924: Epoch time: 138.15 s\n",
      "2025-12-08 15:30:00.030276: \n",
      "2025-12-08 15:30:00.030276: Epoch 711\n",
      "2025-12-08 15:30:00.030276: Current learning rate: 0.00327\n",
      "2025-12-08 15:32:18.297550: train_loss -0.8539\n",
      "2025-12-08 15:32:18.297550: val_loss -0.8782\n",
      "2025-12-08 15:32:18.297550: Pseudo dice [0.9284, 0.9569, 0.9488]\n",
      "2025-12-08 15:32:18.317600: Epoch time: 138.27 s\n",
      "2025-12-08 15:32:19.092685: \n",
      "2025-12-08 15:32:19.092685: Epoch 712\n",
      "2025-12-08 15:32:19.108673: Current learning rate: 0.00326\n",
      "2025-12-08 15:34:37.249512: train_loss -0.8508\n",
      "2025-12-08 15:34:37.249512: val_loss -0.8741\n",
      "2025-12-08 15:34:37.249512: Pseudo dice [0.9206, 0.9496, 0.9506]\n",
      "2025-12-08 15:34:37.263076: Epoch time: 138.16 s\n",
      "2025-12-08 15:34:37.906699: \n",
      "2025-12-08 15:34:37.906699: Epoch 713\n",
      "2025-12-08 15:34:37.906699: Current learning rate: 0.00325\n",
      "2025-12-08 15:36:55.968328: train_loss -0.8538\n",
      "2025-12-08 15:36:55.968328: val_loss -0.8734\n",
      "2025-12-08 15:36:55.983577: Pseudo dice [0.9196, 0.9496, 0.9496]\n",
      "2025-12-08 15:36:55.987581: Epoch time: 138.06 s\n",
      "2025-12-08 15:36:56.642704: \n",
      "2025-12-08 15:36:56.642704: Epoch 714\n",
      "2025-12-08 15:36:56.642704: Current learning rate: 0.00324\n",
      "2025-12-08 15:39:14.765977: train_loss -0.8568\n",
      "2025-12-08 15:39:14.765977: val_loss -0.8758\n",
      "2025-12-08 15:39:14.772019: Pseudo dice [0.9245, 0.9564, 0.9502]\n",
      "2025-12-08 15:39:14.777043: Epoch time: 138.12 s\n",
      "2025-12-08 15:39:15.763789: \n",
      "2025-12-08 15:39:15.763789: Epoch 715\n",
      "2025-12-08 15:39:15.770974: Current learning rate: 0.00323\n",
      "2025-12-08 15:41:34.061836: train_loss -0.8515\n",
      "2025-12-08 15:41:34.061836: val_loss -0.8789\n",
      "2025-12-08 15:41:34.077581: Pseudo dice [0.9279, 0.9584, 0.9528]\n",
      "2025-12-08 15:41:34.077581: Epoch time: 138.3 s\n",
      "2025-12-08 15:41:34.733670: \n",
      "2025-12-08 15:41:34.733670: Epoch 716\n",
      "2025-12-08 15:41:34.733670: Current learning rate: 0.00322\n",
      "2025-12-08 15:43:52.670922: train_loss -0.859\n",
      "2025-12-08 15:43:52.670922: val_loss -0.8717\n",
      "2025-12-08 15:43:52.686578: Pseudo dice [0.92, 0.9524, 0.9521]\n",
      "2025-12-08 15:43:52.686578: Epoch time: 137.95 s\n",
      "2025-12-08 15:43:53.326801: \n",
      "2025-12-08 15:43:53.326801: Epoch 717\n",
      "2025-12-08 15:43:53.342566: Current learning rate: 0.00321\n",
      "2025-12-08 15:46:11.600828: train_loss -0.857\n",
      "2025-12-08 15:46:11.600828: val_loss -0.8657\n",
      "2025-12-08 15:46:11.604832: Pseudo dice [0.9172, 0.9487, 0.9437]\n",
      "2025-12-08 15:46:11.610838: Epoch time: 138.27 s\n",
      "2025-12-08 15:46:12.358357: \n",
      "2025-12-08 15:46:12.358357: Epoch 718\n",
      "2025-12-08 15:46:12.374274: Current learning rate: 0.0032\n",
      "2025-12-08 15:48:30.355407: train_loss -0.8506\n",
      "2025-12-08 15:48:30.357409: val_loss -0.8806\n",
      "2025-12-08 15:48:30.363153: Pseudo dice [0.9254, 0.957, 0.9509]\n",
      "2025-12-08 15:48:30.369159: Epoch time: 138.0 s\n",
      "2025-12-08 15:48:31.030609: \n",
      "2025-12-08 15:48:31.030609: Epoch 719\n",
      "2025-12-08 15:48:31.030609: Current learning rate: 0.00319\n",
      "2025-12-08 15:50:49.069439: train_loss -0.8579\n",
      "2025-12-08 15:50:49.070441: val_loss -0.878\n",
      "2025-12-08 15:50:49.075457: Pseudo dice [0.9242, 0.9577, 0.9509]\n",
      "2025-12-08 15:50:49.076459: Epoch time: 138.04 s\n",
      "2025-12-08 15:50:49.731134: \n",
      "2025-12-08 15:50:49.731134: Epoch 720\n",
      "2025-12-08 15:50:49.737223: Current learning rate: 0.00318\n",
      "2025-12-08 15:53:07.853988: train_loss -0.8553\n",
      "2025-12-08 15:53:07.853988: val_loss -0.8811\n",
      "2025-12-08 15:53:07.858010: Pseudo dice [0.9274, 0.9573, 0.949]\n",
      "2025-12-08 15:53:07.858010: Epoch time: 138.12 s\n",
      "2025-12-08 15:53:08.842216: \n",
      "2025-12-08 15:53:08.842216: Epoch 721\n",
      "2025-12-08 15:53:08.842216: Current learning rate: 0.00317\n",
      "2025-12-08 15:55:26.796894: train_loss -0.8569\n",
      "2025-12-08 15:55:26.796894: val_loss -0.8779\n",
      "2025-12-08 15:55:26.812653: Pseudo dice [0.9265, 0.9556, 0.9524]\n",
      "2025-12-08 15:55:26.812653: Epoch time: 137.96 s\n",
      "2025-12-08 15:55:27.468003: \n",
      "2025-12-08 15:55:27.468003: Epoch 722\n",
      "2025-12-08 15:55:27.468003: Current learning rate: 0.00316\n",
      "2025-12-08 15:57:45.437224: train_loss -0.8562\n",
      "2025-12-08 15:57:45.437224: val_loss -0.8776\n",
      "2025-12-08 15:57:45.437224: Pseudo dice [0.9257, 0.9533, 0.9447]\n",
      "2025-12-08 15:57:45.437224: Epoch time: 137.97 s\n",
      "2025-12-08 15:57:46.092521: \n",
      "2025-12-08 15:57:46.092521: Epoch 723\n",
      "2025-12-08 15:57:46.092521: Current learning rate: 0.00315\n",
      "2025-12-08 16:00:04.237739: train_loss -0.8545\n",
      "2025-12-08 16:00:04.239741: val_loss -0.8695\n",
      "2025-12-08 16:00:04.249504: Pseudo dice [0.9209, 0.9524, 0.9505]\n",
      "2025-12-08 16:00:04.252740: Epoch time: 138.15 s\n",
      "2025-12-08 16:00:05.014384: \n",
      "2025-12-08 16:00:05.016387: Epoch 724\n",
      "2025-12-08 16:00:05.018391: Current learning rate: 0.00314\n",
      "2025-12-08 16:02:23.216543: train_loss -0.8496\n",
      "2025-12-08 16:02:23.216543: val_loss -0.8692\n",
      "2025-12-08 16:02:23.222549: Pseudo dice [0.9173, 0.9505, 0.9507]\n",
      "2025-12-08 16:02:23.228292: Epoch time: 138.2 s\n",
      "2025-12-08 16:02:23.873899: \n",
      "2025-12-08 16:02:23.873899: Epoch 725\n",
      "2025-12-08 16:02:23.889968: Current learning rate: 0.00313\n",
      "2025-12-08 16:04:41.911976: train_loss -0.8565\n",
      "2025-12-08 16:04:41.911976: val_loss -0.8796\n",
      "2025-12-08 16:04:41.921727: Pseudo dice [0.9274, 0.9563, 0.9506]\n",
      "2025-12-08 16:04:41.927736: Epoch time: 138.04 s\n",
      "2025-12-08 16:04:42.577304: \n",
      "2025-12-08 16:04:42.577304: Epoch 726\n",
      "2025-12-08 16:04:42.593094: Current learning rate: 0.00312\n",
      "2025-12-08 16:07:00.827195: train_loss -0.8585\n",
      "2025-12-08 16:07:00.827195: val_loss -0.8738\n",
      "2025-12-08 16:07:00.834531: Pseudo dice [0.921, 0.9533, 0.9515]\n",
      "2025-12-08 16:07:00.840537: Epoch time: 138.25 s\n",
      "2025-12-08 16:07:01.783680: \n",
      "2025-12-08 16:07:01.784683: Epoch 727\n",
      "2025-12-08 16:07:01.790703: Current learning rate: 0.00311\n",
      "2025-12-08 16:09:19.836418: train_loss -0.8551\n",
      "2025-12-08 16:09:19.836418: val_loss -0.8833\n",
      "2025-12-08 16:09:19.842504: Pseudo dice [0.9295, 0.9613, 0.9428]\n",
      "2025-12-08 16:09:19.848091: Epoch time: 138.05 s\n",
      "2025-12-08 16:09:20.548191: \n",
      "2025-12-08 16:09:20.548191: Epoch 728\n",
      "2025-12-08 16:09:20.553205: Current learning rate: 0.0031\n",
      "2025-12-08 16:11:38.705555: train_loss -0.8547\n",
      "2025-12-08 16:11:38.707558: val_loss -0.8867\n",
      "2025-12-08 16:11:38.713565: Pseudo dice [0.933, 0.9584, 0.9507]\n",
      "2025-12-08 16:11:38.717569: Epoch time: 138.17 s\n",
      "2025-12-08 16:11:39.374153: \n",
      "2025-12-08 16:11:39.374153: Epoch 729\n",
      "2025-12-08 16:11:39.374153: Current learning rate: 0.00309\n",
      "2025-12-08 16:13:57.492908: train_loss -0.8544\n",
      "2025-12-08 16:13:57.493910: val_loss -0.8826\n",
      "2025-12-08 16:13:57.498925: Pseudo dice [0.9316, 0.956, 0.9509]\n",
      "2025-12-08 16:13:57.505085: Epoch time: 138.12 s\n",
      "2025-12-08 16:13:58.280923: \n",
      "2025-12-08 16:13:58.280923: Epoch 730\n",
      "2025-12-08 16:13:58.296801: Current learning rate: 0.00308\n",
      "2025-12-08 16:16:16.356356: train_loss -0.8573\n",
      "2025-12-08 16:16:16.356356: val_loss -0.8693\n",
      "2025-12-08 16:16:16.358096: Pseudo dice [0.9209, 0.9515, 0.941]\n",
      "2025-12-08 16:16:16.358096: Epoch time: 138.08 s\n",
      "2025-12-08 16:16:17.030430: \n",
      "2025-12-08 16:16:17.030430: Epoch 731\n",
      "2025-12-08 16:16:17.030430: Current learning rate: 0.00307\n",
      "2025-12-08 16:18:35.155793: train_loss -0.8526\n",
      "2025-12-08 16:18:35.155793: val_loss -0.8743\n",
      "2025-12-08 16:18:35.155793: Pseudo dice [0.9222, 0.9545, 0.9486]\n",
      "2025-12-08 16:18:35.155793: Epoch time: 138.14 s\n",
      "2025-12-08 16:18:35.843461: \n",
      "2025-12-08 16:18:35.843461: Epoch 732\n",
      "2025-12-08 16:18:35.858978: Current learning rate: 0.00306\n",
      "2025-12-08 16:20:54.015244: train_loss -0.854\n",
      "2025-12-08 16:20:54.015244: val_loss -0.8866\n",
      "2025-12-08 16:20:54.035081: Pseudo dice [0.9321, 0.9578, 0.9483]\n",
      "2025-12-08 16:20:54.039448: Epoch time: 138.17 s\n",
      "2025-12-08 16:20:54.686515: \n",
      "2025-12-08 16:20:54.686515: Epoch 733\n",
      "2025-12-08 16:20:54.694112: Current learning rate: 0.00305\n",
      "2025-12-08 16:23:12.841616: train_loss -0.8571\n",
      "2025-12-08 16:23:12.842620: val_loss -0.8839\n",
      "2025-12-08 16:23:12.848621: Pseudo dice [0.9305, 0.9571, 0.9482]\n",
      "2025-12-08 16:23:12.852570: Epoch time: 138.16 s\n",
      "2025-12-08 16:23:13.540438: \n",
      "2025-12-08 16:23:13.540438: Epoch 734\n",
      "2025-12-08 16:23:13.549816: Current learning rate: 0.00304\n",
      "2025-12-08 16:25:31.499775: train_loss -0.8519\n",
      "2025-12-08 16:25:31.499775: val_loss -0.8679\n",
      "2025-12-08 16:25:31.507985: Pseudo dice [0.916, 0.9514, 0.9461]\n",
      "2025-12-08 16:25:31.513629: Epoch time: 137.96 s\n",
      "2025-12-08 16:25:32.156291: \n",
      "2025-12-08 16:25:32.156291: Epoch 735\n",
      "2025-12-08 16:25:32.162977: Current learning rate: 0.00303\n",
      "2025-12-08 16:27:50.219733: train_loss -0.8563\n",
      "2025-12-08 16:27:50.219733: val_loss -0.8811\n",
      "2025-12-08 16:27:50.230608: Pseudo dice [0.9235, 0.9591, 0.9477]\n",
      "2025-12-08 16:27:50.237345: Epoch time: 138.06 s\n",
      "2025-12-08 16:27:50.952289: \n",
      "2025-12-08 16:27:50.968227: Epoch 736\n",
      "2025-12-08 16:27:50.968227: Current learning rate: 0.00302\n",
      "2025-12-08 16:30:09.076736: train_loss -0.851\n",
      "2025-12-08 16:30:09.076736: val_loss -0.8865\n",
      "2025-12-08 16:30:09.086309: Pseudo dice [0.9326, 0.9608, 0.9481]\n",
      "2025-12-08 16:30:09.090828: Epoch time: 138.12 s\n",
      "2025-12-08 16:30:09.749058: \n",
      "2025-12-08 16:30:09.749058: Epoch 737\n",
      "2025-12-08 16:30:09.749058: Current learning rate: 0.00301\n",
      "2025-12-08 16:32:27.873856: train_loss -0.8532\n",
      "2025-12-08 16:32:27.875858: val_loss -0.8699\n",
      "2025-12-08 16:32:27.881864: Pseudo dice [0.9196, 0.9534, 0.9483]\n",
      "2025-12-08 16:32:27.885868: Epoch time: 138.12 s\n",
      "2025-12-08 16:32:28.551021: \n",
      "2025-12-08 16:32:28.551021: Epoch 738\n",
      "2025-12-08 16:32:28.557045: Current learning rate: 0.003\n",
      "2025-12-08 16:34:46.699082: train_loss -0.8559\n",
      "2025-12-08 16:34:46.699082: val_loss -0.8763\n",
      "2025-12-08 16:34:46.706011: Pseudo dice [0.9256, 0.9546, 0.9472]\n",
      "2025-12-08 16:34:46.706011: Epoch time: 138.15 s\n",
      "2025-12-08 16:34:47.515096: \n",
      "2025-12-08 16:34:47.515096: Epoch 739\n",
      "2025-12-08 16:34:47.530977: Current learning rate: 0.00299\n",
      "2025-12-08 16:37:06.613407: train_loss -0.8493\n",
      "2025-12-08 16:37:06.614411: val_loss -0.8827\n",
      "2025-12-08 16:37:06.619438: Pseudo dice [0.9308, 0.9597, 0.9492]\n",
      "2025-12-08 16:37:06.623824: Epoch time: 139.1 s\n",
      "2025-12-08 16:37:07.280059: \n",
      "2025-12-08 16:37:07.280059: Epoch 740\n",
      "2025-12-08 16:37:07.280059: Current learning rate: 0.00297\n",
      "2025-12-08 16:39:25.779861: train_loss -0.8539\n",
      "2025-12-08 16:39:25.779861: val_loss -0.886\n",
      "2025-12-08 16:39:25.779861: Pseudo dice [0.9303, 0.96, 0.9502]\n",
      "2025-12-08 16:39:25.779861: Epoch time: 138.5 s\n",
      "2025-12-08 16:39:26.464347: \n",
      "2025-12-08 16:39:26.465352: Epoch 741\n",
      "2025-12-08 16:39:26.471559: Current learning rate: 0.00296\n",
      "2025-12-08 16:41:45.323546: train_loss -0.8542\n",
      "2025-12-08 16:41:45.323546: val_loss -0.8789\n",
      "2025-12-08 16:41:45.331293: Pseudo dice [0.9276, 0.9578, 0.948]\n",
      "2025-12-08 16:41:45.335297: Epoch time: 138.86 s\n",
      "2025-12-08 16:41:46.171733: \n",
      "2025-12-08 16:41:46.171733: Epoch 742\n",
      "2025-12-08 16:41:46.186994: Current learning rate: 0.00295\n",
      "2025-12-08 16:44:04.825731: train_loss -0.8509\n",
      "2025-12-08 16:44:04.827472: val_loss -0.87\n",
      "2025-12-08 16:44:04.833478: Pseudo dice [0.9225, 0.9566, 0.9429]\n",
      "2025-12-08 16:44:04.839483: Epoch time: 138.65 s\n",
      "2025-12-08 16:44:05.499713: \n",
      "2025-12-08 16:44:05.499713: Epoch 743\n",
      "2025-12-08 16:44:05.499713: Current learning rate: 0.00294\n",
      "2025-12-08 16:46:23.992178: train_loss -0.8496\n",
      "2025-12-08 16:46:23.992178: val_loss -0.8859\n",
      "2025-12-08 16:46:23.998200: Pseudo dice [0.9317, 0.9574, 0.949]\n",
      "2025-12-08 16:46:24.003062: Epoch time: 138.49 s\n",
      "2025-12-08 16:46:24.655903: \n",
      "2025-12-08 16:46:24.655903: Epoch 744\n",
      "2025-12-08 16:46:24.655903: Current learning rate: 0.00293\n",
      "2025-12-08 16:48:42.784578: train_loss -0.8462\n",
      "2025-12-08 16:48:42.784578: val_loss -0.8757\n",
      "2025-12-08 16:48:42.792586: Pseudo dice [0.9253, 0.9542, 0.9501]\n",
      "2025-12-08 16:48:42.798912: Epoch time: 138.13 s\n",
      "2025-12-08 16:48:43.796178: \n",
      "2025-12-08 16:48:43.796178: Epoch 745\n",
      "2025-12-08 16:48:43.812204: Current learning rate: 0.00292\n",
      "2025-12-08 16:51:01.784065: train_loss -0.8556\n",
      "2025-12-08 16:51:01.784065: val_loss -0.8789\n",
      "2025-12-08 16:51:01.790068: Pseudo dice [0.9273, 0.9582, 0.9425]\n",
      "2025-12-08 16:51:01.794395: Epoch time: 137.99 s\n",
      "2025-12-08 16:51:02.467763: \n",
      "2025-12-08 16:51:02.467763: Epoch 746\n",
      "2025-12-08 16:51:02.467763: Current learning rate: 0.00291\n",
      "2025-12-08 16:53:20.542840: train_loss -0.8506\n",
      "2025-12-08 16:53:20.544844: val_loss -0.8683\n",
      "2025-12-08 16:53:20.551853: Pseudo dice [0.9177, 0.9542, 0.9482]\n",
      "2025-12-08 16:53:20.557859: Epoch time: 138.08 s\n",
      "2025-12-08 16:53:21.250190: \n",
      "2025-12-08 16:53:21.250190: Epoch 747\n",
      "2025-12-08 16:53:21.257366: Current learning rate: 0.0029\n",
      "2025-12-08 16:55:39.219191: train_loss -0.8484\n",
      "2025-12-08 16:55:39.219191: val_loss -0.8795\n",
      "2025-12-08 16:55:39.221194: Pseudo dice [0.9267, 0.9576, 0.9473]\n",
      "2025-12-08 16:55:39.229772: Epoch time: 137.98 s\n",
      "2025-12-08 16:55:39.983727: \n",
      "2025-12-08 16:55:39.983727: Epoch 748\n",
      "2025-12-08 16:55:39.994255: Current learning rate: 0.00289\n",
      "2025-12-08 16:57:58.207278: train_loss -0.8521\n",
      "2025-12-08 16:57:58.208280: val_loss -0.8824\n",
      "2025-12-08 16:57:58.213281: Pseudo dice [0.9296, 0.9584, 0.9465]\n",
      "2025-12-08 16:57:58.217702: Epoch time: 138.22 s\n",
      "2025-12-08 16:57:58.926049: \n",
      "2025-12-08 16:57:58.926049: Epoch 749\n",
      "2025-12-08 16:57:58.932053: Current learning rate: 0.00288\n",
      "2025-12-08 17:00:16.935865: train_loss -0.8499\n",
      "2025-12-08 17:00:16.935865: val_loss -0.8687\n",
      "2025-12-08 17:00:16.941366: Pseudo dice [0.9183, 0.9514, 0.9466]\n",
      "2025-12-08 17:00:16.945370: Epoch time: 138.01 s\n",
      "2025-12-08 17:00:17.858153: \n",
      "2025-12-08 17:00:17.858153: Epoch 750\n",
      "2025-12-08 17:00:17.873908: Current learning rate: 0.00287\n",
      "2025-12-08 17:02:36.045734: train_loss -0.8539\n",
      "2025-12-08 17:02:36.045734: val_loss -0.8804\n",
      "2025-12-08 17:02:36.045734: Pseudo dice [0.9276, 0.9529, 0.9543]\n",
      "2025-12-08 17:02:36.045734: Epoch time: 138.19 s\n",
      "2025-12-08 17:02:37.061607: \n",
      "2025-12-08 17:02:37.061607: Epoch 751\n",
      "2025-12-08 17:02:37.061607: Current learning rate: 0.00286\n",
      "2025-12-08 17:04:55.101326: train_loss -0.859\n",
      "2025-12-08 17:04:55.103330: val_loss -0.8748\n",
      "2025-12-08 17:04:55.112329: Pseudo dice [0.9246, 0.9551, 0.9455]\n",
      "2025-12-08 17:04:55.118330: Epoch time: 138.04 s\n",
      "2025-12-08 17:04:55.843242: \n",
      "2025-12-08 17:04:55.843242: Epoch 752\n",
      "2025-12-08 17:04:55.843242: Current learning rate: 0.00285\n",
      "2025-12-08 17:07:14.030074: train_loss -0.8571\n",
      "2025-12-08 17:07:14.031887: val_loss -0.8714\n",
      "2025-12-08 17:07:14.037893: Pseudo dice [0.9231, 0.9548, 0.9437]\n",
      "2025-12-08 17:07:14.043638: Epoch time: 138.19 s\n",
      "2025-12-08 17:07:14.697573: \n",
      "2025-12-08 17:07:14.698585: Epoch 753\n",
      "2025-12-08 17:07:14.703943: Current learning rate: 0.00284\n",
      "2025-12-08 17:09:32.779102: train_loss -0.8483\n",
      "2025-12-08 17:09:32.780103: val_loss -0.8731\n",
      "2025-12-08 17:09:32.788105: Pseudo dice [0.9219, 0.9529, 0.9531]\n",
      "2025-12-08 17:09:32.794106: Epoch time: 138.08 s\n",
      "2025-12-08 17:09:33.592262: \n",
      "2025-12-08 17:09:33.592262: Epoch 754\n",
      "2025-12-08 17:09:33.608342: Current learning rate: 0.00283\n",
      "2025-12-08 17:11:51.722028: train_loss -0.8516\n",
      "2025-12-08 17:11:51.724029: val_loss -0.8864\n",
      "2025-12-08 17:11:51.725770: Pseudo dice [0.9279, 0.9586, 0.9558]\n",
      "2025-12-08 17:11:51.733921: Epoch time: 138.13 s\n",
      "2025-12-08 17:11:52.436433: \n",
      "2025-12-08 17:11:52.436433: Epoch 755\n",
      "2025-12-08 17:11:52.452170: Current learning rate: 0.00282\n",
      "2025-12-08 17:14:10.565205: train_loss -0.8547\n",
      "2025-12-08 17:14:10.565205: val_loss -0.8768\n",
      "2025-12-08 17:14:10.572953: Pseudo dice [0.9228, 0.9575, 0.9461]\n",
      "2025-12-08 17:14:10.576957: Epoch time: 138.13 s\n",
      "2025-12-08 17:14:11.241823: \n",
      "2025-12-08 17:14:11.241823: Epoch 756\n",
      "2025-12-08 17:14:11.247529: Current learning rate: 0.00281\n",
      "2025-12-08 17:16:29.382347: train_loss -0.8536\n",
      "2025-12-08 17:16:29.389982: val_loss -0.869\n",
      "2025-12-08 17:16:29.393926: Pseudo dice [0.9213, 0.9525, 0.9442]\n",
      "2025-12-08 17:16:29.399932: Epoch time: 138.14 s\n",
      "2025-12-08 17:16:30.358339: \n",
      "2025-12-08 17:16:30.358339: Epoch 757\n",
      "2025-12-08 17:16:30.375442: Current learning rate: 0.0028\n",
      "2025-12-08 17:18:48.580821: train_loss -0.847\n",
      "2025-12-08 17:18:48.580821: val_loss -0.8746\n",
      "2025-12-08 17:18:48.589859: Pseudo dice [0.9246, 0.9586, 0.9421]\n",
      "2025-12-08 17:18:48.592870: Epoch time: 138.22 s\n",
      "2025-12-08 17:18:49.248597: \n",
      "2025-12-08 17:18:49.248597: Epoch 758\n",
      "2025-12-08 17:18:49.248597: Current learning rate: 0.00279\n",
      "2025-12-08 17:21:07.275146: train_loss -0.8554\n",
      "2025-12-08 17:21:07.275146: val_loss -0.8774\n",
      "2025-12-08 17:21:07.284902: Pseudo dice [0.9282, 0.9531, 0.9452]\n",
      "2025-12-08 17:21:07.290908: Epoch time: 138.03 s\n",
      "2025-12-08 17:21:07.948934: \n",
      "2025-12-08 17:21:07.948934: Epoch 759\n",
      "2025-12-08 17:21:07.954946: Current learning rate: 0.00278\n",
      "2025-12-08 17:23:26.061362: train_loss -0.8597\n",
      "2025-12-08 17:23:26.061362: val_loss -0.882\n",
      "2025-12-08 17:23:26.070410: Pseudo dice [0.9282, 0.96, 0.9506]\n",
      "2025-12-08 17:23:26.076131: Epoch time: 138.11 s\n",
      "2025-12-08 17:23:26.756088: \n",
      "2025-12-08 17:23:26.756088: Epoch 760\n",
      "2025-12-08 17:23:26.761101: Current learning rate: 0.00277\n",
      "2025-12-08 17:25:44.873870: train_loss -0.8518\n",
      "2025-12-08 17:25:44.874980: val_loss -0.8777\n",
      "2025-12-08 17:25:44.882015: Pseudo dice [0.9292, 0.9554, 0.9409]\n",
      "2025-12-08 17:25:44.888018: Epoch time: 138.12 s\n",
      "2025-12-08 17:25:45.531067: \n",
      "2025-12-08 17:25:45.531067: Epoch 761\n",
      "2025-12-08 17:25:45.543736: Current learning rate: 0.00276\n",
      "2025-12-08 17:28:03.593118: train_loss -0.8519\n",
      "2025-12-08 17:28:03.593118: val_loss -0.8843\n",
      "2025-12-08 17:28:03.608939: Pseudo dice [0.9289, 0.9555, 0.9504]\n",
      "2025-12-08 17:28:03.613745: Epoch time: 138.06 s\n",
      "2025-12-08 17:28:04.325301: \n",
      "2025-12-08 17:28:04.326306: Epoch 762\n",
      "2025-12-08 17:28:04.331353: Current learning rate: 0.00275\n",
      "2025-12-08 17:30:22.344583: train_loss -0.8541\n",
      "2025-12-08 17:30:22.344583: val_loss -0.8805\n",
      "2025-12-08 17:30:22.350589: Pseudo dice [0.9281, 0.9565, 0.9495]\n",
      "2025-12-08 17:30:22.356595: Epoch time: 138.02 s\n",
      "2025-12-08 17:30:23.187927: \n",
      "2025-12-08 17:30:23.187927: Epoch 763\n",
      "2025-12-08 17:30:23.187927: Current learning rate: 0.00274\n",
      "2025-12-08 17:32:41.344105: train_loss -0.8537\n",
      "2025-12-08 17:32:41.345115: val_loss -0.8706\n",
      "2025-12-08 17:32:41.351178: Pseudo dice [0.9182, 0.9544, 0.9444]\n",
      "2025-12-08 17:32:41.355402: Epoch time: 138.16 s\n",
      "2025-12-08 17:32:42.071291: \n",
      "2025-12-08 17:32:42.071291: Epoch 764\n",
      "2025-12-08 17:32:42.077787: Current learning rate: 0.00273\n",
      "2025-12-08 17:35:00.234174: train_loss -0.855\n",
      "2025-12-08 17:35:00.234174: val_loss -0.8775\n",
      "2025-12-08 17:35:00.234174: Pseudo dice [0.9237, 0.953, 0.9516]\n",
      "2025-12-08 17:35:00.249863: Epoch time: 138.16 s\n",
      "2025-12-08 17:35:00.910100: \n",
      "2025-12-08 17:35:00.910100: Epoch 765\n",
      "2025-12-08 17:35:00.916125: Current learning rate: 0.00272\n",
      "2025-12-08 17:37:19.046699: train_loss -0.8531\n",
      "2025-12-08 17:37:19.062497: val_loss -0.8738\n",
      "2025-12-08 17:37:19.062497: Pseudo dice [0.9253, 0.9538, 0.9493]\n",
      "2025-12-08 17:37:19.062497: Epoch time: 138.14 s\n",
      "2025-12-08 17:37:19.717976: \n",
      "2025-12-08 17:37:19.717976: Epoch 766\n",
      "2025-12-08 17:37:19.733917: Current learning rate: 0.00271\n",
      "2025-12-08 17:39:38.108955: train_loss -0.8552\n",
      "2025-12-08 17:39:38.124943: val_loss -0.8774\n",
      "2025-12-08 17:39:38.124943: Pseudo dice [0.9243, 0.9573, 0.9469]\n",
      "2025-12-08 17:39:38.124943: Epoch time: 138.39 s\n",
      "2025-12-08 17:39:38.780824: \n",
      "2025-12-08 17:39:38.780824: Epoch 767\n",
      "2025-12-08 17:39:38.780824: Current learning rate: 0.0027\n",
      "2025-12-08 17:41:57.091994: train_loss -0.8428\n",
      "2025-12-08 17:41:57.092994: val_loss -0.8786\n",
      "2025-12-08 17:41:57.094779: Pseudo dice [0.927, 0.9584, 0.9484]\n",
      "2025-12-08 17:41:57.102768: Epoch time: 138.31 s\n",
      "2025-12-08 17:41:57.780260: \n",
      "2025-12-08 17:41:57.780260: Epoch 768\n",
      "2025-12-08 17:41:57.784266: Current learning rate: 0.00268\n",
      "2025-12-08 17:44:15.907614: train_loss -0.8485\n",
      "2025-12-08 17:44:15.907614: val_loss -0.8759\n",
      "2025-12-08 17:44:15.907614: Pseudo dice [0.9246, 0.9563, 0.9478]\n",
      "2025-12-08 17:44:15.907614: Epoch time: 138.13 s\n",
      "2025-12-08 17:44:16.753181: \n",
      "2025-12-08 17:44:16.754184: Epoch 769\n",
      "2025-12-08 17:44:16.759426: Current learning rate: 0.00267\n",
      "2025-12-08 17:46:34.939950: train_loss -0.8519\n",
      "2025-12-08 17:46:34.940952: val_loss -0.8789\n",
      "2025-12-08 17:46:34.946960: Pseudo dice [0.9276, 0.9562, 0.948]\n",
      "2025-12-08 17:46:34.952224: Epoch time: 138.19 s\n",
      "2025-12-08 17:46:35.608722: \n",
      "2025-12-08 17:46:35.608722: Epoch 770\n",
      "2025-12-08 17:46:35.620910: Current learning rate: 0.00266\n",
      "2025-12-08 17:48:53.624125: train_loss -0.8519\n",
      "2025-12-08 17:48:53.625933: val_loss -0.8875\n",
      "2025-12-08 17:48:53.631137: Pseudo dice [0.9341, 0.9578, 0.9467]\n",
      "2025-12-08 17:48:53.635141: Epoch time: 138.02 s\n",
      "2025-12-08 17:48:54.327237: \n",
      "2025-12-08 17:48:54.329239: Epoch 771\n",
      "2025-12-08 17:48:54.329239: Current learning rate: 0.00265\n",
      "2025-12-08 17:51:12.543224: train_loss -0.8549\n",
      "2025-12-08 17:51:12.543224: val_loss -0.8813\n",
      "2025-12-08 17:51:12.550232: Pseudo dice [0.929, 0.9565, 0.9477]\n",
      "2025-12-08 17:51:12.556238: Epoch time: 138.22 s\n",
      "2025-12-08 17:51:13.215364: \n",
      "2025-12-08 17:51:13.217366: Epoch 772\n",
      "2025-12-08 17:51:13.217366: Current learning rate: 0.00264\n",
      "2025-12-08 17:53:31.240237: train_loss -0.8515\n",
      "2025-12-08 17:53:31.242238: val_loss -0.8825\n",
      "2025-12-08 17:53:31.251465: Pseudo dice [0.9276, 0.9596, 0.9489]\n",
      "2025-12-08 17:53:31.263483: Epoch time: 138.02 s\n",
      "2025-12-08 17:53:31.982326: \n",
      "2025-12-08 17:53:31.982326: Epoch 773\n",
      "2025-12-08 17:53:31.987885: Current learning rate: 0.00263\n",
      "2025-12-08 17:55:50.096360: train_loss -0.8487\n",
      "2025-12-08 17:55:50.096360: val_loss -0.8709\n",
      "2025-12-08 17:55:50.102367: Pseudo dice [0.9218, 0.9517, 0.9475]\n",
      "2025-12-08 17:55:50.110277: Epoch time: 138.12 s\n",
      "2025-12-08 17:55:50.859495: \n",
      "2025-12-08 17:55:50.859495: Epoch 774\n",
      "2025-12-08 17:55:50.875237: Current learning rate: 0.00262\n",
      "2025-12-08 17:58:08.932349: train_loss -0.8512\n",
      "2025-12-08 17:58:08.932349: val_loss -0.8817\n",
      "2025-12-08 17:58:08.936353: Pseudo dice [0.9285, 0.9581, 0.9474]\n",
      "2025-12-08 17:58:08.945596: Epoch time: 138.07 s\n",
      "2025-12-08 17:58:09.811704: \n",
      "2025-12-08 17:58:09.811704: Epoch 775\n",
      "2025-12-08 17:58:09.827582: Current learning rate: 0.00261\n",
      "2025-12-08 18:00:27.850961: train_loss -0.8564\n",
      "2025-12-08 18:00:27.850961: val_loss -0.8749\n",
      "2025-12-08 18:00:27.856967: Pseudo dice [0.9264, 0.9528, 0.9496]\n",
      "2025-12-08 18:00:27.858969: Epoch time: 138.04 s\n",
      "2025-12-08 18:00:28.514806: \n",
      "2025-12-08 18:00:28.514806: Epoch 776\n",
      "2025-12-08 18:00:28.530729: Current learning rate: 0.0026\n",
      "2025-12-08 18:02:46.358360: train_loss -0.8563\n",
      "2025-12-08 18:02:46.358360: val_loss -0.8878\n",
      "2025-12-08 18:02:46.365446: Pseudo dice [0.9332, 0.961, 0.9487]\n",
      "2025-12-08 18:02:46.371452: Epoch time: 137.84 s\n",
      "2025-12-08 18:02:47.124254: \n",
      "2025-12-08 18:02:47.124254: Epoch 777\n",
      "2025-12-08 18:02:47.140063: Current learning rate: 0.00259\n",
      "2025-12-08 18:05:05.179268: train_loss -0.8529\n",
      "2025-12-08 18:05:05.179268: val_loss -0.8797\n",
      "2025-12-08 18:05:05.190872: Pseudo dice [0.9261, 0.9577, 0.9464]\n",
      "2025-12-08 18:05:05.196878: Epoch time: 138.06 s\n",
      "2025-12-08 18:05:05.914452: \n",
      "2025-12-08 18:05:05.915457: Epoch 778\n",
      "2025-12-08 18:05:05.921471: Current learning rate: 0.00258\n",
      "2025-12-08 18:07:23.995018: train_loss -0.8516\n",
      "2025-12-08 18:07:23.995018: val_loss -0.8762\n",
      "2025-12-08 18:07:24.002979: Pseudo dice [0.9255, 0.9571, 0.9467]\n",
      "2025-12-08 18:07:24.006982: Epoch time: 138.08 s\n",
      "2025-12-08 18:07:24.725975: \n",
      "2025-12-08 18:07:24.726977: Epoch 779\n",
      "2025-12-08 18:07:24.732988: Current learning rate: 0.00257\n",
      "2025-12-08 18:09:42.667406: train_loss -0.8552\n",
      "2025-12-08 18:09:42.669408: val_loss -0.879\n",
      "2025-12-08 18:09:42.675205: Pseudo dice [0.9265, 0.9567, 0.9508]\n",
      "2025-12-08 18:09:42.681211: Epoch time: 137.94 s\n",
      "2025-12-08 18:09:43.484574: \n",
      "2025-12-08 18:09:43.484574: Epoch 780\n",
      "2025-12-08 18:09:43.484574: Current learning rate: 0.00256\n",
      "2025-12-08 18:12:01.529047: train_loss -0.8551\n",
      "2025-12-08 18:12:01.529047: val_loss -0.8816\n",
      "2025-12-08 18:12:01.538178: Pseudo dice [0.9271, 0.9566, 0.9495]\n",
      "2025-12-08 18:12:01.544180: Epoch time: 138.04 s\n",
      "2025-12-08 18:12:02.444346: \n",
      "2025-12-08 18:12:02.444346: Epoch 781\n",
      "2025-12-08 18:12:02.444346: Current learning rate: 0.00255\n",
      "2025-12-08 18:14:20.592723: train_loss -0.8545\n",
      "2025-12-08 18:14:20.592723: val_loss -0.8806\n",
      "2025-12-08 18:14:20.615051: Pseudo dice [0.9284, 0.9569, 0.9516]\n",
      "2025-12-08 18:14:20.619055: Epoch time: 138.15 s\n",
      "2025-12-08 18:14:21.334038: \n",
      "2025-12-08 18:14:21.334038: Epoch 782\n",
      "2025-12-08 18:14:21.340299: Current learning rate: 0.00254\n",
      "2025-12-08 18:16:39.378533: train_loss -0.8568\n",
      "2025-12-08 18:16:39.379534: val_loss -0.8792\n",
      "2025-12-08 18:16:39.385535: Pseudo dice [0.926, 0.9567, 0.9525]\n",
      "2025-12-08 18:16:39.390582: Epoch time: 138.05 s\n",
      "2025-12-08 18:16:40.138682: \n",
      "2025-12-08 18:16:40.142182: Epoch 783\n",
      "2025-12-08 18:16:40.142182: Current learning rate: 0.00253\n",
      "2025-12-08 18:18:58.218020: train_loss -0.8569\n",
      "2025-12-08 18:18:58.218020: val_loss -0.8778\n",
      "2025-12-08 18:18:58.218020: Pseudo dice [0.9218, 0.9569, 0.949]\n",
      "2025-12-08 18:18:58.233988: Epoch time: 138.08 s\n",
      "2025-12-08 18:18:58.874449: \n",
      "2025-12-08 18:18:58.874449: Epoch 784\n",
      "2025-12-08 18:18:58.890095: Current learning rate: 0.00252\n",
      "2025-12-08 18:21:18.236622: train_loss -0.8571\n",
      "2025-12-08 18:21:18.237624: val_loss -0.8816\n",
      "2025-12-08 18:21:18.242642: Pseudo dice [0.9253, 0.9548, 0.9522]\n",
      "2025-12-08 18:21:18.247658: Epoch time: 139.36 s\n",
      "2025-12-08 18:21:18.904968: \n",
      "2025-12-08 18:21:18.904968: Epoch 785\n",
      "2025-12-08 18:21:18.904968: Current learning rate: 0.00251\n",
      "2025-12-08 18:23:37.267286: train_loss -0.8613\n",
      "2025-12-08 18:23:37.267286: val_loss -0.8769\n",
      "2025-12-08 18:23:37.277035: Pseudo dice [0.9227, 0.9547, 0.9509]\n",
      "2025-12-08 18:23:37.283042: Epoch time: 138.36 s\n",
      "2025-12-08 18:23:37.958024: \n",
      "2025-12-08 18:23:37.960026: Epoch 786\n",
      "2025-12-08 18:23:37.960026: Current learning rate: 0.0025\n",
      "2025-12-08 18:25:56.529596: train_loss -0.8605\n",
      "2025-12-08 18:25:56.529596: val_loss -0.8749\n",
      "2025-12-08 18:25:56.529596: Pseudo dice [0.9209, 0.9601, 0.9488]\n",
      "2025-12-08 18:25:56.529596: Epoch time: 138.57 s\n",
      "2025-12-08 18:25:57.420424: \n",
      "2025-12-08 18:25:57.420424: Epoch 787\n",
      "2025-12-08 18:25:57.420424: Current learning rate: 0.00249\n",
      "2025-12-08 18:28:15.919500: train_loss -0.8554\n",
      "2025-12-08 18:28:15.921241: val_loss -0.8781\n",
      "2025-12-08 18:28:15.927110: Pseudo dice [0.9269, 0.9575, 0.9466]\n",
      "2025-12-08 18:28:15.931114: Epoch time: 138.5 s\n",
      "2025-12-08 18:28:16.608731: \n",
      "2025-12-08 18:28:16.608731: Epoch 788\n",
      "2025-12-08 18:28:16.608731: Current learning rate: 0.00248\n",
      "2025-12-08 18:30:35.154491: train_loss -0.861\n",
      "2025-12-08 18:30:35.154491: val_loss -0.8818\n",
      "2025-12-08 18:30:35.155493: Pseudo dice [0.9244, 0.9575, 0.9553]\n",
      "2025-12-08 18:30:35.155493: Epoch time: 138.55 s\n",
      "2025-12-08 18:30:35.843200: \n",
      "2025-12-08 18:30:35.858974: Epoch 789\n",
      "2025-12-08 18:30:35.858974: Current learning rate: 0.00247\n",
      "2025-12-08 18:32:53.984088: train_loss -0.8531\n",
      "2025-12-08 18:32:53.984088: val_loss -0.8613\n",
      "2025-12-08 18:32:53.984088: Pseudo dice [0.915, 0.947, 0.9488]\n",
      "2025-12-08 18:32:53.995827: Epoch time: 138.14 s\n",
      "2025-12-08 18:32:54.765583: \n",
      "2025-12-08 18:32:54.765583: Epoch 790\n",
      "2025-12-08 18:32:54.765583: Current learning rate: 0.00245\n",
      "2025-12-08 18:35:12.765426: train_loss -0.8562\n",
      "2025-12-08 18:35:12.765426: val_loss -0.8705\n",
      "2025-12-08 18:35:12.781476: Pseudo dice [0.9206, 0.9497, 0.9482]\n",
      "2025-12-08 18:35:12.781476: Epoch time: 138.02 s\n",
      "2025-12-08 18:35:13.436925: \n",
      "2025-12-08 18:35:13.436925: Epoch 791\n",
      "2025-12-08 18:35:13.436925: Current learning rate: 0.00244\n",
      "2025-12-08 18:37:31.389206: train_loss -0.8547\n",
      "2025-12-08 18:37:31.389206: val_loss -0.8822\n",
      "2025-12-08 18:37:31.389206: Pseudo dice [0.9283, 0.9618, 0.9488]\n",
      "2025-12-08 18:37:31.389206: Epoch time: 137.95 s\n",
      "2025-12-08 18:37:32.045753: \n",
      "2025-12-08 18:37:32.045753: Epoch 792\n",
      "2025-12-08 18:37:32.045753: Current learning rate: 0.00243\n",
      "2025-12-08 18:39:50.186106: train_loss -0.8547\n",
      "2025-12-08 18:39:50.186106: val_loss -0.8846\n",
      "2025-12-08 18:39:50.186106: Pseudo dice [0.9292, 0.9605, 0.9482]\n",
      "2025-12-08 18:39:50.186106: Epoch time: 138.14 s\n",
      "2025-12-08 18:39:51.155434: \n",
      "2025-12-08 18:39:51.155434: Epoch 793\n",
      "2025-12-08 18:39:51.170934: Current learning rate: 0.00242\n",
      "2025-12-08 18:42:09.201605: train_loss -0.8574\n",
      "2025-12-08 18:42:09.201605: val_loss -0.877\n",
      "2025-12-08 18:42:09.214620: Pseudo dice [0.9229, 0.9539, 0.9476]\n",
      "2025-12-08 18:42:09.217625: Epoch time: 138.05 s\n",
      "2025-12-08 18:42:09.885997: \n",
      "2025-12-08 18:42:09.885997: Epoch 794\n",
      "2025-12-08 18:42:09.889933: Current learning rate: 0.00241\n",
      "2025-12-08 18:44:28.041142: train_loss -0.8555\n",
      "2025-12-08 18:44:28.041142: val_loss -0.8762\n",
      "2025-12-08 18:44:28.046873: Pseudo dice [0.9229, 0.9579, 0.9486]\n",
      "2025-12-08 18:44:28.048730: Epoch time: 138.16 s\n",
      "2025-12-08 18:44:28.704955: \n",
      "2025-12-08 18:44:28.704955: Epoch 795\n",
      "2025-12-08 18:44:28.718768: Current learning rate: 0.0024\n",
      "2025-12-08 18:46:46.733986: train_loss -0.8572\n",
      "2025-12-08 18:46:46.733986: val_loss -0.8724\n",
      "2025-12-08 18:46:46.750016: Pseudo dice [0.9203, 0.9551, 0.943]\n",
      "2025-12-08 18:46:46.754516: Epoch time: 138.03 s\n",
      "2025-12-08 18:46:47.562033: \n",
      "2025-12-08 18:46:47.562033: Epoch 796\n",
      "2025-12-08 18:46:47.578003: Current learning rate: 0.00239\n",
      "2025-12-08 18:49:05.655196: train_loss -0.853\n",
      "2025-12-08 18:49:05.655196: val_loss -0.8818\n",
      "2025-12-08 18:49:05.671305: Pseudo dice [0.931, 0.9586, 0.9485]\n",
      "2025-12-08 18:49:05.671305: Epoch time: 138.09 s\n",
      "2025-12-08 18:49:06.327442: \n",
      "2025-12-08 18:49:06.327442: Epoch 797\n",
      "2025-12-08 18:49:06.327442: Current learning rate: 0.00238\n",
      "2025-12-08 18:51:24.657383: train_loss -0.86\n",
      "2025-12-08 18:51:24.658382: val_loss -0.8785\n",
      "2025-12-08 18:51:24.668852: Pseudo dice [0.9265, 0.9549, 0.9475]\n",
      "2025-12-08 18:51:24.674781: Epoch time: 138.33 s\n",
      "2025-12-08 18:51:25.342435: \n",
      "2025-12-08 18:51:25.342435: Epoch 798\n",
      "2025-12-08 18:51:25.342435: Current learning rate: 0.00237\n",
      "2025-12-08 18:53:43.420893: train_loss -0.8549\n",
      "2025-12-08 18:53:43.420893: val_loss -0.8881\n",
      "2025-12-08 18:53:43.420893: Pseudo dice [0.9343, 0.9605, 0.9461]\n",
      "2025-12-08 18:53:43.432140: Epoch time: 138.09 s\n",
      "2025-12-08 18:53:44.406447: \n",
      "2025-12-08 18:53:44.406447: Epoch 799\n",
      "2025-12-08 18:53:44.413548: Current learning rate: 0.00236\n",
      "2025-12-08 18:56:02.453576: train_loss -0.8515\n",
      "2025-12-08 18:56:02.453576: val_loss -0.8693\n",
      "2025-12-08 18:56:02.453576: Pseudo dice [0.9198, 0.9544, 0.9476]\n",
      "2025-12-08 18:56:02.469539: Epoch time: 138.05 s\n",
      "2025-12-08 18:56:03.411101: \n",
      "2025-12-08 18:56:03.411101: Epoch 800\n",
      "2025-12-08 18:56:03.411101: Current learning rate: 0.00235\n",
      "2025-12-08 18:58:21.290149: train_loss -0.8578\n",
      "2025-12-08 18:58:21.292151: val_loss -0.8794\n",
      "2025-12-08 18:58:21.297896: Pseudo dice [0.9241, 0.9607, 0.9532]\n",
      "2025-12-08 18:58:21.303901: Epoch time: 137.88 s\n",
      "2025-12-08 18:58:21.967714: \n",
      "2025-12-08 18:58:21.967714: Epoch 801\n",
      "2025-12-08 18:58:21.967714: Current learning rate: 0.00234\n",
      "2025-12-08 19:00:39.870021: train_loss -0.8628\n",
      "2025-12-08 19:00:39.872024: val_loss -0.8782\n",
      "2025-12-08 19:00:39.873766: Pseudo dice [0.9262, 0.9566, 0.9464]\n",
      "2025-12-08 19:00:39.873766: Epoch time: 137.9 s\n",
      "2025-12-08 19:00:40.593046: \n",
      "2025-12-08 19:00:40.593046: Epoch 802\n",
      "2025-12-08 19:00:40.600563: Current learning rate: 0.00233\n",
      "2025-12-08 19:02:58.573763: train_loss -0.8585\n",
      "2025-12-08 19:02:58.573763: val_loss -0.8741\n",
      "2025-12-08 19:02:58.581004: Pseudo dice [0.9217, 0.952, 0.9528]\n",
      "2025-12-08 19:02:58.585966: Epoch time: 138.0 s\n",
      "2025-12-08 19:02:59.311864: \n",
      "2025-12-08 19:02:59.327599: Epoch 803\n",
      "2025-12-08 19:02:59.331664: Current learning rate: 0.00232\n",
      "2025-12-08 19:05:17.248889: train_loss -0.8562\n",
      "2025-12-08 19:05:17.248889: val_loss -0.8736\n",
      "2025-12-08 19:05:17.268758: Pseudo dice [0.9231, 0.955, 0.9449]\n",
      "2025-12-08 19:05:17.272762: Epoch time: 137.94 s\n",
      "2025-12-08 19:05:17.986116: \n",
      "2025-12-08 19:05:17.986116: Epoch 804\n",
      "2025-12-08 19:05:17.992145: Current learning rate: 0.00231\n",
      "2025-12-08 19:07:35.967156: train_loss -0.8542\n",
      "2025-12-08 19:07:35.967156: val_loss -0.8822\n",
      "2025-12-08 19:07:35.982802: Pseudo dice [0.9265, 0.9583, 0.9514]\n",
      "2025-12-08 19:07:35.982802: Epoch time: 137.98 s\n",
      "2025-12-08 19:07:36.835065: \n",
      "2025-12-08 19:07:36.835065: Epoch 805\n",
      "2025-12-08 19:07:36.841093: Current learning rate: 0.0023\n",
      "2025-12-08 19:09:54.837471: train_loss -0.8581\n",
      "2025-12-08 19:09:54.839473: val_loss -0.8812\n",
      "2025-12-08 19:09:54.851233: Pseudo dice [0.9281, 0.9591, 0.9506]\n",
      "2025-12-08 19:09:54.860997: Epoch time: 138.0 s\n",
      "2025-12-08 19:09:55.515122: \n",
      "2025-12-08 19:09:55.515122: Epoch 806\n",
      "2025-12-08 19:09:55.533233: Current learning rate: 0.00229\n",
      "2025-12-08 19:12:13.488602: train_loss -0.855\n",
      "2025-12-08 19:12:13.489602: val_loss -0.887\n",
      "2025-12-08 19:12:13.498604: Pseudo dice [0.9313, 0.9601, 0.9503]\n",
      "2025-12-08 19:12:13.504680: Epoch time: 137.97 s\n",
      "2025-12-08 19:12:14.233557: \n",
      "2025-12-08 19:12:14.233557: Epoch 807\n",
      "2025-12-08 19:12:14.233557: Current learning rate: 0.00228\n",
      "2025-12-08 19:14:32.229224: train_loss -0.8567\n",
      "2025-12-08 19:14:32.229224: val_loss -0.8858\n",
      "2025-12-08 19:14:32.236920: Pseudo dice [0.9301, 0.9575, 0.9474]\n",
      "2025-12-08 19:14:32.241268: Epoch time: 138.01 s\n",
      "2025-12-08 19:14:32.901841: \n",
      "2025-12-08 19:14:32.901841: Epoch 808\n",
      "2025-12-08 19:14:32.904853: Current learning rate: 0.00226\n",
      "2025-12-08 19:16:50.946065: train_loss -0.8552\n",
      "2025-12-08 19:16:50.948067: val_loss -0.8841\n",
      "2025-12-08 19:16:50.956075: Pseudo dice [0.9271, 0.9575, 0.9506]\n",
      "2025-12-08 19:16:50.962081: Epoch time: 138.05 s\n",
      "2025-12-08 19:16:51.639742: \n",
      "2025-12-08 19:16:51.639742: Epoch 809\n",
      "2025-12-08 19:16:51.639742: Current learning rate: 0.00225\n",
      "2025-12-08 19:19:09.685284: train_loss -0.8575\n",
      "2025-12-08 19:19:09.686284: val_loss -0.8829\n",
      "2025-12-08 19:19:09.691285: Pseudo dice [0.9277, 0.957, 0.9508]\n",
      "2025-12-08 19:19:09.697287: Epoch time: 138.05 s\n",
      "2025-12-08 19:19:10.577804: \n",
      "2025-12-08 19:19:10.577804: Epoch 810\n",
      "2025-12-08 19:19:10.577804: Current learning rate: 0.00224\n",
      "2025-12-08 19:21:28.538909: train_loss -0.8557\n",
      "2025-12-08 19:21:28.538909: val_loss -0.8815\n",
      "2025-12-08 19:21:28.544916: Pseudo dice [0.9237, 0.9586, 0.9543]\n",
      "2025-12-08 19:21:28.550661: Epoch time: 137.96 s\n",
      "2025-12-08 19:21:29.296805: \n",
      "2025-12-08 19:21:29.296805: Epoch 811\n",
      "2025-12-08 19:21:29.312562: Current learning rate: 0.00223\n",
      "2025-12-08 19:23:47.386775: train_loss -0.855\n",
      "2025-12-08 19:23:47.388777: val_loss -0.8829\n",
      "2025-12-08 19:23:47.395020: Pseudo dice [0.9265, 0.9589, 0.9522]\n",
      "2025-12-08 19:23:47.401026: Epoch time: 138.09 s\n",
      "2025-12-08 19:23:48.072648: \n",
      "2025-12-08 19:23:48.072648: Epoch 812\n",
      "2025-12-08 19:23:48.078875: Current learning rate: 0.00222\n",
      "2025-12-08 19:26:06.366960: train_loss -0.8575\n",
      "2025-12-08 19:26:06.368962: val_loss -0.8844\n",
      "2025-12-08 19:26:06.378475: Pseudo dice [0.9312, 0.9599, 0.9504]\n",
      "2025-12-08 19:26:06.384481: Epoch time: 138.3 s\n",
      "2025-12-08 19:26:07.046837: \n",
      "2025-12-08 19:26:07.046837: Epoch 813\n",
      "2025-12-08 19:26:07.046837: Current learning rate: 0.00221\n",
      "2025-12-08 19:28:25.103093: train_loss -0.8618\n",
      "2025-12-08 19:28:25.105095: val_loss -0.8786\n",
      "2025-12-08 19:28:25.111225: Pseudo dice [0.924, 0.9558, 0.9501]\n",
      "2025-12-08 19:28:25.116226: Epoch time: 138.06 s\n",
      "2025-12-08 19:28:25.780141: \n",
      "2025-12-08 19:28:25.780141: Epoch 814\n",
      "2025-12-08 19:28:25.780141: Current learning rate: 0.0022\n",
      "2025-12-08 19:30:43.970251: train_loss -0.8568\n",
      "2025-12-08 19:30:43.970251: val_loss -0.8811\n",
      "2025-12-08 19:30:43.983253: Pseudo dice [0.9311, 0.9547, 0.9478]\n",
      "2025-12-08 19:30:43.989281: Epoch time: 138.19 s\n",
      "2025-12-08 19:30:44.717874: \n",
      "2025-12-08 19:30:44.717874: Epoch 815\n",
      "2025-12-08 19:30:44.723909: Current learning rate: 0.00219\n",
      "2025-12-08 19:33:02.874304: train_loss -0.8612\n",
      "2025-12-08 19:33:02.890149: val_loss -0.8867\n",
      "2025-12-08 19:33:02.893602: Pseudo dice [0.9312, 0.9588, 0.9503]\n",
      "2025-12-08 19:33:02.893602: Epoch time: 138.16 s\n",
      "2025-12-08 19:33:03.732724: \n",
      "2025-12-08 19:33:03.732724: Epoch 816\n",
      "2025-12-08 19:33:03.732724: Current learning rate: 0.00218\n",
      "2025-12-08 19:35:21.878896: train_loss -0.8551\n",
      "2025-12-08 19:35:21.878896: val_loss -0.8781\n",
      "2025-12-08 19:35:21.886904: Pseudo dice [0.9248, 0.9566, 0.9486]\n",
      "2025-12-08 19:35:21.894794: Epoch time: 138.15 s\n",
      "2025-12-08 19:35:22.631432: \n",
      "2025-12-08 19:35:22.631432: Epoch 817\n",
      "2025-12-08 19:35:22.638453: Current learning rate: 0.00217\n",
      "2025-12-08 19:37:40.792414: train_loss -0.8566\n",
      "2025-12-08 19:37:40.792414: val_loss -0.8795\n",
      "2025-12-08 19:37:40.799288: Pseudo dice [0.926, 0.9564, 0.9542]\n",
      "2025-12-08 19:37:40.801291: Epoch time: 138.16 s\n",
      "2025-12-08 19:37:41.476904: \n",
      "2025-12-08 19:37:41.476904: Epoch 818\n",
      "2025-12-08 19:37:41.482933: Current learning rate: 0.00216\n",
      "2025-12-08 19:39:59.741018: train_loss -0.8564\n",
      "2025-12-08 19:39:59.741018: val_loss -0.8831\n",
      "2025-12-08 19:39:59.747183: Pseudo dice [0.9274, 0.9598, 0.95]\n",
      "2025-12-08 19:39:59.753190: Epoch time: 138.27 s\n",
      "2025-12-08 19:40:00.508635: \n",
      "2025-12-08 19:40:00.509640: Epoch 819\n",
      "2025-12-08 19:40:00.514655: Current learning rate: 0.00215\n",
      "2025-12-08 19:42:18.545534: train_loss -0.8555\n",
      "2025-12-08 19:42:18.545534: val_loss -0.8755\n",
      "2025-12-08 19:42:18.564286: Pseudo dice [0.9208, 0.9538, 0.9517]\n",
      "2025-12-08 19:42:18.570292: Epoch time: 138.04 s\n",
      "2025-12-08 19:42:19.212644: \n",
      "2025-12-08 19:42:19.212644: Epoch 820\n",
      "2025-12-08 19:42:19.218464: Current learning rate: 0.00214\n",
      "2025-12-08 19:44:37.373867: train_loss -0.8565\n",
      "2025-12-08 19:44:37.375871: val_loss -0.8765\n",
      "2025-12-08 19:44:37.385870: Pseudo dice [0.9228, 0.9559, 0.9527]\n",
      "2025-12-08 19:44:37.395963: Epoch time: 138.16 s\n",
      "2025-12-08 19:44:38.015818: \n",
      "2025-12-08 19:44:38.015818: Epoch 821\n",
      "2025-12-08 19:44:38.031682: Current learning rate: 0.00213\n",
      "2025-12-08 19:46:56.190500: train_loss -0.8566\n",
      "2025-12-08 19:46:56.190500: val_loss -0.8667\n",
      "2025-12-08 19:46:56.198762: Pseudo dice [0.919, 0.9488, 0.9483]\n",
      "2025-12-08 19:46:56.202494: Epoch time: 138.17 s\n",
      "2025-12-08 19:46:57.154943: \n",
      "2025-12-08 19:46:57.154943: Epoch 822\n",
      "2025-12-08 19:46:57.154943: Current learning rate: 0.00212\n",
      "2025-12-08 19:49:15.203988: train_loss -0.8596\n",
      "2025-12-08 19:49:15.205991: val_loss -0.8825\n",
      "2025-12-08 19:49:15.214209: Pseudo dice [0.9243, 0.9589, 0.9553]\n",
      "2025-12-08 19:49:15.219954: Epoch time: 138.05 s\n",
      "2025-12-08 19:49:15.858793: \n",
      "2025-12-08 19:49:15.858793: Epoch 823\n",
      "2025-12-08 19:49:15.874435: Current learning rate: 0.0021\n",
      "2025-12-08 19:51:34.046142: train_loss -0.8593\n",
      "2025-12-08 19:51:34.046142: val_loss -0.8871\n",
      "2025-12-08 19:51:34.065470: Pseudo dice [0.9309, 0.9607, 0.9501]\n",
      "2025-12-08 19:51:34.069469: Epoch time: 138.19 s\n",
      "2025-12-08 19:51:34.686307: \n",
      "2025-12-08 19:51:34.686307: Epoch 824\n",
      "2025-12-08 19:51:34.702253: Current learning rate: 0.00209\n",
      "2025-12-08 19:53:52.631636: train_loss -0.8592\n",
      "2025-12-08 19:53:52.631636: val_loss -0.8909\n",
      "2025-12-08 19:53:52.637643: Pseudo dice [0.932, 0.9642, 0.9519]\n",
      "2025-12-08 19:53:52.643388: Epoch time: 137.95 s\n",
      "2025-12-08 19:53:52.649393: Yayy! New best EMA pseudo Dice: 0.945\n",
      "2025-12-08 19:53:53.660229: \n",
      "2025-12-08 19:53:53.661233: Epoch 825\n",
      "2025-12-08 19:53:53.667261: Current learning rate: 0.00208\n",
      "2025-12-08 19:56:11.577435: train_loss -0.8591\n",
      "2025-12-08 19:56:11.577435: val_loss -0.8753\n",
      "2025-12-08 19:56:11.583827: Pseudo dice [0.9256, 0.9542, 0.9467]\n",
      "2025-12-08 19:56:11.589834: Epoch time: 137.92 s\n",
      "2025-12-08 19:56:12.217752: \n",
      "2025-12-08 19:56:12.217752: Epoch 826\n",
      "2025-12-08 19:56:12.233420: Current learning rate: 0.00207\n",
      "2025-12-08 19:58:30.307872: train_loss -0.8563\n",
      "2025-12-08 19:58:30.309875: val_loss -0.8838\n",
      "2025-12-08 19:58:30.317624: Pseudo dice [0.9291, 0.9575, 0.9549]\n",
      "2025-12-08 19:58:30.323368: Epoch time: 138.09 s\n",
      "2025-12-08 19:58:31.019893: \n",
      "2025-12-08 19:58:31.019893: Epoch 827\n",
      "2025-12-08 19:58:31.026908: Current learning rate: 0.00206\n",
      "2025-12-08 20:00:49.189579: train_loss -0.8601\n",
      "2025-12-08 20:00:49.189579: val_loss -0.878\n",
      "2025-12-08 20:00:49.197075: Pseudo dice [0.9276, 0.9548, 0.9442]\n",
      "2025-12-08 20:00:49.201079: Epoch time: 138.17 s\n",
      "2025-12-08 20:00:49.920916: \n",
      "2025-12-08 20:00:49.920916: Epoch 828\n",
      "2025-12-08 20:00:49.920916: Current learning rate: 0.00205\n",
      "2025-12-08 20:03:08.014956: train_loss -0.858\n",
      "2025-12-08 20:03:08.014956: val_loss -0.8756\n",
      "2025-12-08 20:03:08.014956: Pseudo dice [0.9222, 0.9536, 0.9557]\n",
      "2025-12-08 20:03:08.030941: Epoch time: 138.09 s\n",
      "2025-12-08 20:03:08.844064: \n",
      "2025-12-08 20:03:08.844064: Epoch 829\n",
      "2025-12-08 20:03:08.844064: Current learning rate: 0.00204\n",
      "2025-12-08 20:05:26.963784: train_loss -0.8537\n",
      "2025-12-08 20:05:26.965786: val_loss -0.8708\n",
      "2025-12-08 20:05:26.971930: Pseudo dice [0.9202, 0.951, 0.9517]\n",
      "2025-12-08 20:05:26.976949: Epoch time: 138.12 s\n",
      "2025-12-08 20:05:27.670239: \n",
      "2025-12-08 20:05:27.670239: Epoch 830\n",
      "2025-12-08 20:05:27.685898: Current learning rate: 0.00203\n",
      "2025-12-08 20:07:45.759203: train_loss -0.8546\n",
      "2025-12-08 20:07:45.761205: val_loss -0.8773\n",
      "2025-12-08 20:07:45.766949: Pseudo dice [0.9256, 0.9574, 0.9459]\n",
      "2025-12-08 20:07:45.766949: Epoch time: 138.09 s\n",
      "2025-12-08 20:07:46.471664: \n",
      "2025-12-08 20:07:46.471664: Epoch 831\n",
      "2025-12-08 20:07:46.478684: Current learning rate: 0.00202\n",
      "2025-12-08 20:10:04.654881: train_loss -0.8578\n",
      "2025-12-08 20:10:04.654881: val_loss -0.8734\n",
      "2025-12-08 20:10:04.654881: Pseudo dice [0.9214, 0.9533, 0.9481]\n",
      "2025-12-08 20:10:04.670535: Epoch time: 138.18 s\n",
      "2025-12-08 20:10:05.295662: \n",
      "2025-12-08 20:10:05.295662: Epoch 832\n",
      "2025-12-08 20:10:05.295662: Current learning rate: 0.00201\n",
      "2025-12-08 20:12:23.362376: train_loss -0.8535\n",
      "2025-12-08 20:12:23.362376: val_loss -0.8737\n",
      "2025-12-08 20:12:23.364377: Pseudo dice [0.9232, 0.9536, 0.947]\n",
      "2025-12-08 20:12:23.374270: Epoch time: 138.07 s\n",
      "2025-12-08 20:12:24.014679: \n",
      "2025-12-08 20:12:24.014679: Epoch 833\n",
      "2025-12-08 20:12:24.014679: Current learning rate: 0.002\n",
      "2025-12-08 20:14:42.233750: train_loss -0.8587\n",
      "2025-12-08 20:14:42.233750: val_loss -0.8798\n",
      "2025-12-08 20:14:42.239757: Pseudo dice [0.9266, 0.9543, 0.9503]\n",
      "2025-12-08 20:14:42.245763: Epoch time: 138.22 s\n",
      "2025-12-08 20:14:42.883367: \n",
      "2025-12-08 20:14:42.885369: Epoch 834\n",
      "2025-12-08 20:14:42.889818: Current learning rate: 0.00199\n",
      "2025-12-08 20:17:01.014964: train_loss -0.8632\n",
      "2025-12-08 20:17:01.014964: val_loss -0.8852\n",
      "2025-12-08 20:17:01.035213: Pseudo dice [0.9287, 0.9602, 0.9501]\n",
      "2025-12-08 20:17:01.040236: Epoch time: 138.13 s\n",
      "2025-12-08 20:17:01.826707: \n",
      "2025-12-08 20:17:01.826707: Epoch 835\n",
      "2025-12-08 20:17:01.842831: Current learning rate: 0.00198\n",
      "2025-12-08 20:19:20.014034: train_loss -0.8602\n",
      "2025-12-08 20:19:20.014034: val_loss -0.8887\n",
      "2025-12-08 20:19:20.014034: Pseudo dice [0.9332, 0.9615, 0.9526]\n",
      "2025-12-08 20:19:20.014034: Epoch time: 138.19 s\n",
      "2025-12-08 20:19:20.718606: \n",
      "2025-12-08 20:19:20.718606: Epoch 836\n",
      "2025-12-08 20:19:20.718606: Current learning rate: 0.00196\n",
      "2025-12-08 20:21:38.971270: train_loss -0.8625\n",
      "2025-12-08 20:21:38.971270: val_loss -0.8737\n",
      "2025-12-08 20:21:38.980756: Pseudo dice [0.9199, 0.9551, 0.95]\n",
      "2025-12-08 20:21:38.986763: Epoch time: 138.25 s\n",
      "2025-12-08 20:21:39.686005: \n",
      "2025-12-08 20:21:39.686005: Epoch 837\n",
      "2025-12-08 20:21:39.686005: Current learning rate: 0.00195\n",
      "2025-12-08 20:23:57.736133: train_loss -0.8513\n",
      "2025-12-08 20:23:57.736133: val_loss -0.8845\n",
      "2025-12-08 20:23:57.736133: Pseudo dice [0.9287, 0.9604, 0.9523]\n",
      "2025-12-08 20:23:57.751778: Epoch time: 138.05 s\n",
      "2025-12-08 20:23:58.467860: \n",
      "2025-12-08 20:23:58.467860: Epoch 838\n",
      "2025-12-08 20:23:58.467860: Current learning rate: 0.00194\n",
      "2025-12-08 20:26:16.561401: train_loss -0.8596\n",
      "2025-12-08 20:26:16.561401: val_loss -0.8673\n",
      "2025-12-08 20:26:16.561401: Pseudo dice [0.9161, 0.9475, 0.9477]\n",
      "2025-12-08 20:26:16.561401: Epoch time: 138.09 s\n",
      "2025-12-08 20:26:17.217859: \n",
      "2025-12-08 20:26:17.217859: Epoch 839\n",
      "2025-12-08 20:26:17.217859: Current learning rate: 0.00193\n",
      "2025-12-08 20:28:35.380162: train_loss -0.8555\n",
      "2025-12-08 20:28:35.381164: val_loss -0.8805\n",
      "2025-12-08 20:28:35.387178: Pseudo dice [0.9263, 0.9524, 0.9493]\n",
      "2025-12-08 20:28:35.389186: Epoch time: 138.16 s\n",
      "2025-12-08 20:28:36.077481: \n",
      "2025-12-08 20:28:36.077481: Epoch 840\n",
      "2025-12-08 20:28:36.093191: Current learning rate: 0.00192\n",
      "2025-12-08 20:30:54.314471: train_loss -0.8562\n",
      "2025-12-08 20:30:54.314471: val_loss -0.8889\n",
      "2025-12-08 20:30:54.321669: Pseudo dice [0.933, 0.9555, 0.955]\n",
      "2025-12-08 20:30:54.327664: Epoch time: 138.24 s\n",
      "2025-12-08 20:30:55.124665: \n",
      "2025-12-08 20:30:55.124665: Epoch 841\n",
      "2025-12-08 20:30:55.124665: Current learning rate: 0.00191\n",
      "2025-12-08 20:33:13.278963: train_loss -0.8593\n",
      "2025-12-08 20:33:13.280703: val_loss -0.8821\n",
      "2025-12-08 20:33:13.286712: Pseudo dice [0.9275, 0.9558, 0.951]\n",
      "2025-12-08 20:33:13.294420: Epoch time: 138.15 s\n",
      "2025-12-08 20:33:13.984424: \n",
      "2025-12-08 20:33:13.984424: Epoch 842\n",
      "2025-12-08 20:33:13.984424: Current learning rate: 0.0019\n",
      "2025-12-08 20:35:32.043719: train_loss -0.8603\n",
      "2025-12-08 20:35:32.043719: val_loss -0.8814\n",
      "2025-12-08 20:35:32.053228: Pseudo dice [0.9243, 0.9537, 0.9521]\n",
      "2025-12-08 20:35:32.060974: Epoch time: 138.06 s\n",
      "2025-12-08 20:35:32.701906: \n",
      "2025-12-08 20:35:32.701906: Epoch 843\n",
      "2025-12-08 20:35:32.701906: Current learning rate: 0.00189\n",
      "2025-12-08 20:37:50.873913: train_loss -0.858\n",
      "2025-12-08 20:37:50.873913: val_loss -0.884\n",
      "2025-12-08 20:37:50.881185: Pseudo dice [0.9272, 0.9554, 0.9561]\n",
      "2025-12-08 20:37:50.887192: Epoch time: 138.19 s\n",
      "2025-12-08 20:37:51.700486: \n",
      "2025-12-08 20:37:51.700486: Epoch 844\n",
      "2025-12-08 20:37:51.702228: Current learning rate: 0.00188\n",
      "2025-12-08 20:40:09.827553: train_loss -0.8563\n",
      "2025-12-08 20:40:09.827553: val_loss -0.8759\n",
      "2025-12-08 20:40:09.845301: Pseudo dice [0.9238, 0.9503, 0.953]\n",
      "2025-12-08 20:40:09.845301: Epoch time: 138.13 s\n",
      "2025-12-08 20:40:10.484773: \n",
      "2025-12-08 20:40:10.484773: Epoch 845\n",
      "2025-12-08 20:40:10.484773: Current learning rate: 0.00187\n",
      "2025-12-08 20:42:28.462438: train_loss -0.8628\n",
      "2025-12-08 20:42:28.462438: val_loss -0.8854\n",
      "2025-12-08 20:42:28.470188: Pseudo dice [0.93, 0.9579, 0.948]\n",
      "2025-12-08 20:42:28.478196: Epoch time: 137.98 s\n",
      "2025-12-08 20:42:29.170966: \n",
      "2025-12-08 20:42:29.170966: Epoch 846\n",
      "2025-12-08 20:42:29.170966: Current learning rate: 0.00186\n",
      "2025-12-08 20:44:47.204086: train_loss -0.861\n",
      "2025-12-08 20:44:47.204086: val_loss -0.8729\n",
      "2025-12-08 20:44:47.211810: Pseudo dice [0.9201, 0.955, 0.9525]\n",
      "2025-12-08 20:44:47.217554: Epoch time: 138.03 s\n",
      "2025-12-08 20:44:48.076614: \n",
      "2025-12-08 20:44:48.092267: Epoch 847\n",
      "2025-12-08 20:44:48.092267: Current learning rate: 0.00185\n",
      "2025-12-08 20:47:05.996790: train_loss -0.8612\n",
      "2025-12-08 20:47:05.996790: val_loss -0.891\n",
      "2025-12-08 20:47:06.000708: Pseudo dice [0.9325, 0.9597, 0.9503]\n",
      "2025-12-08 20:47:06.008218: Epoch time: 137.92 s\n",
      "2025-12-08 20:47:06.763971: \n",
      "2025-12-08 20:47:06.763971: Epoch 848\n",
      "2025-12-08 20:47:06.783162: Current learning rate: 0.00184\n",
      "2025-12-08 20:49:24.874495: train_loss -0.8621\n",
      "2025-12-08 20:49:24.874495: val_loss -0.8761\n",
      "2025-12-08 20:49:24.874495: Pseudo dice [0.9235, 0.9538, 0.9484]\n",
      "2025-12-08 20:49:24.890402: Epoch time: 138.11 s\n",
      "2025-12-08 20:49:25.514901: \n",
      "2025-12-08 20:49:25.514901: Epoch 849\n",
      "2025-12-08 20:49:25.514901: Current learning rate: 0.00182\n",
      "2025-12-08 20:51:43.609506: train_loss -0.8558\n",
      "2025-12-08 20:51:43.610506: val_loss -0.8798\n",
      "2025-12-08 20:51:43.616956: Pseudo dice [0.924, 0.9554, 0.9568]\n",
      "2025-12-08 20:51:43.622957: Epoch time: 138.09 s\n",
      "2025-12-08 20:51:44.528773: \n",
      "2025-12-08 20:51:44.528773: Epoch 850\n",
      "2025-12-08 20:51:44.529777: Current learning rate: 0.00181\n",
      "2025-12-08 20:54:02.690138: train_loss -0.8524\n",
      "2025-12-08 20:54:02.692140: val_loss -0.8446\n",
      "2025-12-08 20:54:02.692140: Pseudo dice [0.9128, 0.9415, 0.9414]\n",
      "2025-12-08 20:54:02.705089: Epoch time: 138.16 s\n",
      "2025-12-08 20:54:03.436871: \n",
      "2025-12-08 20:54:03.436871: Epoch 851\n",
      "2025-12-08 20:54:03.436871: Current learning rate: 0.0018\n",
      "2025-12-08 20:56:21.548438: train_loss -0.8302\n",
      "2025-12-08 20:56:21.548438: val_loss -0.8582\n",
      "2025-12-08 20:56:21.554444: Pseudo dice [0.9151, 0.9502, 0.9469]\n",
      "2025-12-08 20:56:21.560450: Epoch time: 138.11 s\n",
      "2025-12-08 20:56:22.200755: \n",
      "2025-12-08 20:56:22.201755: Epoch 852\n",
      "2025-12-08 20:56:22.203681: Current learning rate: 0.00179\n",
      "2025-12-08 20:58:41.119598: train_loss -0.8453\n",
      "2025-12-08 20:58:41.119598: val_loss -0.886\n",
      "2025-12-08 20:58:41.129493: Pseudo dice [0.9305, 0.9579, 0.9532]\n",
      "2025-12-08 20:58:41.138668: Epoch time: 138.92 s\n",
      "2025-12-08 20:58:42.050856: \n",
      "2025-12-08 20:58:42.050856: Epoch 853\n",
      "2025-12-08 20:58:42.063916: Current learning rate: 0.00178\n",
      "2025-12-08 21:01:01.642681: train_loss -0.848\n",
      "2025-12-08 21:01:01.643681: val_loss -0.8755\n",
      "2025-12-08 21:01:01.648371: Pseudo dice [0.9246, 0.957, 0.9475]\n",
      "2025-12-08 21:01:01.654559: Epoch time: 139.59 s\n",
      "2025-12-08 21:01:02.292306: \n",
      "2025-12-08 21:01:02.292306: Epoch 854\n",
      "2025-12-08 21:01:02.306144: Current learning rate: 0.00177\n",
      "2025-12-08 21:03:20.985643: train_loss -0.8533\n",
      "2025-12-08 21:03:20.987646: val_loss -0.8835\n",
      "2025-12-08 21:03:20.995830: Pseudo dice [0.9276, 0.9543, 0.9545]\n",
      "2025-12-08 21:03:21.003596: Epoch time: 138.69 s\n",
      "2025-12-08 21:03:21.784941: \n",
      "2025-12-08 21:03:21.784941: Epoch 855\n",
      "2025-12-08 21:03:21.796477: Current learning rate: 0.00176\n",
      "2025-12-08 21:05:40.503781: train_loss -0.8538\n",
      "2025-12-08 21:05:40.503781: val_loss -0.8887\n",
      "2025-12-08 21:05:40.503781: Pseudo dice [0.9308, 0.958, 0.9536]\n",
      "2025-12-08 21:05:40.514116: Epoch time: 138.72 s\n",
      "2025-12-08 21:05:41.155556: \n",
      "2025-12-08 21:05:41.155556: Epoch 856\n",
      "2025-12-08 21:05:41.155556: Current learning rate: 0.00175\n",
      "2025-12-08 21:08:00.077375: train_loss -0.8537\n",
      "2025-12-08 21:08:00.077375: val_loss -0.8702\n",
      "2025-12-08 21:08:00.097130: Pseudo dice [0.9173, 0.9503, 0.9521]\n",
      "2025-12-08 21:08:00.103135: Epoch time: 138.92 s\n",
      "2025-12-08 21:08:00.780620: \n",
      "2025-12-08 21:08:00.780620: Epoch 857\n",
      "2025-12-08 21:08:00.796667: Current learning rate: 0.00174\n",
      "2025-12-08 21:10:19.943634: train_loss -0.8575\n",
      "2025-12-08 21:10:19.950293: val_loss -0.883\n",
      "2025-12-08 21:10:19.955333: Pseudo dice [0.9303, 0.9583, 0.9499]\n",
      "2025-12-08 21:10:19.961608: Epoch time: 139.16 s\n",
      "2025-12-08 21:10:20.865422: \n",
      "2025-12-08 21:10:20.865422: Epoch 858\n",
      "2025-12-08 21:10:20.871698: Current learning rate: 0.00173\n",
      "2025-12-08 21:12:41.916282: train_loss -0.8581\n",
      "2025-12-08 21:12:41.917288: val_loss -0.8786\n",
      "2025-12-08 21:12:41.923925: Pseudo dice [0.9289, 0.9533, 0.951]\n",
      "2025-12-08 21:12:41.929927: Epoch time: 141.06 s\n",
      "2025-12-08 21:12:42.702041: \n",
      "2025-12-08 21:12:42.702041: Epoch 859\n",
      "2025-12-08 21:12:42.710253: Current learning rate: 0.00172\n",
      "2025-12-08 21:15:03.118529: train_loss -0.8599\n",
      "2025-12-08 21:15:03.119529: val_loss -0.8761\n",
      "2025-12-08 21:15:03.128207: Pseudo dice [0.9212, 0.955, 0.9516]\n",
      "2025-12-08 21:15:03.135149: Epoch time: 140.42 s\n",
      "2025-12-08 21:15:04.085816: \n",
      "2025-12-08 21:15:04.085816: Epoch 860\n",
      "2025-12-08 21:15:04.088742: Current learning rate: 0.0017\n",
      "2025-12-08 21:17:24.331582: train_loss -0.8577\n",
      "2025-12-08 21:17:24.332582: val_loss -0.8793\n",
      "2025-12-08 21:17:24.341471: Pseudo dice [0.9261, 0.9546, 0.9498]\n",
      "2025-12-08 21:17:24.347914: Epoch time: 140.25 s\n",
      "2025-12-08 21:17:25.018127: \n",
      "2025-12-08 21:17:25.018127: Epoch 861\n",
      "2025-12-08 21:17:25.026038: Current learning rate: 0.00169\n",
      "2025-12-08 21:19:45.673506: train_loss -0.8546\n",
      "2025-12-08 21:19:45.673506: val_loss -0.8708\n",
      "2025-12-08 21:19:45.679512: Pseudo dice [0.9191, 0.9523, 0.9491]\n",
      "2025-12-08 21:19:45.685518: Epoch time: 140.66 s\n",
      "2025-12-08 21:19:46.339595: \n",
      "2025-12-08 21:19:46.339595: Epoch 862\n",
      "2025-12-08 21:19:46.346446: Current learning rate: 0.00168\n",
      "2025-12-08 21:22:06.889476: train_loss -0.8525\n",
      "2025-12-08 21:22:06.890523: val_loss -0.8894\n",
      "2025-12-08 21:22:06.896544: Pseudo dice [0.9341, 0.96, 0.9458]\n",
      "2025-12-08 21:22:06.902570: Epoch time: 140.55 s\n",
      "2025-12-08 21:22:07.554304: \n",
      "2025-12-08 21:22:07.555304: Epoch 863\n",
      "2025-12-08 21:22:07.561334: Current learning rate: 0.00167\n",
      "2025-12-08 21:24:27.262975: train_loss -0.8541\n",
      "2025-12-08 21:24:27.263976: val_loss -0.8802\n",
      "2025-12-08 21:24:27.270982: Pseudo dice [0.9268, 0.9564, 0.9506]\n",
      "2025-12-08 21:24:27.277278: Epoch time: 139.71 s\n",
      "2025-12-08 21:24:28.105320: \n",
      "2025-12-08 21:24:28.105320: Epoch 864\n",
      "2025-12-08 21:24:28.112329: Current learning rate: 0.00166\n",
      "2025-12-08 21:26:48.230374: train_loss -0.8571\n",
      "2025-12-08 21:26:48.230374: val_loss -0.8724\n",
      "2025-12-08 21:26:48.238349: Pseudo dice [0.9232, 0.9493, 0.9506]\n",
      "2025-12-08 21:26:48.243042: Epoch time: 140.13 s\n",
      "2025-12-08 21:26:48.873294: \n",
      "2025-12-08 21:26:48.873294: Epoch 865\n",
      "2025-12-08 21:26:48.877955: Current learning rate: 0.00165\n",
      "2025-12-08 21:29:10.475772: train_loss -0.855\n",
      "2025-12-08 21:29:10.475772: val_loss -0.8629\n",
      "2025-12-08 21:29:10.483131: Pseudo dice [0.9171, 0.9469, 0.941]\n",
      "2025-12-08 21:29:10.489356: Epoch time: 141.6 s\n",
      "2025-12-08 21:29:11.312366: \n",
      "2025-12-08 21:29:11.312366: Epoch 866\n",
      "2025-12-08 21:29:11.328161: Current learning rate: 0.00164\n",
      "2025-12-08 21:31:32.805335: train_loss -0.8595\n",
      "2025-12-08 21:31:32.805335: val_loss -0.8812\n",
      "2025-12-08 21:31:32.811336: Pseudo dice [0.9255, 0.9605, 0.951]\n",
      "2025-12-08 21:31:32.817176: Epoch time: 141.49 s\n",
      "2025-12-08 21:31:33.457348: \n",
      "2025-12-08 21:31:33.457348: Epoch 867\n",
      "2025-12-08 21:31:33.464374: Current learning rate: 0.00163\n",
      "2025-12-08 21:33:54.321192: train_loss -0.859\n",
      "2025-12-08 21:33:54.322195: val_loss -0.8866\n",
      "2025-12-08 21:33:54.327221: Pseudo dice [0.9311, 0.9589, 0.9502]\n",
      "2025-12-08 21:33:54.334760: Epoch time: 140.86 s\n",
      "2025-12-08 21:33:55.003225: \n",
      "2025-12-08 21:33:55.005228: Epoch 868\n",
      "2025-12-08 21:33:55.005228: Current learning rate: 0.00162\n",
      "2025-12-08 21:36:13.499388: train_loss -0.8625\n",
      "2025-12-08 21:36:13.506887: val_loss -0.8785\n",
      "2025-12-08 21:36:13.512893: Pseudo dice [0.9255, 0.953, 0.9545]\n",
      "2025-12-08 21:36:13.518517: Epoch time: 138.5 s\n",
      "2025-12-08 21:36:14.202589: \n",
      "2025-12-08 21:36:14.202589: Epoch 869\n",
      "2025-12-08 21:36:14.206095: Current learning rate: 0.00161\n",
      "2025-12-08 21:38:32.749557: train_loss -0.8629\n",
      "2025-12-08 21:38:32.749557: val_loss -0.8825\n",
      "2025-12-08 21:38:32.765322: Pseudo dice [0.9282, 0.9544, 0.9518]\n",
      "2025-12-08 21:38:32.765322: Epoch time: 138.55 s\n",
      "2025-12-08 21:38:33.421762: \n",
      "2025-12-08 21:38:33.421762: Epoch 870\n",
      "2025-12-08 21:38:33.441218: Current learning rate: 0.00159\n",
      "2025-12-08 21:40:52.140175: train_loss -0.8615\n",
      "2025-12-08 21:40:52.155946: val_loss -0.8763\n",
      "2025-12-08 21:40:52.164306: Pseudo dice [0.9238, 0.9542, 0.9492]\n",
      "2025-12-08 21:40:52.169307: Epoch time: 138.72 s\n",
      "2025-12-08 21:40:52.874351: \n",
      "2025-12-08 21:40:52.874351: Epoch 871\n",
      "2025-12-08 21:40:52.874351: Current learning rate: 0.00158\n",
      "2025-12-08 21:43:11.198414: train_loss -0.8636\n",
      "2025-12-08 21:43:11.199415: val_loss -0.889\n",
      "2025-12-08 21:43:11.207117: Pseudo dice [0.9325, 0.9595, 0.9505]\n",
      "2025-12-08 21:43:11.213122: Epoch time: 138.32 s\n",
      "2025-12-08 21:43:11.848518: \n",
      "2025-12-08 21:43:11.848518: Epoch 872\n",
      "2025-12-08 21:43:11.854541: Current learning rate: 0.00157\n",
      "2025-12-08 21:45:30.452594: train_loss -0.8607\n",
      "2025-12-08 21:45:30.452594: val_loss -0.8753\n",
      "2025-12-08 21:45:30.458177: Pseudo dice [0.9231, 0.9532, 0.9475]\n",
      "2025-12-08 21:45:30.464183: Epoch time: 138.61 s\n",
      "2025-12-08 21:45:31.406051: \n",
      "2025-12-08 21:45:31.406051: Epoch 873\n",
      "2025-12-08 21:45:31.406051: Current learning rate: 0.00156\n",
      "2025-12-08 21:47:49.483148: train_loss -0.861\n",
      "2025-12-08 21:47:49.498960: val_loss -0.8881\n",
      "2025-12-08 21:47:49.498960: Pseudo dice [0.9297, 0.9632, 0.9494]\n",
      "2025-12-08 21:47:49.498960: Epoch time: 138.08 s\n",
      "2025-12-08 21:47:50.139557: \n",
      "2025-12-08 21:47:50.140624: Epoch 874\n",
      "2025-12-08 21:47:50.146776: Current learning rate: 0.00155\n",
      "2025-12-08 21:50:07.967714: train_loss -0.8636\n",
      "2025-12-08 21:50:07.967714: val_loss -0.8847\n",
      "2025-12-08 21:50:07.983816: Pseudo dice [0.9278, 0.9603, 0.953]\n",
      "2025-12-08 21:50:07.983816: Epoch time: 137.83 s\n",
      "2025-12-08 21:50:08.614652: \n",
      "2025-12-08 21:50:08.614652: Epoch 875\n",
      "2025-12-08 21:50:08.620672: Current learning rate: 0.00154\n",
      "2025-12-08 21:52:26.723044: train_loss -0.8579\n",
      "2025-12-08 21:52:26.723044: val_loss -0.8813\n",
      "2025-12-08 21:52:26.723044: Pseudo dice [0.9246, 0.9565, 0.9518]\n",
      "2025-12-08 21:52:26.733360: Epoch time: 138.11 s\n",
      "2025-12-08 21:52:27.356089: \n",
      "2025-12-08 21:52:27.356089: Epoch 876\n",
      "2025-12-08 21:52:27.357830: Current learning rate: 0.00153\n",
      "2025-12-08 21:54:45.450783: train_loss -0.8595\n",
      "2025-12-08 21:54:45.450783: val_loss -0.8794\n",
      "2025-12-08 21:54:45.458277: Pseudo dice [0.9254, 0.9556, 0.9504]\n",
      "2025-12-08 21:54:45.462283: Epoch time: 138.09 s\n",
      "2025-12-08 21:54:46.158461: \n",
      "2025-12-08 21:54:46.158461: Epoch 877\n",
      "2025-12-08 21:54:46.165479: Current learning rate: 0.00152\n",
      "2025-12-08 21:57:04.170472: train_loss -0.864\n",
      "2025-12-08 21:57:04.172474: val_loss -0.8712\n",
      "2025-12-08 21:57:04.178480: Pseudo dice [0.9191, 0.9507, 0.9546]\n",
      "2025-12-08 21:57:04.184486: Epoch time: 138.01 s\n",
      "2025-12-08 21:57:04.884621: \n",
      "2025-12-08 21:57:04.886361: Epoch 878\n",
      "2025-12-08 21:57:04.889124: Current learning rate: 0.00151\n",
      "2025-12-08 21:59:23.076955: train_loss -0.8618\n",
      "2025-12-08 21:59:23.078959: val_loss -0.8692\n",
      "2025-12-08 21:59:23.088970: Pseudo dice [0.9169, 0.9521, 0.9463]\n",
      "2025-12-08 21:59:23.094714: Epoch time: 138.19 s\n",
      "2025-12-08 21:59:23.889063: \n",
      "2025-12-08 21:59:23.889063: Epoch 879\n",
      "2025-12-08 21:59:23.904813: Current learning rate: 0.00149\n",
      "2025-12-08 22:01:41.844756: train_loss -0.8625\n",
      "2025-12-08 22:01:41.846759: val_loss -0.8849\n",
      "2025-12-08 22:01:41.854505: Pseudo dice [0.9326, 0.9594, 0.9481]\n",
      "2025-12-08 22:01:41.863594: Epoch time: 137.96 s\n",
      "2025-12-08 22:01:42.501500: \n",
      "2025-12-08 22:01:42.501500: Epoch 880\n",
      "2025-12-08 22:01:42.501500: Current learning rate: 0.00148\n",
      "2025-12-08 22:04:00.426143: train_loss -0.8583\n",
      "2025-12-08 22:04:00.426143: val_loss -0.8804\n",
      "2025-12-08 22:04:00.436153: Pseudo dice [0.926, 0.9559, 0.9498]\n",
      "2025-12-08 22:04:00.443901: Epoch time: 137.93 s\n",
      "2025-12-08 22:04:01.191188: \n",
      "2025-12-08 22:04:01.192188: Epoch 881\n",
      "2025-12-08 22:04:01.197696: Current learning rate: 0.00147\n",
      "2025-12-08 22:06:19.326842: train_loss -0.8625\n",
      "2025-12-08 22:06:19.326842: val_loss -0.8805\n",
      "2025-12-08 22:06:19.337438: Pseudo dice [0.9249, 0.9559, 0.9506]\n",
      "2025-12-08 22:06:19.342943: Epoch time: 138.14 s\n",
      "2025-12-08 22:06:20.014690: \n",
      "2025-12-08 22:06:20.014690: Epoch 882\n",
      "2025-12-08 22:06:20.014690: Current learning rate: 0.00146\n",
      "2025-12-08 22:08:38.093192: train_loss -0.8574\n",
      "2025-12-08 22:08:38.093192: val_loss -0.8873\n",
      "2025-12-08 22:08:38.104438: Pseudo dice [0.9313, 0.9634, 0.9529]\n",
      "2025-12-08 22:08:38.108442: Epoch time: 138.08 s\n",
      "2025-12-08 22:08:38.810440: \n",
      "2025-12-08 22:08:38.811445: Epoch 883\n",
      "2025-12-08 22:08:38.811445: Current learning rate: 0.00145\n",
      "2025-12-08 22:10:56.702685: train_loss -0.8604\n",
      "2025-12-08 22:10:56.702685: val_loss -0.8678\n",
      "2025-12-08 22:10:56.710687: Pseudo dice [0.9167, 0.9519, 0.9478]\n",
      "2025-12-08 22:10:56.715696: Epoch time: 137.89 s\n",
      "2025-12-08 22:10:57.467975: \n",
      "2025-12-08 22:10:57.468980: Epoch 884\n",
      "2025-12-08 22:10:57.476024: Current learning rate: 0.00144\n",
      "2025-12-08 22:13:15.498992: train_loss -0.8595\n",
      "2025-12-08 22:13:15.498992: val_loss -0.8835\n",
      "2025-12-08 22:13:15.498992: Pseudo dice [0.9268, 0.9584, 0.953]\n",
      "2025-12-08 22:13:15.498992: Epoch time: 138.03 s\n",
      "2025-12-08 22:13:16.125149: \n",
      "2025-12-08 22:13:16.125149: Epoch 885\n",
      "2025-12-08 22:13:16.125149: Current learning rate: 0.00143\n",
      "2025-12-08 22:15:34.179533: train_loss -0.8609\n",
      "2025-12-08 22:15:34.181535: val_loss -0.8883\n",
      "2025-12-08 22:15:34.187541: Pseudo dice [0.9312, 0.9626, 0.9537]\n",
      "2025-12-08 22:15:34.192547: Epoch time: 138.05 s\n",
      "2025-12-08 22:15:35.062373: \n",
      "2025-12-08 22:15:35.078081: Epoch 886\n",
      "2025-12-08 22:15:35.078081: Current learning rate: 0.00142\n",
      "2025-12-08 22:17:53.236816: train_loss -0.8613\n",
      "2025-12-08 22:17:53.237818: val_loss -0.8897\n",
      "2025-12-08 22:17:53.244820: Pseudo dice [0.9315, 0.9597, 0.954]\n",
      "2025-12-08 22:17:53.250865: Epoch time: 138.17 s\n",
      "2025-12-08 22:17:53.952148: \n",
      "2025-12-08 22:17:53.952148: Epoch 887\n",
      "2025-12-08 22:17:53.967852: Current learning rate: 0.00141\n",
      "2025-12-08 22:20:11.998894: train_loss -0.86\n",
      "2025-12-08 22:20:11.998894: val_loss -0.8767\n",
      "2025-12-08 22:20:12.014554: Pseudo dice [0.9242, 0.9562, 0.9515]\n",
      "2025-12-08 22:20:12.014554: Epoch time: 138.05 s\n",
      "2025-12-08 22:20:12.639304: \n",
      "2025-12-08 22:20:12.639304: Epoch 888\n",
      "2025-12-08 22:20:12.639304: Current learning rate: 0.00139\n",
      "2025-12-08 22:22:30.558100: train_loss -0.8604\n",
      "2025-12-08 22:22:30.558100: val_loss -0.8739\n",
      "2025-12-08 22:22:30.561843: Pseudo dice [0.9195, 0.9534, 0.9565]\n",
      "2025-12-08 22:22:30.567724: Epoch time: 137.92 s\n",
      "2025-12-08 22:22:31.235452: \n",
      "2025-12-08 22:22:31.235452: Epoch 889\n",
      "2025-12-08 22:22:31.251581: Current learning rate: 0.00138\n",
      "2025-12-08 22:24:49.188931: train_loss -0.8659\n",
      "2025-12-08 22:24:49.188931: val_loss -0.8789\n",
      "2025-12-08 22:24:49.202699: Pseudo dice [0.9251, 0.9549, 0.9517]\n",
      "2025-12-08 22:24:49.208707: Epoch time: 137.95 s\n",
      "2025-12-08 22:24:49.859715: \n",
      "2025-12-08 22:24:49.859715: Epoch 890\n",
      "2025-12-08 22:24:49.875382: Current learning rate: 0.00137\n",
      "2025-12-08 22:27:08.209919: train_loss -0.8569\n",
      "2025-12-08 22:27:08.209919: val_loss -0.8911\n",
      "2025-12-08 22:27:08.219671: Pseudo dice [0.9322, 0.9656, 0.9523]\n",
      "2025-12-08 22:27:08.229685: Epoch time: 138.35 s\n",
      "2025-12-08 22:27:08.237433: Yayy! New best EMA pseudo Dice: 0.9451\n",
      "2025-12-08 22:27:09.221732: \n",
      "2025-12-08 22:27:09.221732: Epoch 891\n",
      "2025-12-08 22:27:09.221732: Current learning rate: 0.00136\n",
      "2025-12-08 22:29:27.490243: train_loss -0.8578\n",
      "2025-12-08 22:29:27.490243: val_loss -0.8854\n",
      "2025-12-08 22:29:27.496248: Pseudo dice [0.9305, 0.9593, 0.9496]\n",
      "2025-12-08 22:29:27.500252: Epoch time: 138.27 s\n",
      "2025-12-08 22:29:27.506665: Yayy! New best EMA pseudo Dice: 0.9452\n",
      "2025-12-08 22:29:28.729039: \n",
      "2025-12-08 22:29:28.730057: Epoch 892\n",
      "2025-12-08 22:29:28.736173: Current learning rate: 0.00135\n",
      "2025-12-08 22:31:46.659729: train_loss -0.8598\n",
      "2025-12-08 22:31:46.659729: val_loss -0.8869\n",
      "2025-12-08 22:31:46.668731: Pseudo dice [0.9314, 0.9628, 0.9513]\n",
      "2025-12-08 22:31:46.676522: Epoch time: 137.93 s\n",
      "2025-12-08 22:31:46.682527: Yayy! New best EMA pseudo Dice: 0.9456\n",
      "2025-12-08 22:31:47.770421: \n",
      "2025-12-08 22:31:47.770421: Epoch 893\n",
      "2025-12-08 22:31:47.777450: Current learning rate: 0.00134\n",
      "2025-12-08 22:34:05.751729: train_loss -0.8589\n",
      "2025-12-08 22:34:05.751729: val_loss -0.8813\n",
      "2025-12-08 22:34:05.761739: Pseudo dice [0.9243, 0.9593, 0.9536]\n",
      "2025-12-08 22:34:05.767484: Epoch time: 137.98 s\n",
      "2025-12-08 22:34:05.773489: Yayy! New best EMA pseudo Dice: 0.9456\n",
      "2025-12-08 22:34:06.718661: \n",
      "2025-12-08 22:34:06.718661: Epoch 894\n",
      "2025-12-08 22:34:06.734374: Current learning rate: 0.00133\n",
      "2025-12-08 22:36:24.640201: train_loss -0.8666\n",
      "2025-12-08 22:36:24.640201: val_loss -0.8818\n",
      "2025-12-08 22:36:24.655457: Pseudo dice [0.9265, 0.9564, 0.9516]\n",
      "2025-12-08 22:36:24.661721: Epoch time: 137.92 s\n",
      "2025-12-08 22:36:25.295728: \n",
      "2025-12-08 22:36:25.295728: Epoch 895\n",
      "2025-12-08 22:36:25.295728: Current learning rate: 0.00132\n",
      "2025-12-08 22:38:43.231928: train_loss -0.8609\n",
      "2025-12-08 22:38:43.233668: val_loss -0.8941\n",
      "2025-12-08 22:38:43.241679: Pseudo dice [0.9325, 0.9667, 0.9542]\n",
      "2025-12-08 22:38:43.249425: Epoch time: 137.94 s\n",
      "2025-12-08 22:38:43.256501: Yayy! New best EMA pseudo Dice: 0.9461\n",
      "2025-12-08 22:38:44.180835: \n",
      "2025-12-08 22:38:44.180835: Epoch 896\n",
      "2025-12-08 22:38:44.186863: Current learning rate: 0.0013\n",
      "2025-12-08 22:41:02.190971: train_loss -0.8609\n",
      "2025-12-08 22:41:02.200574: val_loss -0.8717\n",
      "2025-12-08 22:41:02.208055: Pseudo dice [0.9185, 0.9529, 0.952]\n",
      "2025-12-08 22:41:02.215539: Epoch time: 138.02 s\n",
      "2025-12-08 22:41:02.842445: \n",
      "2025-12-08 22:41:02.842445: Epoch 897\n",
      "2025-12-08 22:41:02.842445: Current learning rate: 0.00129\n",
      "2025-12-08 22:43:20.907752: train_loss -0.8627\n",
      "2025-12-08 22:43:20.907752: val_loss -0.8887\n",
      "2025-12-08 22:43:20.915497: Pseudo dice [0.9305, 0.9604, 0.9505]\n",
      "2025-12-08 22:43:20.921241: Epoch time: 138.07 s\n",
      "2025-12-08 22:43:21.715903: \n",
      "2025-12-08 22:43:21.715903: Epoch 898\n",
      "2025-12-08 22:43:21.723046: Current learning rate: 0.00128\n",
      "2025-12-08 22:45:39.736147: train_loss -0.8588\n",
      "2025-12-08 22:45:39.736147: val_loss -0.8792\n",
      "2025-12-08 22:45:39.751767: Pseudo dice [0.9294, 0.9563, 0.9402]\n",
      "2025-12-08 22:45:39.751767: Epoch time: 138.02 s\n",
      "2025-12-08 22:45:40.376297: \n",
      "2025-12-08 22:45:40.376297: Epoch 899\n",
      "2025-12-08 22:45:40.390313: Current learning rate: 0.00127\n",
      "2025-12-08 22:47:58.344433: train_loss -0.8601\n",
      "2025-12-08 22:47:58.344433: val_loss -0.8874\n",
      "2025-12-08 22:47:58.360085: Pseudo dice [0.9322, 0.96, 0.953]\n",
      "2025-12-08 22:47:58.360085: Epoch time: 137.97 s\n",
      "2025-12-08 22:47:59.289320: \n",
      "2025-12-08 22:47:59.290323: Epoch 900\n",
      "2025-12-08 22:47:59.296319: Current learning rate: 0.00126\n",
      "2025-12-08 22:50:17.514726: train_loss -0.8584\n",
      "2025-12-08 22:50:17.514726: val_loss -0.8851\n",
      "2025-12-08 22:50:17.522221: Pseudo dice [0.9273, 0.9536, 0.9546]\n",
      "2025-12-08 22:50:17.528227: Epoch time: 138.23 s\n",
      "2025-12-08 22:50:18.165370: \n",
      "2025-12-08 22:50:18.165370: Epoch 901\n",
      "2025-12-08 22:50:18.171767: Current learning rate: 0.00125\n",
      "2025-12-08 22:52:36.173424: train_loss -0.8587\n",
      "2025-12-08 22:52:36.175427: val_loss -0.8747\n",
      "2025-12-08 22:52:36.180800: Pseudo dice [0.9201, 0.9509, 0.9505]\n",
      "2025-12-08 22:52:36.186806: Epoch time: 138.01 s\n",
      "2025-12-08 22:52:36.889045: \n",
      "2025-12-08 22:52:36.889045: Epoch 902\n",
      "2025-12-08 22:52:36.905158: Current learning rate: 0.00124\n",
      "2025-12-08 22:54:54.796586: train_loss -0.8591\n",
      "2025-12-08 22:54:54.796586: val_loss -0.8848\n",
      "2025-12-08 22:54:54.807936: Pseudo dice [0.9287, 0.9556, 0.9522]\n",
      "2025-12-08 22:54:54.815944: Epoch time: 137.91 s\n",
      "2025-12-08 22:54:55.437339: \n",
      "2025-12-08 22:54:55.437339: Epoch 903\n",
      "2025-12-08 22:54:55.453102: Current learning rate: 0.00122\n",
      "2025-12-08 22:57:13.640755: train_loss -0.8582\n",
      "2025-12-08 22:57:13.640755: val_loss -0.8817\n",
      "2025-12-08 22:57:13.650427: Pseudo dice [0.9277, 0.9578, 0.9521]\n",
      "2025-12-08 22:57:13.656440: Epoch time: 138.2 s\n",
      "2025-12-08 22:57:14.422769: \n",
      "2025-12-08 22:57:14.422769: Epoch 904\n",
      "2025-12-08 22:57:14.441635: Current learning rate: 0.00121\n",
      "2025-12-08 22:59:32.436190: train_loss -0.8566\n",
      "2025-12-08 22:59:32.436190: val_loss -0.8837\n",
      "2025-12-08 22:59:32.443238: Pseudo dice [0.9274, 0.9602, 0.9494]\n",
      "2025-12-08 22:59:32.449259: Epoch time: 138.01 s\n",
      "2025-12-08 22:59:33.249120: \n",
      "2025-12-08 22:59:33.249120: Epoch 905\n",
      "2025-12-08 22:59:33.249120: Current learning rate: 0.0012\n",
      "2025-12-08 23:01:51.368309: train_loss -0.8617\n",
      "2025-12-08 23:01:51.368309: val_loss -0.8718\n",
      "2025-12-08 23:01:51.380071: Pseudo dice [0.9181, 0.9512, 0.9537]\n",
      "2025-12-08 23:01:51.388081: Epoch time: 138.12 s\n",
      "2025-12-08 23:01:52.014668: \n",
      "2025-12-08 23:01:52.014668: Epoch 906\n",
      "2025-12-08 23:01:52.014668: Current learning rate: 0.00119\n",
      "2025-12-08 23:04:10.116211: train_loss -0.861\n",
      "2025-12-08 23:04:10.116211: val_loss -0.877\n",
      "2025-12-08 23:04:10.122217: Pseudo dice [0.921, 0.9551, 0.951]\n",
      "2025-12-08 23:04:10.129313: Epoch time: 138.1 s\n",
      "2025-12-08 23:04:10.811831: \n",
      "2025-12-08 23:04:10.827578: Epoch 907\n",
      "2025-12-08 23:04:10.827578: Current learning rate: 0.00118\n",
      "2025-12-08 23:06:28.966268: train_loss -0.8582\n",
      "2025-12-08 23:06:28.968009: val_loss -0.8812\n",
      "2025-12-08 23:06:28.974829: Pseudo dice [0.9264, 0.9615, 0.9494]\n",
      "2025-12-08 23:06:28.978833: Epoch time: 138.15 s\n",
      "2025-12-08 23:06:29.608177: \n",
      "2025-12-08 23:06:29.608177: Epoch 908\n",
      "2025-12-08 23:06:29.608177: Current learning rate: 0.00117\n",
      "2025-12-08 23:08:47.782441: train_loss -0.8607\n",
      "2025-12-08 23:08:47.782441: val_loss -0.8907\n",
      "2025-12-08 23:08:47.800372: Pseudo dice [0.9317, 0.959, 0.9561]\n",
      "2025-12-08 23:08:47.805387: Epoch time: 138.17 s\n",
      "2025-12-08 23:08:48.467901: \n",
      "2025-12-08 23:08:48.467901: Epoch 909\n",
      "2025-12-08 23:08:48.467901: Current learning rate: 0.00116\n",
      "2025-12-08 23:11:06.564006: train_loss -0.8595\n",
      "2025-12-08 23:11:06.565008: val_loss -0.8835\n",
      "2025-12-08 23:11:06.571022: Pseudo dice [0.9289, 0.9574, 0.9554]\n",
      "2025-12-08 23:11:06.577047: Epoch time: 138.1 s\n",
      "2025-12-08 23:11:07.265009: \n",
      "2025-12-08 23:11:07.265009: Epoch 910\n",
      "2025-12-08 23:11:07.265009: Current learning rate: 0.00115\n",
      "2025-12-08 23:13:25.530153: train_loss -0.858\n",
      "2025-12-08 23:13:25.530153: val_loss -0.8858\n",
      "2025-12-08 23:13:25.547807: Pseudo dice [0.9306, 0.9628, 0.9497]\n",
      "2025-12-08 23:13:25.547807: Epoch time: 138.27 s\n",
      "2025-12-08 23:13:26.344623: \n",
      "2025-12-08 23:13:26.344623: Epoch 911\n",
      "2025-12-08 23:13:26.344623: Current learning rate: 0.00113\n",
      "2025-12-08 23:15:44.509355: train_loss -0.8583\n",
      "2025-12-08 23:15:44.509355: val_loss -0.8912\n",
      "2025-12-08 23:15:44.517368: Pseudo dice [0.9337, 0.961, 0.9473]\n",
      "2025-12-08 23:15:44.523370: Epoch time: 138.16 s\n",
      "2025-12-08 23:15:45.209562: \n",
      "2025-12-08 23:15:45.209562: Epoch 912\n",
      "2025-12-08 23:15:45.209562: Current learning rate: 0.00112\n",
      "2025-12-08 23:18:03.229896: train_loss -0.8647\n",
      "2025-12-08 23:18:03.231898: val_loss -0.8835\n",
      "2025-12-08 23:18:03.239645: Pseudo dice [0.9281, 0.9571, 0.949]\n",
      "2025-12-08 23:18:03.243648: Epoch time: 138.02 s\n",
      "2025-12-08 23:18:03.983905: \n",
      "2025-12-08 23:18:03.983905: Epoch 913\n",
      "2025-12-08 23:18:04.001696: Current learning rate: 0.00111\n",
      "2025-12-08 23:20:22.108779: train_loss -0.8608\n",
      "2025-12-08 23:20:22.108779: val_loss -0.8902\n",
      "2025-12-08 23:20:22.118290: Pseudo dice [0.9302, 0.9596, 0.9555]\n",
      "2025-12-08 23:20:22.122294: Epoch time: 138.12 s\n",
      "2025-12-08 23:20:22.758726: \n",
      "2025-12-08 23:20:22.759738: Epoch 914\n",
      "2025-12-08 23:20:22.766513: Current learning rate: 0.0011\n",
      "2025-12-08 23:22:40.584112: train_loss -0.8621\n",
      "2025-12-08 23:22:40.586115: val_loss -0.8874\n",
      "2025-12-08 23:22:40.593861: Pseudo dice [0.9276, 0.9589, 0.9537]\n",
      "2025-12-08 23:22:40.599867: Epoch time: 137.83 s\n",
      "2025-12-08 23:22:41.232179: \n",
      "2025-12-08 23:22:41.232179: Epoch 915\n",
      "2025-12-08 23:22:41.233184: Current learning rate: 0.00109\n",
      "2025-12-08 23:24:59.337909: train_loss -0.86\n",
      "2025-12-08 23:24:59.339912: val_loss -0.8854\n",
      "2025-12-08 23:24:59.345658: Pseudo dice [0.9305, 0.9569, 0.9514]\n",
      "2025-12-08 23:24:59.351664: Epoch time: 138.11 s\n",
      "2025-12-08 23:24:59.968017: \n",
      "2025-12-08 23:24:59.968017: Epoch 916\n",
      "2025-12-08 23:24:59.983812: Current learning rate: 0.00108\n",
      "2025-12-08 23:27:18.297062: train_loss -0.8564\n",
      "2025-12-08 23:27:18.297062: val_loss -0.8716\n",
      "2025-12-08 23:27:18.304075: Pseudo dice [0.9211, 0.9512, 0.9451]\n",
      "2025-12-08 23:27:18.310076: Epoch time: 138.33 s\n",
      "2025-12-08 23:27:19.108407: \n",
      "2025-12-08 23:27:19.108407: Epoch 917\n",
      "2025-12-08 23:27:19.108407: Current learning rate: 0.00106\n",
      "2025-12-08 23:29:37.317205: train_loss -0.8618\n",
      "2025-12-08 23:29:37.317205: val_loss -0.8847\n",
      "2025-12-08 23:29:37.325213: Pseudo dice [0.931, 0.958, 0.9469]\n",
      "2025-12-08 23:29:37.330716: Epoch time: 138.21 s\n",
      "2025-12-08 23:29:38.015276: \n",
      "2025-12-08 23:29:38.015276: Epoch 918\n",
      "2025-12-08 23:29:38.022293: Current learning rate: 0.00105\n",
      "2025-12-08 23:31:55.987942: train_loss -0.8651\n",
      "2025-12-08 23:31:55.987942: val_loss -0.8799\n",
      "2025-12-08 23:31:55.999693: Pseudo dice [0.9269, 0.9578, 0.9473]\n",
      "2025-12-08 23:31:56.011711: Epoch time: 137.97 s\n",
      "2025-12-08 23:31:56.648056: \n",
      "2025-12-08 23:31:56.648056: Epoch 919\n",
      "2025-12-08 23:31:56.655580: Current learning rate: 0.00104\n",
      "2025-12-08 23:34:14.614665: train_loss -0.8643\n",
      "2025-12-08 23:34:14.614665: val_loss -0.8836\n",
      "2025-12-08 23:34:14.622673: Pseudo dice [0.9287, 0.956, 0.9518]\n",
      "2025-12-08 23:34:14.628290: Epoch time: 137.97 s\n",
      "2025-12-08 23:34:15.252282: \n",
      "2025-12-08 23:34:15.253282: Epoch 920\n",
      "2025-12-08 23:34:15.258828: Current learning rate: 0.00103\n",
      "2025-12-08 23:36:33.295356: train_loss -0.8654\n",
      "2025-12-08 23:36:33.295356: val_loss -0.8828\n",
      "2025-12-08 23:36:33.304069: Pseudo dice [0.9267, 0.9576, 0.954]\n",
      "2025-12-08 23:36:33.308873: Epoch time: 138.04 s\n",
      "2025-12-08 23:36:33.967107: \n",
      "2025-12-08 23:36:33.967107: Epoch 921\n",
      "2025-12-08 23:36:33.982939: Current learning rate: 0.00102\n",
      "2025-12-08 23:38:52.155399: train_loss -0.859\n",
      "2025-12-08 23:38:52.155399: val_loss -0.8824\n",
      "2025-12-08 23:38:52.167423: Pseudo dice [0.927, 0.9572, 0.951]\n",
      "2025-12-08 23:38:52.175170: Epoch time: 138.19 s\n",
      "2025-12-08 23:38:52.795760: \n",
      "2025-12-08 23:38:52.795760: Epoch 922\n",
      "2025-12-08 23:38:52.811668: Current learning rate: 0.00101\n",
      "2025-12-08 23:41:12.357891: train_loss -0.8637\n",
      "2025-12-08 23:41:12.357891: val_loss -0.8775\n",
      "2025-12-08 23:41:12.364308: Pseudo dice [0.925, 0.9566, 0.9495]\n",
      "2025-12-08 23:41:12.368819: Epoch time: 139.56 s\n",
      "2025-12-08 23:41:12.991977: \n",
      "2025-12-08 23:41:12.991977: Epoch 923\n",
      "2025-12-08 23:41:12.997108: Current learning rate: 0.001\n",
      "2025-12-08 23:43:31.474879: train_loss -0.8646\n",
      "2025-12-08 23:43:31.474879: val_loss -0.8865\n",
      "2025-12-08 23:43:31.484891: Pseudo dice [0.931, 0.96, 0.9514]\n",
      "2025-12-08 23:43:31.490636: Epoch time: 138.5 s\n",
      "2025-12-08 23:43:32.295345: \n",
      "2025-12-08 23:43:32.295345: Epoch 924\n",
      "2025-12-08 23:43:32.295345: Current learning rate: 0.00098\n",
      "2025-12-08 23:45:50.639628: train_loss -0.866\n",
      "2025-12-08 23:45:50.639628: val_loss -0.8926\n",
      "2025-12-08 23:45:50.647376: Pseudo dice [0.934, 0.9597, 0.9518]\n",
      "2025-12-08 23:45:50.653839: Epoch time: 138.34 s\n",
      "2025-12-08 23:45:51.396692: \n",
      "2025-12-08 23:45:51.396692: Epoch 925\n",
      "2025-12-08 23:45:51.404704: Current learning rate: 0.00097\n",
      "2025-12-08 23:48:09.797849: train_loss -0.8641\n",
      "2025-12-08 23:48:09.797849: val_loss -0.8894\n",
      "2025-12-08 23:48:09.797849: Pseudo dice [0.9326, 0.9621, 0.9511]\n",
      "2025-12-08 23:48:09.807821: Epoch time: 138.4 s\n",
      "2025-12-08 23:48:10.433750: \n",
      "2025-12-08 23:48:10.433750: Epoch 926\n",
      "2025-12-08 23:48:10.433750: Current learning rate: 0.00096\n",
      "2025-12-08 23:50:28.875886: train_loss -0.8646\n",
      "2025-12-08 23:50:28.875886: val_loss -0.8941\n",
      "2025-12-08 23:50:28.881891: Pseudo dice [0.9339, 0.9636, 0.9516]\n",
      "2025-12-08 23:50:28.889260: Epoch time: 138.44 s\n",
      "2025-12-08 23:50:28.895266: Yayy! New best EMA pseudo Dice: 0.9463\n",
      "2025-12-08 23:50:29.798206: \n",
      "2025-12-08 23:50:29.798206: Epoch 927\n",
      "2025-12-08 23:50:29.804480: Current learning rate: 0.00095\n",
      "2025-12-08 23:52:47.961868: train_loss -0.8594\n",
      "2025-12-08 23:52:47.961868: val_loss -0.8813\n",
      "2025-12-08 23:52:47.977778: Pseudo dice [0.9236, 0.9567, 0.9502]\n",
      "2025-12-08 23:52:47.977778: Epoch time: 138.17 s\n",
      "2025-12-08 23:52:48.838809: \n",
      "2025-12-08 23:52:48.838809: Epoch 928\n",
      "2025-12-08 23:52:48.838809: Current learning rate: 0.00094\n",
      "2025-12-08 23:55:06.899708: train_loss -0.8655\n",
      "2025-12-08 23:55:06.899708: val_loss -0.887\n",
      "2025-12-08 23:55:06.899708: Pseudo dice [0.9289, 0.9589, 0.9506]\n",
      "2025-12-08 23:55:06.915591: Epoch time: 138.06 s\n",
      "2025-12-08 23:55:07.532890: \n",
      "2025-12-08 23:55:07.532890: Epoch 929\n",
      "2025-12-08 23:55:07.532890: Current learning rate: 0.00092\n",
      "2025-12-08 23:57:25.828827: train_loss -0.8551\n",
      "2025-12-08 23:57:25.828827: val_loss -0.8833\n",
      "2025-12-08 23:57:25.838576: Pseudo dice [0.929, 0.9584, 0.9497]\n",
      "2025-12-08 23:57:25.842580: Epoch time: 138.3 s\n",
      "2025-12-08 23:57:26.470056: \n",
      "2025-12-08 23:57:26.470056: Epoch 930\n",
      "2025-12-08 23:57:26.470056: Current learning rate: 0.00091\n",
      "2025-12-08 23:59:44.581079: train_loss -0.8614\n",
      "2025-12-08 23:59:44.581079: val_loss -0.8825\n",
      "2025-12-08 23:59:44.588994: Pseudo dice [0.9282, 0.9602, 0.952]\n",
      "2025-12-08 23:59:44.595001: Epoch time: 138.11 s\n",
      "2025-12-08 23:59:45.569155: \n",
      "2025-12-08 23:59:45.569155: Epoch 931\n",
      "2025-12-08 23:59:45.586920: Current learning rate: 0.0009\n",
      "2025-12-09 00:02:03.747560: train_loss -0.8663\n",
      "2025-12-09 00:02:03.747560: val_loss -0.8894\n",
      "2025-12-09 00:02:03.763267: Pseudo dice [0.931, 0.9641, 0.9516]\n",
      "2025-12-09 00:02:03.763267: Epoch time: 138.18 s\n",
      "2025-12-09 00:02:03.763267: Yayy! New best EMA pseudo Dice: 0.9464\n",
      "2025-12-09 00:02:04.761514: \n",
      "2025-12-09 00:02:04.761514: Epoch 932\n",
      "2025-12-09 00:02:04.761514: Current learning rate: 0.00089\n",
      "2025-12-09 00:04:22.904267: train_loss -0.8637\n",
      "2025-12-09 00:04:22.906269: val_loss -0.8758\n",
      "2025-12-09 00:04:22.906269: Pseudo dice [0.9248, 0.9537, 0.9526]\n",
      "2025-12-09 00:04:22.916707: Epoch time: 138.16 s\n",
      "2025-12-09 00:04:23.540768: \n",
      "2025-12-09 00:04:23.540768: Epoch 933\n",
      "2025-12-09 00:04:23.540768: Current learning rate: 0.00088\n",
      "2025-12-09 00:06:41.677257: train_loss -0.863\n",
      "2025-12-09 00:06:41.677257: val_loss -0.884\n",
      "2025-12-09 00:06:41.685265: Pseudo dice [0.9287, 0.959, 0.9534]\n",
      "2025-12-09 00:06:41.691009: Epoch time: 138.14 s\n",
      "2025-12-09 00:06:42.495827: \n",
      "2025-12-09 00:06:42.495827: Epoch 934\n",
      "2025-12-09 00:06:42.495827: Current learning rate: 0.00087\n",
      "2025-12-09 00:09:00.511299: train_loss -0.8664\n",
      "2025-12-09 00:09:00.511299: val_loss -0.8878\n",
      "2025-12-09 00:09:00.529116: Pseudo dice [0.9308, 0.9612, 0.9498]\n",
      "2025-12-09 00:09:00.535122: Epoch time: 138.02 s\n",
      "2025-12-09 00:09:01.253980: \n",
      "2025-12-09 00:09:01.253980: Epoch 935\n",
      "2025-12-09 00:09:01.270051: Current learning rate: 0.00085\n",
      "2025-12-09 00:11:19.373273: train_loss -0.8651\n",
      "2025-12-09 00:11:19.375276: val_loss -0.8794\n",
      "2025-12-09 00:11:19.381283: Pseudo dice [0.9217, 0.957, 0.9553]\n",
      "2025-12-09 00:11:19.385287: Epoch time: 138.12 s\n",
      "2025-12-09 00:11:20.020909: \n",
      "2025-12-09 00:11:20.021912: Epoch 936\n",
      "2025-12-09 00:11:20.021912: Current learning rate: 0.00084\n",
      "2025-12-09 00:13:38.175034: train_loss -0.8598\n",
      "2025-12-09 00:13:38.175034: val_loss -0.8856\n",
      "2025-12-09 00:13:38.191150: Pseudo dice [0.9302, 0.9595, 0.9489]\n",
      "2025-12-09 00:13:38.191150: Epoch time: 138.16 s\n",
      "2025-12-09 00:13:39.204703: \n",
      "2025-12-09 00:13:39.204703: Epoch 937\n",
      "2025-12-09 00:13:39.204703: Current learning rate: 0.00083\n",
      "2025-12-09 00:15:57.221310: train_loss -0.8633\n",
      "2025-12-09 00:15:57.221310: val_loss -0.8824\n",
      "2025-12-09 00:15:57.221310: Pseudo dice [0.9272, 0.9573, 0.9496]\n",
      "2025-12-09 00:15:57.221310: Epoch time: 138.02 s\n",
      "2025-12-09 00:15:57.863778: \n",
      "2025-12-09 00:15:57.863778: Epoch 938\n",
      "2025-12-09 00:15:57.868090: Current learning rate: 0.00082\n",
      "2025-12-09 00:18:16.194149: train_loss -0.8671\n",
      "2025-12-09 00:18:16.194149: val_loss -0.8855\n",
      "2025-12-09 00:18:16.207900: Pseudo dice [0.932, 0.9599, 0.9442]\n",
      "2025-12-09 00:18:16.207900: Epoch time: 138.33 s\n",
      "2025-12-09 00:18:16.842175: \n",
      "2025-12-09 00:18:16.842175: Epoch 939\n",
      "2025-12-09 00:18:16.842175: Current learning rate: 0.00081\n",
      "2025-12-09 00:20:35.744829: train_loss -0.864\n",
      "2025-12-09 00:20:35.744829: val_loss -0.8803\n",
      "2025-12-09 00:20:35.744829: Pseudo dice [0.927, 0.953, 0.9521]\n",
      "2025-12-09 00:20:35.760588: Epoch time: 138.9 s\n",
      "2025-12-09 00:20:36.552299: \n",
      "2025-12-09 00:20:36.552299: Epoch 940\n",
      "2025-12-09 00:20:36.552299: Current learning rate: 0.00079\n",
      "2025-12-09 00:22:55.125354: train_loss -0.8628\n",
      "2025-12-09 00:22:55.125354: val_loss -0.8739\n",
      "2025-12-09 00:22:55.133100: Pseudo dice [0.9185, 0.953, 0.9553]\n",
      "2025-12-09 00:22:55.138898: Epoch time: 138.59 s\n",
      "2025-12-09 00:22:55.831434: \n",
      "2025-12-09 00:22:55.831434: Epoch 941\n",
      "2025-12-09 00:22:55.846762: Current learning rate: 0.00078\n",
      "2025-12-09 00:25:14.514328: train_loss -0.8619\n",
      "2025-12-09 00:25:14.514328: val_loss -0.8742\n",
      "2025-12-09 00:25:14.521967: Pseudo dice [0.9191, 0.9534, 0.951]\n",
      "2025-12-09 00:25:14.529975: Epoch time: 138.68 s\n",
      "2025-12-09 00:25:15.229328: \n",
      "2025-12-09 00:25:15.229328: Epoch 942\n",
      "2025-12-09 00:25:15.229328: Current learning rate: 0.00077\n",
      "2025-12-09 00:27:33.639635: train_loss -0.8634\n",
      "2025-12-09 00:27:33.639635: val_loss -0.8848\n",
      "2025-12-09 00:27:33.639635: Pseudo dice [0.9255, 0.9591, 0.9521]\n",
      "2025-12-09 00:27:33.659510: Epoch time: 138.41 s\n",
      "2025-12-09 00:27:34.492669: \n",
      "2025-12-09 00:27:34.508582: Epoch 943\n",
      "2025-12-09 00:27:34.514591: Current learning rate: 0.00076\n",
      "2025-12-09 00:29:52.860782: train_loss -0.8605\n",
      "2025-12-09 00:29:52.860782: val_loss -0.8753\n",
      "2025-12-09 00:29:52.876546: Pseudo dice [0.9225, 0.956, 0.947]\n",
      "2025-12-09 00:29:52.876546: Epoch time: 138.37 s\n",
      "2025-12-09 00:29:53.515744: \n",
      "2025-12-09 00:29:53.515744: Epoch 944\n",
      "2025-12-09 00:29:53.515744: Current learning rate: 0.00075\n",
      "2025-12-09 00:32:11.615592: train_loss -0.8606\n",
      "2025-12-09 00:32:11.615592: val_loss -0.8898\n",
      "2025-12-09 00:32:11.615592: Pseudo dice [0.9296, 0.9614, 0.958]\n",
      "2025-12-09 00:32:11.631644: Epoch time: 138.1 s\n",
      "2025-12-09 00:32:12.262644: \n",
      "2025-12-09 00:32:12.262644: Epoch 945\n",
      "2025-12-09 00:32:12.262644: Current learning rate: 0.00074\n",
      "2025-12-09 00:34:30.539233: train_loss -0.8648\n",
      "2025-12-09 00:34:30.539233: val_loss -0.8897\n",
      "2025-12-09 00:34:30.546257: Pseudo dice [0.9316, 0.9555, 0.9553]\n",
      "2025-12-09 00:34:30.550475: Epoch time: 138.28 s\n",
      "2025-12-09 00:34:31.180989: \n",
      "2025-12-09 00:34:31.182991: Epoch 946\n",
      "2025-12-09 00:34:31.187928: Current learning rate: 0.00072\n",
      "2025-12-09 00:36:49.151145: train_loss -0.8601\n",
      "2025-12-09 00:36:49.151145: val_loss -0.8814\n",
      "2025-12-09 00:36:49.170823: Pseudo dice [0.9291, 0.9557, 0.9525]\n",
      "2025-12-09 00:36:49.178832: Epoch time: 137.97 s\n",
      "2025-12-09 00:36:49.809514: \n",
      "2025-12-09 00:36:49.811516: Epoch 947\n",
      "2025-12-09 00:36:49.817551: Current learning rate: 0.00071\n",
      "2025-12-09 00:39:07.989095: train_loss -0.8619\n",
      "2025-12-09 00:39:07.989095: val_loss -0.8903\n",
      "2025-12-09 00:39:07.989095: Pseudo dice [0.9319, 0.9603, 0.9518]\n",
      "2025-12-09 00:39:08.004878: Epoch time: 138.18 s\n",
      "2025-12-09 00:39:08.628475: \n",
      "2025-12-09 00:39:08.628475: Epoch 948\n",
      "2025-12-09 00:39:08.628475: Current learning rate: 0.0007\n",
      "2025-12-09 00:41:26.635899: train_loss -0.8655\n",
      "2025-12-09 00:41:26.635899: val_loss -0.8798\n",
      "2025-12-09 00:41:26.643907: Pseudo dice [0.9246, 0.9542, 0.9531]\n",
      "2025-12-09 00:41:26.649913: Epoch time: 138.01 s\n",
      "2025-12-09 00:41:27.295447: \n",
      "2025-12-09 00:41:27.297450: Epoch 949\n",
      "2025-12-09 00:41:27.297450: Current learning rate: 0.00069\n",
      "2025-12-09 00:43:45.249300: train_loss -0.8662\n",
      "2025-12-09 00:43:45.249300: val_loss -0.8795\n",
      "2025-12-09 00:43:45.259310: Pseudo dice [0.9253, 0.9543, 0.9488]\n",
      "2025-12-09 00:43:45.265055: Epoch time: 137.95 s\n",
      "2025-12-09 00:43:46.405988: \n",
      "2025-12-09 00:43:46.407990: Epoch 950\n",
      "2025-12-09 00:43:46.414438: Current learning rate: 0.00067\n",
      "2025-12-09 00:46:04.558399: train_loss -0.8659\n",
      "2025-12-09 00:46:04.558399: val_loss -0.8762\n",
      "2025-12-09 00:46:04.566364: Pseudo dice [0.921, 0.9525, 0.951]\n",
      "2025-12-09 00:46:04.572370: Epoch time: 138.15 s\n",
      "2025-12-09 00:46:05.240777: \n",
      "2025-12-09 00:46:05.240777: Epoch 951\n",
      "2025-12-09 00:46:05.240777: Current learning rate: 0.00066\n",
      "2025-12-09 00:48:23.652069: train_loss -0.863\n",
      "2025-12-09 00:48:23.654071: val_loss -0.881\n",
      "2025-12-09 00:48:23.659577: Pseudo dice [0.9242, 0.9544, 0.9544]\n",
      "2025-12-09 00:48:23.659577: Epoch time: 138.41 s\n",
      "2025-12-09 00:48:24.307471: \n",
      "2025-12-09 00:48:24.307471: Epoch 952\n",
      "2025-12-09 00:48:24.323316: Current learning rate: 0.00065\n",
      "2025-12-09 00:50:42.407940: train_loss -0.8602\n",
      "2025-12-09 00:50:42.409942: val_loss -0.8828\n",
      "2025-12-09 00:50:42.415507: Pseudo dice [0.9266, 0.9568, 0.9534]\n",
      "2025-12-09 00:50:42.421512: Epoch time: 138.1 s\n",
      "2025-12-09 00:50:43.084547: \n",
      "2025-12-09 00:50:43.084547: Epoch 953\n",
      "2025-12-09 00:50:43.096294: Current learning rate: 0.00064\n",
      "2025-12-09 00:53:01.278290: train_loss -0.8608\n",
      "2025-12-09 00:53:01.278290: val_loss -0.8875\n",
      "2025-12-09 00:53:01.288038: Pseudo dice [0.9301, 0.9565, 0.9592]\n",
      "2025-12-09 00:53:01.292042: Epoch time: 138.19 s\n",
      "2025-12-09 00:53:02.010962: \n",
      "2025-12-09 00:53:02.010962: Epoch 954\n",
      "2025-12-09 00:53:02.026595: Current learning rate: 0.00063\n",
      "2025-12-09 00:55:20.077012: train_loss -0.8644\n",
      "2025-12-09 00:55:20.077012: val_loss -0.8822\n",
      "2025-12-09 00:55:20.092705: Pseudo dice [0.9266, 0.9582, 0.9539]\n",
      "2025-12-09 00:55:20.092705: Epoch time: 138.07 s\n",
      "2025-12-09 00:55:20.798968: \n",
      "2025-12-09 00:55:20.798968: Epoch 955\n",
      "2025-12-09 00:55:20.803137: Current learning rate: 0.00061\n",
      "2025-12-09 00:57:38.984826: train_loss -0.8656\n",
      "2025-12-09 00:57:38.984826: val_loss -0.8819\n",
      "2025-12-09 00:57:38.994449: Pseudo dice [0.9287, 0.9571, 0.9491]\n",
      "2025-12-09 00:57:39.002457: Epoch time: 138.19 s\n",
      "2025-12-09 00:57:39.892729: \n",
      "2025-12-09 00:57:39.892729: Epoch 956\n",
      "2025-12-09 00:57:39.892729: Current learning rate: 0.0006\n",
      "2025-12-09 00:59:58.071404: train_loss -0.8603\n",
      "2025-12-09 00:59:58.071404: val_loss -0.8862\n",
      "2025-12-09 00:59:58.085416: Pseudo dice [0.9279, 0.9565, 0.9524]\n",
      "2025-12-09 00:59:58.091020: Epoch time: 138.18 s\n",
      "2025-12-09 00:59:58.719701: \n",
      "2025-12-09 00:59:58.719701: Epoch 957\n",
      "2025-12-09 00:59:58.735532: Current learning rate: 0.00059\n",
      "2025-12-09 01:02:16.957267: train_loss -0.8643\n",
      "2025-12-09 01:02:16.972902: val_loss -0.885\n",
      "2025-12-09 01:02:16.978910: Pseudo dice [0.9301, 0.9585, 0.9503]\n",
      "2025-12-09 01:02:16.984916: Epoch time: 138.24 s\n",
      "2025-12-09 01:02:17.619654: \n",
      "2025-12-09 01:02:17.619654: Epoch 958\n",
      "2025-12-09 01:02:17.619654: Current learning rate: 0.00058\n",
      "2025-12-09 01:04:35.755251: train_loss -0.8652\n",
      "2025-12-09 01:04:35.757253: val_loss -0.8845\n",
      "2025-12-09 01:04:35.764808: Pseudo dice [0.9273, 0.9579, 0.9528]\n",
      "2025-12-09 01:04:35.770814: Epoch time: 138.14 s\n",
      "2025-12-09 01:04:36.498950: \n",
      "2025-12-09 01:04:36.498950: Epoch 959\n",
      "2025-12-09 01:04:36.505442: Current learning rate: 0.00056\n",
      "2025-12-09 01:06:54.695382: train_loss -0.8645\n",
      "2025-12-09 01:06:54.695382: val_loss -0.8847\n",
      "2025-12-09 01:06:54.695382: Pseudo dice [0.9288, 0.9594, 0.9541]\n",
      "2025-12-09 01:06:54.707583: Epoch time: 138.2 s\n",
      "2025-12-09 01:06:55.337204: \n",
      "2025-12-09 01:06:55.337204: Epoch 960\n",
      "2025-12-09 01:06:55.337204: Current learning rate: 0.00055\n",
      "2025-12-09 01:09:13.858275: train_loss -0.8634\n",
      "2025-12-09 01:09:13.860277: val_loss -0.8915\n",
      "2025-12-09 01:09:13.862279: Pseudo dice [0.9342, 0.9598, 0.9548]\n",
      "2025-12-09 01:09:13.862279: Epoch time: 138.52 s\n",
      "2025-12-09 01:09:14.621254: \n",
      "2025-12-09 01:09:14.621254: Epoch 961\n",
      "2025-12-09 01:09:14.621254: Current learning rate: 0.00054\n",
      "2025-12-09 01:11:32.862940: train_loss -0.8607\n",
      "2025-12-09 01:11:32.862940: val_loss -0.8803\n",
      "2025-12-09 01:11:32.862940: Pseudo dice [0.9255, 0.9584, 0.9511]\n",
      "2025-12-09 01:11:32.878955: Epoch time: 138.24 s\n",
      "2025-12-09 01:11:33.512436: \n",
      "2025-12-09 01:11:33.512436: Epoch 962\n",
      "2025-12-09 01:11:33.512436: Current learning rate: 0.00053\n",
      "2025-12-09 01:13:51.594019: train_loss -0.8654\n",
      "2025-12-09 01:13:51.594019: val_loss -0.8851\n",
      "2025-12-09 01:13:51.609855: Pseudo dice [0.9314, 0.9564, 0.9477]\n",
      "2025-12-09 01:13:51.609855: Epoch time: 138.08 s\n",
      "2025-12-09 01:13:52.416860: \n",
      "2025-12-09 01:13:52.416860: Epoch 963\n",
      "2025-12-09 01:13:52.432760: Current learning rate: 0.00051\n",
      "2025-12-09 01:16:10.649755: train_loss -0.864\n",
      "2025-12-09 01:16:10.649755: val_loss -0.8841\n",
      "2025-12-09 01:16:10.665553: Pseudo dice [0.9272, 0.9602, 0.9503]\n",
      "2025-12-09 01:16:10.669557: Epoch time: 138.23 s\n",
      "2025-12-09 01:16:11.305384: \n",
      "2025-12-09 01:16:11.307389: Epoch 964\n",
      "2025-12-09 01:16:11.314831: Current learning rate: 0.0005\n",
      "2025-12-09 01:18:29.386487: train_loss -0.8616\n",
      "2025-12-09 01:18:29.388228: val_loss -0.8833\n",
      "2025-12-09 01:18:29.388228: Pseudo dice [0.93, 0.9581, 0.9526]\n",
      "2025-12-09 01:18:29.401474: Epoch time: 138.08 s\n",
      "2025-12-09 01:18:30.097550: \n",
      "2025-12-09 01:18:30.097550: Epoch 965\n",
      "2025-12-09 01:18:30.097550: Current learning rate: 0.00049\n",
      "2025-12-09 01:20:48.279490: train_loss -0.8656\n",
      "2025-12-09 01:20:48.279490: val_loss -0.883\n",
      "2025-12-09 01:20:48.293246: Pseudo dice [0.9264, 0.9606, 0.9478]\n",
      "2025-12-09 01:20:48.300995: Epoch time: 138.18 s\n",
      "2025-12-09 01:20:48.931604: \n",
      "2025-12-09 01:20:48.931604: Epoch 966\n",
      "2025-12-09 01:20:48.947270: Current learning rate: 0.00048\n",
      "2025-12-09 01:23:07.049186: train_loss -0.8659\n",
      "2025-12-09 01:23:07.049186: val_loss -0.8913\n",
      "2025-12-09 01:23:07.056933: Pseudo dice [0.9331, 0.9608, 0.9506]\n",
      "2025-12-09 01:23:07.064479: Epoch time: 138.12 s\n",
      "2025-12-09 01:23:07.802176: \n",
      "2025-12-09 01:23:07.802176: Epoch 967\n",
      "2025-12-09 01:23:07.802176: Current learning rate: 0.00046\n",
      "2025-12-09 01:25:25.862397: train_loss -0.8677\n",
      "2025-12-09 01:25:25.864398: val_loss -0.8843\n",
      "2025-12-09 01:25:25.870404: Pseudo dice [0.9261, 0.9565, 0.9521]\n",
      "2025-12-09 01:25:25.877412: Epoch time: 138.06 s\n",
      "2025-12-09 01:25:26.567099: \n",
      "2025-12-09 01:25:26.567099: Epoch 968\n",
      "2025-12-09 01:25:26.567099: Current learning rate: 0.00045\n",
      "2025-12-09 01:27:44.668184: train_loss -0.8645\n",
      "2025-12-09 01:27:44.670187: val_loss -0.8844\n",
      "2025-12-09 01:27:44.677935: Pseudo dice [0.9284, 0.9582, 0.9485]\n",
      "2025-12-09 01:27:44.682157: Epoch time: 138.11 s\n",
      "2025-12-09 01:27:45.532036: \n",
      "2025-12-09 01:27:45.532036: Epoch 969\n",
      "2025-12-09 01:27:45.532036: Current learning rate: 0.00044\n",
      "2025-12-09 01:30:03.694713: train_loss -0.8636\n",
      "2025-12-09 01:30:03.694713: val_loss -0.8784\n",
      "2025-12-09 01:30:03.694713: Pseudo dice [0.922, 0.9575, 0.9535]\n",
      "2025-12-09 01:30:03.710473: Epoch time: 138.18 s\n",
      "2025-12-09 01:30:04.392874: \n",
      "2025-12-09 01:30:04.392874: Epoch 970\n",
      "2025-12-09 01:30:04.408733: Current learning rate: 0.00043\n",
      "2025-12-09 01:32:22.729078: train_loss -0.8597\n",
      "2025-12-09 01:32:22.729078: val_loss -0.8875\n",
      "2025-12-09 01:32:22.739089: Pseudo dice [0.93, 0.9588, 0.955]\n",
      "2025-12-09 01:32:22.741091: Epoch time: 138.34 s\n",
      "2025-12-09 01:32:23.476565: \n",
      "2025-12-09 01:32:23.476565: Epoch 971\n",
      "2025-12-09 01:32:23.476565: Current learning rate: 0.00041\n",
      "2025-12-09 01:34:41.579323: train_loss -0.8634\n",
      "2025-12-09 01:34:41.579323: val_loss -0.8851\n",
      "2025-12-09 01:34:41.587331: Pseudo dice [0.929, 0.9595, 0.9517]\n",
      "2025-12-09 01:34:41.592813: Epoch time: 138.1 s\n",
      "2025-12-09 01:34:42.288915: \n",
      "2025-12-09 01:34:42.288915: Epoch 972\n",
      "2025-12-09 01:34:42.306998: Current learning rate: 0.0004\n",
      "2025-12-09 01:37:00.478932: train_loss -0.8634\n",
      "2025-12-09 01:37:00.478932: val_loss -0.8842\n",
      "2025-12-09 01:37:00.494721: Pseudo dice [0.9262, 0.9599, 0.9526]\n",
      "2025-12-09 01:37:00.494721: Epoch time: 138.19 s\n",
      "2025-12-09 01:37:01.207094: \n",
      "2025-12-09 01:37:01.207094: Epoch 973\n",
      "2025-12-09 01:37:01.223079: Current learning rate: 0.00039\n",
      "2025-12-09 01:39:19.627141: train_loss -0.8657\n",
      "2025-12-09 01:39:19.634888: val_loss -0.8892\n",
      "2025-12-09 01:39:19.640894: Pseudo dice [0.9302, 0.9607, 0.9543]\n",
      "2025-12-09 01:39:19.646900: Epoch time: 138.42 s\n",
      "2025-12-09 01:39:20.271695: \n",
      "2025-12-09 01:39:20.271695: Epoch 974\n",
      "2025-12-09 01:39:20.287609: Current learning rate: 0.00037\n",
      "2025-12-09 01:41:38.321416: train_loss -0.8645\n",
      "2025-12-09 01:41:38.321416: val_loss -0.8838\n",
      "2025-12-09 01:41:38.337166: Pseudo dice [0.9291, 0.9596, 0.9488]\n",
      "2025-12-09 01:41:38.337166: Epoch time: 138.05 s\n",
      "2025-12-09 01:41:39.129686: \n",
      "2025-12-09 01:41:39.145750: Epoch 975\n",
      "2025-12-09 01:41:39.145750: Current learning rate: 0.00036\n",
      "2025-12-09 01:43:57.420134: train_loss -0.8616\n",
      "2025-12-09 01:43:57.420134: val_loss -0.8872\n",
      "2025-12-09 01:43:57.435943: Pseudo dice [0.9289, 0.9576, 0.953]\n",
      "2025-12-09 01:43:57.440474: Epoch time: 138.29 s\n",
      "2025-12-09 01:43:58.150448: \n",
      "2025-12-09 01:43:58.150448: Epoch 976\n",
      "2025-12-09 01:43:58.166267: Current learning rate: 0.00035\n",
      "2025-12-09 01:46:16.406887: train_loss -0.8606\n",
      "2025-12-09 01:46:16.406887: val_loss -0.8971\n",
      "2025-12-09 01:46:16.422758: Pseudo dice [0.9366, 0.9638, 0.9549]\n",
      "2025-12-09 01:46:16.424760: Epoch time: 138.26 s\n",
      "2025-12-09 01:46:16.430976: Yayy! New best EMA pseudo Dice: 0.9468\n",
      "2025-12-09 01:46:17.390983: \n",
      "2025-12-09 01:46:17.390983: Epoch 977\n",
      "2025-12-09 01:46:17.406730: Current learning rate: 0.00034\n",
      "2025-12-09 01:48:35.605344: train_loss -0.8655\n",
      "2025-12-09 01:48:35.605344: val_loss -0.8892\n",
      "2025-12-09 01:48:35.613089: Pseudo dice [0.9327, 0.9629, 0.9482]\n",
      "2025-12-09 01:48:35.613089: Epoch time: 138.21 s\n",
      "2025-12-09 01:48:35.621944: Yayy! New best EMA pseudo Dice: 0.9469\n",
      "2025-12-09 01:48:36.623237: \n",
      "2025-12-09 01:48:36.623237: Epoch 978\n",
      "2025-12-09 01:48:36.639202: Current learning rate: 0.00032\n",
      "2025-12-09 01:50:54.869846: train_loss -0.8614\n",
      "2025-12-09 01:50:54.871848: val_loss -0.8886\n",
      "2025-12-09 01:50:54.879596: Pseudo dice [0.9313, 0.9583, 0.9514]\n",
      "2025-12-09 01:50:54.887605: Epoch time: 138.25 s\n",
      "2025-12-09 01:50:54.893351: Yayy! New best EMA pseudo Dice: 0.9469\n",
      "2025-12-09 01:50:55.823470: \n",
      "2025-12-09 01:50:55.823470: Epoch 979\n",
      "2025-12-09 01:50:55.841502: Current learning rate: 0.00031\n",
      "2025-12-09 01:53:14.038652: train_loss -0.8684\n",
      "2025-12-09 01:53:14.038652: val_loss -0.8924\n",
      "2025-12-09 01:53:14.042658: Pseudo dice [0.9344, 0.9618, 0.9537]\n",
      "2025-12-09 01:53:14.049202: Epoch time: 138.22 s\n",
      "2025-12-09 01:53:14.058419: Yayy! New best EMA pseudo Dice: 0.9472\n",
      "2025-12-09 01:53:15.230993: \n",
      "2025-12-09 01:53:15.230993: Epoch 980\n",
      "2025-12-09 01:53:15.230993: Current learning rate: 0.0003\n",
      "2025-12-09 01:55:33.594475: train_loss -0.8619\n",
      "2025-12-09 01:55:33.594475: val_loss -0.8923\n",
      "2025-12-09 01:55:33.612240: Pseudo dice [0.9336, 0.9598, 0.9545]\n",
      "2025-12-09 01:55:33.612240: Epoch time: 138.36 s\n",
      "2025-12-09 01:55:33.626110: Yayy! New best EMA pseudo Dice: 0.9474\n",
      "2025-12-09 01:55:34.547577: \n",
      "2025-12-09 01:55:34.547577: Epoch 981\n",
      "2025-12-09 01:55:34.547577: Current learning rate: 0.00028\n",
      "2025-12-09 01:57:52.868595: train_loss -0.8649\n",
      "2025-12-09 01:57:52.868595: val_loss -0.8866\n",
      "2025-12-09 01:57:52.884368: Pseudo dice [0.928, 0.9586, 0.9529]\n",
      "2025-12-09 01:57:52.884368: Epoch time: 138.32 s\n",
      "2025-12-09 01:57:53.567270: \n",
      "2025-12-09 01:57:53.567270: Epoch 982\n",
      "2025-12-09 01:57:53.567270: Current learning rate: 0.00027\n",
      "2025-12-09 02:00:11.860437: train_loss -0.8639\n",
      "2025-12-09 02:00:11.862439: val_loss -0.8811\n",
      "2025-12-09 02:00:11.867587: Pseudo dice [0.9248, 0.9565, 0.9526]\n",
      "2025-12-09 02:00:11.872750: Epoch time: 138.29 s\n",
      "2025-12-09 02:00:12.609561: \n",
      "2025-12-09 02:00:12.609561: Epoch 983\n",
      "2025-12-09 02:00:12.625496: Current learning rate: 0.00026\n",
      "2025-12-09 02:02:30.793948: train_loss -0.8661\n",
      "2025-12-09 02:02:30.793948: val_loss -0.885\n",
      "2025-12-09 02:02:30.793948: Pseudo dice [0.9284, 0.962, 0.9508]\n",
      "2025-12-09 02:02:30.809686: Epoch time: 138.18 s\n",
      "2025-12-09 02:02:31.475857: \n",
      "2025-12-09 02:02:31.475857: Epoch 984\n",
      "2025-12-09 02:02:31.491630: Current learning rate: 0.00024\n",
      "2025-12-09 02:04:49.883636: train_loss -0.8562\n",
      "2025-12-09 02:04:49.883636: val_loss -0.8827\n",
      "2025-12-09 02:04:49.899409: Pseudo dice [0.9242, 0.9534, 0.9555]\n",
      "2025-12-09 02:04:49.899409: Epoch time: 138.41 s\n",
      "2025-12-09 02:04:50.616789: \n",
      "2025-12-09 02:04:50.616789: Epoch 985\n",
      "2025-12-09 02:04:50.616789: Current learning rate: 0.00023\n",
      "2025-12-09 02:07:08.865412: train_loss -0.866\n",
      "2025-12-09 02:07:08.865412: val_loss -0.8907\n",
      "2025-12-09 02:07:08.873179: Pseudo dice [0.9312, 0.9588, 0.9569]\n",
      "2025-12-09 02:07:08.881446: Epoch time: 138.25 s\n",
      "2025-12-09 02:07:09.567440: \n",
      "2025-12-09 02:07:09.567440: Epoch 986\n",
      "2025-12-09 02:07:09.585062: Current learning rate: 0.00021\n",
      "2025-12-09 02:09:27.925254: train_loss -0.8646\n",
      "2025-12-09 02:09:27.925254: val_loss -0.8779\n",
      "2025-12-09 02:09:27.927256: Pseudo dice [0.923, 0.9559, 0.9507]\n",
      "2025-12-09 02:09:27.935135: Epoch time: 138.36 s\n",
      "2025-12-09 02:09:28.844705: \n",
      "2025-12-09 02:09:28.844705: Epoch 987\n",
      "2025-12-09 02:09:28.860594: Current learning rate: 0.0002\n",
      "2025-12-09 02:11:47.059606: train_loss -0.8686\n",
      "2025-12-09 02:11:47.059606: val_loss -0.8871\n",
      "2025-12-09 02:11:47.065350: Pseudo dice [0.9316, 0.9597, 0.9486]\n",
      "2025-12-09 02:11:47.069172: Epoch time: 138.21 s\n",
      "2025-12-09 02:11:47.694001: \n",
      "2025-12-09 02:11:47.694001: Epoch 988\n",
      "2025-12-09 02:11:47.709911: Current learning rate: 0.00019\n",
      "2025-12-09 02:14:05.961917: train_loss -0.8617\n",
      "2025-12-09 02:14:05.961917: val_loss -0.8821\n",
      "2025-12-09 02:14:05.977945: Pseudo dice [0.9267, 0.959, 0.9509]\n",
      "2025-12-09 02:14:05.977945: Epoch time: 138.27 s\n",
      "2025-12-09 02:14:06.770595: \n",
      "2025-12-09 02:14:06.770595: Epoch 989\n",
      "2025-12-09 02:14:06.770595: Current learning rate: 0.00017\n",
      "2025-12-09 02:16:24.882771: train_loss -0.8636\n",
      "2025-12-09 02:16:24.882771: val_loss -0.883\n",
      "2025-12-09 02:16:24.898553: Pseudo dice [0.9285, 0.9587, 0.9478]\n",
      "2025-12-09 02:16:24.902525: Epoch time: 138.11 s\n",
      "2025-12-09 02:16:25.545879: \n",
      "2025-12-09 02:16:25.545879: Epoch 990\n",
      "2025-12-09 02:16:25.545879: Current learning rate: 0.00016\n",
      "2025-12-09 02:18:43.854167: train_loss -0.8622\n",
      "2025-12-09 02:18:43.854167: val_loss -0.8911\n",
      "2025-12-09 02:18:43.869926: Pseudo dice [0.934, 0.9606, 0.9545]\n",
      "2025-12-09 02:18:43.869926: Epoch time: 138.31 s\n",
      "2025-12-09 02:18:44.595631: \n",
      "2025-12-09 02:18:44.595631: Epoch 991\n",
      "2025-12-09 02:18:44.598637: Current learning rate: 0.00014\n",
      "2025-12-09 02:21:02.719771: train_loss -0.8668\n",
      "2025-12-09 02:21:02.719771: val_loss -0.8909\n",
      "2025-12-09 02:21:02.719771: Pseudo dice [0.9321, 0.9624, 0.9555]\n",
      "2025-12-09 02:21:02.735515: Epoch time: 138.13 s\n",
      "2025-12-09 02:21:03.467211: \n",
      "2025-12-09 02:21:03.467211: Epoch 992\n",
      "2025-12-09 02:21:03.481219: Current learning rate: 0.00013\n",
      "2025-12-09 02:23:21.878032: train_loss -0.8651\n",
      "2025-12-09 02:23:21.878032: val_loss -0.8922\n",
      "2025-12-09 02:23:21.889797: Pseudo dice [0.9334, 0.9602, 0.9488]\n",
      "2025-12-09 02:23:21.898426: Epoch time: 138.41 s\n",
      "2025-12-09 02:23:22.697762: \n",
      "2025-12-09 02:23:22.697762: Epoch 993\n",
      "2025-12-09 02:23:22.697762: Current learning rate: 0.00011\n",
      "2025-12-09 02:25:40.839727: train_loss -0.8682\n",
      "2025-12-09 02:25:40.839727: val_loss -0.8969\n",
      "2025-12-09 02:25:40.855485: Pseudo dice [0.9369, 0.9623, 0.9564]\n",
      "2025-12-09 02:25:40.855485: Epoch time: 138.14 s\n",
      "2025-12-09 02:25:40.871516: Yayy! New best EMA pseudo Dice: 0.9476\n",
      "2025-12-09 02:25:41.760688: \n",
      "2025-12-09 02:25:41.760688: Epoch 994\n",
      "2025-12-09 02:25:41.777892: Current learning rate: 0.0001\n",
      "2025-12-09 02:28:00.041387: train_loss -0.8618\n",
      "2025-12-09 02:28:00.041387: val_loss -0.8913\n",
      "2025-12-09 02:28:00.041387: Pseudo dice [0.9336, 0.9615, 0.954]\n",
      "2025-12-09 02:28:00.055396: Epoch time: 138.28 s\n",
      "2025-12-09 02:28:00.055396: Yayy! New best EMA pseudo Dice: 0.9478\n",
      "2025-12-09 02:28:01.196252: \n",
      "2025-12-09 02:28:01.196252: Epoch 995\n",
      "2025-12-09 02:28:01.204263: Current learning rate: 8e-05\n",
      "2025-12-09 02:30:19.399535: train_loss -0.8672\n",
      "2025-12-09 02:30:19.399535: val_loss -0.8823\n",
      "2025-12-09 02:30:19.407468: Pseudo dice [0.9281, 0.9583, 0.9505]\n",
      "2025-12-09 02:30:19.407468: Epoch time: 138.21 s\n",
      "2025-12-09 02:30:20.145894: \n",
      "2025-12-09 02:30:20.145894: Epoch 996\n",
      "2025-12-09 02:30:20.152395: Current learning rate: 7e-05\n",
      "2025-12-09 02:32:38.283765: train_loss -0.8677\n",
      "2025-12-09 02:32:38.283765: val_loss -0.8786\n",
      "2025-12-09 02:32:38.291015: Pseudo dice [0.9223, 0.9523, 0.9543]\n",
      "2025-12-09 02:32:38.298761: Epoch time: 138.14 s\n",
      "2025-12-09 02:32:39.008296: \n",
      "2025-12-09 02:32:39.008296: Epoch 997\n",
      "2025-12-09 02:32:39.008296: Current learning rate: 5e-05\n",
      "2025-12-09 02:34:57.088151: train_loss -0.8654\n",
      "2025-12-09 02:34:57.088151: val_loss -0.8792\n",
      "2025-12-09 02:34:57.096159: Pseudo dice [0.9234, 0.9565, 0.9514]\n",
      "2025-12-09 02:34:57.103905: Epoch time: 138.08 s\n",
      "2025-12-09 02:34:57.893020: \n",
      "2025-12-09 02:34:57.893020: Epoch 998\n",
      "2025-12-09 02:34:57.909064: Current learning rate: 4e-05\n",
      "2025-12-09 02:37:16.282831: train_loss -0.8659\n",
      "2025-12-09 02:37:16.282831: val_loss -0.8798\n",
      "2025-12-09 02:37:16.293190: Pseudo dice [0.925, 0.9536, 0.9517]\n",
      "2025-12-09 02:37:16.298934: Epoch time: 138.39 s\n",
      "2025-12-09 02:37:17.027145: \n",
      "2025-12-09 02:37:17.027145: Epoch 999\n",
      "2025-12-09 02:37:17.027145: Current learning rate: 2e-05\n",
      "2025-12-09 02:39:35.339341: train_loss -0.8659\n",
      "2025-12-09 02:39:35.341343: val_loss -0.8836\n",
      "2025-12-09 02:39:35.347397: Pseudo dice [0.9251, 0.958, 0.9542]\n",
      "2025-12-09 02:39:35.347397: Epoch time: 138.31 s\n",
      "2025-12-09 02:39:36.421947: Training done.\n",
      "2025-12-09 02:39:36.707715: Using splits from existing split file: C:\\Users\\Anna\\Documents\\TFM\\nnUNet_preprocessed\\Dataset500_MRI\\splits_final.json\n",
      "2025-12-09 02:39:36.713725: The split file contains 5 splits.\n",
      "2025-12-09 02:39:36.752995: Desired fold for training: 0\n",
      "2025-12-09 02:39:36.786520: This split has 400 training and 100 validation cases.\n",
      "2025-12-09 02:39:36.804286: predicting OAS30014_MR_d0196_9\n",
      "2025-12-09 02:39:36.958697: OAS30014_MR_d0196_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:39:58.746167: predicting OAS30017_MR_d0054_1\n",
      "2025-12-09 02:39:58.746167: OAS30017_MR_d0054_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:40:16.451824: predicting OAS30017_MR_d0054_10\n",
      "2025-12-09 02:40:16.467570: OAS30017_MR_d0054_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:40:34.147182: predicting OAS30017_MR_d0054_9\n",
      "2025-12-09 02:40:34.169172: OAS30017_MR_d0054_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:40:51.941931: predicting OAS30025_MR_d0210_7\n",
      "2025-12-09 02:40:51.952480: OAS30025_MR_d0210_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:41:09.690891: predicting OAS30025_MR_d0210_8\n",
      "2025-12-09 02:41:09.710557: OAS30025_MR_d0210_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:41:27.399452: predicting OAS30036_MR_d0059_3\n",
      "2025-12-09 02:41:27.423056: OAS30036_MR_d0059_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:41:45.127167: predicting OAS30039_MR_d1203_1\n",
      "2025-12-09 02:41:45.142930: OAS30039_MR_d1203_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:02.818247: predicting OAS30039_MR_d1203_10\n",
      "2025-12-09 02:42:02.840054: OAS30039_MR_d1203_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:20.569011: predicting OAS30039_MR_d1203_8\n",
      "2025-12-09 02:42:20.578493: OAS30039_MR_d1203_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:38.287601: predicting OAS30052_MR_d0693_2\n",
      "2025-12-09 02:42:38.299641: OAS30052_MR_d0693_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:42:55.996156: predicting OAS30052_MR_d0693_5\n",
      "2025-12-09 02:42:56.016531: OAS30052_MR_d0693_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:43:13.673009: predicting OAS30052_MR_d0693_6\n",
      "2025-12-09 02:43:13.695706: OAS30052_MR_d0693_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:43:31.414482: predicting OAS30078_MR_d0210_6\n",
      "2025-12-09 02:43:31.436475: OAS30078_MR_d0210_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:43:49.117414: predicting OAS30078_MR_d0210_8\n",
      "2025-12-09 02:43:49.127428: OAS30078_MR_d0210_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:06.836894: predicting OAS30083_MR_d0465_1\n",
      "2025-12-09 02:44:06.848567: OAS30083_MR_d0465_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:24.511685: predicting OAS30087_MR_d0260_5\n",
      "2025-12-09 02:44:24.523513: OAS30087_MR_d0260_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:42.184021: predicting OAS30099_MR_d0032_1\n",
      "2025-12-09 02:44:42.193348: OAS30099_MR_d0032_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:44:59.885038: predicting OAS30099_MR_d0032_7\n",
      "2025-12-09 02:44:59.905079: OAS30099_MR_d0032_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:45:17.593966: predicting OAS30099_MR_d0032_9\n",
      "2025-12-09 02:45:17.609995: OAS30099_MR_d0032_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:45:35.275320: predicting OAS30102_MR_d0024_1\n",
      "2025-12-09 02:45:35.284915: OAS30102_MR_d0024_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:45:52.958901: predicting OAS30102_MR_d0024_10\n",
      "2025-12-09 02:45:52.968498: OAS30102_MR_d0024_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:46:10.679825: predicting OAS30102_MR_d0024_3\n",
      "2025-12-09 02:46:10.690679: OAS30102_MR_d0024_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:46:28.415468: predicting OAS30104_MR_d0328_1\n",
      "2025-12-09 02:46:28.425479: OAS30104_MR_d0328_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:46:46.099731: predicting OAS30104_MR_d0328_5\n",
      "2025-12-09 02:46:46.110172: OAS30104_MR_d0328_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:03.823339: predicting OAS30107_MR_d0387_3\n",
      "2025-12-09 02:47:03.832535: OAS30107_MR_d0387_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:21.562269: predicting OAS30125_MR_d0201_1\n",
      "2025-12-09 02:47:21.572635: OAS30125_MR_d0201_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:39.250517: predicting OAS30125_MR_d0201_4\n",
      "2025-12-09 02:47:39.259403: OAS30125_MR_d0201_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:47:56.933176: predicting OAS30125_MR_d0201_5\n",
      "2025-12-09 02:47:56.952409: OAS30125_MR_d0201_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:48:14.668004: predicting OAS30127_MR_d0098_4\n",
      "2025-12-09 02:48:14.679119: OAS30127_MR_d0098_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:48:32.346652: predicting OAS30127_MR_d0098_8\n",
      "2025-12-09 02:48:32.357560: OAS30127_MR_d0098_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:48:50.073815: predicting OAS30134_MR_d0080_1\n",
      "2025-12-09 02:48:50.083683: OAS30134_MR_d0080_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:49:07.801308: predicting OAS30134_MR_d0080_4\n",
      "2025-12-09 02:49:07.814556: OAS30134_MR_d0080_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:49:25.501059: predicting OAS30134_MR_d0080_9\n",
      "2025-12-09 02:49:25.516108: OAS30134_MR_d0080_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:49:43.227242: predicting OAS30140_MR_d0172_3\n",
      "2025-12-09 02:49:43.239622: OAS30140_MR_d0172_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:00.957093: predicting OAS30147_MR_d0048_5\n",
      "2025-12-09 02:50:00.978920: OAS30147_MR_d0048_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:18.674383: predicting OAS30165_MR_d1763_1\n",
      "2025-12-09 02:50:18.692338: OAS30165_MR_d1763_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:36.349209: predicting OAS30165_MR_d1763_3\n",
      "2025-12-09 02:50:36.373179: OAS30165_MR_d1763_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:50:54.071485: predicting OAS30165_MR_d1763_4\n",
      "2025-12-09 02:50:54.083168: OAS30165_MR_d1763_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:51:11.778953: predicting OAS30165_MR_d1763_8\n",
      "2025-12-09 02:51:11.796936: OAS30165_MR_d1763_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:51:29.480866: predicting OAS30167_MR_d0111_10\n",
      "2025-12-09 02:51:29.496012: OAS30167_MR_d0111_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:51:47.201452: predicting OAS30167_MR_d0111_6\n",
      "2025-12-09 02:51:47.223147: OAS30167_MR_d0111_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:04.960795: predicting OAS30176_MR_d0000_3\n",
      "2025-12-09 02:52:04.971182: OAS30176_MR_d0000_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:22.647094: predicting OAS30195_MR_d1596_6\n",
      "2025-12-09 02:52:22.659155: OAS30195_MR_d1596_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:40.319597: predicting OAS30226_MR_d0183_1\n",
      "2025-12-09 02:52:40.342119: OAS30226_MR_d0183_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:52:58.008246: predicting OAS30226_MR_d0183_10\n",
      "2025-12-09 02:52:58.024188: OAS30226_MR_d0183_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:53:15.725749: predicting OAS30238_MR_d0037_1\n",
      "2025-12-09 02:53:15.738021: OAS30238_MR_d0037_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:53:33.424500: predicting OAS30238_MR_d0037_10\n",
      "2025-12-09 02:53:33.436773: OAS30238_MR_d0037_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:53:51.119259: predicting OAS30238_MR_d0037_4\n",
      "2025-12-09 02:53:51.144859: OAS30238_MR_d0037_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:54:08.852646: predicting OAS30250_MR_d0389_5\n",
      "2025-12-09 02:54:08.875051: OAS30250_MR_d0389_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:54:26.566837: predicting OAS30274_MR_d3332_10\n",
      "2025-12-09 02:54:26.586530: OAS30274_MR_d3332_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:54:44.272654: predicting OAS30274_MR_d3332_2\n",
      "2025-12-09 02:54:44.296249: OAS30274_MR_d3332_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:01.984726: predicting OAS30274_MR_d3332_4\n",
      "2025-12-09 02:55:01.994468: OAS30274_MR_d3332_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:19.661209: predicting OAS30292_MR_d0165_10\n",
      "2025-12-09 02:55:19.677269: OAS30292_MR_d0165_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:37.352799: predicting OAS30292_MR_d0165_3\n",
      "2025-12-09 02:55:37.368347: OAS30292_MR_d0165_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:55:55.033191: predicting OAS30292_MR_d0165_9\n",
      "2025-12-09 02:55:55.042764: OAS30292_MR_d0165_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:56:12.708555: predicting OAS30297_MR_d1712_2\n",
      "2025-12-09 02:56:12.717768: OAS30297_MR_d1712_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:56:30.434258: predicting OAS30297_MR_d1712_5\n",
      "2025-12-09 02:56:30.443326: OAS30297_MR_d1712_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:56:48.114841: predicting OAS30297_MR_d1712_8\n",
      "2025-12-09 02:56:48.124856: OAS30297_MR_d1712_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:05.801759: predicting OAS30297_MR_d1712_9\n",
      "2025-12-09 02:57:05.816847: OAS30297_MR_d1712_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:23.525357: predicting OAS30300_MR_d0100_3\n",
      "2025-12-09 02:57:23.537528: OAS30300_MR_d0100_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:41.264589: predicting OAS30300_MR_d0100_4\n",
      "2025-12-09 02:57:41.274297: OAS30300_MR_d0100_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:57:58.972664: predicting OAS30302_MR_d0262_1\n",
      "2025-12-09 02:57:58.980417: OAS30302_MR_d0262_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:58:16.658774: predicting OAS30302_MR_d0262_3\n",
      "2025-12-09 02:58:16.669575: OAS30302_MR_d0262_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:58:34.324687: predicting OAS30306_MR_d0028_2\n",
      "2025-12-09 02:58:34.342397: OAS30306_MR_d0028_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:58:52.040785: predicting OAS30321_MR_d3003_2\n",
      "2025-12-09 02:58:52.050913: OAS30321_MR_d3003_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:59:09.753227: predicting OAS30325_MR_d0032_2\n",
      "2025-12-09 02:59:09.774864: OAS30325_MR_d0032_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:59:27.504108: predicting OAS30343_MR_d4178_2\n",
      "2025-12-09 02:59:27.523606: OAS30343_MR_d4178_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 02:59:45.213475: predicting OAS30343_MR_d4178_6\n",
      "2025-12-09 02:59:45.233810: OAS30343_MR_d4178_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:02.922451: predicting OAS30349_MR_d0699_10\n",
      "2025-12-09 03:00:02.937356: OAS30349_MR_d0699_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:20.635502: predicting OAS30349_MR_d0699_2\n",
      "2025-12-09 03:00:20.657226: OAS30349_MR_d0699_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:38.363477: predicting OAS30349_MR_d0699_3\n",
      "2025-12-09 03:00:38.374452: OAS30349_MR_d0699_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:00:56.074534: predicting OAS30349_MR_d0699_5\n",
      "2025-12-09 03:00:56.080540: OAS30349_MR_d0699_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:01:13.807286: predicting OAS30349_MR_d0699_7\n",
      "2025-12-09 03:01:13.821149: OAS30349_MR_d0699_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:01:31.540524: predicting OAS30350_MR_d0018_2\n",
      "2025-12-09 03:01:31.550373: OAS30350_MR_d0018_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:01:49.260570: predicting OAS30352_MR_d0099_10\n",
      "2025-12-09 03:01:49.280241: OAS30352_MR_d0099_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:02:06.984650: predicting OAS30354_MR_d0056_10\n",
      "2025-12-09 03:02:07.004657: OAS30354_MR_d0056_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:02:24.729280: predicting OAS30354_MR_d0056_3\n",
      "2025-12-09 03:02:24.734508: OAS30354_MR_d0056_3, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:02:42.397912: predicting OAS30354_MR_d0056_5\n",
      "2025-12-09 03:02:42.408187: OAS30354_MR_d0056_5, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:00.097838: predicting OAS30355_MR_d0048_1\n",
      "2025-12-09 03:03:00.107090: OAS30355_MR_d0048_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:17.825193: predicting OAS30355_MR_d0048_7\n",
      "2025-12-09 03:03:17.835208: OAS30355_MR_d0048_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:35.542667: predicting OAS30355_MR_d0048_8\n",
      "2025-12-09 03:03:35.554377: OAS30355_MR_d0048_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:03:53.239689: predicting OAS30361_MR_d1457_1\n",
      "2025-12-09 03:03:53.261682: OAS30361_MR_d1457_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:04:10.946823: predicting OAS30361_MR_d1457_4\n",
      "2025-12-09 03:04:10.966559: OAS30361_MR_d1457_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:04:28.656093: predicting OAS30361_MR_d1457_8\n",
      "2025-12-09 03:04:28.670427: OAS30361_MR_d1457_8, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:04:46.365697: predicting OAS30369_MR_d4058_1\n",
      "2025-12-09 03:04:46.376242: OAS30369_MR_d4058_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:04.079599: predicting OAS30371_MR_d0338_6\n",
      "2025-12-09 03:05:04.087613: OAS30371_MR_d0338_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:21.779699: predicting OAS30373_MR_d1211_10\n",
      "2025-12-09 03:05:21.795347: OAS30373_MR_d1211_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:39.535772: predicting OAS30373_MR_d1211_2\n",
      "2025-12-09 03:05:39.548678: OAS30373_MR_d1211_2, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:05:57.254274: predicting OAS30373_MR_d1211_9\n",
      "2025-12-09 03:05:57.278089: OAS30373_MR_d1211_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:06:14.990406: predicting OAS30379_MR_d2106_1\n",
      "2025-12-09 03:06:15.008123: OAS30379_MR_d2106_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:06:32.709626: predicting OAS30379_MR_d2106_10\n",
      "2025-12-09 03:06:32.727318: OAS30379_MR_d2106_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:06:50.425219: predicting OAS30379_MR_d2106_4\n",
      "2025-12-09 03:06:50.440092: OAS30379_MR_d2106_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:07:08.142959: predicting OAS30379_MR_d2106_6\n",
      "2025-12-09 03:07:08.158762: OAS30379_MR_d2106_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:07:25.858183: predicting OAS30379_MR_d2106_9\n",
      "2025-12-09 03:07:25.871461: OAS30379_MR_d2106_9, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:07:43.561249: predicting OAS30380_MR_d3446_4\n",
      "2025-12-09 03:07:43.570802: OAS30380_MR_d3446_4, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:01.243198: predicting OAS30383_MR_d0134_1\n",
      "2025-12-09 03:08:01.260804: OAS30383_MR_d0134_1, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:18.927143: predicting OAS30383_MR_d0134_10\n",
      "2025-12-09 03:08:18.939142: OAS30383_MR_d0134_10, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:36.650287: predicting OAS30388_MR_d0073_6\n",
      "2025-12-09 03:08:36.662050: OAS30388_MR_d0073_6, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:08:54.386204: predicting OAS30388_MR_d0073_7\n",
      "2025-12-09 03:08:54.399407: OAS30388_MR_d0073_7, shape torch.Size([1, 202, 202, 202]), rank 0\n",
      "2025-12-09 03:09:37.678043: Validation complete\n",
      "2025-12-09 03:09:37.678043: Mean Validation Dice:  0.9385947886337376\n"
     ]
    }
   ],
   "source": [
    "# Train nnU-net on 0 folds\n",
    "!nnUNetv2_train 500 3d_lowres 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0a6ae-77a3-4557-b944-2dc3a5383f6b",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c800529-9a10-4525-9727-dfc227174357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "There are 100 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 100 cases that I would like to predict\n",
      "\n",
      "Predicting sub-OAS30003_sess-d4954_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30003_sess-d4954_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30007_ses-d1641_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30007_ses-d1641_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30008_ses-d0061_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30008_ses-d0061_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30009_ses-d2457_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30009_ses-d2457_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30014_ses-d1176_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30014_ses-d1176_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30016_ses-d0021_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30016_ses-d0021_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30018_ses-d0070_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30018_ses-d0070_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30033_ses-d1267_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30033_ses-d1267_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30035_ses-d2218_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30035_ses-d2218_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30043_ses-d0145_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30043_ses-d0145_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30052_ses-d2709_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30052_ses-d2709_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30053_ses-d0428_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30053_ses-d0428_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30061_ses-d0035_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30061_ses-d0035_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30070_ses-d0070_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30070_ses-d0070_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30073_ses-d1587_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30073_ses-d1587_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30074_ses-d2812_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30074_ses-d2812_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30076_ses-d0534_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30076_ses-d0534_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30103_sess-d7334_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30103_sess-d7334_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30109_ses-d0997_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30109_ses-d0997_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30121_ses-d2392_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30121_ses-d2392_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30134_sess-d2927_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30134_sess-d2927_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30135_ses-d0341_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30135_ses-d0341_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30137_sess-d5772_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30137_sess-d5772_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30138_sess-d0074_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30138_sess-d0074_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30141_ses-d0544_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30141_ses-d0544_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30145_ses-d4247_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30145_ses-d4247_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30150_ses-d0100_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30150_ses-d0100_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30164_ses-d0233_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30164_ses-d0233_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30165_ses-d0563_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30165_ses-d0563_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30168_ses-d0059_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30168_ses-d0059_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30171_ses-d0139_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30171_ses-d0139_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30172_ses-d0028_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30172_ses-d0028_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30173_ses-d3841_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30173_ses-d3841_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30178_ses-d0516_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30178_ses-d0516_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30179_ses-d0061_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30179_ses-d0061_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30184_ses-d0876_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30184_ses-d0876_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30189_ses-d0072_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30189_ses-d0072_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30200_ses-d0075_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30200_ses-d0075_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30202_ses-d0175_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30202_ses-d0175_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30205_ses-d0061_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30205_ses-d0061_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30207_sess-d2812_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30207_sess-d2812_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30208_ses-d0436_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30208_ses-d0436_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30228_sess-d1211_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30228_sess-d1211_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30231_ses-d3617_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30231_ses-d3617_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30232_ses-d0695_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30232_ses-d0695_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30233_ses-d3867_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30233_ses-d3867_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30235_ses-d0139_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30235_ses-d0139_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30244_ses-d1526_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30244_ses-d1526_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30247_ses-d0168_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30247_ses-d0168_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30250_ses-d1233_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30250_ses-d1233_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30255_ses-d0019_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30255_ses-d0019_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30261_ses-d0785_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30261_ses-d0785_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30265_ses-d0192_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30265_ses-d0192_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30272_ses-d0057_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30272_ses-d0057_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30276_ses-d1200_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30276_ses-d1200_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30277_ses-d0198_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30277_ses-d0198_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30284_ses-d3510_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30284_ses-d3510_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30285_ses-d0055_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30285_ses-d0055_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30287_ses-d0890_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30287_ses-d0890_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30289_ses-d0160_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30289_ses-d0160_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30298_ses-d0292_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30298_ses-d0292_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30299_ses-d0119_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30299_ses-d0119_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30304_sess-d1272_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30304_sess-d1272_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30306_ses-d0473_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30306_ses-d0473_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30308_ses-d0423_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30308_ses-d0423_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30310_ses-d0191_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30310_ses-d0191_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30311_ses-d0127_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30311_ses-d0127_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30316_ses-d0018_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30316_ses-d0018_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30317_ses-d0088_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30317_ses-d0088_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30318_ses-d2975_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30318_ses-d2975_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30319_ses-d0043_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30319_ses-d0043_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30321_ses-d0075_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30321_ses-d0075_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30322_ses-d1630_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30322_ses-d1630_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30326_ses-d0189_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30326_ses-d0189_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30327_ses-d0281_run-02_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30327_ses-d0281_run-02_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30328_sess-d1274_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30328_sess-d1274_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30329_ses-d0376_run-02_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30329_ses-d0376_run-02_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30331_ses-d2824_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30331_ses-d2824_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30335_ses-d0693_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30335_ses-d0693_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30336_ses-d0012_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30336_ses-d0012_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30344_ses-d1052_run-01_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30344_ses-d1052_run-01_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30350_ses-d0509_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30350_ses-d0509_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30351_sess-d2542_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30351_sess-d2542_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30357_ses-d0521_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30357_ses-d0521_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30361_ses-d2151_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30361_ses-d2151_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30362_ses-d0032_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30362_ses-d0032_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30363_ses-d0087_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30363_ses-d0087_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30368_ses-d0056_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30368_ses-d0056_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30369_ses-d4921_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30369_ses-d4921_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30372_ses-d3464_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30372_ses-d3464_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30373_ses-d0048_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30373_ses-d0048_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30374_ses-d0084_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30374_ses-d0084_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30382_ses-d0051_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30382_ses-d0051_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30387_ses-d0616_run-02_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30387_ses-d0616_run-02_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30390_ses-d2428_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30390_ses-d2428_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30391_ses-d0192_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30391_ses-d0192_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30394_ses-d0039_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30394_ses-d0039_T2w_stripped\n",
      "\n",
      "Predicting sub-OAS30395_ses-d1241_FLAIR_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30395_ses-d1241_FLAIR_stripped\n",
      "\n",
      "Predicting sub-OAS30397_ses-d0043_T1w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30397_ses-d0043_T1w_stripped\n",
      "\n",
      "Predicting sub-OAS30399_ses-d0380_T2w_stripped:\n",
      "perform_everything_on_device: True\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with sub-OAS30399_ses-d0380_T2w_stripped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:02<00:00,  2.26s/it]\n",
      "100%|##########| 1/1 [00:02<00:00,  2.26s/it]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.78it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.11it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.79it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.12it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.11it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.89it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.90it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.93it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.93it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.95it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.95it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.89it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.83it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.11it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.88it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.11it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.79it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.79it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.88it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.72it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.72it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.73it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.73it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.82it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.89it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.89it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.83it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.78it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.94it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.87it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.78it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.87it/s]\n",
      "100%|##########| 2/2 [00:01<00:00,  1.97it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.76it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.76it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.88it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.77it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.72it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.88it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.84it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.88it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.82it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.91it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.86it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.02it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.84it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.90it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.85it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.79it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.80it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.02it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.84it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.93it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.93it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.87it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.87it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.71it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.81it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.81it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.83it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.02it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.80it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.02it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.82it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.76it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.75it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.01it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.79it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.79it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.94it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.80it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.74it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.95it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.77it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.94it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.80it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.93it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.02it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.07it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.77it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.90it/s]\n",
      "100%|##########| 1/1 [00:00<00:00,  2.90it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.85it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.91it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.79it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.83it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.99it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.00it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.92it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.98it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  2.96it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  1.97it/s]\n",
      "100%|##########| 2/2 [00:00<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_predict -i nnUNet_raw/Dataset500_MRI/imagesTs -o C:\\Users\\Anna\\Documents\\TFM\\predictions -d 500 -c 3d_lowres -f 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f382875-0838-4e37-a78b-ee9bec8fb8b8",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e1aca4c-ebdc-4536-a097-31a7b1467aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!D:\\ProgramData\\Anaconda3\\envs\\nnunet\\Scripts\\nnUNetv2_evaluate_simple.exe\n",
    "        \"C:\\Users\\Anna\\Documents\\TFM\\nnUNet_raw\\Dataset500_MRI\\labelsTs\"\n",
    "        \"C:\\Users\\Anna\\Documents\\TFM\\predictions\"\n",
    "        -l 1 2 3\n",
    "        -o \"C:\\Users\\Anna\\Documents\\TFM\\pred_eval\\eval.json\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
